reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
...done
Training on 1724 shots, testing on 857 shots
NO SCALARS ARE USED, ONLY 1D SIGNALS
n_scalars,n_profiles,profile_size= 0 2 64
channels:  2 2
New Quantum convolution with channels  2 2
channels:  2 2
New Quantum convolution with channels  2 2
InputBlock parameters:  0 2 64 ['n2', 'n2'] 3 10 0.08 2
TCN parameters:  3 1 [5, 5, 5, 5, 5, 5, 5, 5] 5 0.08
Using multiple GPUs................ 4
Using multiple GPUs................ 4
Using multiple GPUs................ 4
Using multiple GPUs................ 4
99 epochs left to go

Training Epoch 0/100 starting at 2021-08-24 22:05:22.516805
[1]  [0/1724] loss: 1.495, ave_loss: 1.495
[2]  [20/1724] loss: 2.029, ave_loss: 1.762
[3]  [40/1724] loss: 2.159, ave_loss: 1.894
[4]  [60/1724] loss: 2.126, ave_loss: 1.952
[5]  [80/1724] loss: 2.177, ave_loss: 1.997
[6]  [100/1724] loss: 1.879, ave_loss: 1.978
[7]  [120/1724] loss: 1.580, ave_loss: 1.921
[8]  [140/1724] loss: 2.102, ave_loss: 1.943
[9]  [160/1724] loss: 1.590, ave_loss: 1.904
[10]  [180/1724] loss: 2.141, ave_loss: 1.928
[11]  [200/1724] loss: 2.027, ave_loss: 1.937
[12]  [220/1724] loss: 1.588, ave_loss: 1.908
[13]  [240/1724] loss: 2.149, ave_loss: 1.926
[14]  [260/1724] loss: 2.145, ave_loss: 1.942
[15]  [280/1724] loss: 1.965, ave_loss: 1.943
[16]  [300/1724] loss: 2.302, ave_loss: 1.966
[17]  [320/1724] loss: 1.119, ave_loss: 1.916
[18]  [340/1724] loss: 2.173, ave_loss: 1.930
[19]  [360/1724] loss: 1.454, ave_loss: 1.905
[20]  [380/1724] loss: 1.623, ave_loss: 1.891
[21]  [400/1724] loss: 1.667, ave_loss: 1.881
[22]  [420/1724] loss: 1.492, ave_loss: 1.863
[23]  [440/1724] loss: 1.601, ave_loss: 1.851
[24]  [460/1724] loss: 1.730, ave_loss: 1.846
[25]  [480/1724] loss: 1.590, ave_loss: 1.836
[26]  [500/1724] loss: 1.386, ave_loss: 1.819
[27]  [520/1724] loss: 1.790, ave_loss: 1.818
[28]  [540/1724] loss: 1.104, ave_loss: 1.792
[29]  [560/1724] loss: 1.719, ave_loss: 1.790
[30]  [580/1724] loss: 1.240, ave_loss: 1.771
[31]  [600/1724] loss: 1.380, ave_loss: 1.759
[32]  [620/1724] loss: 1.187, ave_loss: 1.741
[33]  [640/1724] loss: 1.195, ave_loss: 1.724
[34]  [660/1724] loss: 1.671, ave_loss: 1.723
[35]  [680/1724] loss: 1.362, ave_loss: 1.713
[36]  [700/1724] loss: 1.759, ave_loss: 1.714
[37]  [720/1724] loss: 1.249, ave_loss: 1.701
[38]  [740/1724] loss: 0.951, ave_loss: 1.682
[39]  [760/1724] loss: 1.627, ave_loss: 1.680
[40]  [780/1724] loss: 1.323, ave_loss: 1.671
[41]  [800/1724] loss: 1.727, ave_loss: 1.673
[42]  [820/1724] loss: 1.065, ave_loss: 1.658
[43]  [840/1724] loss: 1.261, ave_loss: 1.649
[44]  [860/1724] loss: 1.059, ave_loss: 1.635
[45]  [880/1724] loss: 1.225, ave_loss: 1.626
[46]  [900/1724] loss: 1.373, ave_loss: 1.621
[47]  [920/1724] loss: 1.148, ave_loss: 1.611
[48]  [940/1724] loss: 1.125, ave_loss: 1.601
[49]  [960/1724] loss: 0.907, ave_loss: 1.586
[50]  [980/1724] loss: 1.491, ave_loss: 1.585
[51]  [1000/1724] loss: 1.199, ave_loss: 1.577
[52]  [1020/1724] loss: 1.196, ave_loss: 1.570
[53]  [1040/1724] loss: 1.295, ave_loss: 1.564
[54]  [1060/1724] loss: 0.992, ave_loss: 1.554
[55]  [1080/1724] loss: 0.915, ave_loss: 1.542
[56]  [1100/1724] loss: 0.875, ave_loss: 1.530
[57]  [1120/1724] loss: 1.049, ave_loss: 1.522
[58]  [1140/1724] loss: 1.372, ave_loss: 1.519
[59]  [1160/1724] loss: 1.023, ave_loss: 1.511
[60]  [1180/1724] loss: 0.905, ave_loss: 1.501
[61]  [1200/1724] loss: 0.921, ave_loss: 1.491
[62]  [1220/1724] loss: 1.005, ave_loss: 1.483
[63]  [1240/1724] loss: 0.954, ave_loss: 1.475
[64]  [1260/1724] loss: 0.934, ave_loss: 1.467
[65]  [1280/1724] loss: 0.951, ave_loss: 1.459
[66]  [1300/1724] loss: 0.919, ave_loss: 1.451
[67]  [1320/1724] loss: 0.942, ave_loss: 1.443
[68]  [1340/1724] loss: 0.888, ave_loss: 1.435
[69]  [1360/1724] loss: 0.797, ave_loss: 1.426
[70]  [1380/1724] loss: 0.764, ave_loss: 1.416
[71]  [1400/1724] loss: 0.790, ave_loss: 1.407
[72]  [1420/1724] loss: 0.785, ave_loss: 1.399
[73]  [1440/1724] loss: 0.788, ave_loss: 1.390
[74]  [1460/1724] loss: 0.793, ave_loss: 1.382
[75]  [1480/1724] loss: 0.763, ave_loss: 1.374
[76]  [1500/1724] loss: 0.725, ave_loss: 1.365
[77]  [1520/1724] loss: 0.703, ave_loss: 1.357
[78]  [1540/1724] loss: 0.702, ave_loss: 1.348
[79]  [1560/1724] loss: 0.799, ave_loss: 1.341
[80]  [1580/1724] loss: 0.754, ave_loss: 1.334
[81]  [1600/1724] loss: 0.671, ave_loss: 1.326
[82]  [1620/1724] loss: 0.694, ave_loss: 1.318
[83]  [1640/1724] loss: 0.674, ave_loss: 1.310
[84]  [1660/1724] loss: 0.635, ave_loss: 1.302
[85]  [1680/1724] loss: 0.708, ave_loss: 1.295
[86]  [1700/1724] loss: 0.664, ave_loss: 1.288
[87]  [1720/1724] loss: 0.766, ave_loss: 1.282
[88]  [1740/1724] loss: 0.722, ave_loss: 1.276

Finished Training finishing at 2021-08-24 22:21:46.990588
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 1.276e+00
Validation Loss: 6.013e-01
Validation ROC: 0.3937
Saving model
97.97911832946636 epochs left to go

Training Epoch 1.0208816705336428/100 starting at 2021-08-24 22:35:32.013615
[1]  [0/1724] loss: 0.665, ave_loss: 0.665
[2]  [20/1724] loss: 0.760, ave_loss: 0.712
[3]  [40/1724] loss: 0.682, ave_loss: 0.702
[4]  [60/1724] loss: 0.753, ave_loss: 0.715
[5]  [80/1724] loss: 0.613, ave_loss: 0.695
[6]  [100/1724] loss: 0.644, ave_loss: 0.686
[7]  [120/1724] loss: 0.735, ave_loss: 0.693
[8]  [140/1724] loss: 0.581, ave_loss: 0.679
[9]  [160/1724] loss: 0.742, ave_loss: 0.686
[10]  [180/1724] loss: 0.484, ave_loss: 0.666
[11]  [200/1724] loss: 0.551, ave_loss: 0.655
[12]  [220/1724] loss: 0.634, ave_loss: 0.654
[13]  [240/1724] loss: 0.489, ave_loss: 0.641
[14]  [260/1724] loss: 0.875, ave_loss: 0.658
[15]  [280/1724] loss: 0.667, ave_loss: 0.658
[16]  [300/1724] loss: 0.668, ave_loss: 0.659
[17]  [320/1724] loss: 0.603, ave_loss: 0.656
[18]  [340/1724] loss: 0.662, ave_loss: 0.656
[19]  [360/1724] loss: 0.859, ave_loss: 0.667
[20]  [380/1724] loss: 0.689, ave_loss: 0.668
[21]  [400/1724] loss: 0.731, ave_loss: 0.671
[22]  [420/1724] loss: 0.784, ave_loss: 0.676
[23]  [440/1724] loss: 0.651, ave_loss: 0.675
[24]  [460/1724] loss: 0.674, ave_loss: 0.675
[25]  [480/1724] loss: 0.735, ave_loss: 0.677
[26]  [500/1724] loss: 0.729, ave_loss: 0.679
[27]  [520/1724] loss: 0.819, ave_loss: 0.684
[28]  [540/1724] loss: 0.753, ave_loss: 0.687
[29]  [560/1724] loss: 0.624, ave_loss: 0.685
[30]  [580/1724] loss: 0.740, ave_loss: 0.686
[31]  [600/1724] loss: 0.735, ave_loss: 0.688
[32]  [620/1724] loss: 0.642, ave_loss: 0.687
[33]  [640/1724] loss: 0.724, ave_loss: 0.688
[34]  [660/1724] loss: 0.613, ave_loss: 0.686
[35]  [680/1724] loss: 0.770, ave_loss: 0.688
[36]  [700/1724] loss: 0.723, ave_loss: 0.689
[37]  [720/1724] loss: 0.748, ave_loss: 0.691
[38]  [740/1724] loss: 0.618, ave_loss: 0.689
[39]  [760/1724] loss: 0.709, ave_loss: 0.689
[40]  [780/1724] loss: 0.703, ave_loss: 0.690
[41]  [800/1724] loss: 0.589, ave_loss: 0.687
[42]  [820/1724] loss: 0.806, ave_loss: 0.690
[43]  [840/1724] loss: 0.673, ave_loss: 0.689
[44]  [860/1724] loss: 0.631, ave_loss: 0.688
[45]  [880/1724] loss: 0.669, ave_loss: 0.688
[46]  [900/1724] loss: 0.588, ave_loss: 0.686
[47]  [920/1724] loss: 0.729, ave_loss: 0.686
[48]  [940/1724] loss: 0.587, ave_loss: 0.684
[49]  [960/1724] loss: 0.723, ave_loss: 0.685
[50]  [980/1724] loss: 0.625, ave_loss: 0.684
[51]  [1000/1724] loss: 0.540, ave_loss: 0.681
[52]  [1020/1724] loss: 0.665, ave_loss: 0.681
[53]  [1040/1724] loss: 0.529, ave_loss: 0.678
[54]  [1060/1724] loss: 0.579, ave_loss: 0.676
[55]  [1080/1724] loss: 0.702, ave_loss: 0.677
[56]  [1100/1724] loss: 0.700, ave_loss: 0.677
[57]  [1120/1724] loss: 0.703, ave_loss: 0.677
[58]  [1140/1724] loss: 0.856, ave_loss: 0.681
[59]  [1160/1724] loss: 0.611, ave_loss: 0.679
[60]  [1180/1724] loss: 0.752, ave_loss: 0.681
[61]  [1200/1724] loss: 0.569, ave_loss: 0.679
[62]  [1220/1724] loss: 0.619, ave_loss: 0.678
[63]  [1240/1724] loss: 0.655, ave_loss: 0.677
[64]  [1260/1724] loss: 0.599, ave_loss: 0.676
[65]  [1280/1724] loss: 0.635, ave_loss: 0.676
[66]  [1300/1724] loss: 0.824, ave_loss: 0.678
[67]  [1320/1724] loss: 0.883, ave_loss: 0.681
[68]  [1340/1724] loss: 0.679, ave_loss: 0.681
[69]  [1360/1724] loss: 0.669, ave_loss: 0.681
[70]  [1380/1724] loss: 0.489, ave_loss: 0.678
[71]  [1400/1724] loss: 0.618, ave_loss: 0.677
[72]  [1420/1724] loss: 0.666, ave_loss: 0.677
[73]  [1440/1724] loss: 0.701, ave_loss: 0.677
[74]  [1460/1724] loss: 0.598, ave_loss: 0.676
[75]  [1480/1724] loss: 0.584, ave_loss: 0.675
[76]  [1500/1724] loss: 0.568, ave_loss: 0.674
[77]  [1520/1724] loss: 0.626, ave_loss: 0.673
[78]  [1540/1724] loss: 0.625, ave_loss: 0.672
[79]  [1560/1724] loss: 0.626, ave_loss: 0.672
[80]  [1580/1724] loss: 0.707, ave_loss: 0.672
[81]  [1600/1724] loss: 0.584, ave_loss: 0.671
[82]  [1620/1724] loss: 0.678, ave_loss: 0.671
[83]  [1640/1724] loss: 0.753, ave_loss: 0.672
[84]  [1660/1724] loss: 0.746, ave_loss: 0.673
[85]  [1680/1724] loss: 0.603, ave_loss: 0.672
[86]  [1700/1724] loss: 0.742, ave_loss: 0.673
[87]  [1720/1724] loss: 0.655, ave_loss: 0.673
[88]  [1740/1724] loss: 0.562, ave_loss: 0.672

Finished Training finishing at 2021-08-24 22:50:56.171151
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.716e-01
Validation Loss: 5.902e-01
Validation ROC: 0.3753
No improvement, still saving model
96.95823665893272 epochs left to go

Training Epoch 2.0417633410672855/100 starting at 2021-08-24 23:02:59.847788
[1]  [0/1724] loss: 0.644, ave_loss: 0.644
[2]  [20/1724] loss: 0.732, ave_loss: 0.688
[3]  [40/1724] loss: 0.585, ave_loss: 0.654
[4]  [60/1724] loss: 0.665, ave_loss: 0.657
[5]  [80/1724] loss: 0.744, ave_loss: 0.674
[6]  [100/1724] loss: 0.611, ave_loss: 0.664
[7]  [120/1724] loss: 0.697, ave_loss: 0.668
[8]  [140/1724] loss: 0.700, ave_loss: 0.672
[9]  [160/1724] loss: 0.595, ave_loss: 0.664
[10]  [180/1724] loss: 0.703, ave_loss: 0.668
[11]  [200/1724] loss: 0.719, ave_loss: 0.672
[12]  [220/1724] loss: 0.709, ave_loss: 0.675
[13]  [240/1724] loss: 0.705, ave_loss: 0.678
[14]  [260/1724] loss: 0.450, ave_loss: 0.661
[15]  [280/1724] loss: 0.656, ave_loss: 0.661
[16]  [300/1724] loss: 0.668, ave_loss: 0.661
[17]  [320/1724] loss: 0.583, ave_loss: 0.657
[18]  [340/1724] loss: 0.626, ave_loss: 0.655
[19]  [360/1724] loss: 0.763, ave_loss: 0.661
[20]  [380/1724] loss: 0.758, ave_loss: 0.666
[21]  [400/1724] loss: 0.661, ave_loss: 0.665
[22]  [420/1724] loss: 0.662, ave_loss: 0.665
[23]  [440/1724] loss: 0.619, ave_loss: 0.663
[24]  [460/1724] loss: 0.575, ave_loss: 0.660
[25]  [480/1724] loss: 0.697, ave_loss: 0.661
[26]  [500/1724] loss: 0.587, ave_loss: 0.658
[27]  [520/1724] loss: 0.648, ave_loss: 0.658
[28]  [540/1724] loss: 0.668, ave_loss: 0.658
[29]  [560/1724] loss: 0.737, ave_loss: 0.661
[30]  [580/1724] loss: 0.740, ave_loss: 0.664
[31]  [600/1724] loss: 0.632, ave_loss: 0.663
[32]  [620/1724] loss: 0.816, ave_loss: 0.667
[33]  [640/1724] loss: 0.622, ave_loss: 0.666
[34]  [660/1724] loss: 0.619, ave_loss: 0.665
[35]  [680/1724] loss: 0.730, ave_loss: 0.666
[36]  [700/1724] loss: 0.747, ave_loss: 0.669
[37]  [720/1724] loss: 0.708, ave_loss: 0.670
[38]  [740/1724] loss: 0.719, ave_loss: 0.671
[39]  [760/1724] loss: 0.583, ave_loss: 0.669
[40]  [780/1724] loss: 0.646, ave_loss: 0.668
[41]  [800/1724] loss: 0.613, ave_loss: 0.667
[42]  [820/1724] loss: 0.662, ave_loss: 0.667
[43]  [840/1724] loss: 0.589, ave_loss: 0.665
[44]  [860/1724] loss: 0.743, ave_loss: 0.667
[45]  [880/1724] loss: 0.666, ave_loss: 0.667
[46]  [900/1724] loss: 0.591, ave_loss: 0.665
[47]  [920/1724] loss: 0.637, ave_loss: 0.664
[48]  [940/1724] loss: 0.650, ave_loss: 0.664
[49]  [960/1724] loss: 0.666, ave_loss: 0.664
[50]  [980/1724] loss: 0.540, ave_loss: 0.662
[51]  [1000/1724] loss: 0.568, ave_loss: 0.660
[52]  [1020/1724] loss: 0.751, ave_loss: 0.662
[53]  [1040/1724] loss: 0.788, ave_loss: 0.664
[54]  [1060/1724] loss: 0.647, ave_loss: 0.664
[55]  [1080/1724] loss: 0.683, ave_loss: 0.664
[56]  [1100/1724] loss: 0.598, ave_loss: 0.663
[57]  [1120/1724] loss: 0.524, ave_loss: 0.660
[58]  [1140/1724] loss: 0.757, ave_loss: 0.662
[59]  [1160/1724] loss: 0.532, ave_loss: 0.660
[60]  [1180/1724] loss: 0.840, ave_loss: 0.663
[61]  [1200/1724] loss: 0.678, ave_loss: 0.663
[62]  [1220/1724] loss: 0.605, ave_loss: 0.662
[63]  [1240/1724] loss: 0.765, ave_loss: 0.664
[64]  [1260/1724] loss: 0.599, ave_loss: 0.663
[65]  [1280/1724] loss: 0.587, ave_loss: 0.662
[66]  [1300/1724] loss: 0.639, ave_loss: 0.661
[67]  [1320/1724] loss: 0.575, ave_loss: 0.660
[68]  [1340/1724] loss: 0.593, ave_loss: 0.659
[69]  [1360/1724] loss: 0.663, ave_loss: 0.659
[70]  [1380/1724] loss: 0.490, ave_loss: 0.657
[71]  [1400/1724] loss: 0.773, ave_loss: 0.658
[72]  [1420/1724] loss: 0.603, ave_loss: 0.658
[73]  [1440/1724] loss: 0.678, ave_loss: 0.658
[74]  [1460/1724] loss: 0.691, ave_loss: 0.658
[75]  [1480/1724] loss: 0.533, ave_loss: 0.657
[76]  [1500/1724] loss: 0.605, ave_loss: 0.656
[77]  [1520/1724] loss: 0.575, ave_loss: 0.655
[78]  [1540/1724] loss: 0.605, ave_loss: 0.654
[79]  [1560/1724] loss: 0.837, ave_loss: 0.657
[80]  [1580/1724] loss: 0.664, ave_loss: 0.657
[81]  [1600/1724] loss: 0.731, ave_loss: 0.658
[82]  [1620/1724] loss: 0.571, ave_loss: 0.657
[83]  [1640/1724] loss: 0.682, ave_loss: 0.657
[84]  [1660/1724] loss: 0.708, ave_loss: 0.657
[85]  [1680/1724] loss: 0.756, ave_loss: 0.659
[86]  [1700/1724] loss: 0.657, ave_loss: 0.659
[87]  [1720/1724] loss: 0.828, ave_loss: 0.661
[88]  [1740/1724] loss: 0.671, ave_loss: 0.661

Finished Training finishing at 2021-08-24 23:17:54.796988
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.607e-01
Validation Loss: 5.846e-01
Validation ROC: 0.4105
Saving model
95.93735498839908 epochs left to go

Training Epoch 3.062645011600928/100 starting at 2021-08-24 23:29:57.524154
[1]  [0/1724] loss: 0.687, ave_loss: 0.687
[2]  [20/1724] loss: 0.631, ave_loss: 0.659
[3]  [40/1724] loss: 0.570, ave_loss: 0.629
[4]  [60/1724] loss: 0.830, ave_loss: 0.679
[5]  [80/1724] loss: 0.624, ave_loss: 0.668
[6]  [100/1724] loss: 0.552, ave_loss: 0.649
[7]  [120/1724] loss: 0.568, ave_loss: 0.637
[8]  [140/1724] loss: 0.560, ave_loss: 0.628
[9]  [160/1724] loss: 0.704, ave_loss: 0.636
[10]  [180/1724] loss: 0.657, ave_loss: 0.638
[11]  [200/1724] loss: 0.661, ave_loss: 0.640
[12]  [220/1724] loss: 0.753, ave_loss: 0.650
[13]  [240/1724] loss: 0.772, ave_loss: 0.659
[14]  [260/1724] loss: 0.683, ave_loss: 0.661
[15]  [280/1724] loss: 0.815, ave_loss: 0.671
[16]  [300/1724] loss: 0.717, ave_loss: 0.674
[17]  [320/1724] loss: 0.558, ave_loss: 0.667
[18]  [340/1724] loss: 0.577, ave_loss: 0.662
[19]  [360/1724] loss: 0.630, ave_loss: 0.660
[20]  [380/1724] loss: 0.687, ave_loss: 0.662
[21]  [400/1724] loss: 0.667, ave_loss: 0.662
[22]  [420/1724] loss: 0.663, ave_loss: 0.662
[23]  [440/1724] loss: 0.564, ave_loss: 0.658
[24]  [460/1724] loss: 0.644, ave_loss: 0.657
[25]  [480/1724] loss: 0.642, ave_loss: 0.657
[26]  [500/1724] loss: 0.789, ave_loss: 0.662
[27]  [520/1724] loss: 0.556, ave_loss: 0.658
[28]  [540/1724] loss: 0.675, ave_loss: 0.658
[29]  [560/1724] loss: 0.790, ave_loss: 0.663
[30]  [580/1724] loss: 0.662, ave_loss: 0.663
[31]  [600/1724] loss: 0.599, ave_loss: 0.661
[32]  [620/1724] loss: 0.651, ave_loss: 0.660
[33]  [640/1724] loss: 0.605, ave_loss: 0.659
[34]  [660/1724] loss: 0.618, ave_loss: 0.658
[35]  [680/1724] loss: 0.490, ave_loss: 0.653
[36]  [700/1724] loss: 0.622, ave_loss: 0.652
[37]  [720/1724] loss: 0.551, ave_loss: 0.649
[38]  [740/1724] loss: 0.601, ave_loss: 0.648
[39]  [760/1724] loss: 0.570, ave_loss: 0.646
[40]  [780/1724] loss: 0.768, ave_loss: 0.649
[41]  [800/1724] loss: 0.598, ave_loss: 0.648
[42]  [820/1724] loss: 0.590, ave_loss: 0.646
[43]  [840/1724] loss: 0.551, ave_loss: 0.644
[44]  [860/1724] loss: 0.699, ave_loss: 0.645
[45]  [880/1724] loss: 0.584, ave_loss: 0.644
[46]  [900/1724] loss: 0.566, ave_loss: 0.642
[47]  [920/1724] loss: 0.738, ave_loss: 0.644
[48]  [940/1724] loss: 0.786, ave_loss: 0.647
[49]  [960/1724] loss: 0.633, ave_loss: 0.647
[50]  [980/1724] loss: 0.474, ave_loss: 0.644
[51]  [1000/1724] loss: 0.718, ave_loss: 0.645
[52]  [1020/1724] loss: 0.545, ave_loss: 0.643
[53]  [1040/1724] loss: 0.658, ave_loss: 0.643
[54]  [1060/1724] loss: 0.571, ave_loss: 0.642
[55]  [1080/1724] loss: 0.453, ave_loss: 0.639
[56]  [1100/1724] loss: 0.599, ave_loss: 0.638
[57]  [1120/1724] loss: 0.788, ave_loss: 0.641
[58]  [1140/1724] loss: 0.476, ave_loss: 0.638
[59]  [1160/1724] loss: 0.584, ave_loss: 0.637
[60]  [1180/1724] loss: 0.854, ave_loss: 0.640
[61]  [1200/1724] loss: 0.700, ave_loss: 0.641
[62]  [1220/1724] loss: 0.644, ave_loss: 0.641
[63]  [1240/1724] loss: 0.561, ave_loss: 0.640
[64]  [1260/1724] loss: 0.729, ave_loss: 0.642
[65]  [1280/1724] loss: 0.623, ave_loss: 0.641
[66]  [1300/1724] loss: 0.672, ave_loss: 0.642
[67]  [1320/1724] loss: 0.462, ave_loss: 0.639
[68]  [1340/1724] loss: 0.539, ave_loss: 0.638
[69]  [1360/1724] loss: 0.661, ave_loss: 0.638
[70]  [1380/1724] loss: 0.487, ave_loss: 0.636
[71]  [1400/1724] loss: 0.578, ave_loss: 0.635
[72]  [1420/1724] loss: 0.535, ave_loss: 0.634
[73]  [1440/1724] loss: 0.507, ave_loss: 0.632
[74]  [1460/1724] loss: 0.763, ave_loss: 0.634
[75]  [1480/1724] loss: 0.633, ave_loss: 0.634
[76]  [1500/1724] loss: 0.844, ave_loss: 0.636
[77]  [1520/1724] loss: 0.664, ave_loss: 0.637
[78]  [1540/1724] loss: 0.687, ave_loss: 0.637
[79]  [1560/1724] loss: 0.672, ave_loss: 0.638
[80]  [1580/1724] loss: 0.702, ave_loss: 0.639
[81]  [1600/1724] loss: 0.588, ave_loss: 0.638
[82]  [1620/1724] loss: 0.825, ave_loss: 0.640
[83]  [1640/1724] loss: 0.622, ave_loss: 0.640
[84]  [1660/1724] loss: 0.681, ave_loss: 0.641
[85]  [1680/1724] loss: 0.660, ave_loss: 0.641
[86]  [1700/1724] loss: 0.674, ave_loss: 0.641
[87]  [1720/1724] loss: 0.657, ave_loss: 0.641
[88]  [1740/1724] loss: 0.511, ave_loss: 0.640

Finished Training finishing at 2021-08-24 23:44:34.871288
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.398e-01
Validation Loss: 5.738e-01
Validation ROC: 0.4570
Saving model
94.91647331786542 epochs left to go

Training Epoch 4.083526682134571/100 starting at 2021-08-24 23:56:33.043690
[1]  [0/1724] loss: 0.659, ave_loss: 0.659
[2]  [20/1724] loss: 0.700, ave_loss: 0.679
[3]  [40/1724] loss: 0.654, ave_loss: 0.671
[4]  [60/1724] loss: 0.564, ave_loss: 0.644
[5]  [80/1724] loss: 0.648, ave_loss: 0.645
[6]  [100/1724] loss: 0.618, ave_loss: 0.640
[7]  [120/1724] loss: 0.688, ave_loss: 0.647
[8]  [140/1724] loss: 0.604, ave_loss: 0.642
[9]  [160/1724] loss: 0.525, ave_loss: 0.629
[10]  [180/1724] loss: 0.668, ave_loss: 0.633
[11]  [200/1724] loss: 0.726, ave_loss: 0.641
[12]  [220/1724] loss: 0.702, ave_loss: 0.646
[13]  [240/1724] loss: 0.584, ave_loss: 0.641
[14]  [260/1724] loss: 0.512, ave_loss: 0.632
[15]  [280/1724] loss: 0.704, ave_loss: 0.637
[16]  [300/1724] loss: 0.712, ave_loss: 0.642
[17]  [320/1724] loss: 0.670, ave_loss: 0.643
[18]  [340/1724] loss: 0.693, ave_loss: 0.646
[19]  [360/1724] loss: 0.715, ave_loss: 0.650
[20]  [380/1724] loss: 0.645, ave_loss: 0.650
[21]  [400/1724] loss: 0.771, ave_loss: 0.655
[22]  [420/1724] loss: 0.665, ave_loss: 0.656
[23]  [440/1724] loss: 0.619, ave_loss: 0.654
[24]  [460/1724] loss: 0.769, ave_loss: 0.659
[25]  [480/1724] loss: 0.712, ave_loss: 0.661
[26]  [500/1724] loss: 0.588, ave_loss: 0.658
[27]  [520/1724] loss: 0.595, ave_loss: 0.656
[28]  [540/1724] loss: 0.616, ave_loss: 0.654
[29]  [560/1724] loss: 0.780, ave_loss: 0.659
[30]  [580/1724] loss: 0.612, ave_loss: 0.657
[31]  [600/1724] loss: 0.693, ave_loss: 0.658
[32]  [620/1724] loss: 0.595, ave_loss: 0.656
[33]  [640/1724] loss: 0.537, ave_loss: 0.653
[34]  [660/1724] loss: 0.575, ave_loss: 0.650
[35]  [680/1724] loss: 0.690, ave_loss: 0.652
[36]  [700/1724] loss: 0.649, ave_loss: 0.652
[37]  [720/1724] loss: 0.654, ave_loss: 0.652
[38]  [740/1724] loss: 0.654, ave_loss: 0.652
[39]  [760/1724] loss: 0.774, ave_loss: 0.655
[40]  [780/1724] loss: 0.677, ave_loss: 0.655
[41]  [800/1724] loss: 0.548, ave_loss: 0.653
[42]  [820/1724] loss: 0.717, ave_loss: 0.654
[43]  [840/1724] loss: 0.669, ave_loss: 0.655
[44]  [860/1724] loss: 0.669, ave_loss: 0.655
[45]  [880/1724] loss: 0.645, ave_loss: 0.655
[46]  [900/1724] loss: 0.683, ave_loss: 0.655
[47]  [920/1724] loss: 0.568, ave_loss: 0.653
[48]  [940/1724] loss: 0.684, ave_loss: 0.654
[49]  [960/1724] loss: 0.700, ave_loss: 0.655
[50]  [980/1724] loss: 0.544, ave_loss: 0.653
[51]  [1000/1724] loss: 0.610, ave_loss: 0.652
[52]  [1020/1724] loss: 0.737, ave_loss: 0.654
[53]  [1040/1724] loss: 0.634, ave_loss: 0.653
[54]  [1060/1724] loss: 0.595, ave_loss: 0.652
[55]  [1080/1724] loss: 0.612, ave_loss: 0.651
[56]  [1100/1724] loss: 0.748, ave_loss: 0.653
[57]  [1120/1724] loss: 0.558, ave_loss: 0.651
[58]  [1140/1724] loss: 0.556, ave_loss: 0.650
[59]  [1160/1724] loss: 0.564, ave_loss: 0.648
[60]  [1180/1724] loss: 0.881, ave_loss: 0.652
[61]  [1200/1724] loss: 0.542, ave_loss: 0.650
[62]  [1220/1724] loss: 0.638, ave_loss: 0.650
[63]  [1240/1724] loss: 0.770, ave_loss: 0.652
[64]  [1260/1724] loss: 0.618, ave_loss: 0.652
[65]  [1280/1724] loss: 0.652, ave_loss: 0.652
[66]  [1300/1724] loss: 0.581, ave_loss: 0.651
[67]  [1320/1724] loss: 0.711, ave_loss: 0.651
[68]  [1340/1724] loss: 0.633, ave_loss: 0.651
[69]  [1360/1724] loss: 0.626, ave_loss: 0.651
[70]  [1380/1724] loss: 0.575, ave_loss: 0.650
[71]  [1400/1724] loss: 0.638, ave_loss: 0.650
[72]  [1420/1724] loss: 0.616, ave_loss: 0.649
[73]  [1440/1724] loss: 0.591, ave_loss: 0.648
[74]  [1460/1724] loss: 0.754, ave_loss: 0.650
[75]  [1480/1724] loss: 0.504, ave_loss: 0.648
[76]  [1500/1724] loss: 0.613, ave_loss: 0.647
[77]  [1520/1724] loss: 0.730, ave_loss: 0.648
[78]  [1540/1724] loss: 0.555, ave_loss: 0.647
[79]  [1560/1724] loss: 0.688, ave_loss: 0.648
[80]  [1580/1724] loss: 0.606, ave_loss: 0.647
[81]  [1600/1724] loss: 0.566, ave_loss: 0.646
[82]  [1620/1724] loss: 0.489, ave_loss: 0.644
[83]  [1640/1724] loss: 0.778, ave_loss: 0.646
[84]  [1660/1724] loss: 0.549, ave_loss: 0.645
[85]  [1680/1724] loss: 0.571, ave_loss: 0.644
[86]  [1700/1724] loss: 0.687, ave_loss: 0.644
[87]  [1720/1724] loss: 0.554, ave_loss: 0.643
[88]  [1740/1724] loss: 0.750, ave_loss: 0.645

Finished Training finishing at 2021-08-25 00:11:08.216081
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.445e-01
Validation Loss: 5.277e-01
Validation ROC: 0.5048
Saving model
93.89559164733178 epochs left to go

Training Epoch 5.104408352668213/100 starting at 2021-08-25 00:23:05.730621
[1]  [0/1724] loss: 0.776, ave_loss: 0.776
[2]  [20/1724] loss: 0.509, ave_loss: 0.643
[3]  [40/1724] loss: 0.786, ave_loss: 0.691
[4]  [60/1724] loss: 0.662, ave_loss: 0.683
[5]  [80/1724] loss: 0.612, ave_loss: 0.669
[6]  [100/1724] loss: 0.864, ave_loss: 0.702
[7]  [120/1724] loss: 0.650, ave_loss: 0.694
[8]  [140/1724] loss: 0.707, ave_loss: 0.696
[9]  [160/1724] loss: 0.579, ave_loss: 0.683
[10]  [180/1724] loss: 0.686, ave_loss: 0.683
[11]  [200/1724] loss: 0.673, ave_loss: 0.682
[12]  [220/1724] loss: 0.497, ave_loss: 0.667
[13]  [240/1724] loss: 0.663, ave_loss: 0.666
[14]  [260/1724] loss: 0.578, ave_loss: 0.660
[15]  [280/1724] loss: 0.494, ave_loss: 0.649
[16]  [300/1724] loss: 0.581, ave_loss: 0.645
[17]  [320/1724] loss: 0.613, ave_loss: 0.643
[18]  [340/1724] loss: 0.781, ave_loss: 0.651
[19]  [360/1724] loss: 0.660, ave_loss: 0.651
[20]  [380/1724] loss: 0.669, ave_loss: 0.652
[21]  [400/1724] loss: 0.606, ave_loss: 0.650
[22]  [420/1724] loss: 0.750, ave_loss: 0.654
[23]  [440/1724] loss: 0.640, ave_loss: 0.654
[24]  [460/1724] loss: 0.530, ave_loss: 0.648
[25]  [480/1724] loss: 0.693, ave_loss: 0.650
[26]  [500/1724] loss: 0.820, ave_loss: 0.657
[27]  [520/1724] loss: 0.775, ave_loss: 0.661
[28]  [540/1724] loss: 0.580, ave_loss: 0.658
[29]  [560/1724] loss: 0.734, ave_loss: 0.661
[30]  [580/1724] loss: 0.677, ave_loss: 0.661
[31]  [600/1724] loss: 0.592, ave_loss: 0.659
[32]  [620/1724] loss: 0.552, ave_loss: 0.656
[33]  [640/1724] loss: 0.624, ave_loss: 0.655
[34]  [660/1724] loss: 0.690, ave_loss: 0.656
[35]  [680/1724] loss: 0.596, ave_loss: 0.654
[36]  [700/1724] loss: 0.593, ave_loss: 0.653
[37]  [720/1724] loss: 0.699, ave_loss: 0.654
[38]  [740/1724] loss: 0.637, ave_loss: 0.653
[39]  [760/1724] loss: 0.729, ave_loss: 0.655
[40]  [780/1724] loss: 0.712, ave_loss: 0.657
[41]  [800/1724] loss: 0.582, ave_loss: 0.655
[42]  [820/1724] loss: 0.743, ave_loss: 0.657
[43]  [840/1724] loss: 0.670, ave_loss: 0.657
[44]  [860/1724] loss: 0.642, ave_loss: 0.657
[45]  [880/1724] loss: 0.568, ave_loss: 0.655
[46]  [900/1724] loss: 0.705, ave_loss: 0.656
[47]  [920/1724] loss: 0.614, ave_loss: 0.655
[48]  [940/1724] loss: 0.660, ave_loss: 0.655
[49]  [960/1724] loss: 0.642, ave_loss: 0.655
[50]  [980/1724] loss: 0.601, ave_loss: 0.654
[51]  [1000/1724] loss: 0.611, ave_loss: 0.653
[52]  [1020/1724] loss: 0.617, ave_loss: 0.652
[53]  [1040/1724] loss: 0.501, ave_loss: 0.649
[54]  [1060/1724] loss: 0.640, ave_loss: 0.649
[55]  [1080/1724] loss: 0.577, ave_loss: 0.648
[56]  [1100/1724] loss: 0.740, ave_loss: 0.650
[57]  [1120/1724] loss: 0.678, ave_loss: 0.650
[58]  [1140/1724] loss: 0.660, ave_loss: 0.650
[59]  [1160/1724] loss: 0.685, ave_loss: 0.651
[60]  [1180/1724] loss: 0.634, ave_loss: 0.651
[61]  [1200/1724] loss: 0.642, ave_loss: 0.650
[62]  [1220/1724] loss: 0.662, ave_loss: 0.651
[63]  [1240/1724] loss: 0.636, ave_loss: 0.650
[64]  [1260/1724] loss: 0.660, ave_loss: 0.651
[65]  [1280/1724] loss: 0.634, ave_loss: 0.650
[66]  [1300/1724] loss: 0.514, ave_loss: 0.648
[67]  [1320/1724] loss: 0.516, ave_loss: 0.646
[68]  [1340/1724] loss: 0.695, ave_loss: 0.647
[69]  [1360/1724] loss: 0.611, ave_loss: 0.646
[70]  [1380/1724] loss: 0.682, ave_loss: 0.647
[71]  [1400/1724] loss: 0.678, ave_loss: 0.647
[72]  [1420/1724] loss: 0.498, ave_loss: 0.645
[73]  [1440/1724] loss: 0.545, ave_loss: 0.644
[74]  [1460/1724] loss: 0.516, ave_loss: 0.642
[75]  [1480/1724] loss: 0.715, ave_loss: 0.643
[76]  [1500/1724] loss: 0.642, ave_loss: 0.643
[77]  [1520/1724] loss: 0.534, ave_loss: 0.642
[78]  [1540/1724] loss: 0.601, ave_loss: 0.641
[79]  [1560/1724] loss: 0.549, ave_loss: 0.640
[80]  [1580/1724] loss: 0.646, ave_loss: 0.640
[81]  [1600/1724] loss: 0.707, ave_loss: 0.641
[82]  [1620/1724] loss: 0.690, ave_loss: 0.642
[83]  [1640/1724] loss: 0.587, ave_loss: 0.641
[84]  [1660/1724] loss: 0.638, ave_loss: 0.641
[85]  [1680/1724] loss: 0.593, ave_loss: 0.640
[86]  [1700/1724] loss: 0.740, ave_loss: 0.642
[87]  [1720/1724] loss: 0.639, ave_loss: 0.642
[88]  [1740/1724] loss: 0.732, ave_loss: 0.643

Finished Training finishing at 2021-08-25 00:37:31.517391
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.425e-01
Validation Loss: 5.465e-01
Validation ROC: 0.5256
Saving model
92.87470997679814 epochs left to go

Training Epoch 6.125290023201856/100 starting at 2021-08-25 00:49:25.327520
[1]  [0/1724] loss: 0.708, ave_loss: 0.708
[2]  [20/1724] loss: 0.551, ave_loss: 0.629
[3]  [40/1724] loss: 0.675, ave_loss: 0.645
[4]  [60/1724] loss: 0.757, ave_loss: 0.673
[5]  [80/1724] loss: 0.833, ave_loss: 0.705
[6]  [100/1724] loss: 0.619, ave_loss: 0.690
[7]  [120/1724] loss: 0.617, ave_loss: 0.680
[8]  [140/1724] loss: 0.653, ave_loss: 0.677
[9]  [160/1724] loss: 0.696, ave_loss: 0.679
[10]  [180/1724] loss: 0.581, ave_loss: 0.669
[11]  [200/1724] loss: 0.721, ave_loss: 0.674
[12]  [220/1724] loss: 0.585, ave_loss: 0.666
[13]  [240/1724] loss: 0.541, ave_loss: 0.657
[14]  [260/1724] loss: 0.662, ave_loss: 0.657
[15]  [280/1724] loss: 0.739, ave_loss: 0.663
[16]  [300/1724] loss: 0.579, ave_loss: 0.657
[17]  [320/1724] loss: 0.513, ave_loss: 0.649
[18]  [340/1724] loss: 0.575, ave_loss: 0.645
[19]  [360/1724] loss: 0.595, ave_loss: 0.642
[20]  [380/1724] loss: 0.580, ave_loss: 0.639
[21]  [400/1724] loss: 0.595, ave_loss: 0.637
[22]  [420/1724] loss: 0.573, ave_loss: 0.634
[23]  [440/1724] loss: 0.579, ave_loss: 0.632
[24]  [460/1724] loss: 0.584, ave_loss: 0.630
[25]  [480/1724] loss: 0.603, ave_loss: 0.629
[26]  [500/1724] loss: 0.639, ave_loss: 0.629
[27]  [520/1724] loss: 0.617, ave_loss: 0.629
[28]  [540/1724] loss: 0.691, ave_loss: 0.631
[29]  [560/1724] loss: 0.713, ave_loss: 0.634
[30]  [580/1724] loss: 0.659, ave_loss: 0.634
[31]  [600/1724] loss: 0.482, ave_loss: 0.629
[32]  [620/1724] loss: 0.512, ave_loss: 0.626
[33]  [640/1724] loss: 0.607, ave_loss: 0.625
[34]  [660/1724] loss: 0.617, ave_loss: 0.625
[35]  [680/1724] loss: 0.624, ave_loss: 0.625
[36]  [700/1724] loss: 0.820, ave_loss: 0.630
[37]  [720/1724] loss: 0.758, ave_loss: 0.634
[38]  [740/1724] loss: 0.586, ave_loss: 0.633
[39]  [760/1724] loss: 0.605, ave_loss: 0.632
[40]  [780/1724] loss: 0.741, ave_loss: 0.635
[41]  [800/1724] loss: 0.617, ave_loss: 0.634
[42]  [820/1724] loss: 0.670, ave_loss: 0.635
[43]  [840/1724] loss: 0.652, ave_loss: 0.635
[44]  [860/1724] loss: 0.655, ave_loss: 0.636
[45]  [880/1724] loss: 0.821, ave_loss: 0.640
[46]  [900/1724] loss: 0.631, ave_loss: 0.640
[47]  [920/1724] loss: 0.649, ave_loss: 0.640
[48]  [940/1724] loss: 0.679, ave_loss: 0.641
[49]  [960/1724] loss: 0.698, ave_loss: 0.642
[50]  [980/1724] loss: 0.549, ave_loss: 0.640
[51]  [1000/1724] loss: 0.576, ave_loss: 0.639
[52]  [1020/1724] loss: 0.592, ave_loss: 0.638
[53]  [1040/1724] loss: 0.570, ave_loss: 0.637
[54]  [1060/1724] loss: 0.660, ave_loss: 0.637
[55]  [1080/1724] loss: 0.564, ave_loss: 0.636
[56]  [1100/1724] loss: 0.648, ave_loss: 0.636
[57]  [1120/1724] loss: 0.566, ave_loss: 0.635
[58]  [1140/1724] loss: 0.733, ave_loss: 0.636
[59]  [1160/1724] loss: 0.627, ave_loss: 0.636
[60]  [1180/1724] loss: 0.569, ave_loss: 0.635
[61]  [1200/1724] loss: 0.613, ave_loss: 0.635
[62]  [1220/1724] loss: 0.610, ave_loss: 0.634
[63]  [1240/1724] loss: 0.548, ave_loss: 0.633
[64]  [1260/1724] loss: 0.620, ave_loss: 0.633
[65]  [1280/1724] loss: 0.614, ave_loss: 0.633
[66]  [1300/1724] loss: 0.683, ave_loss: 0.633
[67]  [1320/1724] loss: 0.543, ave_loss: 0.632
[68]  [1340/1724] loss: 0.747, ave_loss: 0.634
[69]  [1360/1724] loss: 0.546, ave_loss: 0.632
[70]  [1380/1724] loss: 0.618, ave_loss: 0.632
[71]  [1400/1724] loss: 0.610, ave_loss: 0.632
[72]  [1420/1724] loss: 0.637, ave_loss: 0.632
[73]  [1440/1724] loss: 0.609, ave_loss: 0.632
[74]  [1460/1724] loss: 0.818, ave_loss: 0.634
[75]  [1480/1724] loss: 0.832, ave_loss: 0.637
[76]  [1500/1724] loss: 0.806, ave_loss: 0.639
[77]  [1520/1724] loss: 0.734, ave_loss: 0.640
[78]  [1540/1724] loss: 0.617, ave_loss: 0.640
[79]  [1560/1724] loss: 0.649, ave_loss: 0.640
[80]  [1580/1724] loss: 0.640, ave_loss: 0.640
[81]  [1600/1724] loss: 0.627, ave_loss: 0.640
[82]  [1620/1724] loss: 0.598, ave_loss: 0.639
[83]  [1640/1724] loss: 0.738, ave_loss: 0.641
[84]  [1660/1724] loss: 0.675, ave_loss: 0.641
[85]  [1680/1724] loss: 0.677, ave_loss: 0.641
[86]  [1700/1724] loss: 0.663, ave_loss: 0.642
[87]  [1720/1724] loss: 0.526, ave_loss: 0.640
[88]  [1740/1724] loss: 0.646, ave_loss: 0.640

Finished Training finishing at 2021-08-25 01:03:44.341301
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.404e-01
Validation Loss: 5.908e-01
Validation ROC: 0.5374
Saving model
91.8538283062645 epochs left to go

Training Epoch 7.146171693735499/100 starting at 2021-08-25 01:15:36.958102
[1]  [0/1724] loss: 0.584, ave_loss: 0.584
[2]  [20/1724] loss: 0.514, ave_loss: 0.549
[3]  [40/1724] loss: 0.783, ave_loss: 0.627
[4]  [60/1724] loss: 0.557, ave_loss: 0.609
[5]  [80/1724] loss: 0.601, ave_loss: 0.608
[6]  [100/1724] loss: 0.672, ave_loss: 0.619
[7]  [120/1724] loss: 0.587, ave_loss: 0.614
[8]  [140/1724] loss: 0.640, ave_loss: 0.617
[9]  [160/1724] loss: 0.669, ave_loss: 0.623
[10]  [180/1724] loss: 0.657, ave_loss: 0.626
[11]  [200/1724] loss: 0.628, ave_loss: 0.626
[12]  [220/1724] loss: 0.645, ave_loss: 0.628
[13]  [240/1724] loss: 0.674, ave_loss: 0.632
[14]  [260/1724] loss: 0.727, ave_loss: 0.638
[15]  [280/1724] loss: 0.777, ave_loss: 0.648
[16]  [300/1724] loss: 0.632, ave_loss: 0.647
[17]  [320/1724] loss: 0.704, ave_loss: 0.650
[18]  [340/1724] loss: 0.661, ave_loss: 0.651
[19]  [360/1724] loss: 0.626, ave_loss: 0.649
[20]  [380/1724] loss: 0.671, ave_loss: 0.650
[21]  [400/1724] loss: 0.613, ave_loss: 0.649
[22]  [420/1724] loss: 0.607, ave_loss: 0.647
[23]  [440/1724] loss: 0.691, ave_loss: 0.649
[24]  [460/1724] loss: 0.604, ave_loss: 0.647
[25]  [480/1724] loss: 0.748, ave_loss: 0.651
[26]  [500/1724] loss: 0.586, ave_loss: 0.648
[27]  [520/1724] loss: 0.688, ave_loss: 0.650
[28]  [540/1724] loss: 0.538, ave_loss: 0.646
[29]  [560/1724] loss: 0.595, ave_loss: 0.644
[30]  [580/1724] loss: 0.645, ave_loss: 0.644
[31]  [600/1724] loss: 0.655, ave_loss: 0.644
[32]  [620/1724] loss: 0.706, ave_loss: 0.646
[33]  [640/1724] loss: 0.650, ave_loss: 0.646
[34]  [660/1724] loss: 0.621, ave_loss: 0.646
[35]  [680/1724] loss: 0.628, ave_loss: 0.645
[36]  [700/1724] loss: 0.553, ave_loss: 0.643
[37]  [720/1724] loss: 0.645, ave_loss: 0.643
[38]  [740/1724] loss: 0.578, ave_loss: 0.641
[39]  [760/1724] loss: 0.597, ave_loss: 0.640
[40]  [780/1724] loss: 0.633, ave_loss: 0.640
[41]  [800/1724] loss: 0.644, ave_loss: 0.640
[42]  [820/1724] loss: 0.654, ave_loss: 0.640
[43]  [840/1724] loss: 0.596, ave_loss: 0.639
[44]  [860/1724] loss: 0.579, ave_loss: 0.638
[45]  [880/1724] loss: 0.634, ave_loss: 0.638
[46]  [900/1724] loss: 0.575, ave_loss: 0.636
[47]  [920/1724] loss: 0.610, ave_loss: 0.636
[48]  [940/1724] loss: 0.664, ave_loss: 0.636
[49]  [960/1724] loss: 0.783, ave_loss: 0.639
[50]  [980/1724] loss: 0.545, ave_loss: 0.637
[51]  [1000/1724] loss: 0.606, ave_loss: 0.637
[52]  [1020/1724] loss: 0.659, ave_loss: 0.637
[53]  [1040/1724] loss: 0.515, ave_loss: 0.635
[54]  [1060/1724] loss: 0.622, ave_loss: 0.635
[55]  [1080/1724] loss: 0.641, ave_loss: 0.635
[56]  [1100/1724] loss: 0.589, ave_loss: 0.634
[57]  [1120/1724] loss: 0.727, ave_loss: 0.636
[58]  [1140/1724] loss: 0.662, ave_loss: 0.636
[59]  [1160/1724] loss: 0.733, ave_loss: 0.638
[60]  [1180/1724] loss: 0.665, ave_loss: 0.638
[61]  [1200/1724] loss: 0.725, ave_loss: 0.640
[62]  [1220/1724] loss: 0.634, ave_loss: 0.640
[63]  [1240/1724] loss: 0.589, ave_loss: 0.639
[64]  [1260/1724] loss: 0.497, ave_loss: 0.637
[65]  [1280/1724] loss: 0.723, ave_loss: 0.638
[66]  [1300/1724] loss: 0.521, ave_loss: 0.636
[67]  [1320/1724] loss: 0.725, ave_loss: 0.637
[68]  [1340/1724] loss: 0.478, ave_loss: 0.635
[69]  [1360/1724] loss: 0.603, ave_loss: 0.635
[70]  [1380/1724] loss: 0.642, ave_loss: 0.635
[71]  [1400/1724] loss: 0.549, ave_loss: 0.634
[72]  [1420/1724] loss: 0.651, ave_loss: 0.634
[73]  [1440/1724] loss: 0.630, ave_loss: 0.634
[74]  [1460/1724] loss: 0.648, ave_loss: 0.634
[75]  [1480/1724] loss: 0.650, ave_loss: 0.634
[76]  [1500/1724] loss: 0.530, ave_loss: 0.633
[77]  [1520/1724] loss: 0.656, ave_loss: 0.633
[78]  [1540/1724] loss: 0.520, ave_loss: 0.632
[79]  [1560/1724] loss: 0.568, ave_loss: 0.631
[80]  [1580/1724] loss: 0.574, ave_loss: 0.630
[81]  [1600/1724] loss: 0.596, ave_loss: 0.630
[82]  [1620/1724] loss: 0.602, ave_loss: 0.629
[83]  [1640/1724] loss: 0.607, ave_loss: 0.629
[84]  [1660/1724] loss: 0.695, ave_loss: 0.630
[85]  [1680/1724] loss: 0.589, ave_loss: 0.629
[86]  [1700/1724] loss: 0.545, ave_loss: 0.628
[87]  [1720/1724] loss: 0.608, ave_loss: 0.628
[88]  [1740/1724] loss: 0.558, ave_loss: 0.627

Finished Training finishing at 2021-08-25 01:30:00.529216
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.274e-01
Validation Loss: 5.159e-01
Validation ROC: 0.5464
Saving model
90.83294663573086 epochs left to go

Training Epoch 8.167053364269142/100 starting at 2021-08-25 01:41:59.559879
[1]  [0/1724] loss: 0.514, ave_loss: 0.514
[2]  [20/1724] loss: 0.683, ave_loss: 0.598
[3]  [40/1724] loss: 0.690, ave_loss: 0.629
[4]  [60/1724] loss: 0.611, ave_loss: 0.625
[5]  [80/1724] loss: 0.669, ave_loss: 0.634
[6]  [100/1724] loss: 0.652, ave_loss: 0.637
[7]  [120/1724] loss: 0.490, ave_loss: 0.616
[8]  [140/1724] loss: 0.699, ave_loss: 0.626
[9]  [160/1724] loss: 0.770, ave_loss: 0.642
[10]  [180/1724] loss: 0.610, ave_loss: 0.639
[11]  [200/1724] loss: 0.672, ave_loss: 0.642
[12]  [220/1724] loss: 0.587, ave_loss: 0.637
[13]  [240/1724] loss: 0.665, ave_loss: 0.639
[14]  [260/1724] loss: 0.588, ave_loss: 0.636
[15]  [280/1724] loss: 0.632, ave_loss: 0.636
[16]  [300/1724] loss: 0.677, ave_loss: 0.638
[17]  [320/1724] loss: 0.653, ave_loss: 0.639
[18]  [340/1724] loss: 0.786, ave_loss: 0.647
[19]  [360/1724] loss: 0.472, ave_loss: 0.638
[20]  [380/1724] loss: 0.695, ave_loss: 0.641
[21]  [400/1724] loss: 0.604, ave_loss: 0.639
[22]  [420/1724] loss: 0.607, ave_loss: 0.638
[23]  [440/1724] loss: 0.657, ave_loss: 0.638
[24]  [460/1724] loss: 0.700, ave_loss: 0.641
[25]  [480/1724] loss: 0.649, ave_loss: 0.641
[26]  [500/1724] loss: 0.512, ave_loss: 0.636
[27]  [520/1724] loss: 0.654, ave_loss: 0.637
[28]  [540/1724] loss: 0.532, ave_loss: 0.633
[29]  [560/1724] loss: 0.619, ave_loss: 0.633
[30]  [580/1724] loss: 0.561, ave_loss: 0.630
[31]  [600/1724] loss: 0.766, ave_loss: 0.635
[32]  [620/1724] loss: 0.652, ave_loss: 0.635
[33]  [640/1724] loss: 0.551, ave_loss: 0.633
[34]  [660/1724] loss: 0.595, ave_loss: 0.632
[35]  [680/1724] loss: 0.667, ave_loss: 0.633
[36]  [700/1724] loss: 0.688, ave_loss: 0.634
[37]  [720/1724] loss: 0.688, ave_loss: 0.636
[38]  [740/1724] loss: 0.686, ave_loss: 0.637
[39]  [760/1724] loss: 0.602, ave_loss: 0.636
[40]  [780/1724] loss: 0.600, ave_loss: 0.635
[41]  [800/1724] loss: 0.579, ave_loss: 0.634
[42]  [820/1724] loss: 0.618, ave_loss: 0.633
[43]  [840/1724] loss: 0.667, ave_loss: 0.634
[44]  [860/1724] loss: 0.631, ave_loss: 0.634
[45]  [880/1724] loss: 0.577, ave_loss: 0.633
[46]  [900/1724] loss: 0.570, ave_loss: 0.631
[47]  [920/1724] loss: 0.538, ave_loss: 0.629
[48]  [940/1724] loss: 0.650, ave_loss: 0.630
[49]  [960/1724] loss: 0.568, ave_loss: 0.629
[50]  [980/1724] loss: 0.492, ave_loss: 0.626
[51]  [1000/1724] loss: 0.633, ave_loss: 0.626
[52]  [1020/1724] loss: 0.593, ave_loss: 0.625
[53]  [1040/1724] loss: 0.682, ave_loss: 0.627
[54]  [1060/1724] loss: 0.652, ave_loss: 0.627
[55]  [1080/1724] loss: 0.456, ave_loss: 0.624
[56]  [1100/1724] loss: 0.608, ave_loss: 0.624
[57]  [1120/1724] loss: 0.613, ave_loss: 0.623
[58]  [1140/1724] loss: 0.556, ave_loss: 0.622
[59]  [1160/1724] loss: 0.837, ave_loss: 0.626
[60]  [1180/1724] loss: 0.677, ave_loss: 0.627
[61]  [1200/1724] loss: 0.728, ave_loss: 0.628
[62]  [1220/1724] loss: 0.786, ave_loss: 0.631
[63]  [1240/1724] loss: 0.640, ave_loss: 0.631
[64]  [1260/1724] loss: 0.458, ave_loss: 0.628
[65]  [1280/1724] loss: 0.640, ave_loss: 0.629
[66]  [1300/1724] loss: 0.667, ave_loss: 0.629
[67]  [1320/1724] loss: 0.715, ave_loss: 0.630
[68]  [1340/1724] loss: 0.546, ave_loss: 0.629
[69]  [1360/1724] loss: 0.490, ave_loss: 0.627
[70]  [1380/1724] loss: 0.523, ave_loss: 0.626
[71]  [1400/1724] loss: 0.602, ave_loss: 0.625
[72]  [1420/1724] loss: 0.543, ave_loss: 0.624
[73]  [1440/1724] loss: 0.618, ave_loss: 0.624
[74]  [1460/1724] loss: 0.598, ave_loss: 0.624
[75]  [1480/1724] loss: 0.548, ave_loss: 0.623
[76]  [1500/1724] loss: 0.691, ave_loss: 0.624
[77]  [1520/1724] loss: 0.526, ave_loss: 0.622
[78]  [1540/1724] loss: 0.695, ave_loss: 0.623
[79]  [1560/1724] loss: 0.828, ave_loss: 0.626
[80]  [1580/1724] loss: 0.519, ave_loss: 0.625
[81]  [1600/1724] loss: 0.512, ave_loss: 0.623
[82]  [1620/1724] loss: 0.475, ave_loss: 0.621
[83]  [1640/1724] loss: 0.709, ave_loss: 0.622
[84]  [1660/1724] loss: 0.623, ave_loss: 0.622
[85]  [1680/1724] loss: 0.522, ave_loss: 0.621
[86]  [1700/1724] loss: 0.550, ave_loss: 0.620
[87]  [1720/1724] loss: 0.739, ave_loss: 0.622
[88]  [1740/1724] loss: 0.544, ave_loss: 0.621

Finished Training finishing at 2021-08-25 01:56:26.723953
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.209e-01
Validation Loss: 5.245e-01
Validation ROC: 0.5740
Saving model
89.81206496519721 epochs left to go

Training Epoch 9.187935034802784/100 starting at 2021-08-25 02:08:25.721028
[1]  [0/1724] loss: 0.699, ave_loss: 0.699
[2]  [20/1724] loss: 0.624, ave_loss: 0.661
[3]  [40/1724] loss: 0.549, ave_loss: 0.624
[4]  [60/1724] loss: 0.586, ave_loss: 0.614
[5]  [80/1724] loss: 0.671, ave_loss: 0.626
[6]  [100/1724] loss: 0.675, ave_loss: 0.634
[7]  [120/1724] loss: 0.725, ave_loss: 0.647
[8]  [140/1724] loss: 0.663, ave_loss: 0.649
[9]  [160/1724] loss: 0.528, ave_loss: 0.635
[10]  [180/1724] loss: 0.604, ave_loss: 0.632
[11]  [200/1724] loss: 0.592, ave_loss: 0.629
[12]  [220/1724] loss: 0.631, ave_loss: 0.629
[13]  [240/1724] loss: 0.636, ave_loss: 0.629
[14]  [260/1724] loss: 0.733, ave_loss: 0.637
[15]  [280/1724] loss: 0.749, ave_loss: 0.644
[16]  [300/1724] loss: 0.608, ave_loss: 0.642
[17]  [320/1724] loss: 0.589, ave_loss: 0.639
[18]  [340/1724] loss: 0.623, ave_loss: 0.638
[19]  [360/1724] loss: 0.549, ave_loss: 0.633
[20]  [380/1724] loss: 0.607, ave_loss: 0.632
[21]  [400/1724] loss: 0.596, ave_loss: 0.630
[22]  [420/1724] loss: 0.542, ave_loss: 0.626
[23]  [440/1724] loss: 0.602, ave_loss: 0.625
[24]  [460/1724] loss: 0.618, ave_loss: 0.625
[25]  [480/1724] loss: 0.579, ave_loss: 0.623
[26]  [500/1724] loss: 0.570, ave_loss: 0.621
[27]  [520/1724] loss: 0.615, ave_loss: 0.621
[28]  [540/1724] loss: 0.570, ave_loss: 0.619
[29]  [560/1724] loss: 0.557, ave_loss: 0.617
[30]  [580/1724] loss: 0.665, ave_loss: 0.618
[31]  [600/1724] loss: 0.550, ave_loss: 0.616
[32]  [620/1724] loss: 0.736, ave_loss: 0.620
[33]  [640/1724] loss: 0.724, ave_loss: 0.623
[34]  [660/1724] loss: 0.646, ave_loss: 0.624
[35]  [680/1724] loss: 0.631, ave_loss: 0.624
[36]  [700/1724] loss: 0.566, ave_loss: 0.622
[37]  [720/1724] loss: 0.510, ave_loss: 0.619
[38]  [740/1724] loss: 0.698, ave_loss: 0.621
[39]  [760/1724] loss: 0.482, ave_loss: 0.618
[40]  [780/1724] loss: 0.644, ave_loss: 0.619
[41]  [800/1724] loss: 0.601, ave_loss: 0.618
[42]  [820/1724] loss: 0.555, ave_loss: 0.617
[43]  [840/1724] loss: 0.587, ave_loss: 0.616
[44]  [860/1724] loss: 0.554, ave_loss: 0.615
[45]  [880/1724] loss: 0.579, ave_loss: 0.614
[46]  [900/1724] loss: 0.689, ave_loss: 0.615
[47]  [920/1724] loss: 0.649, ave_loss: 0.616
[48]  [940/1724] loss: 0.678, ave_loss: 0.617
[49]  [960/1724] loss: 0.638, ave_loss: 0.618
[50]  [980/1724] loss: 0.618, ave_loss: 0.618
[51]  [1000/1724] loss: 0.734, ave_loss: 0.620
[52]  [1020/1724] loss: 0.657, ave_loss: 0.621
[53]  [1040/1724] loss: 0.637, ave_loss: 0.621
[54]  [1060/1724] loss: 0.684, ave_loss: 0.622
[55]  [1080/1724] loss: 0.491, ave_loss: 0.620
[56]  [1100/1724] loss: 0.619, ave_loss: 0.620
[57]  [1120/1724] loss: 0.538, ave_loss: 0.618
[58]  [1140/1724] loss: 0.573, ave_loss: 0.618
[59]  [1160/1724] loss: 0.705, ave_loss: 0.619
[60]  [1180/1724] loss: 0.651, ave_loss: 0.620
[61]  [1200/1724] loss: 0.684, ave_loss: 0.621
[62]  [1220/1724] loss: 0.606, ave_loss: 0.620
[63]  [1240/1724] loss: 0.697, ave_loss: 0.622
[64]  [1260/1724] loss: 0.615, ave_loss: 0.622
[65]  [1280/1724] loss: 0.761, ave_loss: 0.624
[66]  [1300/1724] loss: 0.700, ave_loss: 0.625
[67]  [1320/1724] loss: 0.655, ave_loss: 0.625
[68]  [1340/1724] loss: 0.719, ave_loss: 0.627
[69]  [1360/1724] loss: 0.577, ave_loss: 0.626
[70]  [1380/1724] loss: 0.608, ave_loss: 0.626
[71]  [1400/1724] loss: 0.624, ave_loss: 0.626
[72]  [1420/1724] loss: 0.623, ave_loss: 0.626
[73]  [1440/1724] loss: 0.680, ave_loss: 0.626
[74]  [1460/1724] loss: 0.593, ave_loss: 0.626
[75]  [1480/1724] loss: 0.583, ave_loss: 0.625
[76]  [1500/1724] loss: 0.517, ave_loss: 0.624
[77]  [1520/1724] loss: 0.636, ave_loss: 0.624
[78]  [1540/1724] loss: 0.667, ave_loss: 0.625
[79]  [1560/1724] loss: 0.623, ave_loss: 0.625
[80]  [1580/1724] loss: 0.717, ave_loss: 0.626
[81]  [1600/1724] loss: 0.675, ave_loss: 0.626
[82]  [1620/1724] loss: 0.563, ave_loss: 0.626
[83]  [1640/1724] loss: 0.566, ave_loss: 0.625
[84]  [1660/1724] loss: 0.598, ave_loss: 0.625
[85]  [1680/1724] loss: 0.695, ave_loss: 0.625
[86]  [1700/1724] loss: 0.593, ave_loss: 0.625
[87]  [1720/1724] loss: 0.681, ave_loss: 0.626
[88]  [1740/1724] loss: 0.646, ave_loss: 0.626

Finished Training finishing at 2021-08-25 02:22:53.116166
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.259e-01
Validation Loss: 5.487e-01
Validation ROC: 0.5994
Saving model
88.79118329466357 epochs left to go

Training Epoch 10.208816705336426/100 starting at 2021-08-25 02:34:47.627784
[1]  [0/1724] loss: 0.749, ave_loss: 0.749
[2]  [20/1724] loss: 0.700, ave_loss: 0.725
[3]  [40/1724] loss: 0.519, ave_loss: 0.656
[4]  [60/1724] loss: 0.662, ave_loss: 0.657
[5]  [80/1724] loss: 0.627, ave_loss: 0.651
[6]  [100/1724] loss: 0.598, ave_loss: 0.643
[7]  [120/1724] loss: 0.597, ave_loss: 0.636
[8]  [140/1724] loss: 0.506, ave_loss: 0.620
[9]  [160/1724] loss: 0.655, ave_loss: 0.624
[10]  [180/1724] loss: 0.606, ave_loss: 0.622
[11]  [200/1724] loss: 0.592, ave_loss: 0.619
[12]  [220/1724] loss: 0.632, ave_loss: 0.620
[13]  [240/1724] loss: 0.621, ave_loss: 0.620
[14]  [260/1724] loss: 0.597, ave_loss: 0.619
[15]  [280/1724] loss: 0.532, ave_loss: 0.613
[16]  [300/1724] loss: 0.546, ave_loss: 0.609
[17]  [320/1724] loss: 0.679, ave_loss: 0.613
[18]  [340/1724] loss: 0.752, ave_loss: 0.621
[19]  [360/1724] loss: 0.525, ave_loss: 0.616
[20]  [380/1724] loss: 0.528, ave_loss: 0.611
[21]  [400/1724] loss: 0.677, ave_loss: 0.614
[22]  [420/1724] loss: 0.483, ave_loss: 0.608
[23]  [440/1724] loss: 0.594, ave_loss: 0.608
[24]  [460/1724] loss: 0.635, ave_loss: 0.609
[25]  [480/1724] loss: 0.540, ave_loss: 0.606
[26]  [500/1724] loss: 0.652, ave_loss: 0.608
[27]  [520/1724] loss: 0.707, ave_loss: 0.612
[28]  [540/1724] loss: 0.631, ave_loss: 0.612
[29]  [560/1724] loss: 0.536, ave_loss: 0.610
[30]  [580/1724] loss: 0.548, ave_loss: 0.608
[31]  [600/1724] loss: 0.578, ave_loss: 0.607
[32]  [620/1724] loss: 0.599, ave_loss: 0.606
[33]  [640/1724] loss: 0.509, ave_loss: 0.603
[34]  [660/1724] loss: 0.651, ave_loss: 0.605
[35]  [680/1724] loss: 0.632, ave_loss: 0.606
[36]  [700/1724] loss: 0.565, ave_loss: 0.604
[37]  [720/1724] loss: 0.591, ave_loss: 0.604
[38]  [740/1724] loss: 0.596, ave_loss: 0.604
[39]  [760/1724] loss: 0.507, ave_loss: 0.601
[40]  [780/1724] loss: 0.650, ave_loss: 0.603
[41]  [800/1724] loss: 0.712, ave_loss: 0.605
[42]  [820/1724] loss: 0.579, ave_loss: 0.605
[43]  [840/1724] loss: 0.513, ave_loss: 0.602
[44]  [860/1724] loss: 0.466, ave_loss: 0.599
[45]  [880/1724] loss: 0.620, ave_loss: 0.600
[46]  [900/1724] loss: 0.482, ave_loss: 0.597
[47]  [920/1724] loss: 0.481, ave_loss: 0.595
[48]  [940/1724] loss: 0.702, ave_loss: 0.597
[49]  [960/1724] loss: 0.621, ave_loss: 0.598
[50]  [980/1724] loss: 0.557, ave_loss: 0.597
[51]  [1000/1724] loss: 0.502, ave_loss: 0.595
[52]  [1020/1724] loss: 0.772, ave_loss: 0.598
[53]  [1040/1724] loss: 0.661, ave_loss: 0.599
[54]  [1060/1724] loss: 0.653, ave_loss: 0.600
[55]  [1080/1724] loss: 0.615, ave_loss: 0.601
[56]  [1100/1724] loss: 0.527, ave_loss: 0.599
[57]  [1120/1724] loss: 0.615, ave_loss: 0.600
[58]  [1140/1724] loss: 0.783, ave_loss: 0.603
[59]  [1160/1724] loss: 0.737, ave_loss: 0.605
[60]  [1180/1724] loss: 0.601, ave_loss: 0.605
[61]  [1200/1724] loss: 0.620, ave_loss: 0.605
[62]  [1220/1724] loss: 0.625, ave_loss: 0.606
[63]  [1240/1724] loss: 0.619, ave_loss: 0.606
[64]  [1260/1724] loss: 0.631, ave_loss: 0.606
[65]  [1280/1724] loss: 0.692, ave_loss: 0.607
[66]  [1300/1724] loss: 0.571, ave_loss: 0.607
[67]  [1320/1724] loss: 0.587, ave_loss: 0.607
[68]  [1340/1724] loss: 0.549, ave_loss: 0.606
[69]  [1360/1724] loss: 0.637, ave_loss: 0.606
[70]  [1380/1724] loss: 0.568, ave_loss: 0.606
[71]  [1400/1724] loss: 0.583, ave_loss: 0.605
[72]  [1420/1724] loss: 0.654, ave_loss: 0.606
[73]  [1440/1724] loss: 0.562, ave_loss: 0.605
[74]  [1460/1724] loss: 0.541, ave_loss: 0.605
[75]  [1480/1724] loss: 0.573, ave_loss: 0.604
[76]  [1500/1724] loss: 0.627, ave_loss: 0.604
[77]  [1520/1724] loss: 0.547, ave_loss: 0.604
[78]  [1540/1724] loss: 0.543, ave_loss: 0.603
[79]  [1560/1724] loss: 0.573, ave_loss: 0.603
[80]  [1580/1724] loss: 0.581, ave_loss: 0.602
[81]  [1600/1724] loss: 0.578, ave_loss: 0.602
[82]  [1620/1724] loss: 0.525, ave_loss: 0.601
[83]  [1640/1724] loss: 0.490, ave_loss: 0.600
[84]  [1660/1724] loss: 0.538, ave_loss: 0.599
[85]  [1680/1724] loss: 0.523, ave_loss: 0.598
[86]  [1700/1724] loss: 0.562, ave_loss: 0.598
[87]  [1720/1724] loss: 0.789, ave_loss: 0.600
[88]  [1740/1724] loss: 0.429, ave_loss: 0.598

Finished Training finishing at 2021-08-25 02:49:14.209825
printing_out epoch  11.22969837587007 learning rate: 0.0005153561248318907
0.00036863495918228726
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.979e-01
Validation Loss: 5.279e-01
Validation ROC: 0.6094
Saving model
87.77030162412993 epochs left to go

Training Epoch 11.22969837587007/100 starting at 2021-08-25 03:01:20.382158
[1]  [0/1724] loss: 0.673, ave_loss: 0.673
[2]  [20/1724] loss: 0.636, ave_loss: 0.654
[3]  [40/1724] loss: 0.483, ave_loss: 0.597
[4]  [60/1724] loss: 0.636, ave_loss: 0.607
[5]  [80/1724] loss: 0.569, ave_loss: 0.599
[6]  [100/1724] loss: 0.692, ave_loss: 0.615
[7]  [120/1724] loss: 0.613, ave_loss: 0.615
[8]  [140/1724] loss: 0.695, ave_loss: 0.625
[9]  [160/1724] loss: 0.521, ave_loss: 0.613
[10]  [180/1724] loss: 0.732, ave_loss: 0.625
[11]  [200/1724] loss: 0.540, ave_loss: 0.617
[12]  [220/1724] loss: 0.635, ave_loss: 0.619
[13]  [240/1724] loss: 0.612, ave_loss: 0.618
[14]  [260/1724] loss: 0.646, ave_loss: 0.620
[15]  [280/1724] loss: 0.672, ave_loss: 0.624
[16]  [300/1724] loss: 0.573, ave_loss: 0.620
[17]  [320/1724] loss: 0.779, ave_loss: 0.630
[18]  [340/1724] loss: 0.590, ave_loss: 0.628
[19]  [360/1724] loss: 0.553, ave_loss: 0.624
[20]  [380/1724] loss: 0.630, ave_loss: 0.624
[21]  [400/1724] loss: 0.618, ave_loss: 0.624
[22]  [420/1724] loss: 0.436, ave_loss: 0.615
[23]  [440/1724] loss: 0.539, ave_loss: 0.612
[24]  [460/1724] loss: 0.566, ave_loss: 0.610
[25]  [480/1724] loss: 0.570, ave_loss: 0.608
[26]  [500/1724] loss: 0.715, ave_loss: 0.612
[27]  [520/1724] loss: 0.538, ave_loss: 0.610
[28]  [540/1724] loss: 0.740, ave_loss: 0.614
[29]  [560/1724] loss: 0.615, ave_loss: 0.614
[30]  [580/1724] loss: 0.722, ave_loss: 0.618
[31]  [600/1724] loss: 0.659, ave_loss: 0.619
[32]  [620/1724] loss: 0.560, ave_loss: 0.617
[33]  [640/1724] loss: 0.649, ave_loss: 0.618
[34]  [660/1724] loss: 0.588, ave_loss: 0.617
[35]  [680/1724] loss: 0.563, ave_loss: 0.616
[36]  [700/1724] loss: 0.613, ave_loss: 0.616
[37]  [720/1724] loss: 0.572, ave_loss: 0.615
[38]  [740/1724] loss: 0.630, ave_loss: 0.615
[39]  [760/1724] loss: 0.638, ave_loss: 0.616
[40]  [780/1724] loss: 0.558, ave_loss: 0.614
[41]  [800/1724] loss: 0.494, ave_loss: 0.611
[42]  [820/1724] loss: 0.562, ave_loss: 0.610
[43]  [840/1724] loss: 0.753, ave_loss: 0.613
[44]  [860/1724] loss: 0.721, ave_loss: 0.616
[45]  [880/1724] loss: 0.597, ave_loss: 0.615
[46]  [900/1724] loss: 0.605, ave_loss: 0.615
[47]  [920/1724] loss: 0.683, ave_loss: 0.617
[48]  [940/1724] loss: 0.544, ave_loss: 0.615
[49]  [960/1724] loss: 0.603, ave_loss: 0.615
[50]  [980/1724] loss: 0.617, ave_loss: 0.615
[51]  [1000/1724] loss: 0.558, ave_loss: 0.614
[52]  [1020/1724] loss: 0.567, ave_loss: 0.613
[53]  [1040/1724] loss: 0.542, ave_loss: 0.612
[54]  [1060/1724] loss: 0.673, ave_loss: 0.613
[55]  [1080/1724] loss: 0.733, ave_loss: 0.615
[56]  [1100/1724] loss: 0.520, ave_loss: 0.613
[57]  [1120/1724] loss: 0.609, ave_loss: 0.613
[58]  [1140/1724] loss: 0.624, ave_loss: 0.613
[59]  [1160/1724] loss: 0.518, ave_loss: 0.612
[60]  [1180/1724] loss: 0.585, ave_loss: 0.611
[61]  [1200/1724] loss: 0.556, ave_loss: 0.610
[62]  [1220/1724] loss: 0.603, ave_loss: 0.610
[63]  [1240/1724] loss: 0.686, ave_loss: 0.611
[64]  [1260/1724] loss: 0.747, ave_loss: 0.614
[65]  [1280/1724] loss: 0.709, ave_loss: 0.615
[66]  [1300/1724] loss: 0.579, ave_loss: 0.614
[67]  [1320/1724] loss: 0.660, ave_loss: 0.615
[68]  [1340/1724] loss: 0.524, ave_loss: 0.614
[69]  [1360/1724] loss: 0.555, ave_loss: 0.613
[70]  [1380/1724] loss: 0.552, ave_loss: 0.612
[71]  [1400/1724] loss: 0.575, ave_loss: 0.612
[72]  [1420/1724] loss: 0.529, ave_loss: 0.610
[73]  [1440/1724] loss: 0.532, ave_loss: 0.609
[74]  [1460/1724] loss: 0.467, ave_loss: 0.607
[75]  [1480/1724] loss: 0.531, ave_loss: 0.606
[76]  [1500/1724] loss: 0.617, ave_loss: 0.607
[77]  [1520/1724] loss: 0.594, ave_loss: 0.606
[78]  [1540/1724] loss: 0.492, ave_loss: 0.605
[79]  [1560/1724] loss: 0.548, ave_loss: 0.604
[80]  [1580/1724] loss: 0.648, ave_loss: 0.605
[81]  [1600/1724] loss: 0.606, ave_loss: 0.605
[82]  [1620/1724] loss: 0.628, ave_loss: 0.605
[83]  [1640/1724] loss: 0.529, ave_loss: 0.604
[84]  [1660/1724] loss: 0.731, ave_loss: 0.606
[85]  [1680/1724] loss: 0.537, ave_loss: 0.605
[86]  [1700/1724] loss: 0.796, ave_loss: 0.607
[87]  [1720/1724] loss: 0.681, ave_loss: 0.608
[88]  [1740/1724] loss: 0.743, ave_loss: 0.609

Finished Training finishing at 2021-08-25 03:15:45.123835
printing_out epoch  12.250580046403712 learning rate: 0.0005153561248318907
0.0003575759104068186
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.094e-01
Validation Loss: 5.275e-01
Validation ROC: 0.6284
Saving model
86.74941995359629 epochs left to go

Training Epoch 12.250580046403712/100 starting at 2021-08-25 03:27:44.138206
[1]  [0/1724] loss: 0.482, ave_loss: 0.482
[2]  [20/1724] loss: 0.472, ave_loss: 0.477
[3]  [40/1724] loss: 0.794, ave_loss: 0.583
[4]  [60/1724] loss: 0.651, ave_loss: 0.600
[5]  [80/1724] loss: 0.510, ave_loss: 0.582
[6]  [100/1724] loss: 0.597, ave_loss: 0.584
[7]  [120/1724] loss: 0.577, ave_loss: 0.583
[8]  [140/1724] loss: 0.536, ave_loss: 0.577
[9]  [160/1724] loss: 0.619, ave_loss: 0.582
[10]  [180/1724] loss: 0.756, ave_loss: 0.599
[11]  [200/1724] loss: 0.556, ave_loss: 0.595
[12]  [220/1724] loss: 0.598, ave_loss: 0.596
[13]  [240/1724] loss: 0.574, ave_loss: 0.594
[14]  [260/1724] loss: 0.537, ave_loss: 0.590
[15]  [280/1724] loss: 0.600, ave_loss: 0.591
[16]  [300/1724] loss: 0.566, ave_loss: 0.589
[17]  [320/1724] loss: 0.590, ave_loss: 0.589
[18]  [340/1724] loss: 0.636, ave_loss: 0.592
[19]  [360/1724] loss: 0.883, ave_loss: 0.607
[20]  [380/1724] loss: 0.590, ave_loss: 0.606
[21]  [400/1724] loss: 0.634, ave_loss: 0.608
[22]  [420/1724] loss: 0.504, ave_loss: 0.603
[23]  [440/1724] loss: 0.536, ave_loss: 0.600
[24]  [460/1724] loss: 0.664, ave_loss: 0.603
[25]  [480/1724] loss: 0.506, ave_loss: 0.599
[26]  [500/1724] loss: 0.671, ave_loss: 0.601
[27]  [520/1724] loss: 0.698, ave_loss: 0.605
[28]  [540/1724] loss: 0.443, ave_loss: 0.599
[29]  [560/1724] loss: 0.561, ave_loss: 0.598
[30]  [580/1724] loss: 0.667, ave_loss: 0.600
[31]  [600/1724] loss: 0.553, ave_loss: 0.599
[32]  [620/1724] loss: 0.552, ave_loss: 0.597
[33]  [640/1724] loss: 0.564, ave_loss: 0.596
[34]  [660/1724] loss: 0.492, ave_loss: 0.593
[35]  [680/1724] loss: 0.522, ave_loss: 0.591
[36]  [700/1724] loss: 0.647, ave_loss: 0.593
[37]  [720/1724] loss: 0.502, ave_loss: 0.590
[38]  [740/1724] loss: 0.760, ave_loss: 0.595
[39]  [760/1724] loss: 0.638, ave_loss: 0.596
[40]  [780/1724] loss: 0.612, ave_loss: 0.596
[41]  [800/1724] loss: 0.479, ave_loss: 0.593
[42]  [820/1724] loss: 0.597, ave_loss: 0.593
[43]  [840/1724] loss: 0.502, ave_loss: 0.591
[44]  [860/1724] loss: 0.724, ave_loss: 0.594
[45]  [880/1724] loss: 0.514, ave_loss: 0.593
[46]  [900/1724] loss: 0.457, ave_loss: 0.590
[47]  [920/1724] loss: 0.607, ave_loss: 0.590
[48]  [940/1724] loss: 0.582, ave_loss: 0.590
[49]  [960/1724] loss: 0.590, ave_loss: 0.590
[50]  [980/1724] loss: 0.573, ave_loss: 0.590
[51]  [1000/1724] loss: 0.463, ave_loss: 0.587
[52]  [1020/1724] loss: 0.691, ave_loss: 0.589
[53]  [1040/1724] loss: 0.653, ave_loss: 0.590
[54]  [1060/1724] loss: 0.744, ave_loss: 0.593
[55]  [1080/1724] loss: 0.664, ave_loss: 0.594
[56]  [1100/1724] loss: 0.551, ave_loss: 0.594
[57]  [1120/1724] loss: 0.574, ave_loss: 0.593
[58]  [1140/1724] loss: 0.678, ave_loss: 0.595
[59]  [1160/1724] loss: 0.550, ave_loss: 0.594
[60]  [1180/1724] loss: 0.458, ave_loss: 0.592
[61]  [1200/1724] loss: 0.589, ave_loss: 0.592
[62]  [1220/1724] loss: 0.562, ave_loss: 0.591
[63]  [1240/1724] loss: 0.688, ave_loss: 0.593
[64]  [1260/1724] loss: 0.534, ave_loss: 0.592
[65]  [1280/1724] loss: 0.532, ave_loss: 0.591
[66]  [1300/1724] loss: 0.620, ave_loss: 0.591
[67]  [1320/1724] loss: 0.627, ave_loss: 0.592
[68]  [1340/1724] loss: 0.542, ave_loss: 0.591
[69]  [1360/1724] loss: 0.705, ave_loss: 0.593
[70]  [1380/1724] loss: 0.587, ave_loss: 0.593
[71]  [1400/1724] loss: 0.724, ave_loss: 0.595
[72]  [1420/1724] loss: 0.698, ave_loss: 0.596
[73]  [1440/1724] loss: 0.583, ave_loss: 0.596
[74]  [1460/1724] loss: 0.621, ave_loss: 0.596
[75]  [1480/1724] loss: 0.818, ave_loss: 0.599
[76]  [1500/1724] loss: 0.683, ave_loss: 0.600
[77]  [1520/1724] loss: 0.575, ave_loss: 0.600
[78]  [1540/1724] loss: 0.674, ave_loss: 0.601
[79]  [1560/1724] loss: 0.592, ave_loss: 0.601
[80]  [1580/1724] loss: 0.508, ave_loss: 0.600
[81]  [1600/1724] loss: 0.657, ave_loss: 0.600
[82]  [1620/1724] loss: 0.676, ave_loss: 0.601
[83]  [1640/1724] loss: 0.614, ave_loss: 0.601
[84]  [1660/1724] loss: 0.603, ave_loss: 0.601
[85]  [1680/1724] loss: 0.574, ave_loss: 0.601
[86]  [1700/1724] loss: 0.669, ave_loss: 0.602
[87]  [1720/1724] loss: 0.590, ave_loss: 0.602
[88]  [1740/1724] loss: 0.684, ave_loss: 0.603

Finished Training finishing at 2021-08-25 03:42:12.504960
printing_out epoch  13.271461716937354 learning rate: 0.0005153561248318907
0.00034684863309461403
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.026e-01
Validation Loss: 6.113e-01
Validation ROC: 0.6515
Saving model
85.72853828306265 epochs left to go

Training Epoch 13.271461716937354/100 starting at 2021-08-25 03:54:09.158905
[1]  [0/1724] loss: 0.615, ave_loss: 0.615
[2]  [20/1724] loss: 0.556, ave_loss: 0.586
[3]  [40/1724] loss: 0.550, ave_loss: 0.574
[4]  [60/1724] loss: 0.676, ave_loss: 0.599
[5]  [80/1724] loss: 0.595, ave_loss: 0.598
[6]  [100/1724] loss: 0.657, ave_loss: 0.608
[7]  [120/1724] loss: 0.673, ave_loss: 0.617
[8]  [140/1724] loss: 0.586, ave_loss: 0.614
[9]  [160/1724] loss: 0.581, ave_loss: 0.610
[10]  [180/1724] loss: 0.698, ave_loss: 0.619
[11]  [200/1724] loss: 0.799, ave_loss: 0.635
[12]  [220/1724] loss: 0.607, ave_loss: 0.633
[13]  [240/1724] loss: 0.480, ave_loss: 0.621
[14]  [260/1724] loss: 0.645, ave_loss: 0.623
[15]  [280/1724] loss: 0.628, ave_loss: 0.623
[16]  [300/1724] loss: 0.592, ave_loss: 0.621
[17]  [320/1724] loss: 0.627, ave_loss: 0.622
[18]  [340/1724] loss: 0.548, ave_loss: 0.617
[19]  [360/1724] loss: 0.579, ave_loss: 0.615
[20]  [380/1724] loss: 0.690, ave_loss: 0.619
[21]  [400/1724] loss: 0.576, ave_loss: 0.617
[22]  [420/1724] loss: 0.490, ave_loss: 0.611
[23]  [440/1724] loss: 0.685, ave_loss: 0.615
[24]  [460/1724] loss: 0.699, ave_loss: 0.618
[25]  [480/1724] loss: 0.589, ave_loss: 0.617
[26]  [500/1724] loss: 0.647, ave_loss: 0.618
[27]  [520/1724] loss: 0.669, ave_loss: 0.620
[28]  [540/1724] loss: 0.559, ave_loss: 0.618
[29]  [560/1724] loss: 0.506, ave_loss: 0.614
[30]  [580/1724] loss: 0.703, ave_loss: 0.617
[31]  [600/1724] loss: 0.625, ave_loss: 0.617
[32]  [620/1724] loss: 0.524, ave_loss: 0.614
[33]  [640/1724] loss: 0.497, ave_loss: 0.611
[34]  [660/1724] loss: 0.780, ave_loss: 0.616
[35]  [680/1724] loss: 0.602, ave_loss: 0.615
[36]  [700/1724] loss: 0.518, ave_loss: 0.613
[37]  [720/1724] loss: 0.580, ave_loss: 0.612
[38]  [740/1724] loss: 0.630, ave_loss: 0.612
[39]  [760/1724] loss: 0.683, ave_loss: 0.614
[40]  [780/1724] loss: 0.642, ave_loss: 0.615
[41]  [800/1724] loss: 0.676, ave_loss: 0.616
[42]  [820/1724] loss: 0.537, ave_loss: 0.614
[43]  [840/1724] loss: 0.655, ave_loss: 0.615
[44]  [860/1724] loss: 0.548, ave_loss: 0.614
[45]  [880/1724] loss: 0.664, ave_loss: 0.615
[46]  [900/1724] loss: 0.617, ave_loss: 0.615
[47]  [920/1724] loss: 0.579, ave_loss: 0.614
[48]  [940/1724] loss: 0.607, ave_loss: 0.614
[49]  [960/1724] loss: 0.603, ave_loss: 0.614
[50]  [980/1724] loss: 0.692, ave_loss: 0.615
[51]  [1000/1724] loss: 0.678, ave_loss: 0.617
[52]  [1020/1724] loss: 0.640, ave_loss: 0.617
[53]  [1040/1724] loss: 0.628, ave_loss: 0.617
[54]  [1060/1724] loss: 0.592, ave_loss: 0.617
[55]  [1080/1724] loss: 0.546, ave_loss: 0.615
[56]  [1100/1724] loss: 0.639, ave_loss: 0.616
[57]  [1120/1724] loss: 0.516, ave_loss: 0.614
[58]  [1140/1724] loss: 0.715, ave_loss: 0.616
[59]  [1160/1724] loss: 0.465, ave_loss: 0.613
[60]  [1180/1724] loss: 0.674, ave_loss: 0.614
[61]  [1200/1724] loss: 0.616, ave_loss: 0.614
[62]  [1220/1724] loss: 0.708, ave_loss: 0.616
[63]  [1240/1724] loss: 0.567, ave_loss: 0.615
[64]  [1260/1724] loss: 0.501, ave_loss: 0.613
[65]  [1280/1724] loss: 0.744, ave_loss: 0.615
[66]  [1300/1724] loss: 0.542, ave_loss: 0.614
[67]  [1320/1724] loss: 0.647, ave_loss: 0.615
[68]  [1340/1724] loss: 0.626, ave_loss: 0.615
[69]  [1360/1724] loss: 0.676, ave_loss: 0.616
[70]  [1380/1724] loss: 0.549, ave_loss: 0.615
[71]  [1400/1724] loss: 0.553, ave_loss: 0.614
[72]  [1420/1724] loss: 0.610, ave_loss: 0.614
[73]  [1440/1724] loss: 0.512, ave_loss: 0.612
[74]  [1460/1724] loss: 0.552, ave_loss: 0.612
[75]  [1480/1724] loss: 0.645, ave_loss: 0.612
[76]  [1500/1724] loss: 0.534, ave_loss: 0.611
[77]  [1520/1724] loss: 0.462, ave_loss: 0.609
[78]  [1540/1724] loss: 0.622, ave_loss: 0.609
[79]  [1560/1724] loss: 0.580, ave_loss: 0.609
[80]  [1580/1724] loss: 0.616, ave_loss: 0.609
[81]  [1600/1724] loss: 0.587, ave_loss: 0.609
[82]  [1620/1724] loss: 0.568, ave_loss: 0.608
[83]  [1640/1724] loss: 0.667, ave_loss: 0.609
[84]  [1660/1724] loss: 0.593, ave_loss: 0.609
[85]  [1680/1724] loss: 0.742, ave_loss: 0.610
[86]  [1700/1724] loss: 0.529, ave_loss: 0.609
[87]  [1720/1724] loss: 0.657, ave_loss: 0.610
[88]  [1740/1724] loss: 0.625, ave_loss: 0.610

Finished Training finishing at 2021-08-25 04:08:43.240572
printing_out epoch  14.292343387470998 learning rate: 0.0005153561248318907
0.0003364431741017756
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.100e-01
Validation Loss: 5.277e-01
Validation ROC: 0.6522
Saving model
84.70765661252901 epochs left to go

Training Epoch 14.292343387470998/100 starting at 2021-08-25 04:20:34.018055
[1]  [0/1724] loss: 0.823, ave_loss: 0.823
[2]  [20/1724] loss: 0.539, ave_loss: 0.681
[3]  [40/1724] loss: 0.590, ave_loss: 0.651
[4]  [60/1724] loss: 0.642, ave_loss: 0.649
[5]  [80/1724] loss: 0.593, ave_loss: 0.637
[6]  [100/1724] loss: 0.562, ave_loss: 0.625
[7]  [120/1724] loss: 0.589, ave_loss: 0.620
[8]  [140/1724] loss: 0.463, ave_loss: 0.600
[9]  [160/1724] loss: 0.639, ave_loss: 0.604
[10]  [180/1724] loss: 0.477, ave_loss: 0.592
[11]  [200/1724] loss: 0.561, ave_loss: 0.589
[12]  [220/1724] loss: 0.714, ave_loss: 0.599
[13]  [240/1724] loss: 0.678, ave_loss: 0.605
[14]  [260/1724] loss: 0.513, ave_loss: 0.599
[15]  [280/1724] loss: 0.533, ave_loss: 0.594
[16]  [300/1724] loss: 0.661, ave_loss: 0.599
[17]  [320/1724] loss: 0.535, ave_loss: 0.595
[18]  [340/1724] loss: 0.706, ave_loss: 0.601
[19]  [360/1724] loss: 0.646, ave_loss: 0.603
[20]  [380/1724] loss: 0.633, ave_loss: 0.605
[21]  [400/1724] loss: 0.573, ave_loss: 0.603
[22]  [420/1724] loss: 0.546, ave_loss: 0.601
[23]  [440/1724] loss: 0.546, ave_loss: 0.598
[24]  [460/1724] loss: 0.606, ave_loss: 0.599
[25]  [480/1724] loss: 0.602, ave_loss: 0.599
[26]  [500/1724] loss: 0.642, ave_loss: 0.600
[27]  [520/1724] loss: 0.533, ave_loss: 0.598
[28]  [540/1724] loss: 0.597, ave_loss: 0.598
[29]  [560/1724] loss: 0.612, ave_loss: 0.598
[30]  [580/1724] loss: 0.842, ave_loss: 0.607
[31]  [600/1724] loss: 0.502, ave_loss: 0.603
[32]  [620/1724] loss: 0.506, ave_loss: 0.600
[33]  [640/1724] loss: 0.584, ave_loss: 0.600
[34]  [660/1724] loss: 0.547, ave_loss: 0.598
[35]  [680/1724] loss: 0.698, ave_loss: 0.601
[36]  [700/1724] loss: 0.574, ave_loss: 0.600
[37]  [720/1724] loss: 0.725, ave_loss: 0.604
[38]  [740/1724] loss: 0.561, ave_loss: 0.603
[39]  [760/1724] loss: 0.721, ave_loss: 0.606
[40]  [780/1724] loss: 0.606, ave_loss: 0.606
[41]  [800/1724] loss: 0.662, ave_loss: 0.607
[42]  [820/1724] loss: 0.568, ave_loss: 0.606
[43]  [840/1724] loss: 0.557, ave_loss: 0.605
[44]  [860/1724] loss: 0.703, ave_loss: 0.607
[45]  [880/1724] loss: 0.737, ave_loss: 0.610
[46]  [900/1724] loss: 0.614, ave_loss: 0.610
[47]  [920/1724] loss: 0.663, ave_loss: 0.611
[48]  [940/1724] loss: 0.624, ave_loss: 0.611
[49]  [960/1724] loss: 0.508, ave_loss: 0.609
[50]  [980/1724] loss: 0.614, ave_loss: 0.609
[51]  [1000/1724] loss: 0.632, ave_loss: 0.610
[52]  [1020/1724] loss: 0.602, ave_loss: 0.610
[53]  [1040/1724] loss: 0.567, ave_loss: 0.609
[54]  [1060/1724] loss: 0.588, ave_loss: 0.609
[55]  [1080/1724] loss: 0.645, ave_loss: 0.609
[56]  [1100/1724] loss: 0.653, ave_loss: 0.610
[57]  [1120/1724] loss: 0.572, ave_loss: 0.609
[58]  [1140/1724] loss: 0.620, ave_loss: 0.610
[59]  [1160/1724] loss: 0.573, ave_loss: 0.609
[60]  [1180/1724] loss: 0.542, ave_loss: 0.608
[61]  [1200/1724] loss: 0.683, ave_loss: 0.609
[62]  [1220/1724] loss: 0.486, ave_loss: 0.607
[63]  [1240/1724] loss: 0.517, ave_loss: 0.606
[64]  [1260/1724] loss: 0.688, ave_loss: 0.607
[65]  [1280/1724] loss: 0.677, ave_loss: 0.608
[66]  [1300/1724] loss: 0.579, ave_loss: 0.608
[67]  [1320/1724] loss: 0.644, ave_loss: 0.608
[68]  [1340/1724] loss: 0.534, ave_loss: 0.607
[69]  [1360/1724] loss: 0.619, ave_loss: 0.607
[70]  [1380/1724] loss: 0.727, ave_loss: 0.609
[71]  [1400/1724] loss: 0.607, ave_loss: 0.609
[72]  [1420/1724] loss: 0.732, ave_loss: 0.611
[73]  [1440/1724] loss: 0.485, ave_loss: 0.609
[74]  [1460/1724] loss: 0.536, ave_loss: 0.608
[75]  [1480/1724] loss: 0.483, ave_loss: 0.606
[76]  [1500/1724] loss: 0.645, ave_loss: 0.607
[77]  [1520/1724] loss: 0.558, ave_loss: 0.606
[78]  [1540/1724] loss: 0.489, ave_loss: 0.605
[79]  [1560/1724] loss: 0.660, ave_loss: 0.605
[80]  [1580/1724] loss: 0.573, ave_loss: 0.605
[81]  [1600/1724] loss: 0.479, ave_loss: 0.603
[82]  [1620/1724] loss: 0.550, ave_loss: 0.603
[83]  [1640/1724] loss: 0.625, ave_loss: 0.603
[84]  [1660/1724] loss: 0.547, ave_loss: 0.602
[85]  [1680/1724] loss: 0.643, ave_loss: 0.603
[86]  [1700/1724] loss: 0.619, ave_loss: 0.603
[87]  [1720/1724] loss: 0.605, ave_loss: 0.603
[88]  [1740/1724] loss: 0.684, ave_loss: 0.604

Finished Training finishing at 2021-08-25 04:34:52.319666
printing_out epoch  15.31322505800464 learning rate: 0.0005153561248318907
0.0003263498788787223
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.039e-01
Validation Loss: 5.159e-01
Validation ROC: 0.6585
Saving model
83.68677494199537 epochs left to go

Training Epoch 15.31322505800464/100 starting at 2021-08-25 04:46:49.148674
[1]  [0/1724] loss: 0.569, ave_loss: 0.569
[2]  [20/1724] loss: 0.735, ave_loss: 0.652
[3]  [40/1724] loss: 0.519, ave_loss: 0.608
[4]  [60/1724] loss: 0.608, ave_loss: 0.608
[5]  [80/1724] loss: 0.625, ave_loss: 0.611
[6]  [100/1724] loss: 0.606, ave_loss: 0.610
[7]  [120/1724] loss: 0.716, ave_loss: 0.626
[8]  [140/1724] loss: 0.565, ave_loss: 0.618
[9]  [160/1724] loss: 0.560, ave_loss: 0.612
[10]  [180/1724] loss: 0.688, ave_loss: 0.619
[11]  [200/1724] loss: 0.520, ave_loss: 0.610
[12]  [220/1724] loss: 0.519, ave_loss: 0.603
[13]  [240/1724] loss: 0.621, ave_loss: 0.604
[14]  [260/1724] loss: 0.544, ave_loss: 0.600
[15]  [280/1724] loss: 0.478, ave_loss: 0.592
[16]  [300/1724] loss: 0.426, ave_loss: 0.581
[17]  [320/1724] loss: 0.538, ave_loss: 0.579
[18]  [340/1724] loss: 0.661, ave_loss: 0.583
[19]  [360/1724] loss: 0.581, ave_loss: 0.583
[20]  [380/1724] loss: 0.706, ave_loss: 0.589
[21]  [400/1724] loss: 0.495, ave_loss: 0.585
[22]  [420/1724] loss: 0.631, ave_loss: 0.587
[23]  [440/1724] loss: 0.656, ave_loss: 0.590
[24]  [460/1724] loss: 0.690, ave_loss: 0.594
[25]  [480/1724] loss: 0.639, ave_loss: 0.596
[26]  [500/1724] loss: 0.669, ave_loss: 0.599
[27]  [520/1724] loss: 0.663, ave_loss: 0.601
[28]  [540/1724] loss: 0.702, ave_loss: 0.605
[29]  [560/1724] loss: 0.707, ave_loss: 0.608
[30]  [580/1724] loss: 0.639, ave_loss: 0.609
[31]  [600/1724] loss: 0.625, ave_loss: 0.610
[32]  [620/1724] loss: 0.546, ave_loss: 0.608
[33]  [640/1724] loss: 0.495, ave_loss: 0.604
[34]  [660/1724] loss: 0.412, ave_loss: 0.599
[35]  [680/1724] loss: 0.660, ave_loss: 0.600
[36]  [700/1724] loss: 0.623, ave_loss: 0.601
[37]  [720/1724] loss: 0.532, ave_loss: 0.599
[38]  [740/1724] loss: 0.631, ave_loss: 0.600
[39]  [760/1724] loss: 0.572, ave_loss: 0.599
[40]  [780/1724] loss: 0.612, ave_loss: 0.600
[41]  [800/1724] loss: 0.498, ave_loss: 0.597
[42]  [820/1724] loss: 0.601, ave_loss: 0.597
[43]  [840/1724] loss: 0.486, ave_loss: 0.595
[44]  [860/1724] loss: 0.651, ave_loss: 0.596
[45]  [880/1724] loss: 0.596, ave_loss: 0.596
[46]  [900/1724] loss: 0.580, ave_loss: 0.596
[47]  [920/1724] loss: 0.502, ave_loss: 0.594
[48]  [940/1724] loss: 0.676, ave_loss: 0.595
[49]  [960/1724] loss: 0.494, ave_loss: 0.593
[50]  [980/1724] loss: 0.514, ave_loss: 0.592
[51]  [1000/1724] loss: 0.711, ave_loss: 0.594
[52]  [1020/1724] loss: 0.620, ave_loss: 0.594
[53]  [1040/1724] loss: 0.664, ave_loss: 0.596
[54]  [1060/1724] loss: 0.607, ave_loss: 0.596
[55]  [1080/1724] loss: 0.501, ave_loss: 0.594
[56]  [1100/1724] loss: 0.530, ave_loss: 0.593
[57]  [1120/1724] loss: 0.640, ave_loss: 0.594
[58]  [1140/1724] loss: 0.520, ave_loss: 0.593
[59]  [1160/1724] loss: 0.516, ave_loss: 0.591
[60]  [1180/1724] loss: 0.605, ave_loss: 0.592
[61]  [1200/1724] loss: 0.513, ave_loss: 0.590
[62]  [1220/1724] loss: 0.649, ave_loss: 0.591
[63]  [1240/1724] loss: 0.519, ave_loss: 0.590
[64]  [1260/1724] loss: 0.618, ave_loss: 0.591
[65]  [1280/1724] loss: 0.622, ave_loss: 0.591
[66]  [1300/1724] loss: 0.517, ave_loss: 0.590
[67]  [1320/1724] loss: 0.575, ave_loss: 0.590
[68]  [1340/1724] loss: 0.387, ave_loss: 0.587
[69]  [1360/1724] loss: 0.629, ave_loss: 0.587
[70]  [1380/1724] loss: 0.575, ave_loss: 0.587
[71]  [1400/1724] loss: 0.660, ave_loss: 0.588
[72]  [1420/1724] loss: 0.578, ave_loss: 0.588
[73]  [1440/1724] loss: 0.646, ave_loss: 0.589
[74]  [1460/1724] loss: 0.597, ave_loss: 0.589
[75]  [1480/1724] loss: 0.542, ave_loss: 0.588
[76]  [1500/1724] loss: 0.561, ave_loss: 0.588
[77]  [1520/1724] loss: 0.478, ave_loss: 0.587
[78]  [1540/1724] loss: 0.655, ave_loss: 0.587
[79]  [1560/1724] loss: 0.648, ave_loss: 0.588
[80]  [1580/1724] loss: 0.608, ave_loss: 0.588
[81]  [1600/1724] loss: 0.613, ave_loss: 0.589
[82]  [1620/1724] loss: 0.664, ave_loss: 0.590
[83]  [1640/1724] loss: 0.433, ave_loss: 0.588
[84]  [1660/1724] loss: 0.536, ave_loss: 0.587
[85]  [1680/1724] loss: 0.612, ave_loss: 0.587
[86]  [1700/1724] loss: 0.453, ave_loss: 0.586
[87]  [1720/1724] loss: 0.634, ave_loss: 0.586
[88]  [1740/1724] loss: 0.489, ave_loss: 0.585

Finished Training finishing at 2021-08-25 05:01:13.781558
printing_out epoch  16.334106728538284 learning rate: 0.0005153561248318907
0.00031655938251236066
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.853e-01
Validation Loss: 5.231e-01
Validation ROC: 0.6615
Saving model
82.66589327146171 epochs left to go

Training Epoch 16.334106728538284/100 starting at 2021-08-25 05:13:09.815256
[1]  [0/1724] loss: 0.643, ave_loss: 0.643
[2]  [20/1724] loss: 0.523, ave_loss: 0.583
[3]  [40/1724] loss: 0.651, ave_loss: 0.606
[4]  [60/1724] loss: 0.606, ave_loss: 0.606
[5]  [80/1724] loss: 0.595, ave_loss: 0.604
[6]  [100/1724] loss: 0.575, ave_loss: 0.599
[7]  [120/1724] loss: 0.417, ave_loss: 0.573
[8]  [140/1724] loss: 0.675, ave_loss: 0.586
[9]  [160/1724] loss: 0.633, ave_loss: 0.591
[10]  [180/1724] loss: 0.657, ave_loss: 0.598
[11]  [200/1724] loss: 0.614, ave_loss: 0.599
[12]  [220/1724] loss: 0.670, ave_loss: 0.605
[13]  [240/1724] loss: 0.625, ave_loss: 0.606
[14]  [260/1724] loss: 0.579, ave_loss: 0.605
[15]  [280/1724] loss: 0.640, ave_loss: 0.607
[16]  [300/1724] loss: 0.717, ave_loss: 0.614
[17]  [320/1724] loss: 0.531, ave_loss: 0.609
[18]  [340/1724] loss: 0.656, ave_loss: 0.612
[19]  [360/1724] loss: 0.533, ave_loss: 0.607
[20]  [380/1724] loss: 0.677, ave_loss: 0.611
[21]  [400/1724] loss: 0.631, ave_loss: 0.612
[22]  [420/1724] loss: 0.548, ave_loss: 0.609
[23]  [440/1724] loss: 0.673, ave_loss: 0.612
[24]  [460/1724] loss: 0.528, ave_loss: 0.608
[25]  [480/1724] loss: 0.702, ave_loss: 0.612
[26]  [500/1724] loss: 0.631, ave_loss: 0.613
[27]  [520/1724] loss: 0.649, ave_loss: 0.614
[28]  [540/1724] loss: 0.603, ave_loss: 0.614
[29]  [560/1724] loss: 0.670, ave_loss: 0.616
[30]  [580/1724] loss: 0.596, ave_loss: 0.615
[31]  [600/1724] loss: 0.576, ave_loss: 0.614
[32]  [620/1724] loss: 0.511, ave_loss: 0.611
[33]  [640/1724] loss: 0.591, ave_loss: 0.610
[34]  [660/1724] loss: 0.598, ave_loss: 0.610
[35]  [680/1724] loss: 0.689, ave_loss: 0.612
[36]  [700/1724] loss: 0.578, ave_loss: 0.611
[37]  [720/1724] loss: 0.598, ave_loss: 0.611
[38]  [740/1724] loss: 0.617, ave_loss: 0.611
[39]  [760/1724] loss: 0.539, ave_loss: 0.609
[40]  [780/1724] loss: 0.552, ave_loss: 0.608
[41]  [800/1724] loss: 0.544, ave_loss: 0.606
[42]  [820/1724] loss: 0.814, ave_loss: 0.611
[43]  [840/1724] loss: 0.611, ave_loss: 0.611
[44]  [860/1724] loss: 0.666, ave_loss: 0.612
[45]  [880/1724] loss: 0.574, ave_loss: 0.611
[46]  [900/1724] loss: 0.550, ave_loss: 0.610
[47]  [920/1724] loss: 0.627, ave_loss: 0.610
[48]  [940/1724] loss: 0.539, ave_loss: 0.609
[49]  [960/1724] loss: 0.615, ave_loss: 0.609
[50]  [980/1724] loss: 0.553, ave_loss: 0.608
[51]  [1000/1724] loss: 0.567, ave_loss: 0.607
[52]  [1020/1724] loss: 0.486, ave_loss: 0.605
[53]  [1040/1724] loss: 0.540, ave_loss: 0.604
[54]  [1060/1724] loss: 0.518, ave_loss: 0.602
[55]  [1080/1724] loss: 0.566, ave_loss: 0.601
[56]  [1100/1724] loss: 0.509, ave_loss: 0.600
[57]  [1120/1724] loss: 0.608, ave_loss: 0.600
[58]  [1140/1724] loss: 0.526, ave_loss: 0.598
[59]  [1160/1724] loss: 0.565, ave_loss: 0.598
[60]  [1180/1724] loss: 0.674, ave_loss: 0.599
[61]  [1200/1724] loss: 0.538, ave_loss: 0.598
[62]  [1220/1724] loss: 0.541, ave_loss: 0.597
[63]  [1240/1724] loss: 0.500, ave_loss: 0.596
[64]  [1260/1724] loss: 0.594, ave_loss: 0.596
[65]  [1280/1724] loss: 0.837, ave_loss: 0.599
[66]  [1300/1724] loss: 0.574, ave_loss: 0.599
[67]  [1320/1724] loss: 0.745, ave_loss: 0.601
[68]  [1340/1724] loss: 0.739, ave_loss: 0.603
[69]  [1360/1724] loss: 0.522, ave_loss: 0.602
[70]  [1380/1724] loss: 0.590, ave_loss: 0.602
[71]  [1400/1724] loss: 0.579, ave_loss: 0.602
[72]  [1420/1724] loss: 0.547, ave_loss: 0.601
[73]  [1440/1724] loss: 0.498, ave_loss: 0.599
[74]  [1460/1724] loss: 0.689, ave_loss: 0.601
[75]  [1480/1724] loss: 0.663, ave_loss: 0.601
[76]  [1500/1724] loss: 0.500, ave_loss: 0.600
[77]  [1520/1724] loss: 0.571, ave_loss: 0.600
[78]  [1540/1724] loss: 0.674, ave_loss: 0.601
[79]  [1560/1724] loss: 0.515, ave_loss: 0.600
[80]  [1580/1724] loss: 0.648, ave_loss: 0.600
[81]  [1600/1724] loss: 0.631, ave_loss: 0.601
[82]  [1620/1724] loss: 0.512, ave_loss: 0.600
[83]  [1640/1724] loss: 0.617, ave_loss: 0.600
[84]  [1660/1724] loss: 0.663, ave_loss: 0.600
[85]  [1680/1724] loss: 0.631, ave_loss: 0.601
[86]  [1700/1724] loss: 0.632, ave_loss: 0.601
[87]  [1720/1724] loss: 0.497, ave_loss: 0.600
[88]  [1740/1724] loss: 0.528, ave_loss: 0.599

Finished Training finishing at 2021-08-25 05:27:23.077584
printing_out epoch  17.354988399071924 learning rate: 0.0005153561248318907
0.00030706260103698985
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.992e-01
Validation Loss: 5.350e-01
Validation ROC: 0.6659
Saving model
81.64501160092807 epochs left to go

Training Epoch 17.354988399071924/100 starting at 2021-08-25 05:39:20.839283
[1]  [0/1724] loss: 0.664, ave_loss: 0.664
[2]  [20/1724] loss: 0.597, ave_loss: 0.631
[3]  [40/1724] loss: 0.808, ave_loss: 0.690
[4]  [60/1724] loss: 0.570, ave_loss: 0.660
[5]  [80/1724] loss: 0.595, ave_loss: 0.647
[6]  [100/1724] loss: 0.579, ave_loss: 0.636
[7]  [120/1724] loss: 0.691, ave_loss: 0.644
[8]  [140/1724] loss: 0.561, ave_loss: 0.633
[9]  [160/1724] loss: 0.655, ave_loss: 0.636
[10]  [180/1724] loss: 0.588, ave_loss: 0.631
[11]  [200/1724] loss: 0.560, ave_loss: 0.624
[12]  [220/1724] loss: 0.713, ave_loss: 0.632
[13]  [240/1724] loss: 0.704, ave_loss: 0.637
[14]  [260/1724] loss: 0.635, ave_loss: 0.637
[15]  [280/1724] loss: 0.593, ave_loss: 0.634
[16]  [300/1724] loss: 0.553, ave_loss: 0.629
[17]  [320/1724] loss: 0.641, ave_loss: 0.630
[18]  [340/1724] loss: 0.595, ave_loss: 0.628
[19]  [360/1724] loss: 0.533, ave_loss: 0.623
[20]  [380/1724] loss: 0.573, ave_loss: 0.620
[21]  [400/1724] loss: 0.580, ave_loss: 0.619
[22]  [420/1724] loss: 0.506, ave_loss: 0.613
[23]  [440/1724] loss: 0.597, ave_loss: 0.613
[24]  [460/1724] loss: 0.601, ave_loss: 0.612
[25]  [480/1724] loss: 0.649, ave_loss: 0.614
[26]  [500/1724] loss: 0.483, ave_loss: 0.609
[27]  [520/1724] loss: 0.670, ave_loss: 0.611
[28]  [540/1724] loss: 0.548, ave_loss: 0.609
[29]  [560/1724] loss: 0.609, ave_loss: 0.609
[30]  [580/1724] loss: 0.680, ave_loss: 0.611
[31]  [600/1724] loss: 0.704, ave_loss: 0.614
[32]  [620/1724] loss: 0.671, ave_loss: 0.616
[33]  [640/1724] loss: 0.541, ave_loss: 0.614
[34]  [660/1724] loss: 0.726, ave_loss: 0.617
[35]  [680/1724] loss: 0.531, ave_loss: 0.614
[36]  [700/1724] loss: 0.477, ave_loss: 0.611
[37]  [720/1724] loss: 0.525, ave_loss: 0.608
[38]  [740/1724] loss: 0.457, ave_loss: 0.604
[39]  [760/1724] loss: 0.661, ave_loss: 0.606
[40]  [780/1724] loss: 0.564, ave_loss: 0.605
[41]  [800/1724] loss: 0.602, ave_loss: 0.605
[42]  [820/1724] loss: 0.603, ave_loss: 0.605
[43]  [840/1724] loss: 0.496, ave_loss: 0.602
[44]  [860/1724] loss: 0.607, ave_loss: 0.602
[45]  [880/1724] loss: 0.567, ave_loss: 0.601
[46]  [900/1724] loss: 0.529, ave_loss: 0.600
[47]  [920/1724] loss: 0.634, ave_loss: 0.601
[48]  [940/1724] loss: 0.478, ave_loss: 0.598
[49]  [960/1724] loss: 0.634, ave_loss: 0.599
[50]  [980/1724] loss: 0.556, ave_loss: 0.598
[51]  [1000/1724] loss: 0.632, ave_loss: 0.599
[52]  [1020/1724] loss: 0.574, ave_loss: 0.598
[53]  [1040/1724] loss: 0.676, ave_loss: 0.600
[54]  [1060/1724] loss: 0.552, ave_loss: 0.599
[55]  [1080/1724] loss: 0.691, ave_loss: 0.600
[56]  [1100/1724] loss: 0.654, ave_loss: 0.601
[57]  [1120/1724] loss: 0.651, ave_loss: 0.602
[58]  [1140/1724] loss: 0.644, ave_loss: 0.603
[59]  [1160/1724] loss: 0.640, ave_loss: 0.604
[60]  [1180/1724] loss: 0.597, ave_loss: 0.603
[61]  [1200/1724] loss: 0.626, ave_loss: 0.604
[62]  [1220/1724] loss: 0.617, ave_loss: 0.604
[63]  [1240/1724] loss: 0.564, ave_loss: 0.603
[64]  [1260/1724] loss: 0.691, ave_loss: 0.605
[65]  [1280/1724] loss: 0.600, ave_loss: 0.605
[66]  [1300/1724] loss: 0.675, ave_loss: 0.606
[67]  [1320/1724] loss: 0.510, ave_loss: 0.604
[68]  [1340/1724] loss: 0.610, ave_loss: 0.604
[69]  [1360/1724] loss: 0.590, ave_loss: 0.604
[70]  [1380/1724] loss: 0.519, ave_loss: 0.603
[71]  [1400/1724] loss: 0.531, ave_loss: 0.602
[72]  [1420/1724] loss: 0.667, ave_loss: 0.603
[73]  [1440/1724] loss: 0.470, ave_loss: 0.601
[74]  [1460/1724] loss: 0.492, ave_loss: 0.600
[75]  [1480/1724] loss: 0.600, ave_loss: 0.600
[76]  [1500/1724] loss: 0.570, ave_loss: 0.599
[77]  [1520/1724] loss: 0.481, ave_loss: 0.598
[78]  [1540/1724] loss: 0.609, ave_loss: 0.598
[79]  [1560/1724] loss: 0.604, ave_loss: 0.598
[80]  [1580/1724] loss: 0.614, ave_loss: 0.598
[81]  [1600/1724] loss: 0.373, ave_loss: 0.595
[82]  [1620/1724] loss: 0.707, ave_loss: 0.597
[83]  [1640/1724] loss: 0.695, ave_loss: 0.598
[84]  [1660/1724] loss: 0.637, ave_loss: 0.598
[85]  [1680/1724] loss: 0.539, ave_loss: 0.598
[86]  [1700/1724] loss: 0.540, ave_loss: 0.597
[87]  [1720/1724] loss: 0.535, ave_loss: 0.596
[88]  [1740/1724] loss: 0.448, ave_loss: 0.595

Finished Training finishing at 2021-08-25 05:53:43.500140
printing_out epoch  18.37587006960557 learning rate: 0.0005153561248318907
0.00029785072300588016
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.945e-01
Validation Loss: 5.099e-01
Validation ROC: 0.6683
Saving model
80.62412993039443 epochs left to go

Training Epoch 18.37587006960557/100 starting at 2021-08-25 06:05:36.826834
[1]  [0/1724] loss: 0.470, ave_loss: 0.470
[2]  [20/1724] loss: 0.666, ave_loss: 0.568
[3]  [40/1724] loss: 0.569, ave_loss: 0.568
[4]  [60/1724] loss: 0.599, ave_loss: 0.576
[5]  [80/1724] loss: 0.542, ave_loss: 0.569
[6]  [100/1724] loss: 0.635, ave_loss: 0.580
[7]  [120/1724] loss: 0.594, ave_loss: 0.582
[8]  [140/1724] loss: 0.672, ave_loss: 0.593
[9]  [160/1724] loss: 0.507, ave_loss: 0.584
[10]  [180/1724] loss: 0.567, ave_loss: 0.582
[11]  [200/1724] loss: 0.660, ave_loss: 0.589
[12]  [220/1724] loss: 0.659, ave_loss: 0.595
[13]  [240/1724] loss: 0.677, ave_loss: 0.601
[14]  [260/1724] loss: 0.470, ave_loss: 0.592
[15]  [280/1724] loss: 0.731, ave_loss: 0.601
[16]  [300/1724] loss: 0.523, ave_loss: 0.596
[17]  [320/1724] loss: 0.668, ave_loss: 0.601
[18]  [340/1724] loss: 0.600, ave_loss: 0.601
[19]  [360/1724] loss: 0.612, ave_loss: 0.601
[20]  [380/1724] loss: 0.702, ave_loss: 0.606
[21]  [400/1724] loss: 0.901, ave_loss: 0.620
[22]  [420/1724] loss: 0.572, ave_loss: 0.618
[23]  [440/1724] loss: 0.661, ave_loss: 0.620
[24]  [460/1724] loss: 0.574, ave_loss: 0.618
[25]  [480/1724] loss: 0.602, ave_loss: 0.617
[26]  [500/1724] loss: 0.630, ave_loss: 0.618
[27]  [520/1724] loss: 0.646, ave_loss: 0.619
[28]  [540/1724] loss: 0.588, ave_loss: 0.618
[29]  [560/1724] loss: 0.564, ave_loss: 0.616
[30]  [580/1724] loss: 0.639, ave_loss: 0.617
[31]  [600/1724] loss: 0.518, ave_loss: 0.613
[32]  [620/1724] loss: 0.583, ave_loss: 0.613
[33]  [640/1724] loss: 0.650, ave_loss: 0.614
[34]  [660/1724] loss: 0.629, ave_loss: 0.614
[35]  [680/1724] loss: 0.668, ave_loss: 0.616
[36]  [700/1724] loss: 0.636, ave_loss: 0.616
[37]  [720/1724] loss: 0.593, ave_loss: 0.616
[38]  [740/1724] loss: 0.501, ave_loss: 0.613
[39]  [760/1724] loss: 0.528, ave_loss: 0.610
[40]  [780/1724] loss: 0.633, ave_loss: 0.611
[41]  [800/1724] loss: 0.652, ave_loss: 0.612
[42]  [820/1724] loss: 0.552, ave_loss: 0.611
[43]  [840/1724] loss: 0.530, ave_loss: 0.609
[44]  [860/1724] loss: 0.602, ave_loss: 0.609
[45]  [880/1724] loss: 0.700, ave_loss: 0.611
[46]  [900/1724] loss: 0.564, ave_loss: 0.610
[47]  [920/1724] loss: 0.647, ave_loss: 0.610
[48]  [940/1724] loss: 0.521, ave_loss: 0.608
[49]  [960/1724] loss: 0.606, ave_loss: 0.608
[50]  [980/1724] loss: 0.550, ave_loss: 0.607
[51]  [1000/1724] loss: 0.590, ave_loss: 0.607
[52]  [1020/1724] loss: 0.525, ave_loss: 0.605
[53]  [1040/1724] loss: 0.706, ave_loss: 0.607
[54]  [1060/1724] loss: 0.415, ave_loss: 0.604
[55]  [1080/1724] loss: 0.574, ave_loss: 0.603
[56]  [1100/1724] loss: 0.509, ave_loss: 0.601
[57]  [1120/1724] loss: 0.549, ave_loss: 0.600
[58]  [1140/1724] loss: 0.610, ave_loss: 0.601
[59]  [1160/1724] loss: 0.774, ave_loss: 0.604
[60]  [1180/1724] loss: 0.516, ave_loss: 0.602
[61]  [1200/1724] loss: 0.664, ave_loss: 0.603
[62]  [1220/1724] loss: 0.625, ave_loss: 0.603
[63]  [1240/1724] loss: 0.500, ave_loss: 0.602
[64]  [1260/1724] loss: 0.595, ave_loss: 0.602
[65]  [1280/1724] loss: 0.635, ave_loss: 0.602
[66]  [1300/1724] loss: 0.629, ave_loss: 0.603
[67]  [1320/1724] loss: 0.738, ave_loss: 0.605
[68]  [1340/1724] loss: 0.580, ave_loss: 0.604
[69]  [1360/1724] loss: 0.577, ave_loss: 0.604
[70]  [1380/1724] loss: 0.561, ave_loss: 0.603
[71]  [1400/1724] loss: 0.568, ave_loss: 0.603
[72]  [1420/1724] loss: 0.561, ave_loss: 0.602
[73]  [1440/1724] loss: 0.680, ave_loss: 0.603
[74]  [1460/1724] loss: 0.514, ave_loss: 0.602
[75]  [1480/1724] loss: 0.554, ave_loss: 0.601
[76]  [1500/1724] loss: 0.624, ave_loss: 0.602
[77]  [1520/1724] loss: 0.513, ave_loss: 0.601
[78]  [1540/1724] loss: 0.592, ave_loss: 0.600
[79]  [1560/1724] loss: 0.526, ave_loss: 0.600
[80]  [1580/1724] loss: 0.697, ave_loss: 0.601
[81]  [1600/1724] loss: 0.541, ave_loss: 0.600
[82]  [1620/1724] loss: 0.596, ave_loss: 0.600
[83]  [1640/1724] loss: 0.487, ave_loss: 0.599
[84]  [1660/1724] loss: 0.688, ave_loss: 0.600
[85]  [1680/1724] loss: 0.518, ave_loss: 0.599
[86]  [1700/1724] loss: 0.554, ave_loss: 0.598
[87]  [1720/1724] loss: 0.601, ave_loss: 0.598
[88]  [1740/1724] loss: 0.630, ave_loss: 0.599

Finished Training finishing at 2021-08-25 06:19:59.266910
printing_out epoch  19.396751740139212 learning rate: 0.0005153561248318907
0.00028891520131570374
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.986e-01
Validation Loss: 5.228e-01
Validation ROC: 0.6730
Saving model
79.60324825986079 epochs left to go

Training Epoch 19.396751740139212/100 starting at 2021-08-25 06:31:53.364256
[1]  [0/1724] loss: 0.512, ave_loss: 0.512
[2]  [20/1724] loss: 0.476, ave_loss: 0.494
[3]  [40/1724] loss: 0.555, ave_loss: 0.514
[4]  [60/1724] loss: 0.627, ave_loss: 0.543
[5]  [80/1724] loss: 0.464, ave_loss: 0.527
[6]  [100/1724] loss: 0.461, ave_loss: 0.516
[7]  [120/1724] loss: 0.689, ave_loss: 0.541
[8]  [140/1724] loss: 0.685, ave_loss: 0.559
[9]  [160/1724] loss: 0.726, ave_loss: 0.577
[10]  [180/1724] loss: 0.498, ave_loss: 0.569
[11]  [200/1724] loss: 0.609, ave_loss: 0.573
[12]  [220/1724] loss: 0.512, ave_loss: 0.568
[13]  [240/1724] loss: 0.654, ave_loss: 0.574
[14]  [260/1724] loss: 0.677, ave_loss: 0.582
[15]  [280/1724] loss: 0.589, ave_loss: 0.582
[16]  [300/1724] loss: 0.633, ave_loss: 0.585
[17]  [320/1724] loss: 0.490, ave_loss: 0.580
[18]  [340/1724] loss: 0.561, ave_loss: 0.579
[19]  [360/1724] loss: 0.502, ave_loss: 0.575
[20]  [380/1724] loss: 0.614, ave_loss: 0.577
[21]  [400/1724] loss: 0.547, ave_loss: 0.575
[22]  [420/1724] loss: 0.557, ave_loss: 0.574
[23]  [440/1724] loss: 0.613, ave_loss: 0.576
[24]  [460/1724] loss: 0.649, ave_loss: 0.579
[25]  [480/1724] loss: 0.612, ave_loss: 0.580
[26]  [500/1724] loss: 0.724, ave_loss: 0.586
[27]  [520/1724] loss: 0.574, ave_loss: 0.585
[28]  [540/1724] loss: 0.630, ave_loss: 0.587
[29]  [560/1724] loss: 0.500, ave_loss: 0.584
[30]  [580/1724] loss: 0.617, ave_loss: 0.585
[31]  [600/1724] loss: 0.683, ave_loss: 0.588
[32]  [620/1724] loss: 0.610, ave_loss: 0.589
[33]  [640/1724] loss: 0.625, ave_loss: 0.590
[34]  [660/1724] loss: 0.627, ave_loss: 0.591
[35]  [680/1724] loss: 0.540, ave_loss: 0.590
[36]  [700/1724] loss: 0.621, ave_loss: 0.591
[37]  [720/1724] loss: 0.548, ave_loss: 0.589
[38]  [740/1724] loss: 0.567, ave_loss: 0.589
[39]  [760/1724] loss: 0.586, ave_loss: 0.589
[40]  [780/1724] loss: 0.523, ave_loss: 0.587
[41]  [800/1724] loss: 0.533, ave_loss: 0.586
[42]  [820/1724] loss: 0.595, ave_loss: 0.586
[43]  [840/1724] loss: 0.597, ave_loss: 0.586
[44]  [860/1724] loss: 0.676, ave_loss: 0.588
[45]  [880/1724] loss: 0.572, ave_loss: 0.588
[46]  [900/1724] loss: 0.619, ave_loss: 0.589
[47]  [920/1724] loss: 0.502, ave_loss: 0.587
[48]  [940/1724] loss: 0.624, ave_loss: 0.588
[49]  [960/1724] loss: 0.537, ave_loss: 0.587
[50]  [980/1724] loss: 0.604, ave_loss: 0.587
[51]  [1000/1724] loss: 0.538, ave_loss: 0.586
[52]  [1020/1724] loss: 0.631, ave_loss: 0.587
[53]  [1040/1724] loss: 0.582, ave_loss: 0.587
[54]  [1060/1724] loss: 0.578, ave_loss: 0.587
[55]  [1080/1724] loss: 0.581, ave_loss: 0.586
[56]  [1100/1724] loss: 0.629, ave_loss: 0.587
[57]  [1120/1724] loss: 0.577, ave_loss: 0.587
[58]  [1140/1724] loss: 0.632, ave_loss: 0.588
[59]  [1160/1724] loss: 0.426, ave_loss: 0.585
[60]  [1180/1724] loss: 0.610, ave_loss: 0.585
[61]  [1200/1724] loss: 0.678, ave_loss: 0.587
[62]  [1220/1724] loss: 0.511, ave_loss: 0.586
[63]  [1240/1724] loss: 0.619, ave_loss: 0.586
[64]  [1260/1724] loss: 0.644, ave_loss: 0.587
[65]  [1280/1724] loss: 0.537, ave_loss: 0.586
[66]  [1300/1724] loss: 0.641, ave_loss: 0.587
[67]  [1320/1724] loss: 0.433, ave_loss: 0.585
[68]  [1340/1724] loss: 0.713, ave_loss: 0.587
[69]  [1360/1724] loss: 0.544, ave_loss: 0.586
[70]  [1380/1724] loss: 0.600, ave_loss: 0.586
[71]  [1400/1724] loss: 0.601, ave_loss: 0.587
[72]  [1420/1724] loss: 0.427, ave_loss: 0.584
[73]  [1440/1724] loss: 0.384, ave_loss: 0.582
[74]  [1460/1724] loss: 0.486, ave_loss: 0.580
[75]  [1480/1724] loss: 0.599, ave_loss: 0.581
[76]  [1500/1724] loss: 0.429, ave_loss: 0.579
[77]  [1520/1724] loss: 0.620, ave_loss: 0.579
[78]  [1540/1724] loss: 0.744, ave_loss: 0.581
[79]  [1560/1724] loss: 0.796, ave_loss: 0.584
[80]  [1580/1724] loss: 0.645, ave_loss: 0.585
[81]  [1600/1724] loss: 0.550, ave_loss: 0.584
[82]  [1620/1724] loss: 0.580, ave_loss: 0.584
[83]  [1640/1724] loss: 0.709, ave_loss: 0.586
[84]  [1660/1724] loss: 0.526, ave_loss: 0.585
[85]  [1680/1724] loss: 0.536, ave_loss: 0.584
[86]  [1700/1724] loss: 0.632, ave_loss: 0.585
[87]  [1720/1724] loss: 0.696, ave_loss: 0.586
[88]  [1740/1724] loss: 0.610, ave_loss: 0.587

Finished Training finishing at 2021-08-25 06:46:23.435780
printing_out epoch  20.417633410672853 learning rate: 0.0005153561248318907
0.0002802477452762326
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.866e-01
Validation Loss: 5.324e-01
Validation ROC: 0.6719
No improvement, still saving model
78.58236658932715 epochs left to go

Training Epoch 20.417633410672853/100 starting at 2021-08-25 06:58:17.105928
[1]  [0/1724] loss: 0.657, ave_loss: 0.657
[2]  [20/1724] loss: 0.577, ave_loss: 0.617
[3]  [40/1724] loss: 0.669, ave_loss: 0.635
[4]  [60/1724] loss: 0.691, ave_loss: 0.649
[5]  [80/1724] loss: 0.523, ave_loss: 0.623
[6]  [100/1724] loss: 0.582, ave_loss: 0.617
[7]  [120/1724] loss: 0.690, ave_loss: 0.627
[8]  [140/1724] loss: 0.483, ave_loss: 0.609
[9]  [160/1724] loss: 0.541, ave_loss: 0.601
[10]  [180/1724] loss: 0.563, ave_loss: 0.598
[11]  [200/1724] loss: 0.620, ave_loss: 0.600
[12]  [220/1724] loss: 0.689, ave_loss: 0.607
[13]  [240/1724] loss: 0.674, ave_loss: 0.612
[14]  [260/1724] loss: 0.594, ave_loss: 0.611
[15]  [280/1724] loss: 0.493, ave_loss: 0.603
[16]  [300/1724] loss: 0.508, ave_loss: 0.597
[17]  [320/1724] loss: 0.502, ave_loss: 0.592
[18]  [340/1724] loss: 0.565, ave_loss: 0.590
[19]  [360/1724] loss: 0.555, ave_loss: 0.588
[20]  [380/1724] loss: 0.562, ave_loss: 0.587
[21]  [400/1724] loss: 0.486, ave_loss: 0.582
[22]  [420/1724] loss: 0.554, ave_loss: 0.581
[23]  [440/1724] loss: 0.652, ave_loss: 0.584
[24]  [460/1724] loss: 0.533, ave_loss: 0.582
[25]  [480/1724] loss: 0.460, ave_loss: 0.577
[26]  [500/1724] loss: 0.627, ave_loss: 0.579
[27]  [520/1724] loss: 0.598, ave_loss: 0.580
[28]  [540/1724] loss: 0.534, ave_loss: 0.578
[29]  [560/1724] loss: 0.690, ave_loss: 0.582
[30]  [580/1724] loss: 0.569, ave_loss: 0.581
[31]  [600/1724] loss: 0.615, ave_loss: 0.582
[32]  [620/1724] loss: 0.576, ave_loss: 0.582
[33]  [640/1724] loss: 0.591, ave_loss: 0.582
[34]  [660/1724] loss: 0.569, ave_loss: 0.582
[35]  [680/1724] loss: 0.747, ave_loss: 0.587
[36]  [700/1724] loss: 0.527, ave_loss: 0.585
[37]  [720/1724] loss: 0.573, ave_loss: 0.585
[38]  [740/1724] loss: 0.594, ave_loss: 0.585
[39]  [760/1724] loss: 0.702, ave_loss: 0.588
[40]  [780/1724] loss: 0.707, ave_loss: 0.591
[41]  [800/1724] loss: 0.597, ave_loss: 0.591
[42]  [820/1724] loss: 0.468, ave_loss: 0.588
[43]  [840/1724] loss: 0.536, ave_loss: 0.587
[44]  [860/1724] loss: 0.611, ave_loss: 0.588
[45]  [880/1724] loss: 0.697, ave_loss: 0.590
[46]  [900/1724] loss: 0.685, ave_loss: 0.592
[47]  [920/1724] loss: 0.661, ave_loss: 0.594
[48]  [940/1724] loss: 0.593, ave_loss: 0.594
[49]  [960/1724] loss: 0.667, ave_loss: 0.595
[50]  [980/1724] loss: 0.696, ave_loss: 0.597
[51]  [1000/1724] loss: 0.603, ave_loss: 0.597
[52]  [1020/1724] loss: 0.798, ave_loss: 0.601
[53]  [1040/1724] loss: 0.514, ave_loss: 0.599
[54]  [1060/1724] loss: 0.810, ave_loss: 0.603
[55]  [1080/1724] loss: 0.633, ave_loss: 0.604
[56]  [1100/1724] loss: 0.658, ave_loss: 0.605
[57]  [1120/1724] loss: 0.553, ave_loss: 0.604
[58]  [1140/1724] loss: 0.573, ave_loss: 0.603
[59]  [1160/1724] loss: 0.596, ave_loss: 0.603
[60]  [1180/1724] loss: 0.650, ave_loss: 0.604
[61]  [1200/1724] loss: 0.591, ave_loss: 0.604
[62]  [1220/1724] loss: 0.588, ave_loss: 0.604
[63]  [1240/1724] loss: 0.623, ave_loss: 0.604
[64]  [1260/1724] loss: 0.679, ave_loss: 0.605
[65]  [1280/1724] loss: 0.654, ave_loss: 0.606
[66]  [1300/1724] loss: 0.701, ave_loss: 0.607
[67]  [1320/1724] loss: 0.592, ave_loss: 0.607
[68]  [1340/1724] loss: 0.524, ave_loss: 0.606
[69]  [1360/1724] loss: 0.522, ave_loss: 0.605
[70]  [1380/1724] loss: 0.523, ave_loss: 0.603
[71]  [1400/1724] loss: 0.548, ave_loss: 0.603
[72]  [1420/1724] loss: 0.605, ave_loss: 0.603
[73]  [1440/1724] loss: 0.576, ave_loss: 0.602
[74]  [1460/1724] loss: 0.647, ave_loss: 0.603
[75]  [1480/1724] loss: 0.555, ave_loss: 0.602
[76]  [1500/1724] loss: 0.621, ave_loss: 0.603
[77]  [1520/1724] loss: 0.637, ave_loss: 0.603
[78]  [1540/1724] loss: 0.636, ave_loss: 0.603
[79]  [1560/1724] loss: 0.769, ave_loss: 0.605
[80]  [1580/1724] loss: 0.639, ave_loss: 0.606
[81]  [1600/1724] loss: 0.625, ave_loss: 0.606
[82]  [1620/1724] loss: 0.523, ave_loss: 0.605
[83]  [1640/1724] loss: 0.634, ave_loss: 0.605
[84]  [1660/1724] loss: 0.557, ave_loss: 0.605
[85]  [1680/1724] loss: 0.495, ave_loss: 0.604
[86]  [1700/1724] loss: 0.565, ave_loss: 0.603
[87]  [1720/1724] loss: 0.608, ave_loss: 0.603
[88]  [1740/1724] loss: 0.663, ave_loss: 0.604

Finished Training finishing at 2021-08-25 07:12:35.738872
printing_out epoch  21.438515081206496 learning rate: 0.0005153561248318907
0.00027184031291794565
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.039e-01
Validation Loss: 5.587e-01
Validation ROC: 0.6643
No improvement, still saving model
77.5614849187935 epochs left to go

Training Epoch 21.438515081206496/100 starting at 2021-08-25 07:24:32.645057
[1]  [0/1724] loss: 0.538, ave_loss: 0.538
[2]  [20/1724] loss: 0.606, ave_loss: 0.572
[3]  [40/1724] loss: 0.527, ave_loss: 0.557
[4]  [60/1724] loss: 0.607, ave_loss: 0.569
[5]  [80/1724] loss: 0.618, ave_loss: 0.579
[6]  [100/1724] loss: 0.682, ave_loss: 0.596
[7]  [120/1724] loss: 0.551, ave_loss: 0.590
[8]  [140/1724] loss: 0.604, ave_loss: 0.592
[9]  [160/1724] loss: 0.618, ave_loss: 0.595
[10]  [180/1724] loss: 0.680, ave_loss: 0.603
[11]  [200/1724] loss: 0.660, ave_loss: 0.608
[12]  [220/1724] loss: 0.598, ave_loss: 0.607
[13]  [240/1724] loss: 0.482, ave_loss: 0.598
[14]  [260/1724] loss: 0.587, ave_loss: 0.597
[15]  [280/1724] loss: 0.492, ave_loss: 0.590
[16]  [300/1724] loss: 0.557, ave_loss: 0.588
[17]  [320/1724] loss: 0.542, ave_loss: 0.585
[18]  [340/1724] loss: 0.550, ave_loss: 0.583
[19]  [360/1724] loss: 0.592, ave_loss: 0.584
[20]  [380/1724] loss: 0.649, ave_loss: 0.587
[21]  [400/1724] loss: 0.571, ave_loss: 0.586
[22]  [420/1724] loss: 0.628, ave_loss: 0.588
[23]  [440/1724] loss: 0.569, ave_loss: 0.587
[24]  [460/1724] loss: 0.624, ave_loss: 0.589
[25]  [480/1724] loss: 0.590, ave_loss: 0.589
[26]  [500/1724] loss: 0.581, ave_loss: 0.589
[27]  [520/1724] loss: 0.632, ave_loss: 0.590
[28]  [540/1724] loss: 0.424, ave_loss: 0.584
[29]  [560/1724] loss: 0.600, ave_loss: 0.585
[30]  [580/1724] loss: 0.520, ave_loss: 0.583
[31]  [600/1724] loss: 0.606, ave_loss: 0.583
[32]  [620/1724] loss: 0.527, ave_loss: 0.582
[33]  [640/1724] loss: 0.660, ave_loss: 0.584
[34]  [660/1724] loss: 0.556, ave_loss: 0.583
[35]  [680/1724] loss: 0.539, ave_loss: 0.582
[36]  [700/1724] loss: 0.589, ave_loss: 0.582
[37]  [720/1724] loss: 0.505, ave_loss: 0.580
[38]  [740/1724] loss: 0.573, ave_loss: 0.580
[39]  [760/1724] loss: 0.540, ave_loss: 0.579
[40]  [780/1724] loss: 0.542, ave_loss: 0.578
[41]  [800/1724] loss: 0.691, ave_loss: 0.581
[42]  [820/1724] loss: 0.682, ave_loss: 0.583
[43]  [840/1724] loss: 0.557, ave_loss: 0.582
[44]  [860/1724] loss: 0.516, ave_loss: 0.581
[45]  [880/1724] loss: 0.575, ave_loss: 0.581
[46]  [900/1724] loss: 0.567, ave_loss: 0.581
[47]  [920/1724] loss: 0.572, ave_loss: 0.580
[48]  [940/1724] loss: 0.568, ave_loss: 0.580
[49]  [960/1724] loss: 0.563, ave_loss: 0.580
[50]  [980/1724] loss: 0.636, ave_loss: 0.581
[51]  [1000/1724] loss: 0.591, ave_loss: 0.581
[52]  [1020/1724] loss: 0.652, ave_loss: 0.582
[53]  [1040/1724] loss: 0.693, ave_loss: 0.584
[54]  [1060/1724] loss: 0.400, ave_loss: 0.581
[55]  [1080/1724] loss: 0.672, ave_loss: 0.583
[56]  [1100/1724] loss: 0.578, ave_loss: 0.583
[57]  [1120/1724] loss: 0.612, ave_loss: 0.583
[58]  [1140/1724] loss: 0.497, ave_loss: 0.582
[59]  [1160/1724] loss: 0.606, ave_loss: 0.582
[60]  [1180/1724] loss: 0.509, ave_loss: 0.581
[61]  [1200/1724] loss: 0.547, ave_loss: 0.580
[62]  [1220/1724] loss: 0.590, ave_loss: 0.580
[63]  [1240/1724] loss: 0.710, ave_loss: 0.583
[64]  [1260/1724] loss: 0.590, ave_loss: 0.583
[65]  [1280/1724] loss: 0.521, ave_loss: 0.582
[66]  [1300/1724] loss: 0.727, ave_loss: 0.584
[67]  [1320/1724] loss: 0.563, ave_loss: 0.584
[68]  [1340/1724] loss: 0.605, ave_loss: 0.584
[69]  [1360/1724] loss: 0.608, ave_loss: 0.584
[70]  [1380/1724] loss: 0.666, ave_loss: 0.585
[71]  [1400/1724] loss: 0.540, ave_loss: 0.585
[72]  [1420/1724] loss: 0.576, ave_loss: 0.585
[73]  [1440/1724] loss: 0.615, ave_loss: 0.585
[74]  [1460/1724] loss: 0.578, ave_loss: 0.585
[75]  [1480/1724] loss: 0.584, ave_loss: 0.585
[76]  [1500/1724] loss: 0.465, ave_loss: 0.583
[77]  [1520/1724] loss: 0.705, ave_loss: 0.585
[78]  [1540/1724] loss: 0.708, ave_loss: 0.587
[79]  [1560/1724] loss: 0.578, ave_loss: 0.586
[80]  [1580/1724] loss: 0.605, ave_loss: 0.587
[81]  [1600/1724] loss: 0.503, ave_loss: 0.586
[82]  [1620/1724] loss: 0.664, ave_loss: 0.587
[83]  [1640/1724] loss: 0.565, ave_loss: 0.586
[84]  [1660/1724] loss: 0.697, ave_loss: 0.588
[85]  [1680/1724] loss: 0.485, ave_loss: 0.586
[86]  [1700/1724] loss: 0.541, ave_loss: 0.586
[87]  [1720/1724] loss: 0.609, ave_loss: 0.586
[88]  [1740/1724] loss: 0.548, ave_loss: 0.586

Finished Training finishing at 2021-08-25 07:38:52.373322
printing_out epoch  22.45939675174014 learning rate: 0.0005153561248318907
0.0002636851035304073
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.857e-01
Validation Loss: 5.679e-01
Validation ROC: 0.6681
No improvement, still saving model
76.54060324825986 epochs left to go

Training Epoch 22.45939675174014/100 starting at 2021-08-25 07:50:46.250808
[1]  [0/1724] loss: 0.562, ave_loss: 0.562
[2]  [20/1724] loss: 0.600, ave_loss: 0.581
[3]  [40/1724] loss: 0.511, ave_loss: 0.558
[4]  [60/1724] loss: 0.534, ave_loss: 0.552
[5]  [80/1724] loss: 0.697, ave_loss: 0.581
[6]  [100/1724] loss: 0.453, ave_loss: 0.560
[7]  [120/1724] loss: 0.703, ave_loss: 0.580
[8]  [140/1724] loss: 0.667, ave_loss: 0.591
[9]  [160/1724] loss: 0.522, ave_loss: 0.583
[10]  [180/1724] loss: 0.420, ave_loss: 0.567
[11]  [200/1724] loss: 0.571, ave_loss: 0.567
[12]  [220/1724] loss: 0.663, ave_loss: 0.575
[13]  [240/1724] loss: 0.532, ave_loss: 0.572
[14]  [260/1724] loss: 0.680, ave_loss: 0.580
[15]  [280/1724] loss: 0.565, ave_loss: 0.579
[16]  [300/1724] loss: 0.776, ave_loss: 0.591
[17]  [320/1724] loss: 0.516, ave_loss: 0.587
[18]  [340/1724] loss: 0.747, ave_loss: 0.596
[19]  [360/1724] loss: 0.683, ave_loss: 0.600
[20]  [380/1724] loss: 0.701, ave_loss: 0.605
[21]  [400/1724] loss: 0.611, ave_loss: 0.605
[22]  [420/1724] loss: 0.647, ave_loss: 0.607
[23]  [440/1724] loss: 0.627, ave_loss: 0.608
[24]  [460/1724] loss: 0.520, ave_loss: 0.605
[25]  [480/1724] loss: 0.528, ave_loss: 0.601
[26]  [500/1724] loss: 0.602, ave_loss: 0.602
[27]  [520/1724] loss: 0.522, ave_loss: 0.599
[28]  [540/1724] loss: 0.631, ave_loss: 0.600
[29]  [560/1724] loss: 0.637, ave_loss: 0.601
[30]  [580/1724] loss: 0.665, ave_loss: 0.603
[31]  [600/1724] loss: 0.673, ave_loss: 0.605
[32]  [620/1724] loss: 0.583, ave_loss: 0.605
[33]  [640/1724] loss: 0.658, ave_loss: 0.606
[34]  [660/1724] loss: 0.508, ave_loss: 0.603
[35]  [680/1724] loss: 0.621, ave_loss: 0.604
[36]  [700/1724] loss: 0.443, ave_loss: 0.599
[37]  [720/1724] loss: 0.506, ave_loss: 0.597
[38]  [740/1724] loss: 0.567, ave_loss: 0.596
[39]  [760/1724] loss: 0.654, ave_loss: 0.598
[40]  [780/1724] loss: 0.643, ave_loss: 0.599
[41]  [800/1724] loss: 0.634, ave_loss: 0.600
[42]  [820/1724] loss: 0.633, ave_loss: 0.600
[43]  [840/1724] loss: 0.475, ave_loss: 0.598
[44]  [860/1724] loss: 0.547, ave_loss: 0.596
[45]  [880/1724] loss: 0.541, ave_loss: 0.595
[46]  [900/1724] loss: 0.644, ave_loss: 0.596
[47]  [920/1724] loss: 0.550, ave_loss: 0.595
[48]  [940/1724] loss: 0.582, ave_loss: 0.595
[49]  [960/1724] loss: 0.589, ave_loss: 0.595
[50]  [980/1724] loss: 0.586, ave_loss: 0.595
[51]  [1000/1724] loss: 0.392, ave_loss: 0.591
[52]  [1020/1724] loss: 0.624, ave_loss: 0.591
[53]  [1040/1724] loss: 0.581, ave_loss: 0.591
[54]  [1060/1724] loss: 0.546, ave_loss: 0.590
[55]  [1080/1724] loss: 0.624, ave_loss: 0.591
[56]  [1100/1724] loss: 0.598, ave_loss: 0.591
[57]  [1120/1724] loss: 0.560, ave_loss: 0.590
[58]  [1140/1724] loss: 0.658, ave_loss: 0.592
[59]  [1160/1724] loss: 0.443, ave_loss: 0.589
[60]  [1180/1724] loss: 0.442, ave_loss: 0.587
[61]  [1200/1724] loss: 0.586, ave_loss: 0.587
[62]  [1220/1724] loss: 0.548, ave_loss: 0.586
[63]  [1240/1724] loss: 0.608, ave_loss: 0.586
[64]  [1260/1724] loss: 0.491, ave_loss: 0.585
[65]  [1280/1724] loss: 0.663, ave_loss: 0.586
[66]  [1300/1724] loss: 0.602, ave_loss: 0.586
[67]  [1320/1724] loss: 0.660, ave_loss: 0.587
[68]  [1340/1724] loss: 0.652, ave_loss: 0.588
[69]  [1360/1724] loss: 0.525, ave_loss: 0.587
[70]  [1380/1724] loss: 0.443, ave_loss: 0.585
[71]  [1400/1724] loss: 0.627, ave_loss: 0.586
[72]  [1420/1724] loss: 0.604, ave_loss: 0.586
[73]  [1440/1724] loss: 0.580, ave_loss: 0.586
[74]  [1460/1724] loss: 0.655, ave_loss: 0.587
[75]  [1480/1724] loss: 0.659, ave_loss: 0.588
[76]  [1500/1724] loss: 0.546, ave_loss: 0.587
[77]  [1520/1724] loss: 0.545, ave_loss: 0.587
[78]  [1540/1724] loss: 0.610, ave_loss: 0.587
[79]  [1560/1724] loss: 0.434, ave_loss: 0.585
[80]  [1580/1724] loss: 0.588, ave_loss: 0.585
[81]  [1600/1724] loss: 0.450, ave_loss: 0.584
[82]  [1620/1724] loss: 0.607, ave_loss: 0.584
[83]  [1640/1724] loss: 0.609, ave_loss: 0.584
[84]  [1660/1724] loss: 0.645, ave_loss: 0.585
[85]  [1680/1724] loss: 0.465, ave_loss: 0.584
[86]  [1700/1724] loss: 0.499, ave_loss: 0.583
[87]  [1720/1724] loss: 0.613, ave_loss: 0.583
[88]  [1740/1724] loss: 0.540, ave_loss: 0.582

Finished Training finishing at 2021-08-25 08:05:02.500652
printing_out epoch  23.48027842227378 learning rate: 0.0005153561248318907
0.00025577455042449505
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.824e-01
Validation Loss: 5.240e-01
Validation ROC: 0.6655
No improvement, still saving model
75.51972157772622 epochs left to go

Training Epoch 23.48027842227378/100 starting at 2021-08-25 08:17:01.221768
[1]  [0/1724] loss: 0.624, ave_loss: 0.624
[2]  [20/1724] loss: 0.548, ave_loss: 0.586
[3]  [40/1724] loss: 0.569, ave_loss: 0.580
[4]  [60/1724] loss: 0.560, ave_loss: 0.575
[5]  [80/1724] loss: 0.635, ave_loss: 0.587
[6]  [100/1724] loss: 0.620, ave_loss: 0.593
[7]  [120/1724] loss: 0.532, ave_loss: 0.584
[8]  [140/1724] loss: 0.685, ave_loss: 0.597
[9]  [160/1724] loss: 0.533, ave_loss: 0.589
[10]  [180/1724] loss: 0.511, ave_loss: 0.582
[11]  [200/1724] loss: 0.497, ave_loss: 0.574
[12]  [220/1724] loss: 0.607, ave_loss: 0.577
[13]  [240/1724] loss: 0.576, ave_loss: 0.577
[14]  [260/1724] loss: 0.552, ave_loss: 0.575
[15]  [280/1724] loss: 0.668, ave_loss: 0.581
[16]  [300/1724] loss: 0.491, ave_loss: 0.575
[17]  [320/1724] loss: 0.517, ave_loss: 0.572
[18]  [340/1724] loss: 0.600, ave_loss: 0.574
[19]  [360/1724] loss: 0.547, ave_loss: 0.572
[20]  [380/1724] loss: 0.512, ave_loss: 0.569
[21]  [400/1724] loss: 0.601, ave_loss: 0.571
[22]  [420/1724] loss: 0.691, ave_loss: 0.576
[23]  [440/1724] loss: 0.628, ave_loss: 0.578
[24]  [460/1724] loss: 0.771, ave_loss: 0.586
[25]  [480/1724] loss: 0.595, ave_loss: 0.587
[26]  [500/1724] loss: 0.572, ave_loss: 0.586
[27]  [520/1724] loss: 0.556, ave_loss: 0.585
[28]  [540/1724] loss: 0.566, ave_loss: 0.584
[29]  [560/1724] loss: 0.647, ave_loss: 0.587
[30]  [580/1724] loss: 0.685, ave_loss: 0.590
[31]  [600/1724] loss: 0.619, ave_loss: 0.591
[32]  [620/1724] loss: 0.518, ave_loss: 0.589
[33]  [640/1724] loss: 0.544, ave_loss: 0.587
[34]  [660/1724] loss: 0.507, ave_loss: 0.585
[35]  [680/1724] loss: 0.655, ave_loss: 0.587
[36]  [700/1724] loss: 0.527, ave_loss: 0.585
[37]  [720/1724] loss: 0.523, ave_loss: 0.584
[38]  [740/1724] loss: 0.702, ave_loss: 0.587
[39]  [760/1724] loss: 0.526, ave_loss: 0.585
[40]  [780/1724] loss: 0.499, ave_loss: 0.583
[41]  [800/1724] loss: 0.524, ave_loss: 0.582
[42]  [820/1724] loss: 0.593, ave_loss: 0.582
[43]  [840/1724] loss: 0.526, ave_loss: 0.580
[44]  [860/1724] loss: 0.634, ave_loss: 0.582
[45]  [880/1724] loss: 0.710, ave_loss: 0.585
[46]  [900/1724] loss: 0.603, ave_loss: 0.585
[47]  [920/1724] loss: 0.515, ave_loss: 0.583
[48]  [940/1724] loss: 0.584, ave_loss: 0.583
[49]  [960/1724] loss: 0.713, ave_loss: 0.586
[50]  [980/1724] loss: 0.544, ave_loss: 0.585
[51]  [1000/1724] loss: 0.625, ave_loss: 0.586
[52]  [1020/1724] loss: 0.559, ave_loss: 0.586
[53]  [1040/1724] loss: 0.577, ave_loss: 0.585
[54]  [1060/1724] loss: 0.548, ave_loss: 0.585
[55]  [1080/1724] loss: 0.576, ave_loss: 0.585
[56]  [1100/1724] loss: 0.619, ave_loss: 0.585
[57]  [1120/1724] loss: 0.541, ave_loss: 0.584
[58]  [1140/1724] loss: 0.507, ave_loss: 0.583
[59]  [1160/1724] loss: 0.522, ave_loss: 0.582
[60]  [1180/1724] loss: 0.578, ave_loss: 0.582
[61]  [1200/1724] loss: 0.579, ave_loss: 0.582
[62]  [1220/1724] loss: 0.646, ave_loss: 0.583
[63]  [1240/1724] loss: 0.643, ave_loss: 0.584
[64]  [1260/1724] loss: 0.501, ave_loss: 0.583
[65]  [1280/1724] loss: 0.527, ave_loss: 0.582
[66]  [1300/1724] loss: 0.499, ave_loss: 0.580
[67]  [1320/1724] loss: 0.697, ave_loss: 0.582
[68]  [1340/1724] loss: 0.464, ave_loss: 0.580
[69]  [1360/1724] loss: 0.481, ave_loss: 0.579
[70]  [1380/1724] loss: 0.719, ave_loss: 0.581
[71]  [1400/1724] loss: 0.506, ave_loss: 0.580
[72]  [1420/1724] loss: 0.540, ave_loss: 0.579
[73]  [1440/1724] loss: 0.503, ave_loss: 0.578
[74]  [1460/1724] loss: 0.694, ave_loss: 0.580
[75]  [1480/1724] loss: 0.539, ave_loss: 0.579
[76]  [1500/1724] loss: 0.520, ave_loss: 0.579
[77]  [1520/1724] loss: 0.656, ave_loss: 0.580
[78]  [1540/1724] loss: 0.585, ave_loss: 0.580
[79]  [1560/1724] loss: 0.561, ave_loss: 0.579
[80]  [1580/1724] loss: 0.622, ave_loss: 0.580
[81]  [1600/1724] loss: 0.470, ave_loss: 0.579
[82]  [1620/1724] loss: 0.466, ave_loss: 0.577
[83]  [1640/1724] loss: 0.637, ave_loss: 0.578
[84]  [1660/1724] loss: 0.530, ave_loss: 0.577
[85]  [1680/1724] loss: 0.441, ave_loss: 0.576
[86]  [1700/1724] loss: 0.530, ave_loss: 0.575
[87]  [1720/1724] loss: 0.603, ave_loss: 0.576
[88]  [1740/1724] loss: 0.518, ave_loss: 0.575

Finished Training finishing at 2021-08-25 08:31:23.938923
printing_out epoch  24.501160092807424 learning rate: 0.0005153561248318907
0.0002481013139117602
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.749e-01
Validation Loss: 5.110e-01
Validation ROC: 0.6708
No improvement, still saving model
74.49883990719258 epochs left to go

Training Epoch 24.501160092807424/100 starting at 2021-08-25 08:43:11.459007
[1]  [0/1724] loss: 0.796, ave_loss: 0.796
[2]  [20/1724] loss: 0.524, ave_loss: 0.660
[3]  [40/1724] loss: 0.566, ave_loss: 0.629
[4]  [60/1724] loss: 0.552, ave_loss: 0.609
[5]  [80/1724] loss: 0.615, ave_loss: 0.611
[6]  [100/1724] loss: 0.483, ave_loss: 0.589
[7]  [120/1724] loss: 0.626, ave_loss: 0.594
[8]  [140/1724] loss: 0.771, ave_loss: 0.617
[9]  [160/1724] loss: 0.599, ave_loss: 0.615
[10]  [180/1724] loss: 0.666, ave_loss: 0.620
[11]  [200/1724] loss: 0.631, ave_loss: 0.621
[12]  [220/1724] loss: 0.561, ave_loss: 0.616
[13]  [240/1724] loss: 0.478, ave_loss: 0.605
[14]  [260/1724] loss: 0.581, ave_loss: 0.603
[15]  [280/1724] loss: 0.578, ave_loss: 0.602
[16]  [300/1724] loss: 0.580, ave_loss: 0.600
[17]  [320/1724] loss: 0.506, ave_loss: 0.595
[18]  [340/1724] loss: 0.522, ave_loss: 0.591
[19]  [360/1724] loss: 0.525, ave_loss: 0.587
[20]  [380/1724] loss: 0.820, ave_loss: 0.599
[21]  [400/1724] loss: 0.609, ave_loss: 0.599
[22]  [420/1724] loss: 0.714, ave_loss: 0.605
[23]  [440/1724] loss: 0.528, ave_loss: 0.601
[24]  [460/1724] loss: 0.612, ave_loss: 0.602
[25]  [480/1724] loss: 0.509, ave_loss: 0.598
[26]  [500/1724] loss: 0.528, ave_loss: 0.595
[27]  [520/1724] loss: 0.704, ave_loss: 0.599
[28]  [540/1724] loss: 0.636, ave_loss: 0.601
[29]  [560/1724] loss: 0.660, ave_loss: 0.603
[30]  [580/1724] loss: 0.607, ave_loss: 0.603
[31]  [600/1724] loss: 0.496, ave_loss: 0.599
[32]  [620/1724] loss: 0.599, ave_loss: 0.599
[33]  [640/1724] loss: 0.544, ave_loss: 0.598
[34]  [660/1724] loss: 0.585, ave_loss: 0.597
[35]  [680/1724] loss: 0.533, ave_loss: 0.596
[36]  [700/1724] loss: 0.626, ave_loss: 0.596
[37]  [720/1724] loss: 0.659, ave_loss: 0.598
[38]  [740/1724] loss: 0.592, ave_loss: 0.598
[39]  [760/1724] loss: 0.535, ave_loss: 0.596
[40]  [780/1724] loss: 0.590, ave_loss: 0.596
[41]  [800/1724] loss: 0.513, ave_loss: 0.594
[42]  [820/1724] loss: 0.511, ave_loss: 0.592
[43]  [840/1724] loss: 0.633, ave_loss: 0.593
[44]  [860/1724] loss: 0.554, ave_loss: 0.592
[45]  [880/1724] loss: 0.588, ave_loss: 0.592
[46]  [900/1724] loss: 0.541, ave_loss: 0.591
[47]  [920/1724] loss: 0.488, ave_loss: 0.589
[48]  [940/1724] loss: 0.585, ave_loss: 0.589
[49]  [960/1724] loss: 0.500, ave_loss: 0.587
[50]  [980/1724] loss: 0.806, ave_loss: 0.591
[51]  [1000/1724] loss: 0.612, ave_loss: 0.592
[52]  [1020/1724] loss: 0.591, ave_loss: 0.592
[53]  [1040/1724] loss: 0.731, ave_loss: 0.594
[54]  [1060/1724] loss: 0.519, ave_loss: 0.593
[55]  [1080/1724] loss: 0.497, ave_loss: 0.591
[56]  [1100/1724] loss: 0.612, ave_loss: 0.592
[57]  [1120/1724] loss: 0.637, ave_loss: 0.592
[58]  [1140/1724] loss: 0.611, ave_loss: 0.593
[59]  [1160/1724] loss: 0.603, ave_loss: 0.593
[60]  [1180/1724] loss: 0.605, ave_loss: 0.593
[61]  [1200/1724] loss: 0.554, ave_loss: 0.592
[62]  [1220/1724] loss: 0.453, ave_loss: 0.590
[63]  [1240/1724] loss: 0.519, ave_loss: 0.589
[64]  [1260/1724] loss: 0.530, ave_loss: 0.588
[65]  [1280/1724] loss: 0.577, ave_loss: 0.588
[66]  [1300/1724] loss: 0.581, ave_loss: 0.588
[67]  [1320/1724] loss: 0.607, ave_loss: 0.588
[68]  [1340/1724] loss: 0.590, ave_loss: 0.588
[69]  [1360/1724] loss: 0.651, ave_loss: 0.589
[70]  [1380/1724] loss: 0.601, ave_loss: 0.589
[71]  [1400/1724] loss: 0.586, ave_loss: 0.589
[72]  [1420/1724] loss: 0.564, ave_loss: 0.589
[73]  [1440/1724] loss: 0.557, ave_loss: 0.588
[74]  [1460/1724] loss: 0.715, ave_loss: 0.590
[75]  [1480/1724] loss: 0.520, ave_loss: 0.589
[76]  [1500/1724] loss: 0.640, ave_loss: 0.590
[77]  [1520/1724] loss: 0.491, ave_loss: 0.589
[78]  [1540/1724] loss: 0.579, ave_loss: 0.588
[79]  [1560/1724] loss: 0.663, ave_loss: 0.589
[80]  [1580/1724] loss: 0.652, ave_loss: 0.590
[81]  [1600/1724] loss: 0.538, ave_loss: 0.590
[82]  [1620/1724] loss: 0.626, ave_loss: 0.590
[83]  [1640/1724] loss: 0.617, ave_loss: 0.590
[84]  [1660/1724] loss: 0.648, ave_loss: 0.591
[85]  [1680/1724] loss: 0.675, ave_loss: 0.592
[86]  [1700/1724] loss: 0.541, ave_loss: 0.591
[87]  [1720/1724] loss: 0.618, ave_loss: 0.592
[88]  [1740/1724] loss: 0.566, ave_loss: 0.591

Finished Training finishing at 2021-08-25 08:57:34.381255
printing_out epoch  25.52204176334107 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.914e-01
Validation Loss: 5.695e-01
Validation ROC: 0.6615
No improvement, still saving model
73.47795823665894 epochs left to go

Training Epoch 25.52204176334107/100 starting at 2021-08-25 09:09:29.790990
[1]  [0/1724] loss: 0.583, ave_loss: 0.583
[2]  [20/1724] loss: 0.578, ave_loss: 0.581
[3]  [40/1724] loss: 0.641, ave_loss: 0.601
[4]  [60/1724] loss: 0.585, ave_loss: 0.597
[5]  [80/1724] loss: 0.571, ave_loss: 0.592
[6]  [100/1724] loss: 0.551, ave_loss: 0.585
[7]  [120/1724] loss: 0.608, ave_loss: 0.588
[8]  [140/1724] loss: 0.597, ave_loss: 0.589
[9]  [160/1724] loss: 0.576, ave_loss: 0.588
[10]  [180/1724] loss: 0.557, ave_loss: 0.585
[11]  [200/1724] loss: 0.484, ave_loss: 0.576
[12]  [220/1724] loss: 0.442, ave_loss: 0.565
[13]  [240/1724] loss: 0.593, ave_loss: 0.567
[14]  [260/1724] loss: 0.624, ave_loss: 0.571
[15]  [280/1724] loss: 0.549, ave_loss: 0.569
[16]  [300/1724] loss: 0.653, ave_loss: 0.575
[17]  [320/1724] loss: 0.581, ave_loss: 0.575
[18]  [340/1724] loss: 0.605, ave_loss: 0.577
[19]  [360/1724] loss: 0.540, ave_loss: 0.575
[20]  [380/1724] loss: 0.490, ave_loss: 0.570
[21]  [400/1724] loss: 0.627, ave_loss: 0.573
[22]  [420/1724] loss: 0.709, ave_loss: 0.579
[23]  [440/1724] loss: 0.612, ave_loss: 0.581
[24]  [460/1724] loss: 0.538, ave_loss: 0.579
[25]  [480/1724] loss: 0.680, ave_loss: 0.583
[26]  [500/1724] loss: 0.550, ave_loss: 0.582
[27]  [520/1724] loss: 0.723, ave_loss: 0.587
[28]  [540/1724] loss: 0.668, ave_loss: 0.590
[29]  [560/1724] loss: 0.581, ave_loss: 0.590
[30]  [580/1724] loss: 0.563, ave_loss: 0.589
[31]  [600/1724] loss: 0.558, ave_loss: 0.588
[32]  [620/1724] loss: 0.651, ave_loss: 0.590
[33]  [640/1724] loss: 0.505, ave_loss: 0.587
[34]  [660/1724] loss: 0.649, ave_loss: 0.589
[35]  [680/1724] loss: 0.548, ave_loss: 0.588
[36]  [700/1724] loss: 0.547, ave_loss: 0.587
[37]  [720/1724] loss: 0.580, ave_loss: 0.586
[38]  [740/1724] loss: 0.554, ave_loss: 0.586
[39]  [760/1724] loss: 0.525, ave_loss: 0.584
[40]  [780/1724] loss: 0.618, ave_loss: 0.585
[41]  [800/1724] loss: 0.684, ave_loss: 0.587
[42]  [820/1724] loss: 0.634, ave_loss: 0.588
[43]  [840/1724] loss: 0.567, ave_loss: 0.588
[44]  [860/1724] loss: 0.583, ave_loss: 0.588
[45]  [880/1724] loss: 0.655, ave_loss: 0.589
[46]  [900/1724] loss: 0.557, ave_loss: 0.589
[47]  [920/1724] loss: 0.620, ave_loss: 0.589
[48]  [940/1724] loss: 0.618, ave_loss: 0.590
[49]  [960/1724] loss: 0.602, ave_loss: 0.590
[50]  [980/1724] loss: 0.527, ave_loss: 0.589
[51]  [1000/1724] loss: 0.708, ave_loss: 0.591
[52]  [1020/1724] loss: 0.431, ave_loss: 0.588
[53]  [1040/1724] loss: 0.651, ave_loss: 0.589
[54]  [1060/1724] loss: 0.494, ave_loss: 0.588
[55]  [1080/1724] loss: 0.497, ave_loss: 0.586
[56]  [1100/1724] loss: 0.504, ave_loss: 0.584
[57]  [1120/1724] loss: 0.704, ave_loss: 0.587
[58]  [1140/1724] loss: 0.719, ave_loss: 0.589
[59]  [1160/1724] loss: 0.598, ave_loss: 0.589
[60]  [1180/1724] loss: 0.619, ave_loss: 0.589
[61]  [1200/1724] loss: 0.646, ave_loss: 0.590
[62]  [1220/1724] loss: 0.606, ave_loss: 0.591
[63]  [1240/1724] loss: 0.604, ave_loss: 0.591
[64]  [1260/1724] loss: 0.553, ave_loss: 0.590
[65]  [1280/1724] loss: 0.600, ave_loss: 0.590
[66]  [1300/1724] loss: 0.509, ave_loss: 0.589
[67]  [1320/1724] loss: 0.538, ave_loss: 0.588
[68]  [1340/1724] loss: 0.613, ave_loss: 0.589
[69]  [1360/1724] loss: 0.588, ave_loss: 0.589
[70]  [1380/1724] loss: 0.502, ave_loss: 0.588
[71]  [1400/1724] loss: 0.591, ave_loss: 0.588
[72]  [1420/1724] loss: 0.510, ave_loss: 0.586
[73]  [1440/1724] loss: 0.595, ave_loss: 0.587
[74]  [1460/1724] loss: 0.566, ave_loss: 0.586
[75]  [1480/1724] loss: 0.533, ave_loss: 0.586
[76]  [1500/1724] loss: 0.475, ave_loss: 0.584
[77]  [1520/1724] loss: 0.555, ave_loss: 0.584
[78]  [1540/1724] loss: 0.671, ave_loss: 0.585
[79]  [1560/1724] loss: 0.585, ave_loss: 0.585
[80]  [1580/1724] loss: 0.501, ave_loss: 0.584
[81]  [1600/1724] loss: 0.601, ave_loss: 0.584
[82]  [1620/1724] loss: 0.553, ave_loss: 0.584
[83]  [1640/1724] loss: 0.542, ave_loss: 0.583
[84]  [1660/1724] loss: 0.676, ave_loss: 0.584
[85]  [1680/1724] loss: 0.587, ave_loss: 0.584
[86]  [1700/1724] loss: 0.736, ave_loss: 0.586
[87]  [1720/1724] loss: 0.616, ave_loss: 0.586
[88]  [1740/1724] loss: 0.550, ave_loss: 0.586

Finished Training finishing at 2021-08-25 09:23:49.653346
printing_out epoch  26.54292343387471 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.860e-01
Validation Loss: 5.575e-01
Validation ROC: 0.6505
No improvement, still saving model
72.4570765661253 epochs left to go

Training Epoch 26.54292343387471/100 starting at 2021-08-25 09:35:44.671787
[1]  [0/1724] loss: 0.612, ave_loss: 0.612
[2]  [20/1724] loss: 0.598, ave_loss: 0.605
[3]  [40/1724] loss: 0.727, ave_loss: 0.646
[4]  [60/1724] loss: 0.501, ave_loss: 0.609
[5]  [80/1724] loss: 0.571, ave_loss: 0.602
[6]  [100/1724] loss: 0.587, ave_loss: 0.599
[7]  [120/1724] loss: 0.476, ave_loss: 0.582
[8]  [140/1724] loss: 0.572, ave_loss: 0.580
[9]  [160/1724] loss: 0.556, ave_loss: 0.578
[10]  [180/1724] loss: 0.570, ave_loss: 0.577
[11]  [200/1724] loss: 0.524, ave_loss: 0.572
[12]  [220/1724] loss: 0.544, ave_loss: 0.570
[13]  [240/1724] loss: 0.589, ave_loss: 0.571
[14]  [260/1724] loss: 0.595, ave_loss: 0.573
[15]  [280/1724] loss: 0.517, ave_loss: 0.569
[16]  [300/1724] loss: 0.623, ave_loss: 0.573
[17]  [320/1724] loss: 0.588, ave_loss: 0.573
[18]  [340/1724] loss: 0.479, ave_loss: 0.568
[19]  [360/1724] loss: 0.645, ave_loss: 0.572
[20]  [380/1724] loss: 0.511, ave_loss: 0.569
[21]  [400/1724] loss: 0.588, ave_loss: 0.570
[22]  [420/1724] loss: 0.609, ave_loss: 0.572
[23]  [440/1724] loss: 0.709, ave_loss: 0.578
[24]  [460/1724] loss: 0.516, ave_loss: 0.575
[25]  [480/1724] loss: 0.475, ave_loss: 0.571
[26]  [500/1724] loss: 0.427, ave_loss: 0.566
[27]  [520/1724] loss: 0.494, ave_loss: 0.563
[28]  [540/1724] loss: 0.547, ave_loss: 0.562
[29]  [560/1724] loss: 0.690, ave_loss: 0.567
[30]  [580/1724] loss: 0.518, ave_loss: 0.565
[31]  [600/1724] loss: 0.580, ave_loss: 0.566
[32]  [620/1724] loss: 0.575, ave_loss: 0.566
[33]  [640/1724] loss: 0.619, ave_loss: 0.568
[34]  [660/1724] loss: 0.612, ave_loss: 0.569
[35]  [680/1724] loss: 0.642, ave_loss: 0.571
[36]  [700/1724] loss: 0.707, ave_loss: 0.575
[37]  [720/1724] loss: 0.489, ave_loss: 0.572
[38]  [740/1724] loss: 0.577, ave_loss: 0.573
[39]  [760/1724] loss: 0.602, ave_loss: 0.573
[40]  [780/1724] loss: 0.499, ave_loss: 0.571
[41]  [800/1724] loss: 0.562, ave_loss: 0.571
[42]  [820/1724] loss: 0.578, ave_loss: 0.571
[43]  [840/1724] loss: 0.529, ave_loss: 0.570
[44]  [860/1724] loss: 0.586, ave_loss: 0.571
[45]  [880/1724] loss: 0.539, ave_loss: 0.570
[46]  [900/1724] loss: 0.680, ave_loss: 0.572
[47]  [920/1724] loss: 0.590, ave_loss: 0.573
[48]  [940/1724] loss: 0.519, ave_loss: 0.572
[49]  [960/1724] loss: 0.574, ave_loss: 0.572
[50]  [980/1724] loss: 0.589, ave_loss: 0.572
[51]  [1000/1724] loss: 0.629, ave_loss: 0.573
[52]  [1020/1724] loss: 0.609, ave_loss: 0.574
[53]  [1040/1724] loss: 0.555, ave_loss: 0.574
[54]  [1060/1724] loss: 0.485, ave_loss: 0.572
[55]  [1080/1724] loss: 0.484, ave_loss: 0.570
[56]  [1100/1724] loss: 0.786, ave_loss: 0.574
[57]  [1120/1724] loss: 0.607, ave_loss: 0.575
[58]  [1140/1724] loss: 0.591, ave_loss: 0.575
[59]  [1160/1724] loss: 0.458, ave_loss: 0.573
[60]  [1180/1724] loss: 0.575, ave_loss: 0.573
[61]  [1200/1724] loss: 0.583, ave_loss: 0.573
[62]  [1220/1724] loss: 0.550, ave_loss: 0.573
[63]  [1240/1724] loss: 0.517, ave_loss: 0.572
[64]  [1260/1724] loss: 0.503, ave_loss: 0.571
[65]  [1280/1724] loss: 0.569, ave_loss: 0.571
[66]  [1300/1724] loss: 0.564, ave_loss: 0.571
[67]  [1320/1724] loss: 0.495, ave_loss: 0.570
[68]  [1340/1724] loss: 0.704, ave_loss: 0.572
[69]  [1360/1724] loss: 0.633, ave_loss: 0.572
[70]  [1380/1724] loss: 0.656, ave_loss: 0.574
[71]  [1400/1724] loss: 0.778, ave_loss: 0.577
[72]  [1420/1724] loss: 0.642, ave_loss: 0.577
[73]  [1440/1724] loss: 0.599, ave_loss: 0.578
[74]  [1460/1724] loss: 0.573, ave_loss: 0.578
[75]  [1480/1724] loss: 0.579, ave_loss: 0.578
[76]  [1500/1724] loss: 0.618, ave_loss: 0.578
[77]  [1520/1724] loss: 0.562, ave_loss: 0.578
[78]  [1540/1724] loss: 0.645, ave_loss: 0.579
[79]  [1560/1724] loss: 0.508, ave_loss: 0.578
[80]  [1580/1724] loss: 0.552, ave_loss: 0.578
[81]  [1600/1724] loss: 0.494, ave_loss: 0.577
[82]  [1620/1724] loss: 0.527, ave_loss: 0.576
[83]  [1640/1724] loss: 0.445, ave_loss: 0.574
[84]  [1660/1724] loss: 0.483, ave_loss: 0.573
[85]  [1680/1724] loss: 0.540, ave_loss: 0.573
[86]  [1700/1724] loss: 0.603, ave_loss: 0.573
[87]  [1720/1724] loss: 0.664, ave_loss: 0.574
[88]  [1740/1724] loss: 0.523, ave_loss: 0.574

Finished Training finishing at 2021-08-25 09:50:11.309439
printing_out epoch  27.563805104408353 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.738e-01
Validation Loss: 5.132e-01
Validation ROC: 0.6660
No improvement, still saving model
71.43619489559165 epochs left to go

Training Epoch 27.563805104408353/100 starting at 2021-08-25 10:02:03.137218
[1]  [0/1724] loss: 0.454, ave_loss: 0.454
[2]  [20/1724] loss: 0.606, ave_loss: 0.530
[3]  [40/1724] loss: 0.545, ave_loss: 0.535
[4]  [60/1724] loss: 0.660, ave_loss: 0.566
[5]  [80/1724] loss: 0.392, ave_loss: 0.532
[6]  [100/1724] loss: 0.555, ave_loss: 0.535
[7]  [120/1724] loss: 0.688, ave_loss: 0.557
[8]  [140/1724] loss: 0.577, ave_loss: 0.560
[9]  [160/1724] loss: 0.529, ave_loss: 0.556
[10]  [180/1724] loss: 0.709, ave_loss: 0.571
[11]  [200/1724] loss: 0.711, ave_loss: 0.584
[12]  [220/1724] loss: 0.679, ave_loss: 0.592
[13]  [240/1724] loss: 0.525, ave_loss: 0.587
[14]  [260/1724] loss: 0.711, ave_loss: 0.596
[15]  [280/1724] loss: 0.679, ave_loss: 0.601
[16]  [300/1724] loss: 0.593, ave_loss: 0.601
[17]  [320/1724] loss: 0.529, ave_loss: 0.597
[18]  [340/1724] loss: 0.615, ave_loss: 0.598
[19]  [360/1724] loss: 0.505, ave_loss: 0.593
[20]  [380/1724] loss: 0.614, ave_loss: 0.594
[21]  [400/1724] loss: 0.562, ave_loss: 0.592
[22]  [420/1724] loss: 0.624, ave_loss: 0.594
[23]  [440/1724] loss: 0.637, ave_loss: 0.596
[24]  [460/1724] loss: 0.579, ave_loss: 0.595
[25]  [480/1724] loss: 0.526, ave_loss: 0.592
[26]  [500/1724] loss: 0.539, ave_loss: 0.590
[27]  [520/1724] loss: 0.638, ave_loss: 0.592
[28]  [540/1724] loss: 0.589, ave_loss: 0.592
[29]  [560/1724] loss: 0.575, ave_loss: 0.591
[30]  [580/1724] loss: 0.640, ave_loss: 0.593
[31]  [600/1724] loss: 0.688, ave_loss: 0.596
[32]  [620/1724] loss: 0.506, ave_loss: 0.593
[33]  [640/1724] loss: 0.583, ave_loss: 0.593
[34]  [660/1724] loss: 0.577, ave_loss: 0.592
[35]  [680/1724] loss: 0.649, ave_loss: 0.594
[36]  [700/1724] loss: 0.585, ave_loss: 0.594
[37]  [720/1724] loss: 0.650, ave_loss: 0.595
[38]  [740/1724] loss: 0.637, ave_loss: 0.596
[39]  [760/1724] loss: 0.581, ave_loss: 0.596
[40]  [780/1724] loss: 0.667, ave_loss: 0.598
[41]  [800/1724] loss: 0.566, ave_loss: 0.597
[42]  [820/1724] loss: 0.650, ave_loss: 0.598
[43]  [840/1724] loss: 0.679, ave_loss: 0.600
[44]  [860/1724] loss: 0.467, ave_loss: 0.597
[45]  [880/1724] loss: 0.499, ave_loss: 0.595
[46]  [900/1724] loss: 0.700, ave_loss: 0.597
[47]  [920/1724] loss: 0.591, ave_loss: 0.597
[48]  [940/1724] loss: 0.536, ave_loss: 0.596
[49]  [960/1724] loss: 0.411, ave_loss: 0.592
[50]  [980/1724] loss: 0.552, ave_loss: 0.591
[51]  [1000/1724] loss: 0.614, ave_loss: 0.592
[52]  [1020/1724] loss: 0.592, ave_loss: 0.592
[53]  [1040/1724] loss: 0.505, ave_loss: 0.590
[54]  [1060/1724] loss: 0.546, ave_loss: 0.589
[55]  [1080/1724] loss: 0.585, ave_loss: 0.589
[56]  [1100/1724] loss: 0.590, ave_loss: 0.589
[57]  [1120/1724] loss: 0.505, ave_loss: 0.588
[58]  [1140/1724] loss: 0.702, ave_loss: 0.590
[59]  [1160/1724] loss: 0.515, ave_loss: 0.588
[60]  [1180/1724] loss: 0.530, ave_loss: 0.587
[61]  [1200/1724] loss: 0.587, ave_loss: 0.587
[62]  [1220/1724] loss: 0.598, ave_loss: 0.588
[63]  [1240/1724] loss: 0.533, ave_loss: 0.587
[64]  [1260/1724] loss: 0.487, ave_loss: 0.585
[65]  [1280/1724] loss: 0.788, ave_loss: 0.588
[66]  [1300/1724] loss: 0.501, ave_loss: 0.587
[67]  [1320/1724] loss: 0.563, ave_loss: 0.587
[68]  [1340/1724] loss: 0.546, ave_loss: 0.586
[69]  [1360/1724] loss: 0.571, ave_loss: 0.586
[70]  [1380/1724] loss: 0.541, ave_loss: 0.585
[71]  [1400/1724] loss: 0.663, ave_loss: 0.586
[72]  [1420/1724] loss: 0.486, ave_loss: 0.585
[73]  [1440/1724] loss: 0.529, ave_loss: 0.584
[74]  [1460/1724] loss: 0.608, ave_loss: 0.584
[75]  [1480/1724] loss: 0.563, ave_loss: 0.584
[76]  [1500/1724] loss: 0.514, ave_loss: 0.583
[77]  [1520/1724] loss: 0.708, ave_loss: 0.585
[78]  [1540/1724] loss: 0.677, ave_loss: 0.586
[79]  [1560/1724] loss: 0.481, ave_loss: 0.585
[80]  [1580/1724] loss: 0.527, ave_loss: 0.584
[81]  [1600/1724] loss: 0.619, ave_loss: 0.584
[82]  [1620/1724] loss: 0.652, ave_loss: 0.585
[83]  [1640/1724] loss: 0.602, ave_loss: 0.585
[84]  [1660/1724] loss: 0.524, ave_loss: 0.585
[85]  [1680/1724] loss: 0.527, ave_loss: 0.584
[86]  [1700/1724] loss: 0.708, ave_loss: 0.585
[87]  [1720/1724] loss: 0.620, ave_loss: 0.586
[88]  [1740/1724] loss: 0.609, ave_loss: 0.586

Finished Training finishing at 2021-08-25 10:16:25.052006
printing_out epoch  28.584686774941996 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.861e-01
Validation Loss: 5.420e-01
Validation ROC: 0.6640
No improvement, still saving model
70.415313225058 epochs left to go

Training Epoch 28.584686774941996/100 starting at 2021-08-25 10:28:25.249707
[1]  [0/1724] loss: 0.582, ave_loss: 0.582
[2]  [20/1724] loss: 0.460, ave_loss: 0.521
[3]  [40/1724] loss: 0.522, ave_loss: 0.521
[4]  [60/1724] loss: 0.543, ave_loss: 0.527
[5]  [80/1724] loss: 0.532, ave_loss: 0.528
[6]  [100/1724] loss: 0.487, ave_loss: 0.521
[7]  [120/1724] loss: 0.603, ave_loss: 0.533
