reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
Arguments:  Namespace(channels_spatial=['c16', 'c8', 'c4', 'c2'], channels_temporal=['c2', 'c2', 'c2', 'c2', 'c2'], input_div=1.0, kernel_spatial=4, kernel_temporal=4, linear_sizes=[20, 5], no_scalars=False, subsampling=10, tcn_hidden=2, tcn_layers=4, tcn_type='c')
...done
Training on 1724 shots, testing on 857 shots
Classical convolution with channels  2 16
Classical convolution with channels  16 8
Classical convolution with channels  8 4
Classical convolution with channels  4 2
InputBlock parameters:  14 2 64 ['c16', 'c8', 'c4', 'c2'] 4 [20, 5] 0.08
TCN parameters:  19 1 ['c2', 'c2', 'c2', 'c2', 'c2'] 4 0.08
29 epochs left to go

Training Epoch 0/30 starting at 2021-08-30 17:52:57.000861
[1]  [0/1724] loss: 1.261, ave_loss: 1.261
[2]  [20/1724] loss: 0.846, ave_loss: 1.053
[3]  [40/1724] loss: 0.675, ave_loss: 0.927
[4]  [60/1724] loss: 0.617, ave_loss: 0.850
[5]  [80/1724] loss: 0.764, ave_loss: 0.833
[6]  [100/1724] loss: 0.805, ave_loss: 0.828
[7]  [120/1724] loss: 0.902, ave_loss: 0.839
[8]  [140/1724] loss: 0.703, ave_loss: 0.822
[9]  [160/1724] loss: 0.848, ave_loss: 0.825
[10]  [180/1724] loss: 0.616, ave_loss: 0.804
[11]  [200/1724] loss: 0.620, ave_loss: 0.787
[12]  [220/1724] loss: 0.724, ave_loss: 0.782
[13]  [240/1724] loss: 0.550, ave_loss: 0.764
[14]  [260/1724] loss: 0.709, ave_loss: 0.760
[15]  [280/1724] loss: 0.520, ave_loss: 0.744
[16]  [300/1724] loss: 0.627, ave_loss: 0.737
[17]  [320/1724] loss: 0.853, ave_loss: 0.744
[18]  [340/1724] loss: 0.749, ave_loss: 0.744
[19]  [360/1724] loss: 0.953, ave_loss: 0.755
[20]  [380/1724] loss: 0.759, ave_loss: 0.755
[21]  [400/1724] loss: 0.698, ave_loss: 0.752
[22]  [420/1724] loss: 0.782, ave_loss: 0.754
[23]  [440/1724] loss: 0.638, ave_loss: 0.749
[24]  [460/1724] loss: 0.852, ave_loss: 0.753
[25]  [480/1724] loss: 0.864, ave_loss: 0.757
[26]  [500/1724] loss: 0.738, ave_loss: 0.757
[27]  [520/1724] loss: 0.679, ave_loss: 0.754
[28]  [540/1724] loss: 0.893, ave_loss: 0.759
[29]  [560/1724] loss: 0.613, ave_loss: 0.754
[30]  [580/1724] loss: 0.847, ave_loss: 0.757
[31]  [600/1724] loss: 0.728, ave_loss: 0.756
[32]  [620/1724] loss: 0.802, ave_loss: 0.757
[33]  [640/1724] loss: 0.672, ave_loss: 0.755
[34]  [660/1724] loss: 0.674, ave_loss: 0.752
[35]  [680/1724] loss: 0.861, ave_loss: 0.756
[36]  [700/1724] loss: 0.572, ave_loss: 0.750
[37]  [720/1724] loss: 0.608, ave_loss: 0.747
[38]  [740/1724] loss: 0.907, ave_loss: 0.751
[39]  [760/1724] loss: 0.583, ave_loss: 0.746
[40]  [780/1724] loss: 0.775, ave_loss: 0.747
[41]  [800/1724] loss: 0.475, ave_loss: 0.741
[42]  [820/1724] loss: 0.621, ave_loss: 0.738
[43]  [840/1724] loss: 0.938, ave_loss: 0.742
[44]  [860/1724] loss: 0.791, ave_loss: 0.743
[45]  [880/1724] loss: 0.595, ave_loss: 0.740
[46]  [900/1724] loss: 0.671, ave_loss: 0.739
[47]  [920/1724] loss: 0.618, ave_loss: 0.736
[48]  [940/1724] loss: 0.632, ave_loss: 0.734
[49]  [960/1724] loss: 0.832, ave_loss: 0.736
[50]  [980/1724] loss: 0.605, ave_loss: 0.733
[51]  [1000/1724] loss: 0.688, ave_loss: 0.732
[52]  [1020/1724] loss: 0.645, ave_loss: 0.731
[53]  [1040/1724] loss: 0.603, ave_loss: 0.728
[54]  [1060/1724] loss: 0.611, ave_loss: 0.726
[55]  [1080/1724] loss: 0.792, ave_loss: 0.727
[56]  [1100/1724] loss: 0.704, ave_loss: 0.727
[57]  [1120/1724] loss: 0.726, ave_loss: 0.727
[58]  [1140/1724] loss: 0.686, ave_loss: 0.726
[59]  [1160/1724] loss: 0.589, ave_loss: 0.724
[60]  [1180/1724] loss: 0.637, ave_loss: 0.722
[61]  [1200/1724] loss: 0.618, ave_loss: 0.721
[62]  [1220/1724] loss: 0.668, ave_loss: 0.720
[63]  [1240/1724] loss: 0.613, ave_loss: 0.718
[64]  [1260/1724] loss: 0.546, ave_loss: 0.716
[65]  [1280/1724] loss: 0.559, ave_loss: 0.713
[66]  [1300/1724] loss: 0.623, ave_loss: 0.712
[67]  [1320/1724] loss: 0.539, ave_loss: 0.709
[68]  [1340/1724] loss: 0.850, ave_loss: 0.711
[69]  [1360/1724] loss: 0.701, ave_loss: 0.711
[70]  [1380/1724] loss: 0.628, ave_loss: 0.710
[71]  [1400/1724] loss: 0.591, ave_loss: 0.708
[72]  [1420/1724] loss: 0.717, ave_loss: 0.708
[73]  [1440/1724] loss: 0.624, ave_loss: 0.707
[74]  [1460/1724] loss: 0.702, ave_loss: 0.707
[75]  [1480/1724] loss: 0.676, ave_loss: 0.707
[76]  [1500/1724] loss: 0.584, ave_loss: 0.705
[77]  [1520/1724] loss: 0.604, ave_loss: 0.704
[78]  [1540/1724] loss: 0.715, ave_loss: 0.704
[79]  [1560/1724] loss: 0.619, ave_loss: 0.703
[80]  [1580/1724] loss: 0.606, ave_loss: 0.702
[81]  [1600/1724] loss: 0.661, ave_loss: 0.701
[82]  [1620/1724] loss: 0.623, ave_loss: 0.700
[83]  [1640/1724] loss: 0.678, ave_loss: 0.700
[84]  [1660/1724] loss: 0.575, ave_loss: 0.698
[85]  [1680/1724] loss: 0.689, ave_loss: 0.698
[86]  [1700/1724] loss: 0.581, ave_loss: 0.697
[87]  [1720/1724] loss: 0.745, ave_loss: 0.698
[88]  [1740/1724] loss: 0.700, ave_loss: 0.698

Finished Training finishing at 2021-08-30 17:56:58.638162
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.975e-01
Validation Loss: 6.310e+03
Validation ROC: 0.5901
Saving model
27.979118329466356 epochs left to go

Training Epoch 1.0208816705336428/30 starting at 2021-08-30 17:59:39.485291
[1]  [0/1724] loss: 0.719, ave_loss: 0.719
[2]  [20/1724] loss: 0.645, ave_loss: 0.682
[3]  [40/1724] loss: 0.655, ave_loss: 0.673
[4]  [60/1724] loss: 0.676, ave_loss: 0.674
[5]  [80/1724] loss: 0.663, ave_loss: 0.672
[6]  [100/1724] loss: 0.671, ave_loss: 0.671
[7]  [120/1724] loss: 0.730, ave_loss: 0.680
[8]  [140/1724] loss: 0.566, ave_loss: 0.666
[9]  [160/1724] loss: 0.681, ave_loss: 0.667
[10]  [180/1724] loss: 0.595, ave_loss: 0.660
[11]  [200/1724] loss: 0.546, ave_loss: 0.650
[12]  [220/1724] loss: 0.724, ave_loss: 0.656
[13]  [240/1724] loss: 0.555, ave_loss: 0.648
[14]  [260/1724] loss: 0.694, ave_loss: 0.651
[15]  [280/1724] loss: 0.718, ave_loss: 0.656
[16]  [300/1724] loss: 0.710, ave_loss: 0.659
[17]  [320/1724] loss: 0.818, ave_loss: 0.669
[18]  [340/1724] loss: 0.668, ave_loss: 0.669
[19]  [360/1724] loss: 0.674, ave_loss: 0.669
[20]  [380/1724] loss: 0.665, ave_loss: 0.669
[21]  [400/1724] loss: 0.815, ave_loss: 0.676
[22]  [420/1724] loss: 0.613, ave_loss: 0.673
[23]  [440/1724] loss: 0.724, ave_loss: 0.675
[24]  [460/1724] loss: 0.651, ave_loss: 0.674
[25]  [480/1724] loss: 0.573, ave_loss: 0.670
[26]  [500/1724] loss: 0.824, ave_loss: 0.676
[27]  [520/1724] loss: 0.726, ave_loss: 0.678
[28]  [540/1724] loss: 0.790, ave_loss: 0.682
[29]  [560/1724] loss: 0.597, ave_loss: 0.679
[30]  [580/1724] loss: 0.652, ave_loss: 0.678
[31]  [600/1724] loss: 0.559, ave_loss: 0.674
[32]  [620/1724] loss: 0.684, ave_loss: 0.674
[33]  [640/1724] loss: 0.710, ave_loss: 0.675
[34]  [660/1724] loss: 0.730, ave_loss: 0.677
[35]  [680/1724] loss: 0.693, ave_loss: 0.678
[36]  [700/1724] loss: 0.634, ave_loss: 0.676
[37]  [720/1724] loss: 0.675, ave_loss: 0.676
[38]  [740/1724] loss: 0.657, ave_loss: 0.676
[39]  [760/1724] loss: 0.649, ave_loss: 0.675
[40]  [780/1724] loss: 0.720, ave_loss: 0.676
[41]  [800/1724] loss: 0.676, ave_loss: 0.676
[42]  [820/1724] loss: 0.648, ave_loss: 0.676
[43]  [840/1724] loss: 0.574, ave_loss: 0.673
[44]  [860/1724] loss: 0.671, ave_loss: 0.673
[45]  [880/1724] loss: 0.703, ave_loss: 0.674
[46]  [900/1724] loss: 0.517, ave_loss: 0.670
[47]  [920/1724] loss: 0.667, ave_loss: 0.670
[48]  [940/1724] loss: 0.646, ave_loss: 0.670
[49]  [960/1724] loss: 0.821, ave_loss: 0.673
[50]  [980/1724] loss: 0.527, ave_loss: 0.670
[51]  [1000/1724] loss: 0.586, ave_loss: 0.668
[52]  [1020/1724] loss: 0.526, ave_loss: 0.666
[53]  [1040/1724] loss: 0.509, ave_loss: 0.663
[54]  [1060/1724] loss: 0.628, ave_loss: 0.662
[55]  [1080/1724] loss: 0.673, ave_loss: 0.662
[56]  [1100/1724] loss: 0.617, ave_loss: 0.661
[57]  [1120/1724] loss: 0.667, ave_loss: 0.661
[58]  [1140/1724] loss: 0.801, ave_loss: 0.664
[59]  [1160/1724] loss: 0.533, ave_loss: 0.662
[60]  [1180/1724] loss: 0.718, ave_loss: 0.663
[61]  [1200/1724] loss: 0.522, ave_loss: 0.660
[62]  [1220/1724] loss: 0.664, ave_loss: 0.660
[63]  [1240/1724] loss: 0.595, ave_loss: 0.659
[64]  [1260/1724] loss: 0.641, ave_loss: 0.659
[65]  [1280/1724] loss: 0.636, ave_loss: 0.659
[66]  [1300/1724] loss: 0.877, ave_loss: 0.662
[67]  [1320/1724] loss: 0.825, ave_loss: 0.664
[68]  [1340/1724] loss: 0.646, ave_loss: 0.664
[69]  [1360/1724] loss: 0.589, ave_loss: 0.663
[70]  [1380/1724] loss: 0.511, ave_loss: 0.661
[71]  [1400/1724] loss: 0.654, ave_loss: 0.661
[72]  [1420/1724] loss: 0.635, ave_loss: 0.660
[73]  [1440/1724] loss: 0.670, ave_loss: 0.661
[74]  [1460/1724] loss: 0.617, ave_loss: 0.660
[75]  [1480/1724] loss: 0.730, ave_loss: 0.661
[76]  [1500/1724] loss: 0.584, ave_loss: 0.660
[77]  [1520/1724] loss: 0.698, ave_loss: 0.660
[78]  [1540/1724] loss: 0.588, ave_loss: 0.659
[79]  [1560/1724] loss: 0.662, ave_loss: 0.660
[80]  [1580/1724] loss: 0.653, ave_loss: 0.659
[81]  [1600/1724] loss: 0.656, ave_loss: 0.659
[82]  [1620/1724] loss: 0.640, ave_loss: 0.659
[83]  [1640/1724] loss: 0.713, ave_loss: 0.660
[84]  [1660/1724] loss: 0.622, ave_loss: 0.659
[85]  [1680/1724] loss: 0.681, ave_loss: 0.660
[86]  [1700/1724] loss: 0.662, ave_loss: 0.660
[87]  [1720/1724] loss: 0.770, ave_loss: 0.661
[88]  [1740/1724] loss: 0.532, ave_loss: 0.659

Finished Training finishing at 2021-08-30 18:01:56.030808
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.594e-01
Validation Loss: 6.072e+03
Validation ROC: 0.4744
No improvement, still saving model
26.958236658932716 epochs left to go

Training Epoch 2.0417633410672855/30 starting at 2021-08-30 18:02:40.386239
[1]  [0/1724] loss: 0.578, ave_loss: 0.578
[2]  [20/1724] loss: 0.644, ave_loss: 0.611
[3]  [40/1724] loss: 0.524, ave_loss: 0.582
[4]  [60/1724] loss: 0.660, ave_loss: 0.601
[5]  [80/1724] loss: 0.648, ave_loss: 0.611
[6]  [100/1724] loss: 0.513, ave_loss: 0.594
[7]  [120/1724] loss: 0.706, ave_loss: 0.610
[8]  [140/1724] loss: 0.604, ave_loss: 0.610
[9]  [160/1724] loss: 0.545, ave_loss: 0.602
[10]  [180/1724] loss: 0.643, ave_loss: 0.607
[11]  [200/1724] loss: 0.515, ave_loss: 0.598
[12]  [220/1724] loss: 0.687, ave_loss: 0.606
[13]  [240/1724] loss: 0.691, ave_loss: 0.612
[14]  [260/1724] loss: 0.591, ave_loss: 0.611
[15]  [280/1724] loss: 0.609, ave_loss: 0.611
[16]  [300/1724] loss: 0.631, ave_loss: 0.612
[17]  [320/1724] loss: 0.549, ave_loss: 0.608
[18]  [340/1724] loss: 0.647, ave_loss: 0.610
[19]  [360/1724] loss: 0.660, ave_loss: 0.613
[20]  [380/1724] loss: 0.688, ave_loss: 0.617
[21]  [400/1724] loss: 0.655, ave_loss: 0.618
[22]  [420/1724] loss: 0.669, ave_loss: 0.621
[23]  [440/1724] loss: 0.607, ave_loss: 0.620
[24]  [460/1724] loss: 0.546, ave_loss: 0.617
[25]  [480/1724] loss: 0.558, ave_loss: 0.615
[26]  [500/1724] loss: 0.539, ave_loss: 0.612
[27]  [520/1724] loss: 0.604, ave_loss: 0.612
[28]  [540/1724] loss: 0.579, ave_loss: 0.610
[29]  [560/1724] loss: 0.600, ave_loss: 0.610
[30]  [580/1724] loss: 0.688, ave_loss: 0.613
[31]  [600/1724] loss: 0.659, ave_loss: 0.614
[32]  [620/1724] loss: 0.740, ave_loss: 0.618
[33]  [640/1724] loss: 0.602, ave_loss: 0.618
[34]  [660/1724] loss: 0.641, ave_loss: 0.618
[35]  [680/1724] loss: 0.754, ave_loss: 0.622
[36]  [700/1724] loss: 0.731, ave_loss: 0.625
[37]  [720/1724] loss: 0.616, ave_loss: 0.625
[38]  [740/1724] loss: 0.675, ave_loss: 0.626
[39]  [760/1724] loss: 0.579, ave_loss: 0.625
[40]  [780/1724] loss: 0.600, ave_loss: 0.624
[41]  [800/1724] loss: 0.534, ave_loss: 0.622
[42]  [820/1724] loss: 0.546, ave_loss: 0.620
[43]  [840/1724] loss: 0.649, ave_loss: 0.621
[44]  [860/1724] loss: 0.836, ave_loss: 0.626
[45]  [880/1724] loss: 0.582, ave_loss: 0.625
[46]  [900/1724] loss: 0.579, ave_loss: 0.624
[47]  [920/1724] loss: 0.629, ave_loss: 0.624
[48]  [940/1724] loss: 0.643, ave_loss: 0.624
[49]  [960/1724] loss: 0.660, ave_loss: 0.625
[50]  [980/1724] loss: 0.595, ave_loss: 0.625
[51]  [1000/1724] loss: 0.642, ave_loss: 0.625
[52]  [1020/1724] loss: 0.694, ave_loss: 0.626
[53]  [1040/1724] loss: 0.804, ave_loss: 0.630
[54]  [1060/1724] loss: 0.696, ave_loss: 0.631
[55]  [1080/1724] loss: 0.651, ave_loss: 0.631
[56]  [1100/1724] loss: 0.650, ave_loss: 0.632
[57]  [1120/1724] loss: 0.576, ave_loss: 0.631
[58]  [1140/1724] loss: 0.668, ave_loss: 0.631
[59]  [1160/1724] loss: 0.575, ave_loss: 0.630
[60]  [1180/1724] loss: 0.678, ave_loss: 0.631
[61]  [1200/1724] loss: 0.636, ave_loss: 0.631
[62]  [1220/1724] loss: 0.573, ave_loss: 0.630
[63]  [1240/1724] loss: 0.692, ave_loss: 0.631
[64]  [1260/1724] loss: 0.665, ave_loss: 0.632
[65]  [1280/1724] loss: 0.574, ave_loss: 0.631
[66]  [1300/1724] loss: 0.722, ave_loss: 0.632
[67]  [1320/1724] loss: 0.574, ave_loss: 0.631
[68]  [1340/1724] loss: 0.573, ave_loss: 0.630
[69]  [1360/1724] loss: 0.662, ave_loss: 0.631
[70]  [1380/1724] loss: 0.530, ave_loss: 0.629
[71]  [1400/1724] loss: 0.703, ave_loss: 0.631
[72]  [1420/1724] loss: 0.644, ave_loss: 0.631
[73]  [1440/1724] loss: 0.564, ave_loss: 0.630
[74]  [1460/1724] loss: 0.616, ave_loss: 0.630
[75]  [1480/1724] loss: 0.594, ave_loss: 0.629
[76]  [1500/1724] loss: 0.645, ave_loss: 0.629
[77]  [1520/1724] loss: 0.608, ave_loss: 0.629
[78]  [1540/1724] loss: 0.582, ave_loss: 0.628
[79]  [1560/1724] loss: 0.731, ave_loss: 0.630
[80]  [1580/1724] loss: 0.642, ave_loss: 0.630
[81]  [1600/1724] loss: 0.713, ave_loss: 0.631
[82]  [1620/1724] loss: 0.589, ave_loss: 0.630
[83]  [1640/1724] loss: 0.671, ave_loss: 0.631
[84]  [1660/1724] loss: 0.744, ave_loss: 0.632
[85]  [1680/1724] loss: 0.626, ave_loss: 0.632
[86]  [1700/1724] loss: 0.715, ave_loss: 0.633
[87]  [1720/1724] loss: 0.879, ave_loss: 0.636
[88]  [1740/1724] loss: 0.659, ave_loss: 0.636

Finished Training finishing at 2021-08-30 18:04:39.895564
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.362e-01
Validation Loss: 6.094e+03
Validation ROC: 0.4590
No improvement, still saving model
25.937354988399072 epochs left to go

Training Epoch 3.062645011600928/30 starting at 2021-08-30 18:05:17.666257
[1]  [0/1724] loss: 0.639, ave_loss: 0.639
[2]  [20/1724] loss: 0.615, ave_loss: 0.627
[3]  [40/1724] loss: 0.604, ave_loss: 0.619
[4]  [60/1724] loss: 0.839, ave_loss: 0.674
[5]  [80/1724] loss: 0.561, ave_loss: 0.651
[6]  [100/1724] loss: 0.571, ave_loss: 0.638
[7]  [120/1724] loss: 0.590, ave_loss: 0.631
[8]  [140/1724] loss: 0.599, ave_loss: 0.627
[9]  [160/1724] loss: 0.730, ave_loss: 0.639
[10]  [180/1724] loss: 0.737, ave_loss: 0.648
[11]  [200/1724] loss: 0.641, ave_loss: 0.648
[12]  [220/1724] loss: 0.749, ave_loss: 0.656
[13]  [240/1724] loss: 0.812, ave_loss: 0.668
[14]  [260/1724] loss: 0.753, ave_loss: 0.674
[15]  [280/1724] loss: 0.808, ave_loss: 0.683
[16]  [300/1724] loss: 0.731, ave_loss: 0.686
[17]  [320/1724] loss: 0.633, ave_loss: 0.683
[18]  [340/1724] loss: 0.481, ave_loss: 0.672
[19]  [360/1724] loss: 0.685, ave_loss: 0.673
[20]  [380/1724] loss: 0.704, ave_loss: 0.674
[21]  [400/1724] loss: 0.636, ave_loss: 0.672
[22]  [420/1724] loss: 0.614, ave_loss: 0.670
[23]  [440/1724] loss: 0.652, ave_loss: 0.669
[24]  [460/1724] loss: 0.718, ave_loss: 0.671
[25]  [480/1724] loss: 0.650, ave_loss: 0.670
[26]  [500/1724] loss: 0.734, ave_loss: 0.673
[27]  [520/1724] loss: 0.596, ave_loss: 0.670
[28]  [540/1724] loss: 0.682, ave_loss: 0.670
[29]  [560/1724] loss: 0.834, ave_loss: 0.676
[30]  [580/1724] loss: 0.631, ave_loss: 0.674
[31]  [600/1724] loss: 0.639, ave_loss: 0.673
[32]  [620/1724] loss: 0.628, ave_loss: 0.672
[33]  [640/1724] loss: 0.583, ave_loss: 0.669
[34]  [660/1724] loss: 0.562, ave_loss: 0.666
[35]  [680/1724] loss: 0.524, ave_loss: 0.662
[36]  [700/1724] loss: 0.696, ave_loss: 0.663
[37]  [720/1724] loss: 0.577, ave_loss: 0.661
[38]  [740/1724] loss: 0.653, ave_loss: 0.660
[39]  [760/1724] loss: 0.626, ave_loss: 0.659
[40]  [780/1724] loss: 0.698, ave_loss: 0.660
[41]  [800/1724] loss: 0.611, ave_loss: 0.659
[42]  [820/1724] loss: 0.593, ave_loss: 0.658
[43]  [840/1724] loss: 0.553, ave_loss: 0.655
[44]  [860/1724] loss: 0.772, ave_loss: 0.658
[45]  [880/1724] loss: 0.623, ave_loss: 0.657
[46]  [900/1724] loss: 0.605, ave_loss: 0.656
[47]  [920/1724] loss: 0.650, ave_loss: 0.656
[48]  [940/1724] loss: 0.796, ave_loss: 0.659
[49]  [960/1724] loss: 0.698, ave_loss: 0.660
[50]  [980/1724] loss: 0.511, ave_loss: 0.657
[51]  [1000/1724] loss: 0.629, ave_loss: 0.656
[52]  [1020/1724] loss: 0.613, ave_loss: 0.655
[53]  [1040/1724] loss: 0.559, ave_loss: 0.653
[54]  [1060/1724] loss: 0.539, ave_loss: 0.651
[55]  [1080/1724] loss: 0.511, ave_loss: 0.649
[56]  [1100/1724] loss: 0.537, ave_loss: 0.647
[57]  [1120/1724] loss: 0.743, ave_loss: 0.648
[58]  [1140/1724] loss: 0.457, ave_loss: 0.645
[59]  [1160/1724] loss: 0.618, ave_loss: 0.645
[60]  [1180/1724] loss: 0.768, ave_loss: 0.647
[61]  [1200/1724] loss: 0.609, ave_loss: 0.646
[62]  [1220/1724] loss: 0.641, ave_loss: 0.646
[63]  [1240/1724] loss: 0.677, ave_loss: 0.646
[64]  [1260/1724] loss: 0.653, ave_loss: 0.647
[65]  [1280/1724] loss: 0.650, ave_loss: 0.647
[66]  [1300/1724] loss: 0.695, ave_loss: 0.647
[67]  [1320/1724] loss: 0.604, ave_loss: 0.647
[68]  [1340/1724] loss: 0.608, ave_loss: 0.646
[69]  [1360/1724] loss: 0.637, ave_loss: 0.646
[70]  [1380/1724] loss: 0.614, ave_loss: 0.646
[71]  [1400/1724] loss: 0.598, ave_loss: 0.645
[72]  [1420/1724] loss: 0.600, ave_loss: 0.644
[73]  [1440/1724] loss: 0.586, ave_loss: 0.643
[74]  [1460/1724] loss: 0.712, ave_loss: 0.644
[75]  [1480/1724] loss: 0.643, ave_loss: 0.644
[76]  [1500/1724] loss: 0.854, ave_loss: 0.647
[77]  [1520/1724] loss: 0.751, ave_loss: 0.648
[78]  [1540/1724] loss: 0.602, ave_loss: 0.648
[79]  [1560/1724] loss: 0.622, ave_loss: 0.648
[80]  [1580/1724] loss: 0.632, ave_loss: 0.647
[81]  [1600/1724] loss: 0.595, ave_loss: 0.647
[82]  [1620/1724] loss: 0.737, ave_loss: 0.648
[83]  [1640/1724] loss: 0.623, ave_loss: 0.647
[84]  [1660/1724] loss: 0.673, ave_loss: 0.648
[85]  [1680/1724] loss: 0.584, ave_loss: 0.647
[86]  [1700/1724] loss: 0.560, ave_loss: 0.646
[87]  [1720/1724] loss: 0.620, ave_loss: 0.646
[88]  [1740/1724] loss: 0.499, ave_loss: 0.644

Finished Training finishing at 2021-08-30 18:07:00.282820
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.441e-01
Validation Loss: 6.041e+03
Validation ROC: 0.4611
No improvement, still saving model
24.916473317865428 epochs left to go

Training Epoch 4.083526682134571/30 starting at 2021-08-30 18:07:37.709415
[1]  [0/1724] loss: 0.652, ave_loss: 0.652
[2]  [20/1724] loss: 0.819, ave_loss: 0.735
[3]  [40/1724] loss: 0.529, ave_loss: 0.667
[4]  [60/1724] loss: 0.518, ave_loss: 0.629
[5]  [80/1724] loss: 0.721, ave_loss: 0.648
[6]  [100/1724] loss: 0.623, ave_loss: 0.644
[7]  [120/1724] loss: 0.666, ave_loss: 0.647
[8]  [140/1724] loss: 0.571, ave_loss: 0.637
[9]  [160/1724] loss: 0.607, ave_loss: 0.634
[10]  [180/1724] loss: 0.593, ave_loss: 0.630
[11]  [200/1724] loss: 0.618, ave_loss: 0.629
[12]  [220/1724] loss: 0.693, ave_loss: 0.634
[13]  [240/1724] loss: 0.611, ave_loss: 0.632
[14]  [260/1724] loss: 0.571, ave_loss: 0.628
[15]  [280/1724] loss: 0.637, ave_loss: 0.629
[16]  [300/1724] loss: 0.686, ave_loss: 0.632
[17]  [320/1724] loss: 0.647, ave_loss: 0.633
[18]  [340/1724] loss: 0.703, ave_loss: 0.637
[19]  [360/1724] loss: 0.679, ave_loss: 0.639
[20]  [380/1724] loss: 0.721, ave_loss: 0.643
[21]  [400/1724] loss: 0.748, ave_loss: 0.648
[22]  [420/1724] loss: 0.662, ave_loss: 0.649
[23]  [440/1724] loss: 0.642, ave_loss: 0.649
[24]  [460/1724] loss: 0.831, ave_loss: 0.656
[25]  [480/1724] loss: 0.745, ave_loss: 0.660
[26]  [500/1724] loss: 0.575, ave_loss: 0.656
[27]  [520/1724] loss: 0.585, ave_loss: 0.654
[28]  [540/1724] loss: 0.639, ave_loss: 0.653
[29]  [560/1724] loss: 0.756, ave_loss: 0.657
[30]  [580/1724] loss: 0.544, ave_loss: 0.653
[31]  [600/1724] loss: 0.728, ave_loss: 0.656
[32]  [620/1724] loss: 0.505, ave_loss: 0.651
[33]  [640/1724] loss: 0.549, ave_loss: 0.648
[34]  [660/1724] loss: 0.605, ave_loss: 0.646
[35]  [680/1724] loss: 0.624, ave_loss: 0.646
[36]  [700/1724] loss: 0.687, ave_loss: 0.647
[37]  [720/1724] loss: 0.661, ave_loss: 0.647
[38]  [740/1724] loss: 0.610, ave_loss: 0.646
[39]  [760/1724] loss: 0.765, ave_loss: 0.649
[40]  [780/1724] loss: 0.703, ave_loss: 0.651
[41]  [800/1724] loss: 0.580, ave_loss: 0.649
[42]  [820/1724] loss: 0.621, ave_loss: 0.648
[43]  [840/1724] loss: 0.600, ave_loss: 0.647
[44]  [860/1724] loss: 0.616, ave_loss: 0.647
[45]  [880/1724] loss: 0.687, ave_loss: 0.647
[46]  [900/1724] loss: 0.555, ave_loss: 0.645
[47]  [920/1724] loss: 0.480, ave_loss: 0.642
[48]  [940/1724] loss: 0.708, ave_loss: 0.643
[49]  [960/1724] loss: 0.599, ave_loss: 0.642
[50]  [980/1724] loss: 0.537, ave_loss: 0.640
[51]  [1000/1724] loss: 0.625, ave_loss: 0.640
[52]  [1020/1724] loss: 0.697, ave_loss: 0.641
[53]  [1040/1724] loss: 0.660, ave_loss: 0.641
[54]  [1060/1724] loss: 0.695, ave_loss: 0.642
[55]  [1080/1724] loss: 0.548, ave_loss: 0.641
[56]  [1100/1724] loss: 0.620, ave_loss: 0.640
[57]  [1120/1724] loss: 0.535, ave_loss: 0.638
[58]  [1140/1724] loss: 0.546, ave_loss: 0.637
[59]  [1160/1724] loss: 0.656, ave_loss: 0.637
[60]  [1180/1724] loss: 0.785, ave_loss: 0.640
[61]  [1200/1724] loss: 0.563, ave_loss: 0.638
[62]  [1220/1724] loss: 0.615, ave_loss: 0.638
[63]  [1240/1724] loss: 0.763, ave_loss: 0.640
[64]  [1260/1724] loss: 0.640, ave_loss: 0.640
[65]  [1280/1724] loss: 0.582, ave_loss: 0.639
[66]  [1300/1724] loss: 0.685, ave_loss: 0.640
[67]  [1320/1724] loss: 0.802, ave_loss: 0.642
[68]  [1340/1724] loss: 0.640, ave_loss: 0.642
[69]  [1360/1724] loss: 0.532, ave_loss: 0.641
[70]  [1380/1724] loss: 0.571, ave_loss: 0.640
[71]  [1400/1724] loss: 0.610, ave_loss: 0.639
[72]  [1420/1724] loss: 0.660, ave_loss: 0.639
[73]  [1440/1724] loss: 0.570, ave_loss: 0.639
[74]  [1460/1724] loss: 0.727, ave_loss: 0.640
[75]  [1480/1724] loss: 0.556, ave_loss: 0.639
[76]  [1500/1724] loss: 0.620, ave_loss: 0.638
[77]  [1520/1724] loss: 0.645, ave_loss: 0.638
[78]  [1540/1724] loss: 0.589, ave_loss: 0.638
[79]  [1560/1724] loss: 0.638, ave_loss: 0.638
[80]  [1580/1724] loss: 0.602, ave_loss: 0.637
[81]  [1600/1724] loss: 0.653, ave_loss: 0.638
[82]  [1620/1724] loss: 0.551, ave_loss: 0.636
[83]  [1640/1724] loss: 0.705, ave_loss: 0.637
[84]  [1660/1724] loss: 0.614, ave_loss: 0.637
[85]  [1680/1724] loss: 0.634, ave_loss: 0.637
[86]  [1700/1724] loss: 0.653, ave_loss: 0.637
[87]  [1720/1724] loss: 0.528, ave_loss: 0.636
[88]  [1740/1724] loss: 0.679, ave_loss: 0.636

Finished Training finishing at 2021-08-30 18:09:09.477393
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.364e-01
Validation Loss: 6.067e+03
Validation ROC: 0.4618
No improvement, still saving model
23.895591647331788 epochs left to go

Training Epoch 5.104408352668213/30 starting at 2021-08-30 18:09:42.996282
[1]  [0/1724] loss: 0.748, ave_loss: 0.748
[2]  [20/1724] loss: 0.469, ave_loss: 0.608
[3]  [40/1724] loss: 0.628, ave_loss: 0.615
[4]  [60/1724] loss: 0.664, ave_loss: 0.627
[5]  [80/1724] loss: 0.663, ave_loss: 0.635
[6]  [100/1724] loss: 0.780, ave_loss: 0.659
[7]  [120/1724] loss: 0.654, ave_loss: 0.658
[8]  [140/1724] loss: 0.667, ave_loss: 0.659
[9]  [160/1724] loss: 0.593, ave_loss: 0.652
[10]  [180/1724] loss: 0.712, ave_loss: 0.658
[11]  [200/1724] loss: 0.645, ave_loss: 0.657
[12]  [220/1724] loss: 0.583, ave_loss: 0.651
[13]  [240/1724] loss: 0.637, ave_loss: 0.650
[14]  [260/1724] loss: 0.648, ave_loss: 0.649
[15]  [280/1724] loss: 0.509, ave_loss: 0.640
[16]  [300/1724] loss: 0.556, ave_loss: 0.635
[17]  [320/1724] loss: 0.636, ave_loss: 0.635
[18]  [340/1724] loss: 0.886, ave_loss: 0.649
[19]  [360/1724] loss: 0.588, ave_loss: 0.646
[20]  [380/1724] loss: 0.613, ave_loss: 0.644
[21]  [400/1724] loss: 0.640, ave_loss: 0.644
[22]  [420/1724] loss: 0.774, ave_loss: 0.650
[23]  [440/1724] loss: 0.745, ave_loss: 0.654
[24]  [460/1724] loss: 0.538, ave_loss: 0.649
[25]  [480/1724] loss: 0.757, ave_loss: 0.653
[26]  [500/1724] loss: 0.873, ave_loss: 0.662
[27]  [520/1724] loss: 0.811, ave_loss: 0.667
[28]  [540/1724] loss: 0.617, ave_loss: 0.666
[29]  [560/1724] loss: 0.787, ave_loss: 0.670
[30]  [580/1724] loss: 0.737, ave_loss: 0.672
[31]  [600/1724] loss: 0.594, ave_loss: 0.669
[32]  [620/1724] loss: 0.584, ave_loss: 0.667
[33]  [640/1724] loss: 0.610, ave_loss: 0.665
[34]  [660/1724] loss: 0.698, ave_loss: 0.666
[35]  [680/1724] loss: 0.548, ave_loss: 0.663
[36]  [700/1724] loss: 0.615, ave_loss: 0.661
[37]  [720/1724] loss: 0.711, ave_loss: 0.663
[38]  [740/1724] loss: 0.644, ave_loss: 0.662
[39]  [760/1724] loss: 0.709, ave_loss: 0.663
[40]  [780/1724] loss: 0.680, ave_loss: 0.664
[41]  [800/1724] loss: 0.583, ave_loss: 0.662
[42]  [820/1724] loss: 0.703, ave_loss: 0.663
[43]  [840/1724] loss: 0.657, ave_loss: 0.663
[44]  [860/1724] loss: 0.698, ave_loss: 0.663
[45]  [880/1724] loss: 0.514, ave_loss: 0.660
[46]  [900/1724] loss: 0.746, ave_loss: 0.662
[47]  [920/1724] loss: 0.738, ave_loss: 0.664
[48]  [940/1724] loss: 0.581, ave_loss: 0.662
[49]  [960/1724] loss: 0.648, ave_loss: 0.662
[50]  [980/1724] loss: 0.640, ave_loss: 0.661
[51]  [1000/1724] loss: 0.585, ave_loss: 0.660
[52]  [1020/1724] loss: 0.524, ave_loss: 0.657
[53]  [1040/1724] loss: 0.505, ave_loss: 0.654
[54]  [1060/1724] loss: 0.698, ave_loss: 0.655
[55]  [1080/1724] loss: 0.600, ave_loss: 0.654
[56]  [1100/1724] loss: 0.660, ave_loss: 0.654
[57]  [1120/1724] loss: 0.620, ave_loss: 0.653
[58]  [1140/1724] loss: 0.667, ave_loss: 0.654
[59]  [1160/1724] loss: 0.720, ave_loss: 0.655
[60]  [1180/1724] loss: 0.526, ave_loss: 0.653
[61]  [1200/1724] loss: 0.557, ave_loss: 0.651
[62]  [1220/1724] loss: 0.731, ave_loss: 0.652
[63]  [1240/1724] loss: 0.614, ave_loss: 0.652
[64]  [1260/1724] loss: 0.646, ave_loss: 0.652
[65]  [1280/1724] loss: 0.696, ave_loss: 0.652
[66]  [1300/1724] loss: 0.579, ave_loss: 0.651
[67]  [1320/1724] loss: 0.615, ave_loss: 0.651
[68]  [1340/1724] loss: 0.594, ave_loss: 0.650
[69]  [1360/1724] loss: 0.625, ave_loss: 0.650
[70]  [1380/1724] loss: 0.635, ave_loss: 0.649
[71]  [1400/1724] loss: 0.653, ave_loss: 0.649
[72]  [1420/1724] loss: 0.605, ave_loss: 0.649
[73]  [1440/1724] loss: 0.558, ave_loss: 0.648
[74]  [1460/1724] loss: 0.570, ave_loss: 0.647
[75]  [1480/1724] loss: 0.775, ave_loss: 0.648
[76]  [1500/1724] loss: 0.635, ave_loss: 0.648
[77]  [1520/1724] loss: 0.634, ave_loss: 0.648
[78]  [1540/1724] loss: 0.630, ave_loss: 0.648
[79]  [1560/1724] loss: 0.587, ave_loss: 0.647
[80]  [1580/1724] loss: 0.686, ave_loss: 0.647
[81]  [1600/1724] loss: 0.678, ave_loss: 0.648
[82]  [1620/1724] loss: 0.675, ave_loss: 0.648
[83]  [1640/1724] loss: 0.628, ave_loss: 0.648
[84]  [1660/1724] loss: 0.691, ave_loss: 0.648
[85]  [1680/1724] loss: 0.577, ave_loss: 0.647
[86]  [1700/1724] loss: 0.705, ave_loss: 0.648
[87]  [1720/1724] loss: 0.616, ave_loss: 0.648
[88]  [1740/1724] loss: 0.764, ave_loss: 0.649

Finished Training finishing at 2021-08-30 18:11:13.053819
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.491e-01
Validation Loss: 5.923e+03
Validation ROC: 0.4618
No improvement, still saving model
22.874709976798144 epochs left to go

Training Epoch 6.125290023201856/30 starting at 2021-08-30 18:11:45.916250
[1]  [0/1724] loss: 0.711, ave_loss: 0.711
[2]  [20/1724] loss: 0.533, ave_loss: 0.622
[3]  [40/1724] loss: 0.656, ave_loss: 0.633
[4]  [60/1724] loss: 0.726, ave_loss: 0.656
[5]  [80/1724] loss: 0.784, ave_loss: 0.682
[6]  [100/1724] loss: 0.635, ave_loss: 0.674
[7]  [120/1724] loss: 0.592, ave_loss: 0.662
[8]  [140/1724] loss: 0.569, ave_loss: 0.651
[9]  [160/1724] loss: 0.719, ave_loss: 0.658
[10]  [180/1724] loss: 0.648, ave_loss: 0.657
[11]  [200/1724] loss: 0.643, ave_loss: 0.656
[12]  [220/1724] loss: 0.509, ave_loss: 0.644
[13]  [240/1724] loss: 0.607, ave_loss: 0.641
[14]  [260/1724] loss: 0.665, ave_loss: 0.643
[15]  [280/1724] loss: 0.713, ave_loss: 0.647
[16]  [300/1724] loss: 0.514, ave_loss: 0.639
[17]  [320/1724] loss: 0.471, ave_loss: 0.629
[18]  [340/1724] loss: 0.616, ave_loss: 0.628
[19]  [360/1724] loss: 0.685, ave_loss: 0.631
[20]  [380/1724] loss: 0.599, ave_loss: 0.630
[21]  [400/1724] loss: 0.618, ave_loss: 0.629
[22]  [420/1724] loss: 0.582, ave_loss: 0.627
[23]  [440/1724] loss: 0.621, ave_loss: 0.627
[24]  [460/1724] loss: 0.606, ave_loss: 0.626
[25]  [480/1724] loss: 0.641, ave_loss: 0.626
[26]  [500/1724] loss: 0.630, ave_loss: 0.627
[27]  [520/1724] loss: 0.565, ave_loss: 0.624
[28]  [540/1724] loss: 0.798, ave_loss: 0.631
[29]  [560/1724] loss: 0.625, ave_loss: 0.630
[30]  [580/1724] loss: 0.619, ave_loss: 0.630
[31]  [600/1724] loss: 0.569, ave_loss: 0.628
[32]  [620/1724] loss: 0.593, ave_loss: 0.627
[33]  [640/1724] loss: 0.637, ave_loss: 0.627
[34]  [660/1724] loss: 0.620, ave_loss: 0.627
[35]  [680/1724] loss: 0.571, ave_loss: 0.625
[36]  [700/1724] loss: 0.871, ave_loss: 0.632
[37]  [720/1724] loss: 0.696, ave_loss: 0.634
[38]  [740/1724] loss: 0.638, ave_loss: 0.634
[39]  [760/1724] loss: 0.521, ave_loss: 0.631
[40]  [780/1724] loss: 0.698, ave_loss: 0.633
[41]  [800/1724] loss: 0.595, ave_loss: 0.632
[42]  [820/1724] loss: 0.653, ave_loss: 0.632
[43]  [840/1724] loss: 0.658, ave_loss: 0.633
[44]  [860/1724] loss: 0.664, ave_loss: 0.634
[45]  [880/1724] loss: 0.891, ave_loss: 0.639
[46]  [900/1724] loss: 0.697, ave_loss: 0.641
[47]  [920/1724] loss: 0.688, ave_loss: 0.642
[48]  [940/1724] loss: 0.691, ave_loss: 0.643
[49]  [960/1724] loss: 0.666, ave_loss: 0.643
[50]  [980/1724] loss: 0.674, ave_loss: 0.644
[51]  [1000/1724] loss: 0.592, ave_loss: 0.643
[52]  [1020/1724] loss: 0.644, ave_loss: 0.643
[53]  [1040/1724] loss: 0.576, ave_loss: 0.641
[54]  [1060/1724] loss: 0.615, ave_loss: 0.641
[55]  [1080/1724] loss: 0.572, ave_loss: 0.640
[56]  [1100/1724] loss: 0.674, ave_loss: 0.640
[57]  [1120/1724] loss: 0.554, ave_loss: 0.639
[58]  [1140/1724] loss: 0.807, ave_loss: 0.642
[59]  [1160/1724] loss: 0.604, ave_loss: 0.641
[60]  [1180/1724] loss: 0.522, ave_loss: 0.639
[61]  [1200/1724] loss: 0.669, ave_loss: 0.640
[62]  [1220/1724] loss: 0.694, ave_loss: 0.640
[63]  [1240/1724] loss: 0.549, ave_loss: 0.639
[64]  [1260/1724] loss: 0.661, ave_loss: 0.639
[65]  [1280/1724] loss: 0.688, ave_loss: 0.640
[66]  [1300/1724] loss: 0.725, ave_loss: 0.641
[67]  [1320/1724] loss: 0.536, ave_loss: 0.640
[68]  [1340/1724] loss: 0.692, ave_loss: 0.641
[69]  [1360/1724] loss: 0.605, ave_loss: 0.640
[70]  [1380/1724] loss: 0.566, ave_loss: 0.639
[71]  [1400/1724] loss: 0.618, ave_loss: 0.639
[72]  [1420/1724] loss: 0.572, ave_loss: 0.638
[73]  [1440/1724] loss: 0.608, ave_loss: 0.637
[74]  [1460/1724] loss: 0.802, ave_loss: 0.640
[75]  [1480/1724] loss: 0.779, ave_loss: 0.641
[76]  [1500/1724] loss: 0.751, ave_loss: 0.643
[77]  [1520/1724] loss: 0.685, ave_loss: 0.643
[78]  [1540/1724] loss: 0.703, ave_loss: 0.644
[79]  [1560/1724] loss: 0.656, ave_loss: 0.644
[80]  [1580/1724] loss: 0.590, ave_loss: 0.644
[81]  [1600/1724] loss: 0.686, ave_loss: 0.644
[82]  [1620/1724] loss: 0.590, ave_loss: 0.644
[83]  [1640/1724] loss: 0.727, ave_loss: 0.645
[84]  [1660/1724] loss: 0.756, ave_loss: 0.646
[85]  [1680/1724] loss: 0.704, ave_loss: 0.647
[86]  [1700/1724] loss: 0.703, ave_loss: 0.647
[87]  [1720/1724] loss: 0.533, ave_loss: 0.646
[88]  [1740/1724] loss: 0.613, ave_loss: 0.646

Finished Training finishing at 2021-08-30 18:13:15.677894
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.456e-01
Validation Loss: 5.880e+03
Validation ROC: 0.4618
No improvement, still saving model
21.8538283062645 epochs left to go

Training Epoch 7.146171693735499/30 starting at 2021-08-30 18:13:52.031234
[1]  [0/1724] loss: 0.587, ave_loss: 0.587
[2]  [20/1724] loss: 0.482, ave_loss: 0.535
[3]  [40/1724] loss: 0.838, ave_loss: 0.636
[4]  [60/1724] loss: 0.529, ave_loss: 0.609
[5]  [80/1724] loss: 0.597, ave_loss: 0.607
[6]  [100/1724] loss: 0.678, ave_loss: 0.619
[7]  [120/1724] loss: 0.613, ave_loss: 0.618
[8]  [140/1724] loss: 0.680, ave_loss: 0.626
[9]  [160/1724] loss: 0.668, ave_loss: 0.630
[10]  [180/1724] loss: 0.682, ave_loss: 0.635
[11]  [200/1724] loss: 0.603, ave_loss: 0.633
[12]  [220/1724] loss: 0.695, ave_loss: 0.638
[13]  [240/1724] loss: 0.735, ave_loss: 0.645
[14]  [260/1724] loss: 0.732, ave_loss: 0.651
[15]  [280/1724] loss: 0.771, ave_loss: 0.659
[16]  [300/1724] loss: 0.565, ave_loss: 0.653
[17]  [320/1724] loss: 0.783, ave_loss: 0.661
[18]  [340/1724] loss: 0.755, ave_loss: 0.666
[19]  [360/1724] loss: 0.676, ave_loss: 0.667
[20]  [380/1724] loss: 0.640, ave_loss: 0.666
[21]  [400/1724] loss: 0.599, ave_loss: 0.662
[22]  [420/1724] loss: 0.660, ave_loss: 0.662
[23]  [440/1724] loss: 0.700, ave_loss: 0.664
[24]  [460/1724] loss: 0.632, ave_loss: 0.663
[25]  [480/1724] loss: 0.704, ave_loss: 0.664
[26]  [500/1724] loss: 0.619, ave_loss: 0.662
[27]  [520/1724] loss: 0.721, ave_loss: 0.665
[28]  [540/1724] loss: 0.539, ave_loss: 0.660
[29]  [560/1724] loss: 0.637, ave_loss: 0.659
[30]  [580/1724] loss: 0.642, ave_loss: 0.659
[31]  [600/1724] loss: 0.733, ave_loss: 0.661
[32]  [620/1724] loss: 0.806, ave_loss: 0.666
[33]  [640/1724] loss: 0.669, ave_loss: 0.666
[34]  [660/1724] loss: 0.668, ave_loss: 0.666
[35]  [680/1724] loss: 0.610, ave_loss: 0.664
[36]  [700/1724] loss: 0.580, ave_loss: 0.662
[37]  [720/1724] loss: 0.658, ave_loss: 0.662
[38]  [740/1724] loss: 0.694, ave_loss: 0.663
[39]  [760/1724] loss: 0.614, ave_loss: 0.661
[40]  [780/1724] loss: 0.687, ave_loss: 0.662
[41]  [800/1724] loss: 0.722, ave_loss: 0.664
[42]  [820/1724] loss: 0.608, ave_loss: 0.662
[43]  [840/1724] loss: 0.587, ave_loss: 0.660
[44]  [860/1724] loss: 0.595, ave_loss: 0.659
[45]  [880/1724] loss: 0.609, ave_loss: 0.658
[46]  [900/1724] loss: 0.643, ave_loss: 0.658
[47]  [920/1724] loss: 0.569, ave_loss: 0.656
[48]  [940/1724] loss: 0.672, ave_loss: 0.656
[49]  [960/1724] loss: 0.847, ave_loss: 0.660
[50]  [980/1724] loss: 0.618, ave_loss: 0.659
[51]  [1000/1724] loss: 0.573, ave_loss: 0.657
[52]  [1020/1724] loss: 0.694, ave_loss: 0.658
[53]  [1040/1724] loss: 0.632, ave_loss: 0.658
[54]  [1060/1724] loss: 0.649, ave_loss: 0.657
[55]  [1080/1724] loss: 0.742, ave_loss: 0.659
[56]  [1100/1724] loss: 0.577, ave_loss: 0.658
[57]  [1120/1724] loss: 0.797, ave_loss: 0.660
[58]  [1140/1724] loss: 0.644, ave_loss: 0.660
[59]  [1160/1724] loss: 0.794, ave_loss: 0.662
[60]  [1180/1724] loss: 0.699, ave_loss: 0.663
[61]  [1200/1724] loss: 0.728, ave_loss: 0.664
[62]  [1220/1724] loss: 0.661, ave_loss: 0.664
[63]  [1240/1724] loss: 0.595, ave_loss: 0.663
[64]  [1260/1724] loss: 0.572, ave_loss: 0.661
[65]  [1280/1724] loss: 0.614, ave_loss: 0.660
[66]  [1300/1724] loss: 0.572, ave_loss: 0.659
[67]  [1320/1724] loss: 0.752, ave_loss: 0.660
[68]  [1340/1724] loss: 0.512, ave_loss: 0.658
[69]  [1360/1724] loss: 0.624, ave_loss: 0.658
[70]  [1380/1724] loss: 0.596, ave_loss: 0.657
[71]  [1400/1724] loss: 0.623, ave_loss: 0.656
[72]  [1420/1724] loss: 0.624, ave_loss: 0.656
[73]  [1440/1724] loss: 0.590, ave_loss: 0.655
[74]  [1460/1724] loss: 0.650, ave_loss: 0.655
[75]  [1480/1724] loss: 0.638, ave_loss: 0.655
[76]  [1500/1724] loss: 0.612, ave_loss: 0.654
[77]  [1520/1724] loss: 0.638, ave_loss: 0.654
[78]  [1540/1724] loss: 0.600, ave_loss: 0.653
[79]  [1560/1724] loss: 0.654, ave_loss: 0.653
[80]  [1580/1724] loss: 0.586, ave_loss: 0.652
[81]  [1600/1724] loss: 0.576, ave_loss: 0.652
[82]  [1620/1724] loss: 0.570, ave_loss: 0.651
[83]  [1640/1724] loss: 0.644, ave_loss: 0.650
[84]  [1660/1724] loss: 0.658, ave_loss: 0.651
[85]  [1680/1724] loss: 0.637, ave_loss: 0.650
[86]  [1700/1724] loss: 0.539, ave_loss: 0.649
[87]  [1720/1724] loss: 0.615, ave_loss: 0.649
[88]  [1740/1724] loss: 0.578, ave_loss: 0.648

Finished Training finishing at 2021-08-30 18:15:21.527226
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.479e-01
Validation Loss: 5.911e+03
Validation ROC: 0.4618
No improvement, still saving model
20.832946635730856 epochs left to go

Training Epoch 8.167053364269142/30 starting at 2021-08-30 18:16:00.903269
[1]  [0/1724] loss: 0.554, ave_loss: 0.554
[2]  [20/1724] loss: 0.639, ave_loss: 0.596
[3]  [40/1724] loss: 0.646, ave_loss: 0.613
[4]  [60/1724] loss: 0.642, ave_loss: 0.620
[5]  [80/1724] loss: 0.710, ave_loss: 0.638
[6]  [100/1724] loss: 0.707, ave_loss: 0.650
[7]  [120/1724] loss: 0.529, ave_loss: 0.632
[8]  [140/1724] loss: 0.667, ave_loss: 0.637
[9]  [160/1724] loss: 0.752, ave_loss: 0.649
[10]  [180/1724] loss: 0.589, ave_loss: 0.643
[11]  [200/1724] loss: 0.648, ave_loss: 0.644
[12]  [220/1724] loss: 0.626, ave_loss: 0.642
[13]  [240/1724] loss: 0.666, ave_loss: 0.644
[14]  [260/1724] loss: 0.599, ave_loss: 0.641
[15]  [280/1724] loss: 0.620, ave_loss: 0.640
[16]  [300/1724] loss: 0.676, ave_loss: 0.642
[17]  [320/1724] loss: 0.611, ave_loss: 0.640
[18]  [340/1724] loss: 0.764, ave_loss: 0.647
[19]  [360/1724] loss: 0.534, ave_loss: 0.641
[20]  [380/1724] loss: 0.692, ave_loss: 0.643
[21]  [400/1724] loss: 0.603, ave_loss: 0.642
[22]  [420/1724] loss: 0.574, ave_loss: 0.638
[23]  [440/1724] loss: 0.664, ave_loss: 0.640
[24]  [460/1724] loss: 0.758, ave_loss: 0.644
[25]  [480/1724] loss: 0.627, ave_loss: 0.644
[26]  [500/1724] loss: 0.573, ave_loss: 0.641
[27]  [520/1724] loss: 0.650, ave_loss: 0.641
[28]  [540/1724] loss: 0.606, ave_loss: 0.640
[29]  [560/1724] loss: 0.746, ave_loss: 0.644
[30]  [580/1724] loss: 0.498, ave_loss: 0.639
[31]  [600/1724] loss: 0.735, ave_loss: 0.642
[32]  [620/1724] loss: 0.617, ave_loss: 0.641
[33]  [640/1724] loss: 0.619, ave_loss: 0.641
[34]  [660/1724] loss: 0.610, ave_loss: 0.640
[35]  [680/1724] loss: 0.674, ave_loss: 0.641
[36]  [700/1724] loss: 0.805, ave_loss: 0.645
[37]  [720/1724] loss: 0.691, ave_loss: 0.647
[38]  [740/1724] loss: 0.789, ave_loss: 0.650
[39]  [760/1724] loss: 0.659, ave_loss: 0.651
[40]  [780/1724] loss: 0.582, ave_loss: 0.649
[41]  [800/1724] loss: 0.573, ave_loss: 0.647
[42]  [820/1724] loss: 0.610, ave_loss: 0.646
[43]  [840/1724] loss: 0.692, ave_loss: 0.647
[44]  [860/1724] loss: 0.660, ave_loss: 0.647
[45]  [880/1724] loss: 0.573, ave_loss: 0.646
[46]  [900/1724] loss: 0.573, ave_loss: 0.644
[47]  [920/1724] loss: 0.534, ave_loss: 0.642
[48]  [940/1724] loss: 0.622, ave_loss: 0.641
[49]  [960/1724] loss: 0.611, ave_loss: 0.641
[50]  [980/1724] loss: 0.489, ave_loss: 0.638
[51]  [1000/1724] loss: 0.724, ave_loss: 0.639
[52]  [1020/1724] loss: 0.531, ave_loss: 0.637
[53]  [1040/1724] loss: 0.570, ave_loss: 0.636
[54]  [1060/1724] loss: 0.589, ave_loss: 0.635
[55]  [1080/1724] loss: 0.529, ave_loss: 0.633
[56]  [1100/1724] loss: 0.653, ave_loss: 0.634
[57]  [1120/1724] loss: 0.614, ave_loss: 0.633
[58]  [1140/1724] loss: 0.716, ave_loss: 0.635
[59]  [1160/1724] loss: 0.870, ave_loss: 0.639
[60]  [1180/1724] loss: 0.727, ave_loss: 0.640
[61]  [1200/1724] loss: 0.836, ave_loss: 0.643
[62]  [1220/1724] loss: 0.937, ave_loss: 0.648
[63]  [1240/1724] loss: 0.684, ave_loss: 0.649
[64]  [1260/1724] loss: 0.525, ave_loss: 0.647
[65]  [1280/1724] loss: 0.659, ave_loss: 0.647
[66]  [1300/1724] loss: 0.604, ave_loss: 0.646
[67]  [1320/1724] loss: 0.668, ave_loss: 0.647
[68]  [1340/1724] loss: 0.554, ave_loss: 0.645
[69]  [1360/1724] loss: 0.544, ave_loss: 0.644
[70]  [1380/1724] loss: 0.604, ave_loss: 0.643
[71]  [1400/1724] loss: 0.687, ave_loss: 0.644
[72]  [1420/1724] loss: 0.583, ave_loss: 0.643
[73]  [1440/1724] loss: 0.611, ave_loss: 0.643
[74]  [1460/1724] loss: 0.632, ave_loss: 0.642
[75]  [1480/1724] loss: 0.552, ave_loss: 0.641
[76]  [1500/1724] loss: 0.624, ave_loss: 0.641
[77]  [1520/1724] loss: 0.568, ave_loss: 0.640
[78]  [1540/1724] loss: 0.744, ave_loss: 0.641
[79]  [1560/1724] loss: 0.766, ave_loss: 0.643
[80]  [1580/1724] loss: 0.550, ave_loss: 0.642
[81]  [1600/1724] loss: 0.529, ave_loss: 0.640
[82]  [1620/1724] loss: 0.590, ave_loss: 0.640
[83]  [1640/1724] loss: 0.740, ave_loss: 0.641
[84]  [1660/1724] loss: 0.585, ave_loss: 0.640
[85]  [1680/1724] loss: 0.591, ave_loss: 0.640
[86]  [1700/1724] loss: 0.575, ave_loss: 0.639
[87]  [1720/1724] loss: 0.742, ave_loss: 0.640
[88]  [1740/1724] loss: 0.612, ave_loss: 0.640

Finished Training finishing at 2021-08-30 18:17:26.208862
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.398e-01
Validation Loss: 5.957e+03
Validation ROC: 0.4625
No improvement, still saving model
19.812064965197216 epochs left to go

Training Epoch 9.187935034802784/30 starting at 2021-08-30 18:18:02.997252
[1]  [0/1724] loss: 0.686, ave_loss: 0.686
[2]  [20/1724] loss: 0.626, ave_loss: 0.656
[3]  [40/1724] loss: 0.551, ave_loss: 0.621
[4]  [60/1724] loss: 0.640, ave_loss: 0.626
[5]  [80/1724] loss: 0.611, ave_loss: 0.623
[6]  [100/1724] loss: 0.676, ave_loss: 0.632
[7]  [120/1724] loss: 0.704, ave_loss: 0.642
[8]  [140/1724] loss: 0.718, ave_loss: 0.652
[9]  [160/1724] loss: 0.628, ave_loss: 0.649
[10]  [180/1724] loss: 0.626, ave_loss: 0.647
[11]  [200/1724] loss: 0.589, ave_loss: 0.641
[12]  [220/1724] loss: 0.636, ave_loss: 0.641
[13]  [240/1724] loss: 0.688, ave_loss: 0.645
[14]  [260/1724] loss: 0.759, ave_loss: 0.653
[15]  [280/1724] loss: 0.799, ave_loss: 0.662
[16]  [300/1724] loss: 0.561, ave_loss: 0.656
[17]  [320/1724] loss: 0.616, ave_loss: 0.654
[18]  [340/1724] loss: 0.608, ave_loss: 0.651
[19]  [360/1724] loss: 0.608, ave_loss: 0.649
[20]  [380/1724] loss: 0.629, ave_loss: 0.648
[21]  [400/1724] loss: 0.580, ave_loss: 0.645
[22]  [420/1724] loss: 0.566, ave_loss: 0.641
[23]  [440/1724] loss: 0.584, ave_loss: 0.639
[24]  [460/1724] loss: 0.621, ave_loss: 0.638
[25]  [480/1724] loss: 0.589, ave_loss: 0.636
[26]  [500/1724] loss: 0.636, ave_loss: 0.636
[27]  [520/1724] loss: 0.675, ave_loss: 0.637
[28]  [540/1724] loss: 0.708, ave_loss: 0.640
[29]  [560/1724] loss: 0.560, ave_loss: 0.637
[30]  [580/1724] loss: 0.638, ave_loss: 0.637
[31]  [600/1724] loss: 0.595, ave_loss: 0.636
[32]  [620/1724] loss: 0.870, ave_loss: 0.643
[33]  [640/1724] loss: 0.743, ave_loss: 0.646
[34]  [660/1724] loss: 0.683, ave_loss: 0.647
[35]  [680/1724] loss: 0.562, ave_loss: 0.645
[36]  [700/1724] loss: 0.531, ave_loss: 0.642
[37]  [720/1724] loss: 0.548, ave_loss: 0.639
[38]  [740/1724] loss: 0.669, ave_loss: 0.640
[39]  [760/1724] loss: 0.543, ave_loss: 0.637
[40]  [780/1724] loss: 0.697, ave_loss: 0.639
[41]  [800/1724] loss: 0.563, ave_loss: 0.637
[42]  [820/1724] loss: 0.603, ave_loss: 0.636
[43]  [840/1724] loss: 0.576, ave_loss: 0.635
[44]  [860/1724] loss: 0.586, ave_loss: 0.634
[45]  [880/1724] loss: 0.623, ave_loss: 0.634
[46]  [900/1724] loss: 0.802, ave_loss: 0.637
[47]  [920/1724] loss: 0.629, ave_loss: 0.637
[48]  [940/1724] loss: 0.741, ave_loss: 0.639
[49]  [960/1724] loss: 0.660, ave_loss: 0.640
[50]  [980/1724] loss: 0.599, ave_loss: 0.639
[51]  [1000/1724] loss: 0.782, ave_loss: 0.642
[52]  [1020/1724] loss: 0.669, ave_loss: 0.642
[53]  [1040/1724] loss: 0.656, ave_loss: 0.642
[54]  [1060/1724] loss: 0.752, ave_loss: 0.644
[55]  [1080/1724] loss: 0.522, ave_loss: 0.642
[56]  [1100/1724] loss: 0.626, ave_loss: 0.642
[57]  [1120/1724] loss: 0.598, ave_loss: 0.641
[58]  [1140/1724] loss: 0.627, ave_loss: 0.641
[59]  [1160/1724] loss: 0.704, ave_loss: 0.642
[60]  [1180/1724] loss: 0.675, ave_loss: 0.643
[61]  [1200/1724] loss: 0.631, ave_loss: 0.642
[62]  [1220/1724] loss: 0.587, ave_loss: 0.641
[63]  [1240/1724] loss: 0.740, ave_loss: 0.643
[64]  [1260/1724] loss: 0.607, ave_loss: 0.642
[65]  [1280/1724] loss: 0.782, ave_loss: 0.645
[66]  [1300/1724] loss: 0.730, ave_loss: 0.646
[67]  [1320/1724] loss: 0.672, ave_loss: 0.646
[68]  [1340/1724] loss: 0.725, ave_loss: 0.647
[69]  [1360/1724] loss: 0.583, ave_loss: 0.646
[70]  [1380/1724] loss: 0.623, ave_loss: 0.646
[71]  [1400/1724] loss: 0.536, ave_loss: 0.645
[72]  [1420/1724] loss: 0.616, ave_loss: 0.644
[73]  [1440/1724] loss: 0.677, ave_loss: 0.645
[74]  [1460/1724] loss: 0.636, ave_loss: 0.645
[75]  [1480/1724] loss: 0.656, ave_loss: 0.645
[76]  [1500/1724] loss: 0.443, ave_loss: 0.642
[77]  [1520/1724] loss: 0.668, ave_loss: 0.642
[78]  [1540/1724] loss: 0.655, ave_loss: 0.643
[79]  [1560/1724] loss: 0.588, ave_loss: 0.642
[80]  [1580/1724] loss: 0.834, ave_loss: 0.644
[81]  [1600/1724] loss: 0.668, ave_loss: 0.645
[82]  [1620/1724] loss: 0.549, ave_loss: 0.643
[83]  [1640/1724] loss: 0.602, ave_loss: 0.643
[84]  [1660/1724] loss: 0.600, ave_loss: 0.642
[85]  [1680/1724] loss: 0.664, ave_loss: 0.643
[86]  [1700/1724] loss: 0.602, ave_loss: 0.642
[87]  [1720/1724] loss: 0.682, ave_loss: 0.643
[88]  [1740/1724] loss: 0.695, ave_loss: 0.643

Finished Training finishing at 2021-08-30 18:19:41.025231
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.432e-01
Validation Loss: 5.910e+03
Validation ROC: 0.4625
No improvement, still saving model
18.791183294663576 epochs left to go

Training Epoch 10.208816705336426/30 starting at 2021-08-30 18:20:18.738242
[1]  [0/1724] loss: 0.702, ave_loss: 0.702
[2]  [20/1724] loss: 0.670, ave_loss: 0.686
[3]  [40/1724] loss: 0.540, ave_loss: 0.637
[4]  [60/1724] loss: 0.630, ave_loss: 0.635
[5]  [80/1724] loss: 0.700, ave_loss: 0.648
[6]  [100/1724] loss: 0.613, ave_loss: 0.642
[7]  [120/1724] loss: 0.610, ave_loss: 0.638
[8]  [140/1724] loss: 0.631, ave_loss: 0.637
[9]  [160/1724] loss: 0.675, ave_loss: 0.641
[10]  [180/1724] loss: 0.660, ave_loss: 0.643
[11]  [200/1724] loss: 0.692, ave_loss: 0.648
[12]  [220/1724] loss: 0.694, ave_loss: 0.651
[13]  [240/1724] loss: 0.598, ave_loss: 0.647
[14]  [260/1724] loss: 0.609, ave_loss: 0.645
[15]  [280/1724] loss: 0.575, ave_loss: 0.640
[16]  [300/1724] loss: 0.530, ave_loss: 0.633
[17]  [320/1724] loss: 0.680, ave_loss: 0.636
[18]  [340/1724] loss: 0.806, ave_loss: 0.645
[19]  [360/1724] loss: 0.572, ave_loss: 0.641
[20]  [380/1724] loss: 0.577, ave_loss: 0.638
[21]  [400/1724] loss: 0.726, ave_loss: 0.642
[22]  [420/1724] loss: 0.533, ave_loss: 0.637
[23]  [440/1724] loss: 0.562, ave_loss: 0.634
[24]  [460/1724] loss: 0.740, ave_loss: 0.639
[25]  [480/1724] loss: 0.616, ave_loss: 0.638
[26]  [500/1724] loss: 0.680, ave_loss: 0.639
[27]  [520/1724] loss: 0.797, ave_loss: 0.645
[28]  [540/1724] loss: 0.677, ave_loss: 0.646
[29]  [560/1724] loss: 0.596, ave_loss: 0.645
[30]  [580/1724] loss: 0.647, ave_loss: 0.645
[31]  [600/1724] loss: 0.683, ave_loss: 0.646
[32]  [620/1724] loss: 0.508, ave_loss: 0.642
[33]  [640/1724] loss: 0.542, ave_loss: 0.639
[34]  [660/1724] loss: 0.708, ave_loss: 0.641
[35]  [680/1724] loss: 0.587, ave_loss: 0.639
[36]  [700/1724] loss: 0.594, ave_loss: 0.638
[37]  [720/1724] loss: 0.700, ave_loss: 0.639
[38]  [740/1724] loss: 0.635, ave_loss: 0.639
[39]  [760/1724] loss: 0.561, ave_loss: 0.637
[40]  [780/1724] loss: 0.656, ave_loss: 0.638
[41]  [800/1724] loss: 0.693, ave_loss: 0.639
[42]  [820/1724] loss: 0.625, ave_loss: 0.639
[43]  [840/1724] loss: 0.565, ave_loss: 0.637
[44]  [860/1724] loss: 0.536, ave_loss: 0.635
[45]  [880/1724] loss: 0.759, ave_loss: 0.638
[46]  [900/1724] loss: 0.522, ave_loss: 0.635
[47]  [920/1724] loss: 0.525, ave_loss: 0.633
[48]  [940/1724] loss: 0.679, ave_loss: 0.634
[49]  [960/1724] loss: 0.587, ave_loss: 0.633
[50]  [980/1724] loss: 0.636, ave_loss: 0.633
[51]  [1000/1724] loss: 0.493, ave_loss: 0.630
[52]  [1020/1724] loss: 0.707, ave_loss: 0.632
[53]  [1040/1724] loss: 0.681, ave_loss: 0.632
[54]  [1060/1724] loss: 0.646, ave_loss: 0.633
[55]  [1080/1724] loss: 0.636, ave_loss: 0.633
[56]  [1100/1724] loss: 0.514, ave_loss: 0.631
[57]  [1120/1724] loss: 0.684, ave_loss: 0.632
[58]  [1140/1724] loss: 0.811, ave_loss: 0.635
[59]  [1160/1724] loss: 0.737, ave_loss: 0.636
[60]  [1180/1724] loss: 0.617, ave_loss: 0.636
[61]  [1200/1724] loss: 0.607, ave_loss: 0.636
[62]  [1220/1724] loss: 0.677, ave_loss: 0.636
[63]  [1240/1724] loss: 0.608, ave_loss: 0.636
[64]  [1260/1724] loss: 0.740, ave_loss: 0.637
[65]  [1280/1724] loss: 0.699, ave_loss: 0.638
[66]  [1300/1724] loss: 0.551, ave_loss: 0.637
[67]  [1320/1724] loss: 0.647, ave_loss: 0.637
[68]  [1340/1724] loss: 0.592, ave_loss: 0.637
[69]  [1360/1724] loss: 0.616, ave_loss: 0.636
[70]  [1380/1724] loss: 0.585, ave_loss: 0.636
[71]  [1400/1724] loss: 0.522, ave_loss: 0.634
[72]  [1420/1724] loss: 0.716, ave_loss: 0.635
[73]  [1440/1724] loss: 0.739, ave_loss: 0.636
[74]  [1460/1724] loss: 0.579, ave_loss: 0.636
[75]  [1480/1724] loss: 0.675, ave_loss: 0.636
[76]  [1500/1724] loss: 0.649, ave_loss: 0.636
[77]  [1520/1724] loss: 0.647, ave_loss: 0.637
[78]  [1540/1724] loss: 0.576, ave_loss: 0.636
[79]  [1560/1724] loss: 0.599, ave_loss: 0.635
[80]  [1580/1724] loss: 0.612, ave_loss: 0.635
[81]  [1600/1724] loss: 0.509, ave_loss: 0.633
[82]  [1620/1724] loss: 0.631, ave_loss: 0.633
[83]  [1640/1724] loss: 0.595, ave_loss: 0.633
[84]  [1660/1724] loss: 0.623, ave_loss: 0.633
[85]  [1680/1724] loss: 0.544, ave_loss: 0.632
[86]  [1700/1724] loss: 0.564, ave_loss: 0.631
[87]  [1720/1724] loss: 0.696, ave_loss: 0.632
[88]  [1740/1724] loss: 0.551, ave_loss: 0.631

Finished Training finishing at 2021-08-30 18:21:57.070948
printing_out epoch  11.22969837587007 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.309e-01
Validation Loss: 6.061e+03
Validation ROC: 0.4625
No improvement, still saving model
17.770301624129928 epochs left to go

Training Epoch 11.22969837587007/30 starting at 2021-08-30 18:22:37.669257
[1]  [0/1724] loss: 0.759, ave_loss: 0.759
[2]  [20/1724] loss: 0.697, ave_loss: 0.728
[3]  [40/1724] loss: 0.576, ave_loss: 0.678
[4]  [60/1724] loss: 0.678, ave_loss: 0.678
[5]  [80/1724] loss: 0.555, ave_loss: 0.653
[6]  [100/1724] loss: 0.701, ave_loss: 0.661
[7]  [120/1724] loss: 0.671, ave_loss: 0.663
[8]  [140/1724] loss: 0.697, ave_loss: 0.667
[9]  [160/1724] loss: 0.491, ave_loss: 0.647
[10]  [180/1724] loss: 0.753, ave_loss: 0.658
[11]  [200/1724] loss: 0.586, ave_loss: 0.651
[12]  [220/1724] loss: 0.629, ave_loss: 0.650
[13]  [240/1724] loss: 0.481, ave_loss: 0.637
[14]  [260/1724] loss: 0.661, ave_loss: 0.638
[15]  [280/1724] loss: 0.589, ave_loss: 0.635
[16]  [300/1724] loss: 0.591, ave_loss: 0.632
[17]  [320/1724] loss: 0.926, ave_loss: 0.650
[18]  [340/1724] loss: 0.595, ave_loss: 0.647
[19]  [360/1724] loss: 0.600, ave_loss: 0.644
[20]  [380/1724] loss: 0.590, ave_loss: 0.641
[21]  [400/1724] loss: 0.678, ave_loss: 0.643
[22]  [420/1724] loss: 0.535, ave_loss: 0.638
[23]  [440/1724] loss: 0.615, ave_loss: 0.637
[24]  [460/1724] loss: 0.656, ave_loss: 0.638
[25]  [480/1724] loss: 0.515, ave_loss: 0.633
[26]  [500/1724] loss: 0.657, ave_loss: 0.634
[27]  [520/1724] loss: 0.546, ave_loss: 0.631
[28]  [540/1724] loss: 0.836, ave_loss: 0.638
[29]  [560/1724] loss: 0.630, ave_loss: 0.638
[30]  [580/1724] loss: 0.725, ave_loss: 0.641
[31]  [600/1724] loss: 0.721, ave_loss: 0.643
[32]  [620/1724] loss: 0.545, ave_loss: 0.640
[33]  [640/1724] loss: 0.674, ave_loss: 0.641
[34]  [660/1724] loss: 0.639, ave_loss: 0.641
[35]  [680/1724] loss: 0.651, ave_loss: 0.641
[36]  [700/1724] loss: 0.636, ave_loss: 0.641
[37]  [720/1724] loss: 0.475, ave_loss: 0.637
[38]  [740/1724] loss: 0.690, ave_loss: 0.638
[39]  [760/1724] loss: 0.681, ave_loss: 0.639
[40]  [780/1724] loss: 0.568, ave_loss: 0.637
[41]  [800/1724] loss: 0.581, ave_loss: 0.636
[42]  [820/1724] loss: 0.684, ave_loss: 0.637
[43]  [840/1724] loss: 0.712, ave_loss: 0.639
[44]  [860/1724] loss: 0.790, ave_loss: 0.642
[45]  [880/1724] loss: 0.624, ave_loss: 0.642
[46]  [900/1724] loss: 0.623, ave_loss: 0.642
[47]  [920/1724] loss: 0.758, ave_loss: 0.644
[48]  [940/1724] loss: 0.505, ave_loss: 0.641
[49]  [960/1724] loss: 0.599, ave_loss: 0.640
[50]  [980/1724] loss: 0.671, ave_loss: 0.641
[51]  [1000/1724] loss: 0.660, ave_loss: 0.641
[52]  [1020/1724] loss: 0.476, ave_loss: 0.638
[53]  [1040/1724] loss: 0.612, ave_loss: 0.638
[54]  [1060/1724] loss: 0.675, ave_loss: 0.638
[55]  [1080/1724] loss: 0.748, ave_loss: 0.640
[56]  [1100/1724] loss: 0.598, ave_loss: 0.640
[57]  [1120/1724] loss: 0.659, ave_loss: 0.640
[58]  [1140/1724] loss: 0.605, ave_loss: 0.639
[59]  [1160/1724] loss: 0.513, ave_loss: 0.637
[60]  [1180/1724] loss: 0.588, ave_loss: 0.636
[61]  [1200/1724] loss: 0.610, ave_loss: 0.636
[62]  [1220/1724] loss: 0.616, ave_loss: 0.636
[63]  [1240/1724] loss: 0.759, ave_loss: 0.638
[64]  [1260/1724] loss: 0.838, ave_loss: 0.641
[65]  [1280/1724] loss: 0.660, ave_loss: 0.641
[66]  [1300/1724] loss: 0.594, ave_loss: 0.640
[67]  [1320/1724] loss: 0.653, ave_loss: 0.640
[68]  [1340/1724] loss: 0.609, ave_loss: 0.640
[69]  [1360/1724] loss: 0.554, ave_loss: 0.639
[70]  [1380/1724] loss: 0.612, ave_loss: 0.638
[71]  [1400/1724] loss: 0.595, ave_loss: 0.638
[72]  [1420/1724] loss: 0.593, ave_loss: 0.637
[73]  [1440/1724] loss: 0.598, ave_loss: 0.637
[74]  [1460/1724] loss: 0.531, ave_loss: 0.635
[75]  [1480/1724] loss: 0.612, ave_loss: 0.635
[76]  [1500/1724] loss: 0.596, ave_loss: 0.634
[77]  [1520/1724] loss: 0.526, ave_loss: 0.633
[78]  [1540/1724] loss: 0.507, ave_loss: 0.631
[79]  [1560/1724] loss: 0.539, ave_loss: 0.630
[80]  [1580/1724] loss: 0.669, ave_loss: 0.631
[81]  [1600/1724] loss: 0.635, ave_loss: 0.631
[82]  [1620/1724] loss: 0.678, ave_loss: 0.631
[83]  [1640/1724] loss: 0.644, ave_loss: 0.631
[84]  [1660/1724] loss: 0.770, ave_loss: 0.633
[85]  [1680/1724] loss: 0.624, ave_loss: 0.633
[86]  [1700/1724] loss: 0.833, ave_loss: 0.635
[87]  [1720/1724] loss: 0.638, ave_loss: 0.635
[88]  [1740/1724] loss: 0.741, ave_loss: 0.637

Finished Training finishing at 2021-08-30 18:24:17.951412
printing_out epoch  12.250580046403712 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.365e-01
Validation Loss: 5.988e+03
Validation ROC: 0.4625
No improvement, still saving model
16.749419953596288 epochs left to go

Training Epoch 12.250580046403712/30 starting at 2021-08-30 18:24:57.184240
[1]  [0/1724] loss: 0.491, ave_loss: 0.491
[2]  [20/1724] loss: 0.550, ave_loss: 0.520
[3]  [40/1724] loss: 0.735, ave_loss: 0.592
[4]  [60/1724] loss: 0.620, ave_loss: 0.599
[5]  [80/1724] loss: 0.624, ave_loss: 0.604
[6]  [100/1724] loss: 0.616, ave_loss: 0.606
[7]  [120/1724] loss: 0.660, ave_loss: 0.614
[8]  [140/1724] loss: 0.589, ave_loss: 0.611
[9]  [160/1724] loss: 0.707, ave_loss: 0.621
[10]  [180/1724] loss: 0.736, ave_loss: 0.633
[11]  [200/1724] loss: 0.522, ave_loss: 0.623
[12]  [220/1724] loss: 0.683, ave_loss: 0.628
[13]  [240/1724] loss: 0.656, ave_loss: 0.630
[14]  [260/1724] loss: 0.565, ave_loss: 0.625
[15]  [280/1724] loss: 0.623, ave_loss: 0.625
[16]  [300/1724] loss: 0.620, ave_loss: 0.625
[17]  [320/1724] loss: 0.682, ave_loss: 0.628
[18]  [340/1724] loss: 0.709, ave_loss: 0.633
[19]  [360/1724] loss: 0.864, ave_loss: 0.645
[20]  [380/1724] loss: 0.601, ave_loss: 0.643
[21]  [400/1724] loss: 0.679, ave_loss: 0.644
[22]  [420/1724] loss: 0.527, ave_loss: 0.639
[23]  [440/1724] loss: 0.688, ave_loss: 0.641
[24]  [460/1724] loss: 0.629, ave_loss: 0.641
[25]  [480/1724] loss: 0.580, ave_loss: 0.638
[26]  [500/1724] loss: 0.691, ave_loss: 0.640
[27]  [520/1724] loss: 0.707, ave_loss: 0.643
[28]  [540/1724] loss: 0.562, ave_loss: 0.640
[29]  [560/1724] loss: 0.635, ave_loss: 0.640
[30]  [580/1724] loss: 0.682, ave_loss: 0.641
[31]  [600/1724] loss: 0.573, ave_loss: 0.639
[32]  [620/1724] loss: 0.558, ave_loss: 0.636
[33]  [640/1724] loss: 0.519, ave_loss: 0.633
[34]  [660/1724] loss: 0.513, ave_loss: 0.629
[35]  [680/1724] loss: 0.576, ave_loss: 0.628
[36]  [700/1724] loss: 0.744, ave_loss: 0.631
[37]  [720/1724] loss: 0.524, ave_loss: 0.628
[38]  [740/1724] loss: 0.731, ave_loss: 0.631
[39]  [760/1724] loss: 0.614, ave_loss: 0.630
[40]  [780/1724] loss: 0.668, ave_loss: 0.631
[41]  [800/1724] loss: 0.507, ave_loss: 0.628
[42]  [820/1724] loss: 0.579, ave_loss: 0.627
[43]  [840/1724] loss: 0.600, ave_loss: 0.627
[44]  [860/1724] loss: 0.788, ave_loss: 0.630
[45]  [880/1724] loss: 0.669, ave_loss: 0.631
[46]  [900/1724] loss: 0.600, ave_loss: 0.630
[47]  [920/1724] loss: 0.551, ave_loss: 0.629
[48]  [940/1724] loss: 0.616, ave_loss: 0.628
[49]  [960/1724] loss: 0.570, ave_loss: 0.627
[50]  [980/1724] loss: 0.679, ave_loss: 0.628
[51]  [1000/1724] loss: 0.502, ave_loss: 0.626
[52]  [1020/1724] loss: 0.731, ave_loss: 0.628
[53]  [1040/1724] loss: 0.695, ave_loss: 0.629
[54]  [1060/1724] loss: 0.741, ave_loss: 0.631
[55]  [1080/1724] loss: 0.645, ave_loss: 0.631
[56]  [1100/1724] loss: 0.606, ave_loss: 0.631
[57]  [1120/1724] loss: 0.623, ave_loss: 0.631
[58]  [1140/1724] loss: 0.730, ave_loss: 0.633
[59]  [1160/1724] loss: 0.633, ave_loss: 0.633
[60]  [1180/1724] loss: 0.462, ave_loss: 0.630
[61]  [1200/1724] loss: 0.714, ave_loss: 0.631
[62]  [1220/1724] loss: 0.678, ave_loss: 0.632
[63]  [1240/1724] loss: 0.677, ave_loss: 0.633
[64]  [1260/1724] loss: 0.634, ave_loss: 0.633
[65]  [1280/1724] loss: 0.591, ave_loss: 0.632
[66]  [1300/1724] loss: 0.694, ave_loss: 0.633
[67]  [1320/1724] loss: 0.653, ave_loss: 0.633
[68]  [1340/1724] loss: 0.545, ave_loss: 0.632
[69]  [1360/1724] loss: 0.774, ave_loss: 0.634
[70]  [1380/1724] loss: 0.625, ave_loss: 0.634
[71]  [1400/1724] loss: 0.784, ave_loss: 0.636
[72]  [1420/1724] loss: 0.777, ave_loss: 0.638
[73]  [1440/1724] loss: 0.610, ave_loss: 0.637
[74]  [1460/1724] loss: 0.590, ave_loss: 0.637
[75]  [1480/1724] loss: 0.786, ave_loss: 0.639
[76]  [1500/1724] loss: 0.767, ave_loss: 0.641
[77]  [1520/1724] loss: 0.585, ave_loss: 0.640
[78]  [1540/1724] loss: 0.689, ave_loss: 0.640
[79]  [1560/1724] loss: 0.616, ave_loss: 0.640
[80]  [1580/1724] loss: 0.516, ave_loss: 0.639
[81]  [1600/1724] loss: 0.660, ave_loss: 0.639
[82]  [1620/1724] loss: 0.770, ave_loss: 0.640
[83]  [1640/1724] loss: 0.597, ave_loss: 0.640
[84]  [1660/1724] loss: 0.671, ave_loss: 0.640
[85]  [1680/1724] loss: 0.535, ave_loss: 0.639
[86]  [1700/1724] loss: 0.675, ave_loss: 0.639
[87]  [1720/1724] loss: 0.659, ave_loss: 0.640
[88]  [1740/1724] loss: 0.721, ave_loss: 0.641

Finished Training finishing at 2021-08-30 18:26:49.759806
printing_out epoch  13.271461716937354 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.406e-01
Validation Loss: 5.956e+03
Validation ROC: 0.4625
No improvement, still saving model
15.728538283062646 epochs left to go

Training Epoch 13.271461716937354/30 starting at 2021-08-30 18:27:36.680266
[1]  [0/1724] loss: 0.664, ave_loss: 0.664
[2]  [20/1724] loss: 0.594, ave_loss: 0.629
[3]  [40/1724] loss: 0.589, ave_loss: 0.616
[4]  [60/1724] loss: 0.779, ave_loss: 0.657
[5]  [80/1724] loss: 0.566, ave_loss: 0.639
[6]  [100/1724] loss: 0.695, ave_loss: 0.648
[7]  [120/1724] loss: 0.657, ave_loss: 0.649
[8]  [140/1724] loss: 0.574, ave_loss: 0.640
[9]  [160/1724] loss: 0.635, ave_loss: 0.639
[10]  [180/1724] loss: 0.661, ave_loss: 0.642
[11]  [200/1724] loss: 0.832, ave_loss: 0.659
[12]  [220/1724] loss: 0.694, ave_loss: 0.662
[13]  [240/1724] loss: 0.563, ave_loss: 0.654
[14]  [260/1724] loss: 0.658, ave_loss: 0.654
[15]  [280/1724] loss: 0.638, ave_loss: 0.653
[16]  [300/1724] loss: 0.607, ave_loss: 0.650
[17]  [320/1724] loss: 0.722, ave_loss: 0.655
[18]  [340/1724] loss: 0.610, ave_loss: 0.652
[19]  [360/1724] loss: 0.642, ave_loss: 0.652
[20]  [380/1724] loss: 0.731, ave_loss: 0.656
[21]  [400/1724] loss: 0.681, ave_loss: 0.657
[22]  [420/1724] loss: 0.478, ave_loss: 0.649
[23]  [440/1724] loss: 0.710, ave_loss: 0.651
[24]  [460/1724] loss: 0.683, ave_loss: 0.653
[25]  [480/1724] loss: 0.630, ave_loss: 0.652
[26]  [500/1724] loss: 0.672, ave_loss: 0.653
[27]  [520/1724] loss: 0.586, ave_loss: 0.650
[28]  [540/1724] loss: 0.588, ave_loss: 0.648
[29]  [560/1724] loss: 0.582, ave_loss: 0.646
[30]  [580/1724] loss: 0.675, ave_loss: 0.647
[31]  [600/1724] loss: 0.722, ave_loss: 0.649
[32]  [620/1724] loss: 0.573, ave_loss: 0.647
[33]  [640/1724] loss: 0.510, ave_loss: 0.643
[34]  [660/1724] loss: 0.791, ave_loss: 0.647
[35]  [680/1724] loss: 0.612, ave_loss: 0.646
[36]  [700/1724] loss: 0.507, ave_loss: 0.642
[37]  [720/1724] loss: 0.648, ave_loss: 0.642
[38]  [740/1724] loss: 0.605, ave_loss: 0.641
[39]  [760/1724] loss: 0.716, ave_loss: 0.643
[40]  [780/1724] loss: 0.745, ave_loss: 0.646
[41]  [800/1724] loss: 0.661, ave_loss: 0.646
[42]  [820/1724] loss: 0.528, ave_loss: 0.643
[43]  [840/1724] loss: 0.630, ave_loss: 0.643
[44]  [860/1724] loss: 0.563, ave_loss: 0.641
[45]  [880/1724] loss: 0.687, ave_loss: 0.642
[46]  [900/1724] loss: 0.590, ave_loss: 0.641
[47]  [920/1724] loss: 0.632, ave_loss: 0.641
[48]  [940/1724] loss: 0.523, ave_loss: 0.638
[49]  [960/1724] loss: 0.677, ave_loss: 0.639
[50]  [980/1724] loss: 0.725, ave_loss: 0.641
[51]  [1000/1724] loss: 0.769, ave_loss: 0.643
[52]  [1020/1724] loss: 0.736, ave_loss: 0.645
[53]  [1040/1724] loss: 0.702, ave_loss: 0.646
[54]  [1060/1724] loss: 0.624, ave_loss: 0.646
[55]  [1080/1724] loss: 0.607, ave_loss: 0.645
[56]  [1100/1724] loss: 0.720, ave_loss: 0.646
[57]  [1120/1724] loss: 0.528, ave_loss: 0.644
[58]  [1140/1724] loss: 0.759, ave_loss: 0.646
[59]  [1160/1724] loss: 0.504, ave_loss: 0.644
[60]  [1180/1724] loss: 0.682, ave_loss: 0.645
[61]  [1200/1724] loss: 0.644, ave_loss: 0.645
[62]  [1220/1724] loss: 0.666, ave_loss: 0.645
[63]  [1240/1724] loss: 0.640, ave_loss: 0.645
[64]  [1260/1724] loss: 0.513, ave_loss: 0.643
[65]  [1280/1724] loss: 0.825, ave_loss: 0.646
[66]  [1300/1724] loss: 0.561, ave_loss: 0.644
[67]  [1320/1724] loss: 0.714, ave_loss: 0.645
[68]  [1340/1724] loss: 0.677, ave_loss: 0.646
[69]  [1360/1724] loss: 0.711, ave_loss: 0.647
[70]  [1380/1724] loss: 0.595, ave_loss: 0.646
[71]  [1400/1724] loss: 0.558, ave_loss: 0.645
[72]  [1420/1724] loss: 0.678, ave_loss: 0.645
[73]  [1440/1724] loss: 0.613, ave_loss: 0.645
[74]  [1460/1724] loss: 0.631, ave_loss: 0.645
[75]  [1480/1724] loss: 0.676, ave_loss: 0.645
[76]  [1500/1724] loss: 0.571, ave_loss: 0.644
[77]  [1520/1724] loss: 0.494, ave_loss: 0.642
[78]  [1540/1724] loss: 0.609, ave_loss: 0.642
[79]  [1560/1724] loss: 0.625, ave_loss: 0.641
[80]  [1580/1724] loss: 0.702, ave_loss: 0.642
[81]  [1600/1724] loss: 0.645, ave_loss: 0.642
[82]  [1620/1724] loss: 0.563, ave_loss: 0.641
[83]  [1640/1724] loss: 0.681, ave_loss: 0.642
[84]  [1660/1724] loss: 0.587, ave_loss: 0.641
[85]  [1680/1724] loss: 0.739, ave_loss: 0.642
[86]  [1700/1724] loss: 0.609, ave_loss: 0.642
[87]  [1720/1724] loss: 0.699, ave_loss: 0.643
[88]  [1740/1724] loss: 0.690, ave_loss: 0.643

Finished Training finishing at 2021-08-30 18:29:21.092328
printing_out epoch  14.292343387470998 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.431e-01
Validation Loss: 5.892e+03
Validation ROC: 0.4625
No improvement, still saving model
14.707656612529002 epochs left to go

Training Epoch 14.292343387470998/30 starting at 2021-08-30 18:30:02.441235
[1]  [0/1724] loss: 0.871, ave_loss: 0.871
[2]  [20/1724] loss: 0.567, ave_loss: 0.719
[3]  [40/1724] loss: 0.639, ave_loss: 0.692
[4]  [60/1724] loss: 0.699, ave_loss: 0.694
[5]  [80/1724] loss: 0.655, ave_loss: 0.686
[6]  [100/1724] loss: 0.555, ave_loss: 0.664
[7]  [120/1724] loss: 0.677, ave_loss: 0.666
[8]  [140/1724] loss: 0.501, ave_loss: 0.646
[9]  [160/1724] loss: 0.661, ave_loss: 0.647
[10]  [180/1724] loss: 0.508, ave_loss: 0.633
[11]  [200/1724] loss: 0.667, ave_loss: 0.636
[12]  [220/1724] loss: 0.684, ave_loss: 0.640
[13]  [240/1724] loss: 0.687, ave_loss: 0.644
[14]  [260/1724] loss: 0.533, ave_loss: 0.636
[15]  [280/1724] loss: 0.604, ave_loss: 0.634
[16]  [300/1724] loss: 0.684, ave_loss: 0.637
[17]  [320/1724] loss: 0.649, ave_loss: 0.638
[18]  [340/1724] loss: 0.709, ave_loss: 0.642
[19]  [360/1724] loss: 0.730, ave_loss: 0.646
[20]  [380/1724] loss: 0.530, ave_loss: 0.641
[21]  [400/1724] loss: 0.579, ave_loss: 0.638
[22]  [420/1724] loss: 0.612, ave_loss: 0.636
[23]  [440/1724] loss: 0.588, ave_loss: 0.634
[24]  [460/1724] loss: 0.640, ave_loss: 0.635
[25]  [480/1724] loss: 0.602, ave_loss: 0.633
[26]  [500/1724] loss: 0.655, ave_loss: 0.634
[27]  [520/1724] loss: 0.604, ave_loss: 0.633
[28]  [540/1724] loss: 0.587, ave_loss: 0.631
[29]  [560/1724] loss: 0.632, ave_loss: 0.631
[30]  [580/1724] loss: 0.760, ave_loss: 0.636
[31]  [600/1724] loss: 0.456, ave_loss: 0.630
[32]  [620/1724] loss: 0.585, ave_loss: 0.628
[33]  [640/1724] loss: 0.569, ave_loss: 0.627
[34]  [660/1724] loss: 0.554, ave_loss: 0.625
[35]  [680/1724] loss: 0.756, ave_loss: 0.628
[36]  [700/1724] loss: 0.706, ave_loss: 0.630
[37]  [720/1724] loss: 0.726, ave_loss: 0.633
[38]  [740/1724] loss: 0.604, ave_loss: 0.632
[39]  [760/1724] loss: 0.805, ave_loss: 0.637
[40]  [780/1724] loss: 0.655, ave_loss: 0.637
[41]  [800/1724] loss: 0.579, ave_loss: 0.636
[42]  [820/1724] loss: 0.634, ave_loss: 0.636
[43]  [840/1724] loss: 0.604, ave_loss: 0.635
[44]  [860/1724] loss: 0.722, ave_loss: 0.637
[45]  [880/1724] loss: 0.822, ave_loss: 0.641
[46]  [900/1724] loss: 0.644, ave_loss: 0.641
[47]  [920/1724] loss: 0.708, ave_loss: 0.643
[48]  [940/1724] loss: 0.702, ave_loss: 0.644
[49]  [960/1724] loss: 0.610, ave_loss: 0.643
[50]  [980/1724] loss: 0.599, ave_loss: 0.642
[51]  [1000/1724] loss: 0.713, ave_loss: 0.644
[52]  [1020/1724] loss: 0.694, ave_loss: 0.645
[53]  [1040/1724] loss: 0.555, ave_loss: 0.643
[54]  [1060/1724] loss: 0.626, ave_loss: 0.643
[55]  [1080/1724] loss: 0.719, ave_loss: 0.644
[56]  [1100/1724] loss: 0.634, ave_loss: 0.644
[57]  [1120/1724] loss: 0.585, ave_loss: 0.643
[58]  [1140/1724] loss: 0.709, ave_loss: 0.644
[59]  [1160/1724] loss: 0.653, ave_loss: 0.644
[60]  [1180/1724] loss: 0.560, ave_loss: 0.643
[61]  [1200/1724] loss: 0.755, ave_loss: 0.644
[62]  [1220/1724] loss: 0.474, ave_loss: 0.642
[63]  [1240/1724] loss: 0.632, ave_loss: 0.642
[64]  [1260/1724] loss: 0.738, ave_loss: 0.643
[65]  [1280/1724] loss: 0.751, ave_loss: 0.645
[66]  [1300/1724] loss: 0.659, ave_loss: 0.645
[67]  [1320/1724] loss: 0.704, ave_loss: 0.646
[68]  [1340/1724] loss: 0.532, ave_loss: 0.644
[69]  [1360/1724] loss: 0.624, ave_loss: 0.644
[70]  [1380/1724] loss: 0.805, ave_loss: 0.646
[71]  [1400/1724] loss: 0.696, ave_loss: 0.647
[72]  [1420/1724] loss: 0.758, ave_loss: 0.648
[73]  [1440/1724] loss: 0.529, ave_loss: 0.647
[74]  [1460/1724] loss: 0.555, ave_loss: 0.646
[75]  [1480/1724] loss: 0.569, ave_loss: 0.644
[76]  [1500/1724] loss: 0.639, ave_loss: 0.644
[77]  [1520/1724] loss: 0.621, ave_loss: 0.644
[78]  [1540/1724] loss: 0.537, ave_loss: 0.643
[79]  [1560/1724] loss: 0.706, ave_loss: 0.644
[80]  [1580/1724] loss: 0.630, ave_loss: 0.643
[81]  [1600/1724] loss: 0.570, ave_loss: 0.642
[82]  [1620/1724] loss: 0.613, ave_loss: 0.642
[83]  [1640/1724] loss: 0.663, ave_loss: 0.642
[84]  [1660/1724] loss: 0.603, ave_loss: 0.642
[85]  [1680/1724] loss: 0.695, ave_loss: 0.643
[86]  [1700/1724] loss: 0.668, ave_loss: 0.643
[87]  [1720/1724] loss: 0.581, ave_loss: 0.642
[88]  [1740/1724] loss: 0.749, ave_loss: 0.643

Finished Training finishing at 2021-08-30 18:31:33.993842
printing_out epoch  15.31322505800464 learning rate: 0.0003199956068772566
0.0003103957386709389
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.433e-01
Validation Loss: 5.911e+03
Validation ROC: 0.4625
No improvement, still saving model
13.68677494199536 epochs left to go

Training Epoch 15.31322505800464/30 starting at 2021-08-30 18:32:07.271265
[1]  [0/1724] loss: 0.622, ave_loss: 0.622
[2]  [20/1724] loss: 0.726, ave_loss: 0.674
[3]  [40/1724] loss: 0.566, ave_loss: 0.638
[4]  [60/1724] loss: 0.633, ave_loss: 0.637
[5]  [80/1724] loss: 0.655, ave_loss: 0.641
[6]  [100/1724] loss: 0.623, ave_loss: 0.638
[7]  [120/1724] loss: 0.713, ave_loss: 0.648
[8]  [140/1724] loss: 0.620, ave_loss: 0.645
[9]  [160/1724] loss: 0.687, ave_loss: 0.650
[10]  [180/1724] loss: 0.684, ave_loss: 0.653
[11]  [200/1724] loss: 0.611, ave_loss: 0.649
[12]  [220/1724] loss: 0.632, ave_loss: 0.648
[13]  [240/1724] loss: 0.647, ave_loss: 0.648
[14]  [260/1724] loss: 0.524, ave_loss: 0.639
[15]  [280/1724] loss: 0.561, ave_loss: 0.634
[16]  [300/1724] loss: 0.509, ave_loss: 0.626
[17]  [320/1724] loss: 0.569, ave_loss: 0.623
[18]  [340/1724] loss: 0.667, ave_loss: 0.625
[19]  [360/1724] loss: 0.613, ave_loss: 0.624
[20]  [380/1724] loss: 0.727, ave_loss: 0.630
[21]  [400/1724] loss: 0.584, ave_loss: 0.627
[22]  [420/1724] loss: 0.661, ave_loss: 0.629
[23]  [440/1724] loss: 0.716, ave_loss: 0.633
[24]  [460/1724] loss: 0.715, ave_loss: 0.636
[25]  [480/1724] loss: 0.647, ave_loss: 0.637
[26]  [500/1724] loss: 0.721, ave_loss: 0.640
[27]  [520/1724] loss: 0.668, ave_loss: 0.641
[28]  [540/1724] loss: 0.697, ave_loss: 0.643
[29]  [560/1724] loss: 0.786, ave_loss: 0.648
[30]  [580/1724] loss: 0.703, ave_loss: 0.650
[31]  [600/1724] loss: 0.656, ave_loss: 0.650
[32]  [620/1724] loss: 0.461, ave_loss: 0.644
[33]  [640/1724] loss: 0.553, ave_loss: 0.641
[34]  [660/1724] loss: 0.480, ave_loss: 0.636
[35]  [680/1724] loss: 0.645, ave_loss: 0.637
[36]  [700/1724] loss: 0.679, ave_loss: 0.638
[37]  [720/1724] loss: 0.652, ave_loss: 0.638
[38]  [740/1724] loss: 0.667, ave_loss: 0.639
[39]  [760/1724] loss: 0.695, ave_loss: 0.640
[40]  [780/1724] loss: 0.665, ave_loss: 0.641
[41]  [800/1724] loss: 0.525, ave_loss: 0.638
[42]  [820/1724] loss: 0.583, ave_loss: 0.637
[43]  [840/1724] loss: 0.587, ave_loss: 0.636
[44]  [860/1724] loss: 0.650, ave_loss: 0.636
[45]  [880/1724] loss: 0.604, ave_loss: 0.635
[46]  [900/1724] loss: 0.599, ave_loss: 0.635
[47]  [920/1724] loss: 0.539, ave_loss: 0.633
[48]  [940/1724] loss: 0.656, ave_loss: 0.633
[49]  [960/1724] loss: 0.565, ave_loss: 0.632
[50]  [980/1724] loss: 0.620, ave_loss: 0.631
[51]  [1000/1724] loss: 0.660, ave_loss: 0.632
[52]  [1020/1724] loss: 0.624, ave_loss: 0.632
[53]  [1040/1724] loss: 0.653, ave_loss: 0.632
[54]  [1060/1724] loss: 0.638, ave_loss: 0.632
[55]  [1080/1724] loss: 0.537, ave_loss: 0.631
[56]  [1100/1724] loss: 0.521, ave_loss: 0.629
[57]  [1120/1724] loss: 0.706, ave_loss: 0.630
[58]  [1140/1724] loss: 0.603, ave_loss: 0.630
[59]  [1160/1724] loss: 0.524, ave_loss: 0.628
[60]  [1180/1724] loss: 0.590, ave_loss: 0.627
[61]  [1200/1724] loss: 0.637, ave_loss: 0.627
[62]  [1220/1724] loss: 0.604, ave_loss: 0.627
[63]  [1240/1724] loss: 0.540, ave_loss: 0.626
[64]  [1260/1724] loss: 0.671, ave_loss: 0.626
[65]  [1280/1724] loss: 0.577, ave_loss: 0.625
[66]  [1300/1724] loss: 0.576, ave_loss: 0.625
[67]  [1320/1724] loss: 0.611, ave_loss: 0.625
[68]  [1340/1724] loss: 0.536, ave_loss: 0.623
[69]  [1360/1724] loss: 0.646, ave_loss: 0.624
[70]  [1380/1724] loss: 0.654, ave_loss: 0.624
[71]  [1400/1724] loss: 0.619, ave_loss: 0.624
[72]  [1420/1724] loss: 0.607, ave_loss: 0.624
[73]  [1440/1724] loss: 0.704, ave_loss: 0.625
[74]  [1460/1724] loss: 0.609, ave_loss: 0.625
[75]  [1480/1724] loss: 0.580, ave_loss: 0.624
[76]  [1500/1724] loss: 0.577, ave_loss: 0.623
[77]  [1520/1724] loss: 0.556, ave_loss: 0.622
[78]  [1540/1724] loss: 0.642, ave_loss: 0.623
[79]  [1560/1724] loss: 0.735, ave_loss: 0.624
[80]  [1580/1724] loss: 0.612, ave_loss: 0.624
[81]  [1600/1724] loss: 0.663, ave_loss: 0.624
[82]  [1620/1724] loss: 0.782, ave_loss: 0.626
[83]  [1640/1724] loss: 0.556, ave_loss: 0.626
[84]  [1660/1724] loss: 0.561, ave_loss: 0.625
[85]  [1680/1724] loss: 0.620, ave_loss: 0.625
[86]  [1700/1724] loss: 0.475, ave_loss: 0.623
[87]  [1720/1724] loss: 0.668, ave_loss: 0.623
[88]  [1740/1724] loss: 0.519, ave_loss: 0.622

Finished Training finishing at 2021-08-30 18:33:40.840845
printing_out epoch  16.334106728538284 learning rate: 0.00029090509716114235
0.0002821779442463081
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.223e-01
Validation Loss: 5.992e+03
Validation ROC: 0.4625
No improvement, still saving model
12.665893271461716 epochs left to go

Training Epoch 16.334106728538284/30 starting at 2021-08-30 18:34:09.902247
[1]  [0/1724] loss: 0.656, ave_loss: 0.656
[2]  [20/1724] loss: 0.672, ave_loss: 0.664
[3]  [40/1724] loss: 0.714, ave_loss: 0.681
[4]  [60/1724] loss: 0.691, ave_loss: 0.683
[5]  [80/1724] loss: 0.642, ave_loss: 0.675
[6]  [100/1724] loss: 0.703, ave_loss: 0.680
[7]  [120/1724] loss: 0.455, ave_loss: 0.648
[8]  [140/1724] loss: 0.796, ave_loss: 0.666
[9]  [160/1724] loss: 0.688, ave_loss: 0.669
[10]  [180/1724] loss: 0.720, ave_loss: 0.674
[11]  [200/1724] loss: 0.566, ave_loss: 0.664
[12]  [220/1724] loss: 0.712, ave_loss: 0.668
[13]  [240/1724] loss: 0.650, ave_loss: 0.667
[14]  [260/1724] loss: 0.619, ave_loss: 0.663
[15]  [280/1724] loss: 0.656, ave_loss: 0.663
[16]  [300/1724] loss: 0.650, ave_loss: 0.662
[17]  [320/1724] loss: 0.583, ave_loss: 0.657
[18]  [340/1724] loss: 0.620, ave_loss: 0.655
[19]  [360/1724] loss: 0.645, ave_loss: 0.655
[20]  [380/1724] loss: 0.747, ave_loss: 0.659
[21]  [400/1724] loss: 0.701, ave_loss: 0.661
[22]  [420/1724] loss: 0.591, ave_loss: 0.658
[23]  [440/1724] loss: 0.732, ave_loss: 0.661
[24]  [460/1724] loss: 0.504, ave_loss: 0.655
[25]  [480/1724] loss: 0.667, ave_loss: 0.655
[26]  [500/1724] loss: 0.685, ave_loss: 0.656
[27]  [520/1724] loss: 0.663, ave_loss: 0.657
[28]  [540/1724] loss: 0.737, ave_loss: 0.659
[29]  [560/1724] loss: 0.790, ave_loss: 0.664
[30]  [580/1724] loss: 0.601, ave_loss: 0.662
[31]  [600/1724] loss: 0.538, ave_loss: 0.658
[32]  [620/1724] loss: 0.530, ave_loss: 0.654
[33]  [640/1724] loss: 0.626, ave_loss: 0.653
[34]  [660/1724] loss: 0.740, ave_loss: 0.656
[35]  [680/1724] loss: 0.637, ave_loss: 0.655
[36]  [700/1724] loss: 0.656, ave_loss: 0.655
[37]  [720/1724] loss: 0.665, ave_loss: 0.655
[38]  [740/1724] loss: 0.653, ave_loss: 0.655
[39]  [760/1724] loss: 0.535, ave_loss: 0.652
[40]  [780/1724] loss: 0.628, ave_loss: 0.652
[41]  [800/1724] loss: 0.584, ave_loss: 0.650
[42]  [820/1724] loss: 0.777, ave_loss: 0.653
[43]  [840/1724] loss: 0.654, ave_loss: 0.653
[44]  [860/1724] loss: 0.690, ave_loss: 0.654
[45]  [880/1724] loss: 0.636, ave_loss: 0.653
[46]  [900/1724] loss: 0.573, ave_loss: 0.652
[47]  [920/1724] loss: 0.661, ave_loss: 0.652
[48]  [940/1724] loss: 0.638, ave_loss: 0.652
[49]  [960/1724] loss: 0.690, ave_loss: 0.652
[50]  [980/1724] loss: 0.629, ave_loss: 0.652
[51]  [1000/1724] loss: 0.617, ave_loss: 0.651
[52]  [1020/1724] loss: 0.555, ave_loss: 0.649
[53]  [1040/1724] loss: 0.621, ave_loss: 0.649
[54]  [1060/1724] loss: 0.612, ave_loss: 0.648
[55]  [1080/1724] loss: 0.628, ave_loss: 0.648
[56]  [1100/1724] loss: 0.557, ave_loss: 0.646
[57]  [1120/1724] loss: 0.654, ave_loss: 0.646
[58]  [1140/1724] loss: 0.602, ave_loss: 0.646
[59]  [1160/1724] loss: 0.625, ave_loss: 0.645
[60]  [1180/1724] loss: 0.704, ave_loss: 0.646
[61]  [1200/1724] loss: 0.558, ave_loss: 0.645
[62]  [1220/1724] loss: 0.602, ave_loss: 0.644
[63]  [1240/1724] loss: 0.558, ave_loss: 0.643
[64]  [1260/1724] loss: 0.629, ave_loss: 0.642
[65]  [1280/1724] loss: 0.850, ave_loss: 0.646
[66]  [1300/1724] loss: 0.677, ave_loss: 0.646
[67]  [1320/1724] loss: 0.844, ave_loss: 0.649
[68]  [1340/1724] loss: 0.881, ave_loss: 0.653
[69]  [1360/1724] loss: 0.581, ave_loss: 0.651
[70]  [1380/1724] loss: 0.624, ave_loss: 0.651
[71]  [1400/1724] loss: 0.681, ave_loss: 0.651
[72]  [1420/1724] loss: 0.577, ave_loss: 0.650
[73]  [1440/1724] loss: 0.530, ave_loss: 0.649
[74]  [1460/1724] loss: 0.650, ave_loss: 0.649
[75]  [1480/1724] loss: 0.626, ave_loss: 0.649
[76]  [1500/1724] loss: 0.568, ave_loss: 0.647
[77]  [1520/1724] loss: 0.611, ave_loss: 0.647
[78]  [1540/1724] loss: 0.657, ave_loss: 0.647
[79]  [1560/1724] loss: 0.579, ave_loss: 0.646
[80]  [1580/1724] loss: 0.739, ave_loss: 0.647
[81]  [1600/1724] loss: 0.714, ave_loss: 0.648
[82]  [1620/1724] loss: 0.541, ave_loss: 0.647
[83]  [1640/1724] loss: 0.595, ave_loss: 0.646
[84]  [1660/1724] loss: 0.728, ave_loss: 0.647
[85]  [1680/1724] loss: 0.656, ave_loss: 0.647
[86]  [1700/1724] loss: 0.713, ave_loss: 0.648
[87]  [1720/1724] loss: 0.564, ave_loss: 0.647
[88]  [1740/1724] loss: 0.594, ave_loss: 0.647

Finished Training finishing at 2021-08-30 18:35:40.529166
printing_out epoch  17.354988399071924 learning rate: 0.00026445917923740214
0.0002565254038602801
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.466e-01
Validation Loss: 5.920e+03
Validation ROC: 0.4625
No improvement, still saving model
11.645011600928076 epochs left to go

Training Epoch 17.354988399071924/30 starting at 2021-08-30 18:36:16.524235
[1]  [0/1724] loss: 0.679, ave_loss: 0.679
[2]  [20/1724] loss: 0.620, ave_loss: 0.649
[3]  [40/1724] loss: 0.764, ave_loss: 0.687
[4]  [60/1724] loss: 0.644, ave_loss: 0.676
[5]  [80/1724] loss: 0.612, ave_loss: 0.664
[6]  [100/1724] loss: 0.591, ave_loss: 0.652
[7]  [120/1724] loss: 0.731, ave_loss: 0.663
[8]  [140/1724] loss: 0.566, ave_loss: 0.651
[9]  [160/1724] loss: 0.585, ave_loss: 0.643
[10]  [180/1724] loss: 0.593, ave_loss: 0.638
[11]  [200/1724] loss: 0.580, ave_loss: 0.633
[12]  [220/1724] loss: 0.811, ave_loss: 0.648
[13]  [240/1724] loss: 0.774, ave_loss: 0.658
[14]  [260/1724] loss: 0.673, ave_loss: 0.659
[15]  [280/1724] loss: 0.527, ave_loss: 0.650
[16]  [300/1724] loss: 0.535, ave_loss: 0.643
[17]  [320/1724] loss: 0.647, ave_loss: 0.643
[18]  [340/1724] loss: 0.566, ave_loss: 0.639
[19]  [360/1724] loss: 0.580, ave_loss: 0.636
[20]  [380/1724] loss: 0.643, ave_loss: 0.636
[21]  [400/1724] loss: 0.603, ave_loss: 0.634
[22]  [420/1724] loss: 0.636, ave_loss: 0.634
[23]  [440/1724] loss: 0.625, ave_loss: 0.634
[24]  [460/1724] loss: 0.685, ave_loss: 0.636
[25]  [480/1724] loss: 0.706, ave_loss: 0.639
[26]  [500/1724] loss: 0.603, ave_loss: 0.638
[27]  [520/1724] loss: 0.700, ave_loss: 0.640
[28]  [540/1724] loss: 0.668, ave_loss: 0.641
[29]  [560/1724] loss: 0.647, ave_loss: 0.641
[30]  [580/1724] loss: 0.759, ave_loss: 0.645
[31]  [600/1724] loss: 0.776, ave_loss: 0.649
[32]  [620/1724] loss: 0.673, ave_loss: 0.650
[33]  [640/1724] loss: 0.716, ave_loss: 0.652
[34]  [660/1724] loss: 0.779, ave_loss: 0.656
[35]  [680/1724] loss: 0.594, ave_loss: 0.654
[36]  [700/1724] loss: 0.609, ave_loss: 0.653
[37]  [720/1724] loss: 0.581, ave_loss: 0.651
[38]  [740/1724] loss: 0.527, ave_loss: 0.648
[39]  [760/1724] loss: 0.701, ave_loss: 0.649
[40]  [780/1724] loss: 0.592, ave_loss: 0.647
[41]  [800/1724] loss: 0.723, ave_loss: 0.649
[42]  [820/1724] loss: 0.779, ave_loss: 0.652
[43]  [840/1724] loss: 0.523, ave_loss: 0.649
[44]  [860/1724] loss: 0.649, ave_loss: 0.649
[45]  [880/1724] loss: 0.631, ave_loss: 0.649
[46]  [900/1724] loss: 0.669, ave_loss: 0.649
[47]  [920/1724] loss: 0.628, ave_loss: 0.649
[48]  [940/1724] loss: 0.579, ave_loss: 0.648
[49]  [960/1724] loss: 0.712, ave_loss: 0.649
[50]  [980/1724] loss: 0.679, ave_loss: 0.649
[51]  [1000/1724] loss: 0.667, ave_loss: 0.650
[52]  [1020/1724] loss: 0.591, ave_loss: 0.649
[53]  [1040/1724] loss: 0.737, ave_loss: 0.650
[54]  [1060/1724] loss: 0.730, ave_loss: 0.652
[55]  [1080/1724] loss: 0.782, ave_loss: 0.654
[56]  [1100/1724] loss: 0.702, ave_loss: 0.655
[57]  [1120/1724] loss: 0.674, ave_loss: 0.655
[58]  [1140/1724] loss: 0.678, ave_loss: 0.656
[59]  [1160/1724] loss: 0.671, ave_loss: 0.656
[60]  [1180/1724] loss: 0.654, ave_loss: 0.656
[61]  [1200/1724] loss: 0.664, ave_loss: 0.656
[62]  [1220/1724] loss: 0.697, ave_loss: 0.657
[63]  [1240/1724] loss: 0.674, ave_loss: 0.657
[64]  [1260/1724] loss: 0.654, ave_loss: 0.657
[65]  [1280/1724] loss: 0.626, ave_loss: 0.657
[66]  [1300/1724] loss: 0.704, ave_loss: 0.657
[67]  [1320/1724] loss: 0.606, ave_loss: 0.656
[68]  [1340/1724] loss: 0.612, ave_loss: 0.656
[69]  [1360/1724] loss: 0.640, ave_loss: 0.656
[70]  [1380/1724] loss: 0.625, ave_loss: 0.655
[71]  [1400/1724] loss: 0.545, ave_loss: 0.654
[72]  [1420/1724] loss: 0.757, ave_loss: 0.655
[73]  [1440/1724] loss: 0.549, ave_loss: 0.654
[74]  [1460/1724] loss: 0.532, ave_loss: 0.652
[75]  [1480/1724] loss: 0.589, ave_loss: 0.651
[76]  [1500/1724] loss: 0.668, ave_loss: 0.651
[77]  [1520/1724] loss: 0.461, ave_loss: 0.649
[78]  [1540/1724] loss: 0.587, ave_loss: 0.648
[79]  [1560/1724] loss: 0.726, ave_loss: 0.649
[80]  [1580/1724] loss: 0.600, ave_loss: 0.648
[81]  [1600/1724] loss: 0.464, ave_loss: 0.646
[82]  [1620/1724] loss: 0.825, ave_loss: 0.648
[83]  [1640/1724] loss: 0.666, ave_loss: 0.649
[84]  [1660/1724] loss: 0.595, ave_loss: 0.648
[85]  [1680/1724] loss: 0.619, ave_loss: 0.648
[86]  [1700/1724] loss: 0.585, ave_loss: 0.647
[87]  [1720/1724] loss: 0.578, ave_loss: 0.646
[88]  [1740/1724] loss: 0.481, ave_loss: 0.644

Finished Training finishing at 2021-08-30 18:37:37.759604
printing_out epoch  18.37587006960557 learning rate: 0.00024041743567036557
0.0002332049126002546
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.442e-01
Validation Loss: 5.909e+03
Validation ROC: 0.4625
No improvement, still saving model
10.624129930394432 epochs left to go

Training Epoch 18.37587006960557/30 starting at 2021-08-30 18:38:09.423263
[1]  [0/1724] loss: 0.529, ave_loss: 0.529
[2]  [20/1724] loss: 0.694, ave_loss: 0.612
[3]  [40/1724] loss: 0.614, ave_loss: 0.612
[4]  [60/1724] loss: 0.601, ave_loss: 0.609
[5]  [80/1724] loss: 0.603, ave_loss: 0.608
[6]  [100/1724] loss: 0.574, ave_loss: 0.602
[7]  [120/1724] loss: 0.609, ave_loss: 0.603
[8]  [140/1724] loss: 0.672, ave_loss: 0.612
[9]  [160/1724] loss: 0.607, ave_loss: 0.611
[10]  [180/1724] loss: 0.631, ave_loss: 0.613
[11]  [200/1724] loss: 0.676, ave_loss: 0.619
[12]  [220/1724] loss: 0.660, ave_loss: 0.622
[13]  [240/1724] loss: 0.752, ave_loss: 0.632
[14]  [260/1724] loss: 0.573, ave_loss: 0.628
[15]  [280/1724] loss: 0.747, ave_loss: 0.636
[16]  [300/1724] loss: 0.563, ave_loss: 0.632
[17]  [320/1724] loss: 0.680, ave_loss: 0.634
[18]  [340/1724] loss: 0.764, ave_loss: 0.642
[19]  [360/1724] loss: 0.661, ave_loss: 0.643
[20]  [380/1724] loss: 0.712, ave_loss: 0.646
[21]  [400/1724] loss: 0.922, ave_loss: 0.659
[22]  [420/1724] loss: 0.625, ave_loss: 0.658
[23]  [440/1724] loss: 0.763, ave_loss: 0.662
[24]  [460/1724] loss: 0.641, ave_loss: 0.661
[25]  [480/1724] loss: 0.587, ave_loss: 0.658
[26]  [500/1724] loss: 0.575, ave_loss: 0.655
[27]  [520/1724] loss: 0.655, ave_loss: 0.655
[28]  [540/1724] loss: 0.676, ave_loss: 0.656
[29]  [560/1724] loss: 0.716, ave_loss: 0.658
[30]  [580/1724] loss: 0.661, ave_loss: 0.658
[31]  [600/1724] loss: 0.589, ave_loss: 0.656
[32]  [620/1724] loss: 0.674, ave_loss: 0.656
[33]  [640/1724] loss: 0.673, ave_loss: 0.657
[34]  [660/1724] loss: 0.643, ave_loss: 0.657
[35]  [680/1724] loss: 0.752, ave_loss: 0.659
[36]  [700/1724] loss: 0.707, ave_loss: 0.661
[37]  [720/1724] loss: 0.598, ave_loss: 0.659
[38]  [740/1724] loss: 0.586, ave_loss: 0.657
[39]  [760/1724] loss: 0.575, ave_loss: 0.655
[40]  [780/1724] loss: 0.605, ave_loss: 0.654
[41]  [800/1724] loss: 0.657, ave_loss: 0.654
[42]  [820/1724] loss: 0.526, ave_loss: 0.651
[43]  [840/1724] loss: 0.594, ave_loss: 0.649
[44]  [860/1724] loss: 0.623, ave_loss: 0.649
[45]  [880/1724] loss: 0.702, ave_loss: 0.650
[46]  [900/1724] loss: 0.647, ave_loss: 0.650
[47]  [920/1724] loss: 0.649, ave_loss: 0.650
[48]  [940/1724] loss: 0.650, ave_loss: 0.650
[49]  [960/1724] loss: 0.641, ave_loss: 0.650
[50]  [980/1724] loss: 0.681, ave_loss: 0.650
[51]  [1000/1724] loss: 0.605, ave_loss: 0.649
[52]  [1020/1724] loss: 0.603, ave_loss: 0.649
[53]  [1040/1724] loss: 0.779, ave_loss: 0.651
[54]  [1060/1724] loss: 0.514, ave_loss: 0.648
[55]  [1080/1724] loss: 0.675, ave_loss: 0.649
[56]  [1100/1724] loss: 0.578, ave_loss: 0.648
[57]  [1120/1724] loss: 0.551, ave_loss: 0.646
[58]  [1140/1724] loss: 0.634, ave_loss: 0.646
[59]  [1160/1724] loss: 0.807, ave_loss: 0.649
[60]  [1180/1724] loss: 0.656, ave_loss: 0.649
[61]  [1200/1724] loss: 0.683, ave_loss: 0.649
[62]  [1220/1724] loss: 0.620, ave_loss: 0.649
[63]  [1240/1724] loss: 0.535, ave_loss: 0.647
[64]  [1260/1724] loss: 0.605, ave_loss: 0.646
[65]  [1280/1724] loss: 0.673, ave_loss: 0.647
[66]  [1300/1724] loss: 0.711, ave_loss: 0.648
[67]  [1320/1724] loss: 0.739, ave_loss: 0.649
[68]  [1340/1724] loss: 0.652, ave_loss: 0.649
[69]  [1360/1724] loss: 0.623, ave_loss: 0.649
[70]  [1380/1724] loss: 0.553, ave_loss: 0.647
[71]  [1400/1724] loss: 0.630, ave_loss: 0.647
[72]  [1420/1724] loss: 0.687, ave_loss: 0.648
[73]  [1440/1724] loss: 0.740, ave_loss: 0.649
[74]  [1460/1724] loss: 0.574, ave_loss: 0.648
[75]  [1480/1724] loss: 0.602, ave_loss: 0.647
[76]  [1500/1724] loss: 0.649, ave_loss: 0.647
[77]  [1520/1724] loss: 0.571, ave_loss: 0.646
[78]  [1540/1724] loss: 0.587, ave_loss: 0.646
[79]  [1560/1724] loss: 0.555, ave_loss: 0.644
[80]  [1580/1724] loss: 0.611, ave_loss: 0.644
[81]  [1600/1724] loss: 0.625, ave_loss: 0.644
[82]  [1620/1724] loss: 0.667, ave_loss: 0.644
[83]  [1640/1724] loss: 0.565, ave_loss: 0.643
[84]  [1660/1724] loss: 0.764, ave_loss: 0.645
[85]  [1680/1724] loss: 0.554, ave_loss: 0.643
[86]  [1700/1724] loss: 0.646, ave_loss: 0.643
[87]  [1720/1724] loss: 0.682, ave_loss: 0.644
[88]  [1740/1724] loss: 0.697, ave_loss: 0.645

Finished Training finishing at 2021-08-30 18:39:40.409994
printing_out epoch  19.396751740139212 learning rate: 0.00021856130515487778
0.00021200446600023144
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.445e-01
Validation Loss: 5.935e+03
Validation ROC: 0.4625
No improvement, still saving model
9.603248259860788 epochs left to go

Training Epoch 19.396751740139212/30 starting at 2021-08-30 18:40:16.647254
[1]  [0/1724] loss: 0.530, ave_loss: 0.530
[2]  [20/1724] loss: 0.580, ave_loss: 0.555
[3]  [40/1724] loss: 0.646, ave_loss: 0.585
[4]  [60/1724] loss: 0.675, ave_loss: 0.608
[5]  [80/1724] loss: 0.584, ave_loss: 0.603
[6]  [100/1724] loss: 0.490, ave_loss: 0.584
[7]  [120/1724] loss: 0.647, ave_loss: 0.593
[8]  [140/1724] loss: 0.696, ave_loss: 0.606
[9]  [160/1724] loss: 0.759, ave_loss: 0.623
[10]  [180/1724] loss: 0.567, ave_loss: 0.617
[11]  [200/1724] loss: 0.651, ave_loss: 0.620
[12]  [220/1724] loss: 0.614, ave_loss: 0.620
[13]  [240/1724] loss: 0.718, ave_loss: 0.627
[14]  [260/1724] loss: 0.711, ave_loss: 0.633
[15]  [280/1724] loss: 0.617, ave_loss: 0.632
[16]  [300/1724] loss: 0.679, ave_loss: 0.635
[17]  [320/1724] loss: 0.530, ave_loss: 0.629
[18]  [340/1724] loss: 0.619, ave_loss: 0.629
[19]  [360/1724] loss: 0.550, ave_loss: 0.624
[20]  [380/1724] loss: 0.604, ave_loss: 0.623
[21]  [400/1724] loss: 0.481, ave_loss: 0.617
[22]  [420/1724] loss: 0.552, ave_loss: 0.614
[23]  [440/1724] loss: 0.706, ave_loss: 0.618
[24]  [460/1724] loss: 0.724, ave_loss: 0.622
[25]  [480/1724] loss: 0.710, ave_loss: 0.626
[26]  [500/1724] loss: 0.729, ave_loss: 0.630
[27]  [520/1724] loss: 0.613, ave_loss: 0.629
[28]  [540/1724] loss: 0.560, ave_loss: 0.627
[29]  [560/1724] loss: 0.602, ave_loss: 0.626
[30]  [580/1724] loss: 0.605, ave_loss: 0.625
[31]  [600/1724] loss: 0.746, ave_loss: 0.629
[32]  [620/1724] loss: 0.585, ave_loss: 0.628
[33]  [640/1724] loss: 0.596, ave_loss: 0.627
[34]  [660/1724] loss: 0.739, ave_loss: 0.630
[35]  [680/1724] loss: 0.570, ave_loss: 0.628
[36]  [700/1724] loss: 0.700, ave_loss: 0.630
[37]  [720/1724] loss: 0.609, ave_loss: 0.630
[38]  [740/1724] loss: 0.539, ave_loss: 0.627
[39]  [760/1724] loss: 0.647, ave_loss: 0.628
[40]  [780/1724] loss: 0.618, ave_loss: 0.627
[41]  [800/1724] loss: 0.570, ave_loss: 0.626
[42]  [820/1724] loss: 0.702, ave_loss: 0.628
[43]  [840/1724] loss: 0.630, ave_loss: 0.628
[44]  [860/1724] loss: 0.642, ave_loss: 0.628
[45]  [880/1724] loss: 0.586, ave_loss: 0.627
[46]  [900/1724] loss: 0.724, ave_loss: 0.629
[47]  [920/1724] loss: 0.525, ave_loss: 0.627
[48]  [940/1724] loss: 0.688, ave_loss: 0.628
[49]  [960/1724] loss: 0.593, ave_loss: 0.628
[50]  [980/1724] loss: 0.622, ave_loss: 0.628
[51]  [1000/1724] loss: 0.655, ave_loss: 0.628
[52]  [1020/1724] loss: 0.718, ave_loss: 0.630
[53]  [1040/1724] loss: 0.631, ave_loss: 0.630
[54]  [1060/1724] loss: 0.605, ave_loss: 0.629
[55]  [1080/1724] loss: 0.732, ave_loss: 0.631
[56]  [1100/1724] loss: 0.667, ave_loss: 0.632
[57]  [1120/1724] loss: 0.579, ave_loss: 0.631
[58]  [1140/1724] loss: 0.727, ave_loss: 0.633
[59]  [1160/1724] loss: 0.470, ave_loss: 0.630
[60]  [1180/1724] loss: 0.709, ave_loss: 0.631
[61]  [1200/1724] loss: 0.667, ave_loss: 0.632
[62]  [1220/1724] loss: 0.617, ave_loss: 0.632
[63]  [1240/1724] loss: 0.687, ave_loss: 0.632
[64]  [1260/1724] loss: 0.688, ave_loss: 0.633
[65]  [1280/1724] loss: 0.574, ave_loss: 0.632
[66]  [1300/1724] loss: 0.591, ave_loss: 0.632
[67]  [1320/1724] loss: 0.519, ave_loss: 0.630
[68]  [1340/1724] loss: 0.754, ave_loss: 0.632
[69]  [1360/1724] loss: 0.565, ave_loss: 0.631
[70]  [1380/1724] loss: 0.590, ave_loss: 0.630
[71]  [1400/1724] loss: 0.687, ave_loss: 0.631
[72]  [1420/1724] loss: 0.544, ave_loss: 0.630
[73]  [1440/1724] loss: 0.481, ave_loss: 0.628
[74]  [1460/1724] loss: 0.535, ave_loss: 0.627
[75]  [1480/1724] loss: 0.695, ave_loss: 0.628
[76]  [1500/1724] loss: 0.552, ave_loss: 0.627
[77]  [1520/1724] loss: 0.639, ave_loss: 0.627
[78]  [1540/1724] loss: 0.808, ave_loss: 0.629
[79]  [1560/1724] loss: 0.722, ave_loss: 0.630
[80]  [1580/1724] loss: 0.688, ave_loss: 0.631
[81]  [1600/1724] loss: 0.574, ave_loss: 0.630
[82]  [1620/1724] loss: 0.626, ave_loss: 0.630
[83]  [1640/1724] loss: 0.752, ave_loss: 0.632
[84]  [1660/1724] loss: 0.587, ave_loss: 0.631
[85]  [1680/1724] loss: 0.559, ave_loss: 0.630
[86]  [1700/1724] loss: 0.625, ave_loss: 0.630
[87]  [1720/1724] loss: 0.762, ave_loss: 0.632
[88]  [1740/1724] loss: 0.625, ave_loss: 0.632

Finished Training finishing at 2021-08-30 18:41:49.339210
printing_out epoch  20.417633410672853 learning rate: 0.00019869209559534342
0.00019273133272748312
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.317e-01
Validation Loss: 5.994e+03
Validation ROC: 0.4625
No improvement, still saving model
8.582366589327147 epochs left to go

Training Epoch 20.417633410672853/30 starting at 2021-08-30 18:42:24.078280
[1]  [0/1724] loss: 0.663, ave_loss: 0.663
[2]  [20/1724] loss: 0.666, ave_loss: 0.664
[3]  [40/1724] loss: 0.727, ave_loss: 0.685
[4]  [60/1724] loss: 0.704, ave_loss: 0.690
[5]  [80/1724] loss: 0.546, ave_loss: 0.661
[6]  [100/1724] loss: 0.591, ave_loss: 0.650
[7]  [120/1724] loss: 0.782, ave_loss: 0.668
[8]  [140/1724] loss: 0.531, ave_loss: 0.651
[9]  [160/1724] loss: 0.519, ave_loss: 0.637
[10]  [180/1724] loss: 0.616, ave_loss: 0.634
[11]  [200/1724] loss: 0.681, ave_loss: 0.639
[12]  [220/1724] loss: 0.770, ave_loss: 0.650
[13]  [240/1724] loss: 0.702, ave_loss: 0.654
[14]  [260/1724] loss: 0.595, ave_loss: 0.650
[15]  [280/1724] loss: 0.518, ave_loss: 0.641
[16]  [300/1724] loss: 0.616, ave_loss: 0.639
[17]  [320/1724] loss: 0.632, ave_loss: 0.639
[18]  [340/1724] loss: 0.517, ave_loss: 0.632
[19]  [360/1724] loss: 0.597, ave_loss: 0.630
[20]  [380/1724] loss: 0.632, ave_loss: 0.630
[21]  [400/1724] loss: 0.520, ave_loss: 0.625
[22]  [420/1724] loss: 0.595, ave_loss: 0.624
[23]  [440/1724] loss: 0.749, ave_loss: 0.629
[24]  [460/1724] loss: 0.617, ave_loss: 0.629
[25]  [480/1724] loss: 0.543, ave_loss: 0.625
[26]  [500/1724] loss: 0.612, ave_loss: 0.625
[27]  [520/1724] loss: 0.725, ave_loss: 0.628
[28]  [540/1724] loss: 0.587, ave_loss: 0.627
[29]  [560/1724] loss: 0.699, ave_loss: 0.629
[30]  [580/1724] loss: 0.574, ave_loss: 0.628
[31]  [600/1724] loss: 0.663, ave_loss: 0.629
[32]  [620/1724] loss: 0.654, ave_loss: 0.629
[33]  [640/1724] loss: 0.629, ave_loss: 0.629
[34]  [660/1724] loss: 0.523, ave_loss: 0.626
[35]  [680/1724] loss: 0.737, ave_loss: 0.630
[36]  [700/1724] loss: 0.622, ave_loss: 0.629
[37]  [720/1724] loss: 0.669, ave_loss: 0.630
[38]  [740/1724] loss: 0.610, ave_loss: 0.630
[39]  [760/1724] loss: 0.795, ave_loss: 0.634
[40]  [780/1724] loss: 0.691, ave_loss: 0.635
[41]  [800/1724] loss: 0.592, ave_loss: 0.634
[42]  [820/1724] loss: 0.539, ave_loss: 0.632
[43]  [840/1724] loss: 0.634, ave_loss: 0.632
[44]  [860/1724] loss: 0.599, ave_loss: 0.631
[45]  [880/1724] loss: 0.656, ave_loss: 0.632
[46]  [900/1724] loss: 0.746, ave_loss: 0.634
[47]  [920/1724] loss: 0.666, ave_loss: 0.635
[48]  [940/1724] loss: 0.745, ave_loss: 0.637
[49]  [960/1724] loss: 0.707, ave_loss: 0.639
[50]  [980/1724] loss: 0.723, ave_loss: 0.640
[51]  [1000/1724] loss: 0.681, ave_loss: 0.641
[52]  [1020/1724] loss: 0.892, ave_loss: 0.646
[53]  [1040/1724] loss: 0.577, ave_loss: 0.645
[54]  [1060/1724] loss: 0.836, ave_loss: 0.648
[55]  [1080/1724] loss: 0.655, ave_loss: 0.648
[56]  [1100/1724] loss: 0.634, ave_loss: 0.648
[57]  [1120/1724] loss: 0.543, ave_loss: 0.646
[58]  [1140/1724] loss: 0.636, ave_loss: 0.646
[59]  [1160/1724] loss: 0.678, ave_loss: 0.647
[60]  [1180/1724] loss: 0.699, ave_loss: 0.648
[61]  [1200/1724] loss: 0.562, ave_loss: 0.646
[62]  [1220/1724] loss: 0.642, ave_loss: 0.646
[63]  [1240/1724] loss: 0.703, ave_loss: 0.647
[64]  [1260/1724] loss: 0.750, ave_loss: 0.649
[65]  [1280/1724] loss: 0.692, ave_loss: 0.649
[66]  [1300/1724] loss: 0.797, ave_loss: 0.652
[67]  [1320/1724] loss: 0.660, ave_loss: 0.652
[68]  [1340/1724] loss: 0.638, ave_loss: 0.651
[69]  [1360/1724] loss: 0.624, ave_loss: 0.651
[70]  [1380/1724] loss: 0.594, ave_loss: 0.650
[71]  [1400/1724] loss: 0.654, ave_loss: 0.650
[72]  [1420/1724] loss: 0.630, ave_loss: 0.650
[73]  [1440/1724] loss: 0.642, ave_loss: 0.650
[74]  [1460/1724] loss: 0.682, ave_loss: 0.650
[75]  [1480/1724] loss: 0.582, ave_loss: 0.649
[76]  [1500/1724] loss: 0.660, ave_loss: 0.650
[77]  [1520/1724] loss: 0.645, ave_loss: 0.650
[78]  [1540/1724] loss: 0.721, ave_loss: 0.650
[79]  [1560/1724] loss: 0.742, ave_loss: 0.652
[80]  [1580/1724] loss: 0.671, ave_loss: 0.652
[81]  [1600/1724] loss: 0.700, ave_loss: 0.652
[82]  [1620/1724] loss: 0.533, ave_loss: 0.651
[83]  [1640/1724] loss: 0.768, ave_loss: 0.652
[84]  [1660/1724] loss: 0.563, ave_loss: 0.651
[85]  [1680/1724] loss: 0.618, ave_loss: 0.651
[86]  [1700/1724] loss: 0.604, ave_loss: 0.650
[87]  [1720/1724] loss: 0.610, ave_loss: 0.650
[88]  [1740/1724] loss: 0.801, ave_loss: 0.652

Finished Training finishing at 2021-08-30 18:43:48.429493
printing_out epoch  21.438515081206496 learning rate: 0.00018062917781394856
0.0001752103024795301
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.516e-01
Validation Loss: 5.942e+03
Validation ROC: 0.4625
No improvement, still saving model
7.561484918793504 epochs left to go

Training Epoch 21.438515081206496/30 starting at 2021-08-30 18:44:24.896271
[1]  [0/1724] loss: 0.644, ave_loss: 0.644
[2]  [20/1724] loss: 0.760, ave_loss: 0.702
[3]  [40/1724] loss: 0.507, ave_loss: 0.637
[4]  [60/1724] loss: 0.583, ave_loss: 0.623
[5]  [80/1724] loss: 0.640, ave_loss: 0.627
[6]  [100/1724] loss: 0.807, ave_loss: 0.657
[7]  [120/1724] loss: 0.633, ave_loss: 0.653
[8]  [140/1724] loss: 0.623, ave_loss: 0.649
[9]  [160/1724] loss: 0.686, ave_loss: 0.653
[10]  [180/1724] loss: 0.700, ave_loss: 0.658
[11]  [200/1724] loss: 0.717, ave_loss: 0.663
[12]  [220/1724] loss: 0.623, ave_loss: 0.660
[13]  [240/1724] loss: 0.541, ave_loss: 0.651
[14]  [260/1724] loss: 0.661, ave_loss: 0.652
[15]  [280/1724] loss: 0.572, ave_loss: 0.646
[16]  [300/1724] loss: 0.644, ave_loss: 0.646
[17]  [320/1724] loss: 0.688, ave_loss: 0.649
[18]  [340/1724] loss: 0.504, ave_loss: 0.641
[19]  [360/1724] loss: 0.582, ave_loss: 0.638
[20]  [380/1724] loss: 0.731, ave_loss: 0.642
[21]  [400/1724] loss: 0.592, ave_loss: 0.640
[22]  [420/1724] loss: 0.620, ave_loss: 0.639
[23]  [440/1724] loss: 0.588, ave_loss: 0.637
[24]  [460/1724] loss: 0.690, ave_loss: 0.639
[25]  [480/1724] loss: 0.579, ave_loss: 0.637
[26]  [500/1724] loss: 0.656, ave_loss: 0.637
[27]  [520/1724] loss: 0.626, ave_loss: 0.637
[28]  [540/1724] loss: 0.560, ave_loss: 0.634
[29]  [560/1724] loss: 0.615, ave_loss: 0.633
[30]  [580/1724] loss: 0.549, ave_loss: 0.631
[31]  [600/1724] loss: 0.651, ave_loss: 0.631
[32]  [620/1724] loss: 0.571, ave_loss: 0.629
[33]  [640/1724] loss: 0.750, ave_loss: 0.633
[34]  [660/1724] loss: 0.610, ave_loss: 0.632
[35]  [680/1724] loss: 0.706, ave_loss: 0.634
[36]  [700/1724] loss: 0.644, ave_loss: 0.635
[37]  [720/1724] loss: 0.570, ave_loss: 0.633
[38]  [740/1724] loss: 0.660, ave_loss: 0.634
[39]  [760/1724] loss: 0.662, ave_loss: 0.634
[40]  [780/1724] loss: 0.567, ave_loss: 0.633
[41]  [800/1724] loss: 0.766, ave_loss: 0.636
[42]  [820/1724] loss: 0.764, ave_loss: 0.639
[43]  [840/1724] loss: 0.591, ave_loss: 0.638
[44]  [860/1724] loss: 0.607, ave_loss: 0.637
[45]  [880/1724] loss: 0.665, ave_loss: 0.638
[46]  [900/1724] loss: 0.602, ave_loss: 0.637
[47]  [920/1724] loss: 0.548, ave_loss: 0.635
[48]  [940/1724] loss: 0.619, ave_loss: 0.635
[49]  [960/1724] loss: 0.592, ave_loss: 0.634
[50]  [980/1724] loss: 0.635, ave_loss: 0.634
[51]  [1000/1724] loss: 0.619, ave_loss: 0.634
[52]  [1020/1724] loss: 0.654, ave_loss: 0.634
[53]  [1040/1724] loss: 0.688, ave_loss: 0.635
[54]  [1060/1724] loss: 0.486, ave_loss: 0.632
[55]  [1080/1724] loss: 0.630, ave_loss: 0.632
[56]  [1100/1724] loss: 0.552, ave_loss: 0.631
[57]  [1120/1724] loss: 0.631, ave_loss: 0.631
[58]  [1140/1724] loss: 0.628, ave_loss: 0.631
[59]  [1160/1724] loss: 0.612, ave_loss: 0.630
[60]  [1180/1724] loss: 0.597, ave_loss: 0.630
[61]  [1200/1724] loss: 0.617, ave_loss: 0.630
[62]  [1220/1724] loss: 0.605, ave_loss: 0.629
[63]  [1240/1724] loss: 0.751, ave_loss: 0.631
[64]  [1260/1724] loss: 0.652, ave_loss: 0.632
[65]  [1280/1724] loss: 0.635, ave_loss: 0.632
[66]  [1300/1724] loss: 0.735, ave_loss: 0.633
[67]  [1320/1724] loss: 0.647, ave_loss: 0.633
[68]  [1340/1724] loss: 0.659, ave_loss: 0.634
[69]  [1360/1724] loss: 0.669, ave_loss: 0.634
[70]  [1380/1724] loss: 0.709, ave_loss: 0.635
[71]  [1400/1724] loss: 0.626, ave_loss: 0.635
[72]  [1420/1724] loss: 0.618, ave_loss: 0.635
[73]  [1440/1724] loss: 0.582, ave_loss: 0.634
[74]  [1460/1724] loss: 0.653, ave_loss: 0.635
[75]  [1480/1724] loss: 0.691, ave_loss: 0.635
[76]  [1500/1724] loss: 0.567, ave_loss: 0.634
[77]  [1520/1724] loss: 0.843, ave_loss: 0.637
[78]  [1540/1724] loss: 0.855, ave_loss: 0.640
[79]  [1560/1724] loss: 0.593, ave_loss: 0.639
[80]  [1580/1724] loss: 0.579, ave_loss: 0.639
[81]  [1600/1724] loss: 0.569, ave_loss: 0.638
[82]  [1620/1724] loss: 0.727, ave_loss: 0.639
[83]  [1640/1724] loss: 0.534, ave_loss: 0.637
[84]  [1660/1724] loss: 0.733, ave_loss: 0.639
[85]  [1680/1724] loss: 0.543, ave_loss: 0.637
[86]  [1700/1724] loss: 0.681, ave_loss: 0.638
[87]  [1720/1724] loss: 0.575, ave_loss: 0.637
[88]  [1740/1724] loss: 0.587, ave_loss: 0.637

Finished Training finishing at 2021-08-30 18:46:04.844577
printing_out epoch  22.45939675174014 learning rate: 0.00016420834346722596
0.00015928209316320917
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.367e-01
Validation Loss: 5.939e+03
Validation ROC: 0.4625
No improvement, still saving model
6.54060324825986 epochs left to go

Training Epoch 22.45939675174014/30 starting at 2021-08-30 18:46:42.374958
[1]  [0/1724] loss: 0.592, ave_loss: 0.592
[2]  [20/1724] loss: 0.683, ave_loss: 0.637
[3]  [40/1724] loss: 0.511, ave_loss: 0.595
[4]  [60/1724] loss: 0.559, ave_loss: 0.586
[5]  [80/1724] loss: 0.688, ave_loss: 0.606
[6]  [100/1724] loss: 0.566, ave_loss: 0.600
[7]  [120/1724] loss: 0.679, ave_loss: 0.611
[8]  [140/1724] loss: 0.714, ave_loss: 0.624
[9]  [160/1724] loss: 0.611, ave_loss: 0.623
[10]  [180/1724] loss: 0.547, ave_loss: 0.615
[11]  [200/1724] loss: 0.584, ave_loss: 0.612
[12]  [220/1724] loss: 0.645, ave_loss: 0.615
[13]  [240/1724] loss: 0.601, ave_loss: 0.614
[14]  [260/1724] loss: 0.677, ave_loss: 0.618
[15]  [280/1724] loss: 0.600, ave_loss: 0.617
[16]  [300/1724] loss: 0.800, ave_loss: 0.629
[17]  [320/1724] loss: 0.588, ave_loss: 0.626
[18]  [340/1724] loss: 0.734, ave_loss: 0.632
[19]  [360/1724] loss: 0.706, ave_loss: 0.636
[20]  [380/1724] loss: 0.710, ave_loss: 0.640
[21]  [400/1724] loss: 0.661, ave_loss: 0.641
[22]  [420/1724] loss: 0.706, ave_loss: 0.644
[23]  [440/1724] loss: 0.680, ave_loss: 0.645
[24]  [460/1724] loss: 0.566, ave_loss: 0.642
[25]  [480/1724] loss: 0.531, ave_loss: 0.638
[26]  [500/1724] loss: 0.664, ave_loss: 0.639
[27]  [520/1724] loss: 0.646, ave_loss: 0.639
[28]  [540/1724] loss: 0.697, ave_loss: 0.641
[29]  [560/1724] loss: 0.659, ave_loss: 0.642
[30]  [580/1724] loss: 0.653, ave_loss: 0.642
[31]  [600/1724] loss: 0.765, ave_loss: 0.646
[32]  [620/1724] loss: 0.661, ave_loss: 0.646
[33]  [640/1724] loss: 0.754, ave_loss: 0.650
[34]  [660/1724] loss: 0.590, ave_loss: 0.648
[35]  [680/1724] loss: 0.714, ave_loss: 0.650
[36]  [700/1724] loss: 0.497, ave_loss: 0.646
[37]  [720/1724] loss: 0.612, ave_loss: 0.645
[38]  [740/1724] loss: 0.652, ave_loss: 0.645
[39]  [760/1724] loss: 0.648, ave_loss: 0.645
[40]  [780/1724] loss: 0.629, ave_loss: 0.644
[41]  [800/1724] loss: 0.640, ave_loss: 0.644
[42]  [820/1724] loss: 0.672, ave_loss: 0.645
[43]  [840/1724] loss: 0.567, ave_loss: 0.643
[44]  [860/1724] loss: 0.606, ave_loss: 0.642
[45]  [880/1724] loss: 0.602, ave_loss: 0.641
[46]  [900/1724] loss: 0.649, ave_loss: 0.642
[47]  [920/1724] loss: 0.615, ave_loss: 0.641
[48]  [940/1724] loss: 0.584, ave_loss: 0.640
[49]  [960/1724] loss: 0.721, ave_loss: 0.642
[50]  [980/1724] loss: 0.713, ave_loss: 0.643
[51]  [1000/1724] loss: 0.492, ave_loss: 0.640
[52]  [1020/1724] loss: 0.689, ave_loss: 0.641
[53]  [1040/1724] loss: 0.591, ave_loss: 0.640
[54]  [1060/1724] loss: 0.609, ave_loss: 0.639
[55]  [1080/1724] loss: 0.575, ave_loss: 0.638
[56]  [1100/1724] loss: 0.697, ave_loss: 0.639
[57]  [1120/1724] loss: 0.619, ave_loss: 0.639
[58]  [1140/1724] loss: 0.671, ave_loss: 0.639
[59]  [1160/1724] loss: 0.494, ave_loss: 0.637
[60]  [1180/1724] loss: 0.527, ave_loss: 0.635
[61]  [1200/1724] loss: 0.545, ave_loss: 0.634
[62]  [1220/1724] loss: 0.594, ave_loss: 0.633
[63]  [1240/1724] loss: 0.678, ave_loss: 0.634
[64]  [1260/1724] loss: 0.532, ave_loss: 0.632
[65]  [1280/1724] loss: 0.738, ave_loss: 0.634
[66]  [1300/1724] loss: 0.627, ave_loss: 0.634
[67]  [1320/1724] loss: 0.654, ave_loss: 0.634
[68]  [1340/1724] loss: 0.616, ave_loss: 0.634
[69]  [1360/1724] loss: 0.553, ave_loss: 0.633
[70]  [1380/1724] loss: 0.525, ave_loss: 0.631
[71]  [1400/1724] loss: 0.661, ave_loss: 0.631
[72]  [1420/1724] loss: 0.683, ave_loss: 0.632
[73]  [1440/1724] loss: 0.586, ave_loss: 0.632
[74]  [1460/1724] loss: 0.610, ave_loss: 0.631
[75]  [1480/1724] loss: 0.650, ave_loss: 0.632
[76]  [1500/1724] loss: 0.561, ave_loss: 0.631
[77]  [1520/1724] loss: 0.630, ave_loss: 0.631
[78]  [1540/1724] loss: 0.633, ave_loss: 0.631
[79]  [1560/1724] loss: 0.526, ave_loss: 0.629
[80]  [1580/1724] loss: 0.665, ave_loss: 0.630
[81]  [1600/1724] loss: 0.538, ave_loss: 0.629
[82]  [1620/1724] loss: 0.678, ave_loss: 0.629
[83]  [1640/1724] loss: 0.720, ave_loss: 0.630
[84]  [1660/1724] loss: 0.758, ave_loss: 0.632
[85]  [1680/1724] loss: 0.550, ave_loss: 0.631
[86]  [1700/1724] loss: 0.585, ave_loss: 0.630
[87]  [1720/1724] loss: 0.689, ave_loss: 0.631
[88]  [1740/1724] loss: 0.551, ave_loss: 0.630

Finished Training finishing at 2021-08-30 18:48:18.786223
printing_out epoch  23.48027842227378 learning rate: 0.0001492803122429327
0.0001448019028756447
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.301e-01
Validation Loss: 5.976e+03
Validation ROC: 0.4632
No improvement, still saving model
5.519721577726219 epochs left to go

Training Epoch 23.48027842227378/30 starting at 2021-08-30 18:48:52.703259
[1]  [0/1724] loss: 0.711, ave_loss: 0.711
[2]  [20/1724] loss: 0.632, ave_loss: 0.672
[3]  [40/1724] loss: 0.673, ave_loss: 0.672
[4]  [60/1724] loss: 0.628, ave_loss: 0.661
[5]  [80/1724] loss: 0.654, ave_loss: 0.660
[6]  [100/1724] loss: 0.633, ave_loss: 0.655
[7]  [120/1724] loss: 0.526, ave_loss: 0.637
[8]  [140/1724] loss: 0.648, ave_loss: 0.638
[9]  [160/1724] loss: 0.657, ave_loss: 0.640
[10]  [180/1724] loss: 0.625, ave_loss: 0.639
[11]  [200/1724] loss: 0.645, ave_loss: 0.639
[12]  [220/1724] loss: 0.605, ave_loss: 0.636
[13]  [240/1724] loss: 0.704, ave_loss: 0.642
[14]  [260/1724] loss: 0.536, ave_loss: 0.634
[15]  [280/1724] loss: 0.741, ave_loss: 0.641
[16]  [300/1724] loss: 0.592, ave_loss: 0.638
[17]  [320/1724] loss: 0.672, ave_loss: 0.640
[18]  [340/1724] loss: 0.505, ave_loss: 0.633
[19]  [360/1724] loss: 0.648, ave_loss: 0.633
[20]  [380/1724] loss: 0.523, ave_loss: 0.628
[21]  [400/1724] loss: 0.653, ave_loss: 0.629
[22]  [420/1724] loss: 0.781, ave_loss: 0.636
[23]  [440/1724] loss: 0.704, ave_loss: 0.639
[24]  [460/1724] loss: 0.753, ave_loss: 0.644
[25]  [480/1724] loss: 0.605, ave_loss: 0.642
[26]  [500/1724] loss: 0.649, ave_loss: 0.642
[27]  [520/1724] loss: 0.609, ave_loss: 0.641
[28]  [540/1724] loss: 0.693, ave_loss: 0.643
[29]  [560/1724] loss: 0.647, ave_loss: 0.643
[30]  [580/1724] loss: 0.690, ave_loss: 0.645
[31]  [600/1724] loss: 0.746, ave_loss: 0.648
[32]  [620/1724] loss: 0.583, ave_loss: 0.646
[33]  [640/1724] loss: 0.564, ave_loss: 0.644
[34]  [660/1724] loss: 0.581, ave_loss: 0.642
[35]  [680/1724] loss: 0.725, ave_loss: 0.644
[36]  [700/1724] loss: 0.657, ave_loss: 0.644
[37]  [720/1724] loss: 0.504, ave_loss: 0.641
[38]  [740/1724] loss: 0.710, ave_loss: 0.642
[39]  [760/1724] loss: 0.637, ave_loss: 0.642
[40]  [780/1724] loss: 0.577, ave_loss: 0.641
[41]  [800/1724] loss: 0.566, ave_loss: 0.639
[42]  [820/1724] loss: 0.632, ave_loss: 0.639
[43]  [840/1724] loss: 0.554, ave_loss: 0.637
[44]  [860/1724] loss: 0.683, ave_loss: 0.638
[45]  [880/1724] loss: 0.691, ave_loss: 0.639
[46]  [900/1724] loss: 0.794, ave_loss: 0.642
[47]  [920/1724] loss: 0.524, ave_loss: 0.640
[48]  [940/1724] loss: 0.684, ave_loss: 0.641
[49]  [960/1724] loss: 0.740, ave_loss: 0.643
[50]  [980/1724] loss: 0.637, ave_loss: 0.643
[51]  [1000/1724] loss: 0.522, ave_loss: 0.640
[52]  [1020/1724] loss: 0.574, ave_loss: 0.639
[53]  [1040/1724] loss: 0.666, ave_loss: 0.640
[54]  [1060/1724] loss: 0.614, ave_loss: 0.639
[55]  [1080/1724] loss: 0.668, ave_loss: 0.640
[56]  [1100/1724] loss: 0.659, ave_loss: 0.640
[57]  [1120/1724] loss: 0.679, ave_loss: 0.641
[58]  [1140/1724] loss: 0.463, ave_loss: 0.638
[59]  [1160/1724] loss: 0.501, ave_loss: 0.635
[60]  [1180/1724] loss: 0.650, ave_loss: 0.636
[61]  [1200/1724] loss: 0.660, ave_loss: 0.636
[62]  [1220/1724] loss: 0.666, ave_loss: 0.636
[63]  [1240/1724] loss: 0.715, ave_loss: 0.638
[64]  [1260/1724] loss: 0.550, ave_loss: 0.636
[65]  [1280/1724] loss: 0.626, ave_loss: 0.636
[66]  [1300/1724] loss: 0.594, ave_loss: 0.635
[67]  [1320/1724] loss: 0.757, ave_loss: 0.637
[68]  [1340/1724] loss: 0.554, ave_loss: 0.636
[69]  [1360/1724] loss: 0.527, ave_loss: 0.634
[70]  [1380/1724] loss: 0.734, ave_loss: 0.636
[71]  [1400/1724] loss: 0.589, ave_loss: 0.635
[72]  [1420/1724] loss: 0.562, ave_loss: 0.634
[73]  [1440/1724] loss: 0.618, ave_loss: 0.634
[74]  [1460/1724] loss: 0.728, ave_loss: 0.635
[75]  [1480/1724] loss: 0.594, ave_loss: 0.635
[76]  [1500/1724] loss: 0.624, ave_loss: 0.635
[77]  [1520/1724] loss: 0.676, ave_loss: 0.635
[78]  [1540/1724] loss: 0.673, ave_loss: 0.636
[79]  [1560/1724] loss: 0.613, ave_loss: 0.635
[80]  [1580/1724] loss: 0.600, ave_loss: 0.635
[81]  [1600/1724] loss: 0.525, ave_loss: 0.634
[82]  [1620/1724] loss: 0.547, ave_loss: 0.632
[83]  [1640/1724] loss: 0.702, ave_loss: 0.633
[84]  [1660/1724] loss: 0.606, ave_loss: 0.633
[85]  [1680/1724] loss: 0.476, ave_loss: 0.631
[86]  [1700/1724] loss: 0.589, ave_loss: 0.631
[87]  [1720/1724] loss: 0.682, ave_loss: 0.631
[88]  [1740/1724] loss: 0.571, ave_loss: 0.631

Finished Training finishing at 2021-08-30 18:50:32.478919
printing_out epoch  24.501160092807424 learning rate: 0.00013570937476630243
0.00013163809352331336
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.306e-01
Validation Loss: 5.996e+03
Validation ROC: 0.4632
No improvement, still saving model
4.4988399071925755 epochs left to go

Training Epoch 24.501160092807424/30 starting at 2021-08-30 18:51:08.153253
[1]  [0/1724] loss: 0.743, ave_loss: 0.743
[2]  [20/1724] loss: 0.577, ave_loss: 0.660
[3]  [40/1724] loss: 0.675, ave_loss: 0.665
[4]  [60/1724] loss: 0.632, ave_loss: 0.657
[5]  [80/1724] loss: 0.695, ave_loss: 0.664
[6]  [100/1724] loss: 0.637, ave_loss: 0.660
[7]  [120/1724] loss: 0.605, ave_loss: 0.652
[8]  [140/1724] loss: 0.872, ave_loss: 0.679
[9]  [160/1724] loss: 0.615, ave_loss: 0.672
[10]  [180/1724] loss: 0.638, ave_loss: 0.669
[11]  [200/1724] loss: 0.613, ave_loss: 0.664
[12]  [220/1724] loss: 0.599, ave_loss: 0.658
[13]  [240/1724] loss: 0.516, ave_loss: 0.647
[14]  [260/1724] loss: 0.624, ave_loss: 0.646
[15]  [280/1724] loss: 0.652, ave_loss: 0.646
[16]  [300/1724] loss: 0.592, ave_loss: 0.643
[17]  [320/1724] loss: 0.587, ave_loss: 0.639
[18]  [340/1724] loss: 0.535, ave_loss: 0.634
[19]  [360/1724] loss: 0.582, ave_loss: 0.631
[20]  [380/1724] loss: 0.795, ave_loss: 0.639
[21]  [400/1724] loss: 0.650, ave_loss: 0.640
[22]  [420/1724] loss: 0.799, ave_loss: 0.647
[23]  [440/1724] loss: 0.608, ave_loss: 0.645
[24]  [460/1724] loss: 0.736, ave_loss: 0.649
[25]  [480/1724] loss: 0.566, ave_loss: 0.646
[26]  [500/1724] loss: 0.593, ave_loss: 0.644
[27]  [520/1724] loss: 0.709, ave_loss: 0.646
[28]  [540/1724] loss: 0.684, ave_loss: 0.647
[29]  [560/1724] loss: 0.599, ave_loss: 0.646
[30]  [580/1724] loss: 0.780, ave_loss: 0.650
[31]  [600/1724] loss: 0.597, ave_loss: 0.648
[32]  [620/1724] loss: 0.594, ave_loss: 0.647
[33]  [640/1724] loss: 0.603, ave_loss: 0.645
[34]  [660/1724] loss: 0.696, ave_loss: 0.647
[35]  [680/1724] loss: 0.687, ave_loss: 0.648
[36]  [700/1724] loss: 0.590, ave_loss: 0.646
[37]  [720/1724] loss: 0.712, ave_loss: 0.648
[38]  [740/1724] loss: 0.612, ave_loss: 0.647
[39]  [760/1724] loss: 0.628, ave_loss: 0.647
[40]  [780/1724] loss: 0.641, ave_loss: 0.647
[41]  [800/1724] loss: 0.591, ave_loss: 0.645
[42]  [820/1724] loss: 0.608, ave_loss: 0.644
[43]  [840/1724] loss: 0.641, ave_loss: 0.644
[44]  [860/1724] loss: 0.549, ave_loss: 0.642
[45]  [880/1724] loss: 0.636, ave_loss: 0.642
[46]  [900/1724] loss: 0.657, ave_loss: 0.642
[47]  [920/1724] loss: 0.533, ave_loss: 0.640
[48]  [940/1724] loss: 0.660, ave_loss: 0.640
[49]  [960/1724] loss: 0.681, ave_loss: 0.641
[50]  [980/1724] loss: 0.833, ave_loss: 0.645
[51]  [1000/1724] loss: 0.631, ave_loss: 0.645
[52]  [1020/1724] loss: 0.688, ave_loss: 0.646
[53]  [1040/1724] loss: 0.778, ave_loss: 0.648
[54]  [1060/1724] loss: 0.564, ave_loss: 0.647
[55]  [1080/1724] loss: 0.582, ave_loss: 0.645
[56]  [1100/1724] loss: 0.616, ave_loss: 0.645
[57]  [1120/1724] loss: 0.656, ave_loss: 0.645
[58]  [1140/1724] loss: 0.668, ave_loss: 0.645
[59]  [1160/1724] loss: 0.618, ave_loss: 0.645
[60]  [1180/1724] loss: 0.633, ave_loss: 0.645
[61]  [1200/1724] loss: 0.640, ave_loss: 0.645
[62]  [1220/1724] loss: 0.518, ave_loss: 0.643
[63]  [1240/1724] loss: 0.573, ave_loss: 0.642
[64]  [1260/1724] loss: 0.549, ave_loss: 0.640
[65]  [1280/1724] loss: 0.650, ave_loss: 0.640
[66]  [1300/1724] loss: 0.592, ave_loss: 0.640
[67]  [1320/1724] loss: 0.722, ave_loss: 0.641
[68]  [1340/1724] loss: 0.594, ave_loss: 0.640
[69]  [1360/1724] loss: 0.672, ave_loss: 0.641
[70]  [1380/1724] loss: 0.612, ave_loss: 0.640
[71]  [1400/1724] loss: 0.603, ave_loss: 0.640
[72]  [1420/1724] loss: 0.611, ave_loss: 0.639
[73]  [1440/1724] loss: 0.597, ave_loss: 0.639
[74]  [1460/1724] loss: 0.700, ave_loss: 0.639
[75]  [1480/1724] loss: 0.519, ave_loss: 0.638
[76]  [1500/1724] loss: 0.769, ave_loss: 0.640
[77]  [1520/1724] loss: 0.575, ave_loss: 0.639
[78]  [1540/1724] loss: 0.650, ave_loss: 0.639
[79]  [1560/1724] loss: 0.617, ave_loss: 0.639
[80]  [1580/1724] loss: 0.653, ave_loss: 0.639
[81]  [1600/1724] loss: 0.504, ave_loss: 0.637
[82]  [1620/1724] loss: 0.665, ave_loss: 0.637
[83]  [1640/1724] loss: 0.699, ave_loss: 0.638
[84]  [1660/1724] loss: 0.719, ave_loss: 0.639
[85]  [1680/1724] loss: 0.839, ave_loss: 0.642
[86]  [1700/1724] loss: 0.645, ave_loss: 0.642
[87]  [1720/1724] loss: 0.612, ave_loss: 0.641
[88]  [1740/1724] loss: 0.598, ave_loss: 0.641

Finished Training finishing at 2021-08-30 18:52:45.911501
printing_out epoch  25.52204176334107 learning rate: 0.00012337215887845676
0.00011967099411210306
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.408e-01
Validation Loss: 5.997e+03
Validation ROC: 0.4632
No improvement, still saving model
3.4779582366589317 epochs left to go

Training Epoch 25.52204176334107/30 starting at 2021-08-30 18:53:19.767235
[1]  [0/1724] loss: 0.614, ave_loss: 0.614
[2]  [20/1724] loss: 0.562, ave_loss: 0.588
[3]  [40/1724] loss: 0.725, ave_loss: 0.634
[4]  [60/1724] loss: 0.650, ave_loss: 0.638
[5]  [80/1724] loss: 0.572, ave_loss: 0.625
[6]  [100/1724] loss: 0.657, ave_loss: 0.630
[7]  [120/1724] loss: 0.768, ave_loss: 0.650
[8]  [140/1724] loss: 0.674, ave_loss: 0.653
[9]  [160/1724] loss: 0.657, ave_loss: 0.653
[10]  [180/1724] loss: 0.492, ave_loss: 0.637
[11]  [200/1724] loss: 0.478, ave_loss: 0.623
[12]  [220/1724] loss: 0.478, ave_loss: 0.611
[13]  [240/1724] loss: 0.717, ave_loss: 0.619
[14]  [260/1724] loss: 0.596, ave_loss: 0.617
[15]  [280/1724] loss: 0.563, ave_loss: 0.614
[16]  [300/1724] loss: 0.651, ave_loss: 0.616
[17]  [320/1724] loss: 0.676, ave_loss: 0.619
[18]  [340/1724] loss: 0.708, ave_loss: 0.624
[19]  [360/1724] loss: 0.548, ave_loss: 0.620
[20]  [380/1724] loss: 0.595, ave_loss: 0.619
[21]  [400/1724] loss: 0.696, ave_loss: 0.623
[22]  [420/1724] loss: 0.808, ave_loss: 0.631
[23]  [440/1724] loss: 0.700, ave_loss: 0.634
[24]  [460/1724] loss: 0.589, ave_loss: 0.632
[25]  [480/1724] loss: 0.841, ave_loss: 0.641
[26]  [500/1724] loss: 0.605, ave_loss: 0.639
[27]  [520/1724] loss: 0.771, ave_loss: 0.644
[28]  [540/1724] loss: 0.762, ave_loss: 0.648
[29]  [560/1724] loss: 0.634, ave_loss: 0.648
[30]  [580/1724] loss: 0.573, ave_loss: 0.645
[31]  [600/1724] loss: 0.648, ave_loss: 0.645
[32]  [620/1724] loss: 0.682, ave_loss: 0.646
[33]  [640/1724] loss: 0.572, ave_loss: 0.644
[34]  [660/1724] loss: 0.704, ave_loss: 0.646
[35]  [680/1724] loss: 0.531, ave_loss: 0.643
[36]  [700/1724] loss: 0.626, ave_loss: 0.642
[37]  [720/1724] loss: 0.579, ave_loss: 0.641
[38]  [740/1724] loss: 0.649, ave_loss: 0.641
[39]  [760/1724] loss: 0.488, ave_loss: 0.637
[40]  [780/1724] loss: 0.639, ave_loss: 0.637
[41]  [800/1724] loss: 0.769, ave_loss: 0.640
[42]  [820/1724] loss: 0.569, ave_loss: 0.638
[43]  [840/1724] loss: 0.712, ave_loss: 0.640
[44]  [860/1724] loss: 0.621, ave_loss: 0.640
[45]  [880/1724] loss: 0.720, ave_loss: 0.642
[46]  [900/1724] loss: 0.553, ave_loss: 0.640
[47]  [920/1724] loss: 0.648, ave_loss: 0.640
[48]  [940/1724] loss: 0.649, ave_loss: 0.640
[49]  [960/1724] loss: 0.665, ave_loss: 0.640
[50]  [980/1724] loss: 0.495, ave_loss: 0.638
[51]  [1000/1724] loss: 0.708, ave_loss: 0.639
[52]  [1020/1724] loss: 0.516, ave_loss: 0.637
[53]  [1040/1724] loss: 0.655, ave_loss: 0.637
[54]  [1060/1724] loss: 0.528, ave_loss: 0.635
[55]  [1080/1724] loss: 0.621, ave_loss: 0.635
[56]  [1100/1724] loss: 0.574, ave_loss: 0.634
[57]  [1120/1724] loss: 0.665, ave_loss: 0.634
[58]  [1140/1724] loss: 0.810, ave_loss: 0.637
[59]  [1160/1724] loss: 0.702, ave_loss: 0.638
[60]  [1180/1724] loss: 0.642, ave_loss: 0.638
[61]  [1200/1724] loss: 0.625, ave_loss: 0.638
[62]  [1220/1724] loss: 0.556, ave_loss: 0.637
[63]  [1240/1724] loss: 0.627, ave_loss: 0.637
[64]  [1260/1724] loss: 0.652, ave_loss: 0.637
[65]  [1280/1724] loss: 0.681, ave_loss: 0.638
[66]  [1300/1724] loss: 0.645, ave_loss: 0.638
[67]  [1320/1724] loss: 0.594, ave_loss: 0.637
[68]  [1340/1724] loss: 0.673, ave_loss: 0.638
[69]  [1360/1724] loss: 0.686, ave_loss: 0.638
[70]  [1380/1724] loss: 0.558, ave_loss: 0.637
[71]  [1400/1724] loss: 0.741, ave_loss: 0.639
[72]  [1420/1724] loss: 0.603, ave_loss: 0.638
[73]  [1440/1724] loss: 0.641, ave_loss: 0.638
[74]  [1460/1724] loss: 0.625, ave_loss: 0.638
[75]  [1480/1724] loss: 0.581, ave_loss: 0.637
[76]  [1500/1724] loss: 0.481, ave_loss: 0.635
[77]  [1520/1724] loss: 0.678, ave_loss: 0.636
[78]  [1540/1724] loss: 0.737, ave_loss: 0.637
[79]  [1560/1724] loss: 0.654, ave_loss: 0.637
[80]  [1580/1724] loss: 0.593, ave_loss: 0.637
[81]  [1600/1724] loss: 0.764, ave_loss: 0.638
[82]  [1620/1724] loss: 0.651, ave_loss: 0.638
[83]  [1640/1724] loss: 0.642, ave_loss: 0.638
[84]  [1660/1724] loss: 0.565, ave_loss: 0.638
[85]  [1680/1724] loss: 0.689, ave_loss: 0.638
[86]  [1700/1724] loss: 0.799, ave_loss: 0.640
[87]  [1720/1724] loss: 0.678, ave_loss: 0.640
[88]  [1740/1724] loss: 0.546, ave_loss: 0.639

Finished Training finishing at 2021-08-30 18:54:54.845769
printing_out epoch  26.54292343387471 learning rate: 0.00011215650807132433
0.0001087918128291846
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.394e-01
Validation Loss: 5.971e+03
Validation ROC: 0.4632
No improvement, still saving model
2.4570765661252914 epochs left to go

Training Epoch 26.54292343387471/30 starting at 2021-08-30 18:55:31.124014
[1]  [0/1724] loss: 0.693, ave_loss: 0.693
[2]  [20/1724] loss: 0.696, ave_loss: 0.695
[3]  [40/1724] loss: 0.884, ave_loss: 0.758
[4]  [60/1724] loss: 0.594, ave_loss: 0.717
[5]  [80/1724] loss: 0.572, ave_loss: 0.688
[6]  [100/1724] loss: 0.659, ave_loss: 0.683
[7]  [120/1724] loss: 0.539, ave_loss: 0.662
[8]  [140/1724] loss: 0.591, ave_loss: 0.654
[9]  [160/1724] loss: 0.590, ave_loss: 0.646
[10]  [180/1724] loss: 0.607, ave_loss: 0.643
[11]  [200/1724] loss: 0.536, ave_loss: 0.633
[12]  [220/1724] loss: 0.592, ave_loss: 0.630
[13]  [240/1724] loss: 0.554, ave_loss: 0.624
[14]  [260/1724] loss: 0.593, ave_loss: 0.622
[15]  [280/1724] loss: 0.612, ave_loss: 0.621
[16]  [300/1724] loss: 0.649, ave_loss: 0.623
[17]  [320/1724] loss: 0.596, ave_loss: 0.621
[18]  [340/1724] loss: 0.561, ave_loss: 0.618
[19]  [360/1724] loss: 0.670, ave_loss: 0.620
[20]  [380/1724] loss: 0.540, ave_loss: 0.616
[21]  [400/1724] loss: 0.736, ave_loss: 0.622
[22]  [420/1724] loss: 0.590, ave_loss: 0.621
[23]  [440/1724] loss: 0.716, ave_loss: 0.625
[24]  [460/1724] loss: 0.523, ave_loss: 0.621
[25]  [480/1724] loss: 0.539, ave_loss: 0.617
[26]  [500/1724] loss: 0.556, ave_loss: 0.615
[27]  [520/1724] loss: 0.560, ave_loss: 0.613
[28]  [540/1724] loss: 0.564, ave_loss: 0.611
[29]  [560/1724] loss: 0.739, ave_loss: 0.616
[30]  [580/1724] loss: 0.568, ave_loss: 0.614
[31]  [600/1724] loss: 0.635, ave_loss: 0.615
[32]  [620/1724] loss: 0.602, ave_loss: 0.614
[33]  [640/1724] loss: 0.695, ave_loss: 0.617
[34]  [660/1724] loss: 0.660, ave_loss: 0.618
[35]  [680/1724] loss: 0.654, ave_loss: 0.619
[36]  [700/1724] loss: 0.752, ave_loss: 0.623
[37]  [720/1724] loss: 0.503, ave_loss: 0.620
[38]  [740/1724] loss: 0.666, ave_loss: 0.621
[39]  [760/1724] loss: 0.615, ave_loss: 0.621
[40]  [780/1724] loss: 0.570, ave_loss: 0.619
[41]  [800/1724] loss: 0.637, ave_loss: 0.620
[42]  [820/1724] loss: 0.618, ave_loss: 0.620
[43]  [840/1724] loss: 0.614, ave_loss: 0.620
[44]  [860/1724] loss: 0.593, ave_loss: 0.619
[45]  [880/1724] loss: 0.569, ave_loss: 0.618
[46]  [900/1724] loss: 0.663, ave_loss: 0.619
[47]  [920/1724] loss: 0.663, ave_loss: 0.620
[48]  [940/1724] loss: 0.610, ave_loss: 0.620
[49]  [960/1724] loss: 0.753, ave_loss: 0.622
[50]  [980/1724] loss: 0.607, ave_loss: 0.622
[51]  [1000/1724] loss: 0.671, ave_loss: 0.623
[52]  [1020/1724] loss: 0.659, ave_loss: 0.624
[53]  [1040/1724] loss: 0.617, ave_loss: 0.623
[54]  [1060/1724] loss: 0.563, ave_loss: 0.622
[55]  [1080/1724] loss: 0.584, ave_loss: 0.622
[56]  [1100/1724] loss: 0.724, ave_loss: 0.624
[57]  [1120/1724] loss: 0.640, ave_loss: 0.624
[58]  [1140/1724] loss: 0.619, ave_loss: 0.624
[59]  [1160/1724] loss: 0.500, ave_loss: 0.622
[60]  [1180/1724] loss: 0.682, ave_loss: 0.623
[61]  [1200/1724] loss: 0.589, ave_loss: 0.622
[62]  [1220/1724] loss: 0.586, ave_loss: 0.621
[63]  [1240/1724] loss: 0.559, ave_loss: 0.621
[64]  [1260/1724] loss: 0.520, ave_loss: 0.619
[65]  [1280/1724] loss: 0.707, ave_loss: 0.620
[66]  [1300/1724] loss: 0.625, ave_loss: 0.620
[67]  [1320/1724] loss: 0.561, ave_loss: 0.619
[68]  [1340/1724] loss: 0.767, ave_loss: 0.622
[69]  [1360/1724] loss: 0.692, ave_loss: 0.623
[70]  [1380/1724] loss: 0.672, ave_loss: 0.623
[71]  [1400/1724] loss: 0.817, ave_loss: 0.626
[72]  [1420/1724] loss: 0.718, ave_loss: 0.627
[73]  [1440/1724] loss: 0.604, ave_loss: 0.627
[74]  [1460/1724] loss: 0.671, ave_loss: 0.628
[75]  [1480/1724] loss: 0.594, ave_loss: 0.627
[76]  [1500/1724] loss: 0.658, ave_loss: 0.628
[77]  [1520/1724] loss: 0.612, ave_loss: 0.627
[78]  [1540/1724] loss: 0.700, ave_loss: 0.628
[79]  [1560/1724] loss: 0.636, ave_loss: 0.628
[80]  [1580/1724] loss: 0.599, ave_loss: 0.628
[81]  [1600/1724] loss: 0.547, ave_loss: 0.627
[82]  [1620/1724] loss: 0.544, ave_loss: 0.626
[83]  [1640/1724] loss: 0.535, ave_loss: 0.625
[84]  [1660/1724] loss: 0.531, ave_loss: 0.624
[85]  [1680/1724] loss: 0.644, ave_loss: 0.624
[86]  [1700/1724] loss: 0.616, ave_loss: 0.624
[87]  [1720/1724] loss: 0.719, ave_loss: 0.625
[88]  [1740/1724] loss: 0.563, ave_loss: 0.624

Finished Training finishing at 2021-08-30 18:57:03.865528
printing_out epoch  27.563805104408353 learning rate: 0.00010196046188302211
9.890164802653145e-05
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.244e-01
Validation Loss: 5.998e+03
Validation ROC: 0.4632
No improvement, still saving model
1.4361948955916475 epochs left to go

Training Epoch 27.563805104408353/30 starting at 2021-08-30 18:57:44.161273
[1]  [0/1724] loss: 0.549, ave_loss: 0.549
[2]  [20/1724] loss: 0.699, ave_loss: 0.624
[3]  [40/1724] loss: 0.554, ave_loss: 0.601
[4]  [60/1724] loss: 0.676, ave_loss: 0.620
[5]  [80/1724] loss: 0.539, ave_loss: 0.604
[6]  [100/1724] loss: 0.615, ave_loss: 0.606
[7]  [120/1724] loss: 0.634, ave_loss: 0.610
[8]  [140/1724] loss: 0.636, ave_loss: 0.613
[9]  [160/1724] loss: 0.575, ave_loss: 0.609
[10]  [180/1724] loss: 0.761, ave_loss: 0.624
[11]  [200/1724] loss: 0.701, ave_loss: 0.631
[12]  [220/1724] loss: 0.708, ave_loss: 0.637
[13]  [240/1724] loss: 0.626, ave_loss: 0.637
[14]  [260/1724] loss: 0.746, ave_loss: 0.644
[15]  [280/1724] loss: 0.759, ave_loss: 0.652
[16]  [300/1724] loss: 0.628, ave_loss: 0.651
[17]  [320/1724] loss: 0.628, ave_loss: 0.649
[18]  [340/1724] loss: 0.695, ave_loss: 0.652
[19]  [360/1724] loss: 0.586, ave_loss: 0.648
[20]  [380/1724] loss: 0.656, ave_loss: 0.649
[21]  [400/1724] loss: 0.668, ave_loss: 0.650
[22]  [420/1724] loss: 0.651, ave_loss: 0.650
[23]  [440/1724] loss: 0.769, ave_loss: 0.655
[24]  [460/1724] loss: 0.608, ave_loss: 0.653
[25]  [480/1724] loss: 0.499, ave_loss: 0.647
[26]  [500/1724] loss: 0.568, ave_loss: 0.644
[27]  [520/1724] loss: 0.649, ave_loss: 0.644
[28]  [540/1724] loss: 0.574, ave_loss: 0.641
[29]  [560/1724] loss: 0.638, ave_loss: 0.641
[30]  [580/1724] loss: 0.699, ave_loss: 0.643
[31]  [600/1724] loss: 0.869, ave_loss: 0.650
[32]  [620/1724] loss: 0.571, ave_loss: 0.648
[33]  [640/1724] loss: 0.613, ave_loss: 0.647
[34]  [660/1724] loss: 0.697, ave_loss: 0.648
[35]  [680/1724] loss: 0.667, ave_loss: 0.649
[36]  [700/1724] loss: 0.731, ave_loss: 0.651
[37]  [720/1724] loss: 0.675, ave_loss: 0.652
[38]  [740/1724] loss: 0.697, ave_loss: 0.653
[39]  [760/1724] loss: 0.572, ave_loss: 0.651
[40]  [780/1724] loss: 0.641, ave_loss: 0.651
[41]  [800/1724] loss: 0.601, ave_loss: 0.650
[42]  [820/1724] loss: 0.629, ave_loss: 0.649
[43]  [840/1724] loss: 0.700, ave_loss: 0.650
[44]  [860/1724] loss: 0.546, ave_loss: 0.648
[45]  [880/1724] loss: 0.577, ave_loss: 0.646
[46]  [900/1724] loss: 0.752, ave_loss: 0.649
[47]  [920/1724] loss: 0.704, ave_loss: 0.650
[48]  [940/1724] loss: 0.623, ave_loss: 0.649
[49]  [960/1724] loss: 0.456, ave_loss: 0.645
[50]  [980/1724] loss: 0.723, ave_loss: 0.647
[51]  [1000/1724] loss: 0.684, ave_loss: 0.648
[52]  [1020/1724] loss: 0.653, ave_loss: 0.648
[53]  [1040/1724] loss: 0.563, ave_loss: 0.646
[54]  [1060/1724] loss: 0.575, ave_loss: 0.645
[55]  [1080/1724] loss: 0.600, ave_loss: 0.644
[56]  [1100/1724] loss: 0.644, ave_loss: 0.644
[57]  [1120/1724] loss: 0.564, ave_loss: 0.643
[58]  [1140/1724] loss: 0.724, ave_loss: 0.644
[59]  [1160/1724] loss: 0.640, ave_loss: 0.644
[60]  [1180/1724] loss: 0.611, ave_loss: 0.643
[61]  [1200/1724] loss: 0.674, ave_loss: 0.644
[62]  [1220/1724] loss: 0.740, ave_loss: 0.645
[63]  [1240/1724] loss: 0.621, ave_loss: 0.645
[64]  [1260/1724] loss: 0.558, ave_loss: 0.644
[65]  [1280/1724] loss: 0.787, ave_loss: 0.646
[66]  [1300/1724] loss: 0.562, ave_loss: 0.645
[67]  [1320/1724] loss: 0.545, ave_loss: 0.643
[68]  [1340/1724] loss: 0.667, ave_loss: 0.643
[69]  [1360/1724] loss: 0.596, ave_loss: 0.643
[70]  [1380/1724] loss: 0.620, ave_loss: 0.642
[71]  [1400/1724] loss: 0.639, ave_loss: 0.642
[72]  [1420/1724] loss: 0.624, ave_loss: 0.642
[73]  [1440/1724] loss: 0.534, ave_loss: 0.641
[74]  [1460/1724] loss: 0.701, ave_loss: 0.641
[75]  [1480/1724] loss: 0.632, ave_loss: 0.641
[76]  [1500/1724] loss: 0.625, ave_loss: 0.641
[77]  [1520/1724] loss: 0.764, ave_loss: 0.643
[78]  [1540/1724] loss: 0.781, ave_loss: 0.644
[79]  [1560/1724] loss: 0.465, ave_loss: 0.642
[80]  [1580/1724] loss: 0.588, ave_loss: 0.642
[81]  [1600/1724] loss: 0.675, ave_loss: 0.642
[82]  [1620/1724] loss: 0.613, ave_loss: 0.642
[83]  [1640/1724] loss: 0.626, ave_loss: 0.641
[84]  [1660/1724] loss: 0.607, ave_loss: 0.641
[85]  [1680/1724] loss: 0.588, ave_loss: 0.640
[86]  [1700/1724] loss: 0.582, ave_loss: 0.640
[87]  [1720/1724] loss: 0.707, ave_loss: 0.640
[88]  [1740/1724] loss: 0.729, ave_loss: 0.641

Finished Training finishing at 2021-08-30 18:59:20.836294
printing_out epoch  28.584686774941996 learning rate: 9.269132898456555e-05
8.991058911502858e-05
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.415e-01
Validation Loss: 5.989e+03
Validation ROC: 0.4632
No improvement, still saving model
0.4153132250580036 epochs left to go

Training Epoch 28.584686774941996/30 starting at 2021-08-30 18:59:55.493092
[1]  [0/1724] loss: 0.601, ave_loss: 0.601
[2]  [20/1724] loss: 0.527, ave_loss: 0.564
[3]  [40/1724] loss: 0.513, ave_loss: 0.547
[4]  [60/1724] loss: 0.651, ave_loss: 0.573
[5]  [80/1724] loss: 0.627, ave_loss: 0.584
[6]  [100/1724] loss: 0.644, ave_loss: 0.594
[7]  [120/1724] loss: 0.625, ave_loss: 0.598
[8]  [140/1724] loss: 0.609, ave_loss: 0.600
[9]  [160/1724] loss: 0.725, ave_loss: 0.614
[10]  [180/1724] loss: 0.566, ave_loss: 0.609
[11]  [200/1724] loss: 0.655, ave_loss: 0.613
[12]  [220/1724] loss: 0.542, ave_loss: 0.607
[13]  [240/1724] loss: 0.589, ave_loss: 0.606
[14]  [260/1724] loss: 0.801, ave_loss: 0.620
[15]  [280/1724] loss: 0.582, ave_loss: 0.617
[16]  [300/1724] loss: 0.581, ave_loss: 0.615
[17]  [320/1724] loss: 0.687, ave_loss: 0.619
[18]  [340/1724] loss: 0.737, ave_loss: 0.626
[19]  [360/1724] loss: 0.649, ave_loss: 0.627
[20]  [380/1724] loss: 0.594, ave_loss: 0.625
[21]  [400/1724] loss: 0.611, ave_loss: 0.625
[22]  [420/1724] loss: 0.448, ave_loss: 0.617
[23]  [440/1724] loss: 0.572, ave_loss: 0.615
[24]  [460/1724] loss: 0.541, ave_loss: 0.612
[25]  [480/1724] loss: 0.537, ave_loss: 0.609
[26]  [500/1724] loss: 0.607, ave_loss: 0.608
[27]  [520/1724] loss: 0.521, ave_loss: 0.605
[28]  [540/1724] loss: 0.589, ave_loss: 0.605
[29]  [560/1724] loss: 0.608, ave_loss: 0.605
[30]  [580/1724] loss: 0.529, ave_loss: 0.602
[31]  [600/1724] loss: 0.654, ave_loss: 0.604
[32]  [620/1724] loss: 0.669, ave_loss: 0.606
[33]  [640/1724] loss: 0.659, ave_loss: 0.608
[34]  [660/1724] loss: 0.517, ave_loss: 0.605
[35]  [680/1724] loss: 0.635, ave_loss: 0.606
[36]  [700/1724] loss: 0.633, ave_loss: 0.607
[37]  [720/1724] loss: 0.727, ave_loss: 0.610
[38]  [740/1724] loss: 0.594, ave_loss: 0.609
[39]  [760/1724] loss: 0.604, ave_loss: 0.609
[40]  [780/1724] loss: 0.666, ave_loss: 0.611
[41]  [800/1724] loss: 0.554, ave_loss: 0.609
[42]  [820/1724] loss: 0.595, ave_loss: 0.609
[43]  [840/1724] loss: 0.562, ave_loss: 0.608
[44]  [860/1724] loss: 0.604, ave_loss: 0.608
[45]  [880/1724] loss: 0.598, ave_loss: 0.608
[46]  [900/1724] loss: 0.520, ave_loss: 0.606
[47]  [920/1724] loss: 0.635, ave_loss: 0.606
[48]  [940/1724] loss: 0.661, ave_loss: 0.607
[49]  [960/1724] loss: 0.693, ave_loss: 0.609
[50]  [980/1724] loss: 0.682, ave_loss: 0.611
[51]  [1000/1724] loss: 0.568, ave_loss: 0.610
[52]  [1020/1724] loss: 0.541, ave_loss: 0.608
[53]  [1040/1724] loss: 0.719, ave_loss: 0.611
[54]  [1060/1724] loss: 0.693, ave_loss: 0.612
[55]  [1080/1724] loss: 0.597, ave_loss: 0.612
[56]  [1100/1724] loss: 0.520, ave_loss: 0.610
[57]  [1120/1724] loss: 0.539, ave_loss: 0.609
[58]  [1140/1724] loss: 0.547, ave_loss: 0.608
[59]  [1160/1724] loss: 0.718, ave_loss: 0.610
[60]  [1180/1724] loss: 0.581, ave_loss: 0.609
[61]  [1200/1724] loss: 0.705, ave_loss: 0.611
[62]  [1220/1724] loss: 0.647, ave_loss: 0.611
[63]  [1240/1724] loss: 0.539, ave_loss: 0.610
[64]  [1260/1724] loss: 0.714, ave_loss: 0.612
[65]  [1280/1724] loss: 0.663, ave_loss: 0.613
[66]  [1300/1724] loss: 0.496, ave_loss: 0.611
[67]  [1320/1724] loss: 0.577, ave_loss: 0.610
[68]  [1340/1724] loss: 0.662, ave_loss: 0.611
[69]  [1360/1724] loss: 0.587, ave_loss: 0.611
[70]  [1380/1724] loss: 0.606, ave_loss: 0.611
[71]  [1400/1724] loss: 0.577, ave_loss: 0.610
[72]  [1420/1724] loss: 0.724, ave_loss: 0.612
[73]  [1440/1724] loss: 0.560, ave_loss: 0.611
[74]  [1460/1724] loss: 0.574, ave_loss: 0.611
[75]  [1480/1724] loss: 0.544, ave_loss: 0.610
[76]  [1500/1724] loss: 0.593, ave_loss: 0.609
[77]  [1520/1724] loss: 0.626, ave_loss: 0.610
[78]  [1540/1724] loss: 0.590, ave_loss: 0.609
[79]  [1560/1724] loss: 0.698, ave_loss: 0.611
[80]  [1580/1724] loss: 0.597, ave_loss: 0.610
[81]  [1600/1724] loss: 0.602, ave_loss: 0.610
[82]  [1620/1724] loss: 0.626, ave_loss: 0.610
[83]  [1640/1724] loss: 0.642, ave_loss: 0.611
[84]  [1660/1724] loss: 0.653, ave_loss: 0.611
[85]  [1680/1724] loss: 0.636, ave_loss: 0.612
[86]  [1700/1724] loss: 0.748, ave_loss: 0.613
[87]  [1720/1724] loss: 0.611, ave_loss: 0.613
[88]  [1740/1724] loss: 0.634, ave_loss: 0.613

Finished Training finishing at 2021-08-30 19:01:25.275347
printing_out epoch  29.605568445475637 learning rate: 8.426484453142322e-05
8.173689919548052e-05
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.134e-01
Validation Loss: 6.063e+03
Validation ROC: 0.4632
No improvement, still saving model
saving results
