reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
Arguments:  Namespace(channels_spatial=['c16', 'c8', 'c4', 'c2'], channels_temporal=['c2', 'd2', 'd2'], input_div=1.0, kernel_spatial=4, kernel_temporal=8, linear_sizes=[20, 5], no_scalars=False, subsampling=10, tcn_hidden=2, tcn_layers=2, tcn_type='d')
...done
Training on 1724 shots, testing on 857 shots
Classical convolution with channels  2 16
Classical convolution with channels  16 8
Classical convolution with channels  8 4
Classical convolution with channels  4 2
InputBlock parameters:  14 2 64 ['c16', 'c8', 'c4', 'c2'] 4 [20, 5] 0.08
TCN parameters:  19 1 ['c2', 'd2', 'd2'] 8 0.08
29 epochs left to go

Training Epoch 0/30 starting at 2021-08-29 18:36:00.158881
[1]  [0/1724] loss: 2.202, ave_loss: 2.202
[2]  [20/1724] loss: 3.107, ave_loss: 2.654
[3]  [40/1724] loss: 2.783, ave_loss: 2.697
[4]  [60/1724] loss: 3.437, ave_loss: 2.882
[5]  [80/1724] loss: 2.904, ave_loss: 2.887
[6]  [100/1724] loss: 2.529, ave_loss: 2.827
[7]  [120/1724] loss: 2.007, ave_loss: 2.710
[8]  [140/1724] loss: 2.218, ave_loss: 2.649
[9]  [160/1724] loss: 2.587, ave_loss: 2.642
[10]  [180/1724] loss: 2.080, ave_loss: 2.586
[11]  [200/1724] loss: 2.069, ave_loss: 2.539
[12]  [220/1724] loss: 5.572, ave_loss: 2.791
[13]  [240/1724] loss: 1.839, ave_loss: 2.718
[14]  [260/1724] loss: 2.176, ave_loss: 2.679
[15]  [280/1724] loss: 2.730, ave_loss: 2.683
[16]  [300/1724] loss: 1.991, ave_loss: 2.640
[17]  [320/1724] loss: 1.168, ave_loss: 2.553
[18]  [340/1724] loss: 2.518, ave_loss: 2.551
[19]  [360/1724] loss: 1.856, ave_loss: 2.514
[20]  [380/1724] loss: 1.938, ave_loss: 2.486
[21]  [400/1724] loss: 2.178, ave_loss: 2.471
[22]  [420/1724] loss: 1.897, ave_loss: 2.445
[23]  [440/1724] loss: 1.623, ave_loss: 2.409
[24]  [460/1724] loss: 1.598, ave_loss: 2.375
[25]  [480/1724] loss: 1.907, ave_loss: 2.357
[26]  [500/1724] loss: 1.787, ave_loss: 2.335
[27]  [520/1724] loss: 1.798, ave_loss: 2.315
[28]  [540/1724] loss: 1.948, ave_loss: 2.302
[29]  [560/1724] loss: 6.810, ave_loss: 2.457
[30]  [580/1724] loss: 1.409, ave_loss: 2.422
[31]  [600/1724] loss: 1.799, ave_loss: 2.402
[32]  [620/1724] loss: 1.712, ave_loss: 2.381
[33]  [640/1724] loss: 1.397, ave_loss: 2.351
[34]  [660/1724] loss: 1.333, ave_loss: 2.321
[35]  [680/1724] loss: 2.316, ave_loss: 2.321
[36]  [700/1724] loss: 1.517, ave_loss: 2.298
[37]  [720/1724] loss: 9.799, ave_loss: 2.501
[38]  [740/1724] loss: 0.923, ave_loss: 2.460
[39]  [760/1724] loss: 1.687, ave_loss: 2.440
[40]  [780/1724] loss: 0.985, ave_loss: 2.403
[41]  [800/1724] loss: 1.735, ave_loss: 2.387
[42]  [820/1724] loss: 1.326, ave_loss: 2.362
[43]  [840/1724] loss: 1.561, ave_loss: 2.343
[44]  [860/1724] loss: 1.398, ave_loss: 2.322
[45]  [880/1724] loss: 1.534, ave_loss: 2.304
[46]  [900/1724] loss: 1.082, ave_loss: 2.278
[47]  [920/1724] loss: 1.121, ave_loss: 2.253
[48]  [940/1724] loss: 1.277, ave_loss: 2.233
[49]  [960/1724] loss: 1.714, ave_loss: 2.222
[50]  [980/1724] loss: 1.497, ave_loss: 2.208
[51]  [1000/1724] loss: 1.298, ave_loss: 2.190
[52]  [1020/1724] loss: 1.350, ave_loss: 2.174
[53]  [1040/1724] loss: 3.017, ave_loss: 2.190
[54]  [1060/1724] loss: 1.741, ave_loss: 2.181
[55]  [1080/1724] loss: 1.191, ave_loss: 2.163
[56]  [1100/1724] loss: 1.057, ave_loss: 2.143
[57]  [1120/1724] loss: 2.830, ave_loss: 2.156
[58]  [1140/1724] loss: 3.281, ave_loss: 2.175
[59]  [1160/1724] loss: 1.310, ave_loss: 2.160
[60]  [1180/1724] loss: 1.380, ave_loss: 2.147
[61]  [1200/1724] loss: 1.557, ave_loss: 2.138
[62]  [1220/1724] loss: 1.847, ave_loss: 2.133
[63]  [1240/1724] loss: 1.743, ave_loss: 2.127
[64]  [1260/1724] loss: 1.379, ave_loss: 2.115
[65]  [1280/1724] loss: 1.090, ave_loss: 2.099
[66]  [1300/1724] loss: 1.104, ave_loss: 2.084
[67]  [1320/1724] loss: 1.089, ave_loss: 2.069
[68]  [1340/1724] loss: 1.447, ave_loss: 2.060
[69]  [1360/1724] loss: 4.084, ave_loss: 2.090
[70]  [1380/1724] loss: 1.024, ave_loss: 2.074
[71]  [1400/1724] loss: 0.915, ave_loss: 2.058
[72]  [1420/1724] loss: 1.294, ave_loss: 2.047
[73]  [1440/1724] loss: 1.067, ave_loss: 2.034
[74]  [1460/1724] loss: 1.257, ave_loss: 2.023
[75]  [1480/1724] loss: 1.010, ave_loss: 2.010
[76]  [1500/1724] loss: 1.382, ave_loss: 2.002
[77]  [1520/1724] loss: 1.199, ave_loss: 1.991
[78]  [1540/1724] loss: 1.555, ave_loss: 1.986
[79]  [1560/1724] loss: 1.407, ave_loss: 1.978
[80]  [1580/1724] loss: 0.758, ave_loss: 1.963
[81]  [1600/1724] loss: 1.324, ave_loss: 1.955
[82]  [1620/1724] loss: 1.346, ave_loss: 1.948
[83]  [1640/1724] loss: 1.378, ave_loss: 1.941
[84]  [1660/1724] loss: 1.075, ave_loss: 1.931
[85]  [1680/1724] loss: 1.305, ave_loss: 1.923
[86]  [1700/1724] loss: 0.965, ave_loss: 1.912
[87]  [1720/1724] loss: 0.983, ave_loss: 1.901
[88]  [1740/1724] loss: 0.975, ave_loss: 1.891

Finished Training finishing at 2021-08-29 18:39:31.001950
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 1.891e+00
Validation Loss: 1.157e+05
Validation ROC: 0.0710
Saving model
27.979118329466356 epochs left to go

Training Epoch 1.0208816705336428/30 starting at 2021-08-29 18:41:59.365510
[1]  [0/1724] loss: 1.325, ave_loss: 1.325
[2]  [20/1724] loss: 0.888, ave_loss: 1.106
[3]  [40/1724] loss: 1.295, ave_loss: 1.169
[4]  [60/1724] loss: 1.219, ave_loss: 1.182
[5]  [80/1724] loss: 0.832, ave_loss: 1.112
[6]  [100/1724] loss: 1.242, ave_loss: 1.133
[7]  [120/1724] loss: 1.232, ave_loss: 1.148
[8]  [140/1724] loss: 0.768, ave_loss: 1.100
[9]  [160/1724] loss: 1.441, ave_loss: 1.138
[10]  [180/1724] loss: 1.131, ave_loss: 1.137
[11]  [200/1724] loss: 0.977, ave_loss: 1.123
[12]  [220/1724] loss: 0.974, ave_loss: 1.110
[13]  [240/1724] loss: 0.806, ave_loss: 1.087
[14]  [260/1724] loss: 1.091, ave_loss: 1.087
[15]  [280/1724] loss: 0.907, ave_loss: 1.075
[16]  [300/1724] loss: 0.820, ave_loss: 1.059
[17]  [320/1724] loss: 1.381, ave_loss: 1.078
[18]  [340/1724] loss: 1.032, ave_loss: 1.076
[19]  [360/1724] loss: 1.042, ave_loss: 1.074
[20]  [380/1724] loss: 1.343, ave_loss: 1.087
[21]  [400/1724] loss: 1.689, ave_loss: 1.116
[22]  [420/1724] loss: 1.045, ave_loss: 1.113
[23]  [440/1724] loss: 1.186, ave_loss: 1.116
[24]  [460/1724] loss: 1.050, ave_loss: 1.113
[25]  [480/1724] loss: 1.260, ave_loss: 1.119
[26]  [500/1724] loss: 0.969, ave_loss: 1.113
[27]  [520/1724] loss: 0.965, ave_loss: 1.108
[28]  [540/1724] loss: 0.831, ave_loss: 1.098
[29]  [560/1724] loss: 0.741, ave_loss: 1.086
[30]  [580/1724] loss: 0.740, ave_loss: 1.074
[31]  [600/1724] loss: 0.977, ave_loss: 1.071
[32]  [620/1724] loss: 1.229, ave_loss: 1.076
[33]  [640/1724] loss: 1.084, ave_loss: 1.076
[34]  [660/1724] loss: 4.334, ave_loss: 1.172
[35]  [680/1724] loss: 1.038, ave_loss: 1.168
[36]  [700/1724] loss: 0.750, ave_loss: 1.157
[37]  [720/1724] loss: 1.181, ave_loss: 1.157
[38]  [740/1724] loss: 1.175, ave_loss: 1.158
[39]  [760/1724] loss: 1.236, ave_loss: 1.160
[40]  [780/1724] loss: 0.968, ave_loss: 1.155
[41]  [800/1724] loss: 0.971, ave_loss: 1.150
[42]  [820/1724] loss: 1.092, ave_loss: 1.149
[43]  [840/1724] loss: 1.065, ave_loss: 1.147
[44]  [860/1724] loss: 1.145, ave_loss: 1.147
[45]  [880/1724] loss: 0.830, ave_loss: 1.140
[46]  [900/1724] loss: 0.839, ave_loss: 1.133
[47]  [920/1724] loss: 0.978, ave_loss: 1.130
[48]  [940/1724] loss: 1.374, ave_loss: 1.135
[49]  [960/1724] loss: 1.028, ave_loss: 1.133
[50]  [980/1724] loss: 1.040, ave_loss: 1.131
[51]  [1000/1724] loss: 0.786, ave_loss: 1.124
[52]  [1020/1724] loss: 0.991, ave_loss: 1.122
[53]  [1040/1724] loss: 0.846, ave_loss: 1.117
[54]  [1060/1724] loss: 1.209, ave_loss: 1.118
[55]  [1080/1724] loss: 0.993, ave_loss: 1.116
[56]  [1100/1724] loss: 0.907, ave_loss: 1.112
[57]  [1120/1724] loss: 1.209, ave_loss: 1.114
[58]  [1140/1724] loss: 0.850, ave_loss: 1.109
[59]  [1160/1724] loss: 0.742, ave_loss: 1.103
[60]  [1180/1724] loss: 1.329, ave_loss: 1.107
[61]  [1200/1724] loss: 0.894, ave_loss: 1.104
[62]  [1220/1724] loss: 0.760, ave_loss: 1.098
[63]  [1240/1724] loss: 1.050, ave_loss: 1.097
[64]  [1260/1724] loss: 0.840, ave_loss: 1.093
[65]  [1280/1724] loss: 0.895, ave_loss: 1.090
[66]  [1300/1724] loss: 0.999, ave_loss: 1.089
[67]  [1320/1724] loss: 1.061, ave_loss: 1.088
[68]  [1340/1724] loss: 0.900, ave_loss: 1.086
[69]  [1360/1724] loss: 0.863, ave_loss: 1.082
[70]  [1380/1724] loss: 0.866, ave_loss: 1.079
[71]  [1400/1724] loss: 0.973, ave_loss: 1.078
[72]  [1420/1724] loss: 1.027, ave_loss: 1.077
[73]  [1440/1724] loss: 0.745, ave_loss: 1.073
[74]  [1460/1724] loss: 0.997, ave_loss: 1.071
[75]  [1480/1724] loss: 1.035, ave_loss: 1.071
[76]  [1500/1724] loss: 1.176, ave_loss: 1.072
[77]  [1520/1724] loss: 1.041, ave_loss: 1.072
[78]  [1540/1724] loss: 0.895, ave_loss: 1.070
[79]  [1560/1724] loss: 0.901, ave_loss: 1.068
[80]  [1580/1724] loss: 1.009, ave_loss: 1.067
[81]  [1600/1724] loss: 0.811, ave_loss: 1.064
[82]  [1620/1724] loss: 0.901, ave_loss: 1.062
[83]  [1640/1724] loss: 1.152, ave_loss: 1.063
[84]  [1660/1724] loss: 0.851, ave_loss: 1.060
[85]  [1680/1724] loss: 0.968, ave_loss: 1.059
[86]  [1700/1724] loss: 0.782, ave_loss: 1.056
[87]  [1720/1724] loss: 3.615, ave_loss: 1.085
[88]  [1740/1724] loss: 0.832, ave_loss: 1.082

Finished Training finishing at 2021-08-29 18:44:59.534730
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 1.082e+00
Validation Loss: 1.333e+05
Validation ROC: 0.1875
Saving model
26.958236658932716 epochs left to go

Training Epoch 2.0417633410672855/30 starting at 2021-08-29 18:47:08.118437
[1]  [0/1724] loss: 0.906, ave_loss: 0.906
[2]  [20/1724] loss: 1.052, ave_loss: 0.979
[3]  [40/1724] loss: 0.799, ave_loss: 0.919
[4]  [60/1724] loss: 0.939, ave_loss: 0.924
[5]  [80/1724] loss: 0.836, ave_loss: 0.906
[6]  [100/1724] loss: 0.751, ave_loss: 0.881
[7]  [120/1724] loss: 0.745, ave_loss: 0.861
[8]  [140/1724] loss: 0.790, ave_loss: 0.852
[9]  [160/1724] loss: 0.688, ave_loss: 0.834
[10]  [180/1724] loss: 0.688, ave_loss: 0.819
[11]  [200/1724] loss: 0.694, ave_loss: 0.808
[12]  [220/1724] loss: 0.896, ave_loss: 0.815
[13]  [240/1724] loss: 0.917, ave_loss: 0.823
[14]  [260/1724] loss: 0.616, ave_loss: 0.808
[15]  [280/1724] loss: 1.014, ave_loss: 0.822
[16]  [300/1724] loss: 0.838, ave_loss: 0.823
[17]  [320/1724] loss: 1.005, ave_loss: 0.834
[18]  [340/1724] loss: 0.926, ave_loss: 0.839
[19]  [360/1724] loss: 0.966, ave_loss: 0.846
[20]  [380/1724] loss: 1.244, ave_loss: 0.866
[21]  [400/1724] loss: 0.983, ave_loss: 0.871
[22]  [420/1724] loss: 0.703, ave_loss: 0.863
[23]  [440/1724] loss: 0.790, ave_loss: 0.860
[24]  [460/1724] loss: 0.729, ave_loss: 0.855
[25]  [480/1724] loss: 0.747, ave_loss: 0.851
[26]  [500/1724] loss: 0.709, ave_loss: 0.845
[27]  [520/1724] loss: 1.023, ave_loss: 0.852
[28]  [540/1724] loss: 0.608, ave_loss: 0.843
[29]  [560/1724] loss: 0.914, ave_loss: 0.845
[30]  [580/1724] loss: 0.782, ave_loss: 0.843
[31]  [600/1724] loss: 0.777, ave_loss: 0.841
[32]  [620/1724] loss: 0.735, ave_loss: 0.838
[33]  [640/1724] loss: 0.865, ave_loss: 0.839
[34]  [660/1724] loss: 0.752, ave_loss: 0.836
[35]  [680/1724] loss: 1.569, ave_loss: 0.857
[36]  [700/1724] loss: 1.432, ave_loss: 0.873
[37]  [720/1724] loss: 0.703, ave_loss: 0.868
[38]  [740/1724] loss: 0.833, ave_loss: 0.867
[39]  [760/1724] loss: 0.873, ave_loss: 0.868
[40]  [780/1724] loss: 0.971, ave_loss: 0.870
[41]  [800/1724] loss: 1.400, ave_loss: 0.883
[42]  [820/1724] loss: 0.838, ave_loss: 0.882
[43]  [840/1724] loss: 0.716, ave_loss: 0.878
[44]  [860/1724] loss: 1.087, ave_loss: 0.883
[45]  [880/1724] loss: 0.820, ave_loss: 0.882
[46]  [900/1724] loss: 0.899, ave_loss: 0.882
[47]  [920/1724] loss: 0.801, ave_loss: 0.880
[48]  [940/1724] loss: 1.230, ave_loss: 0.888
[49]  [960/1724] loss: 0.587, ave_loss: 0.881
[50]  [980/1724] loss: 0.691, ave_loss: 0.878
[51]  [1000/1724] loss: 1.426, ave_loss: 0.888
[52]  [1020/1724] loss: 1.086, ave_loss: 0.892
[53]  [1040/1724] loss: 0.992, ave_loss: 0.894
[54]  [1060/1724] loss: 0.839, ave_loss: 0.893
[55]  [1080/1724] loss: 1.016, ave_loss: 0.895
[56]  [1100/1724] loss: 1.018, ave_loss: 0.897
[57]  [1120/1724] loss: 0.713, ave_loss: 0.894
[58]  [1140/1724] loss: 1.068, ave_loss: 0.897
[59]  [1160/1724] loss: 0.737, ave_loss: 0.894
[60]  [1180/1724] loss: 0.775, ave_loss: 0.892
[61]  [1200/1724] loss: 0.952, ave_loss: 0.893
[62]  [1220/1724] loss: 0.751, ave_loss: 0.891
[63]  [1240/1724] loss: 0.940, ave_loss: 0.892
[64]  [1260/1724] loss: 0.677, ave_loss: 0.889
[65]  [1280/1724] loss: 0.950, ave_loss: 0.890
[66]  [1300/1724] loss: 0.724, ave_loss: 0.887
[67]  [1320/1724] loss: 0.791, ave_loss: 0.886
[68]  [1340/1724] loss: 0.676, ave_loss: 0.883
[69]  [1360/1724] loss: 0.880, ave_loss: 0.882
[70]  [1380/1724] loss: 0.634, ave_loss: 0.879
[71]  [1400/1724] loss: 0.738, ave_loss: 0.877
[72]  [1420/1724] loss: 0.672, ave_loss: 0.874
[73]  [1440/1724] loss: 0.628, ave_loss: 0.871
[74]  [1460/1724] loss: 0.850, ave_loss: 0.870
[75]  [1480/1724] loss: 0.635, ave_loss: 0.867
[76]  [1500/1724] loss: 0.732, ave_loss: 0.865
[77]  [1520/1724] loss: 0.531, ave_loss: 0.861
[78]  [1540/1724] loss: 0.857, ave_loss: 0.861
[79]  [1560/1724] loss: 0.825, ave_loss: 0.861
[80]  [1580/1724] loss: 0.969, ave_loss: 0.862
[81]  [1600/1724] loss: 0.868, ave_loss: 0.862
[82]  [1620/1724] loss: 0.721, ave_loss: 0.860
[83]  [1640/1724] loss: 0.985, ave_loss: 0.862
[84]  [1660/1724] loss: 0.912, ave_loss: 0.862
[85]  [1680/1724] loss: 0.856, ave_loss: 0.862
[86]  [1700/1724] loss: 0.845, ave_loss: 0.862
[87]  [1720/1724] loss: 1.199, ave_loss: 0.866
[88]  [1740/1724] loss: 0.988, ave_loss: 0.867

Finished Training finishing at 2021-08-29 18:50:38.623141
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 8.674e-01
Validation Loss: 1.100e+05
Validation ROC: 0.2189
Saving model
25.937354988399072 epochs left to go

Training Epoch 3.062645011600928/30 starting at 2021-08-29 18:52:37.365681
[1]  [0/1724] loss: 1.062, ave_loss: 1.062
[2]  [20/1724] loss: 0.683, ave_loss: 0.873
[3]  [40/1724] loss: 0.912, ave_loss: 0.886
[4]  [60/1724] loss: 1.002, ave_loss: 0.915
[5]  [80/1724] loss: 0.614, ave_loss: 0.855
[6]  [100/1724] loss: 0.743, ave_loss: 0.836
[7]  [120/1724] loss: 1.050, ave_loss: 0.867
[8]  [140/1724] loss: 0.768, ave_loss: 0.854
[9]  [160/1724] loss: 0.710, ave_loss: 0.838
[10]  [180/1724] loss: 0.747, ave_loss: 0.829
[11]  [200/1724] loss: 0.940, ave_loss: 0.839
[12]  [220/1724] loss: 0.999, ave_loss: 0.852
[13]  [240/1724] loss: 1.090, ave_loss: 0.871
[14]  [260/1724] loss: 1.091, ave_loss: 0.886
[15]  [280/1724] loss: 0.963, ave_loss: 0.892
[16]  [300/1724] loss: 0.763, ave_loss: 0.884
[17]  [320/1724] loss: 0.745, ave_loss: 0.875
[18]  [340/1724] loss: 1.043, ave_loss: 0.885
[19]  [360/1724] loss: 0.965, ave_loss: 0.889
[20]  [380/1724] loss: 0.879, ave_loss: 0.888
[21]  [400/1724] loss: 0.952, ave_loss: 0.891
[22]  [420/1724] loss: 0.567, ave_loss: 0.877
[23]  [440/1724] loss: 0.770, ave_loss: 0.872
[24]  [460/1724] loss: 0.697, ave_loss: 0.865
[25]  [480/1724] loss: 0.917, ave_loss: 0.867
[26]  [500/1724] loss: 0.772, ave_loss: 0.863
[27]  [520/1724] loss: 0.714, ave_loss: 0.858
[28]  [540/1724] loss: 1.258, ave_loss: 0.872
[29]  [560/1724] loss: 0.955, ave_loss: 0.875
[30]  [580/1724] loss: 0.881, ave_loss: 0.875
[31]  [600/1724] loss: 0.645, ave_loss: 0.868
[32]  [620/1724] loss: 0.705, ave_loss: 0.863
[33]  [640/1724] loss: 1.079, ave_loss: 0.869
[34]  [660/1724] loss: 0.804, ave_loss: 0.867
[35]  [680/1724] loss: 0.959, ave_loss: 0.870
[36]  [700/1724] loss: 0.808, ave_loss: 0.868
[37]  [720/1724] loss: 0.834, ave_loss: 0.867
[38]  [740/1724] loss: 1.035, ave_loss: 0.872
[39]  [760/1724] loss: 0.585, ave_loss: 0.864
[40]  [780/1724] loss: 0.635, ave_loss: 0.859
[41]  [800/1724] loss: 1.185, ave_loss: 0.866
[42]  [820/1724] loss: 0.751, ave_loss: 0.864
[43]  [840/1724] loss: 0.663, ave_loss: 0.859
[44]  [860/1724] loss: 0.726, ave_loss: 0.856
[45]  [880/1724] loss: 0.610, ave_loss: 0.851
[46]  [900/1724] loss: 0.843, ave_loss: 0.850
[47]  [920/1724] loss: 0.768, ave_loss: 0.849
[48]  [940/1724] loss: 0.769, ave_loss: 0.847
[49]  [960/1724] loss: 0.855, ave_loss: 0.847
[50]  [980/1724] loss: 0.657, ave_loss: 0.843
[51]  [1000/1724] loss: 0.786, ave_loss: 0.842
[52]  [1020/1724] loss: 0.544, ave_loss: 0.836
[53]  [1040/1724] loss: 0.914, ave_loss: 0.838
[54]  [1060/1724] loss: 0.722, ave_loss: 0.836
[55]  [1080/1724] loss: 0.895, ave_loss: 0.837
[56]  [1100/1724] loss: 0.754, ave_loss: 0.835
[57]  [1120/1724] loss: 0.809, ave_loss: 0.835
[58]  [1140/1724] loss: 0.663, ave_loss: 0.832
[59]  [1160/1724] loss: 1.000, ave_loss: 0.835
[60]  [1180/1724] loss: 0.685, ave_loss: 0.832
[61]  [1200/1724] loss: 0.490, ave_loss: 0.827
[62]  [1220/1724] loss: 1.069, ave_loss: 0.831
[63]  [1240/1724] loss: 0.798, ave_loss: 0.830
[64]  [1260/1724] loss: 0.704, ave_loss: 0.828
[65]  [1280/1724] loss: 0.585, ave_loss: 0.824
[66]  [1300/1724] loss: 0.777, ave_loss: 0.824
[67]  [1320/1724] loss: 0.585, ave_loss: 0.820
[68]  [1340/1724] loss: 0.901, ave_loss: 0.821
[69]  [1360/1724] loss: 0.879, ave_loss: 0.822
[70]  [1380/1724] loss: 0.704, ave_loss: 0.820
[71]  [1400/1724] loss: 0.974, ave_loss: 0.823
[72]  [1420/1724] loss: 0.623, ave_loss: 0.820
[73]  [1440/1724] loss: 0.471, ave_loss: 0.815
[74]  [1460/1724] loss: 0.730, ave_loss: 0.814
[75]  [1480/1724] loss: 0.855, ave_loss: 0.814
[76]  [1500/1724] loss: 0.816, ave_loss: 0.815
[77]  [1520/1724] loss: 0.800, ave_loss: 0.814
[78]  [1540/1724] loss: 0.852, ave_loss: 0.815
[79]  [1560/1724] loss: 0.773, ave_loss: 0.814
[80]  [1580/1724] loss: 0.416, ave_loss: 0.809
[81]  [1600/1724] loss: 0.818, ave_loss: 0.809
[82]  [1620/1724] loss: 0.588, ave_loss: 0.807
[83]  [1640/1724] loss: 1.404, ave_loss: 0.814
[84]  [1660/1724] loss: 0.794, ave_loss: 0.814
[85]  [1680/1724] loss: 0.919, ave_loss: 0.815
[86]  [1700/1724] loss: 0.727, ave_loss: 0.814
[87]  [1720/1724] loss: 0.604, ave_loss: 0.811
[88]  [1740/1724] loss: 0.930, ave_loss: 0.813

Finished Training finishing at 2021-08-29 18:56:04.906196
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 8.128e-01
Validation Loss: 8.434e+04
Validation ROC: 0.3988
Saving model
24.916473317865428 epochs left to go

Training Epoch 4.083526682134571/30 starting at 2021-08-29 18:58:21.176494
[1]  [0/1724] loss: 0.665, ave_loss: 0.665
[2]  [20/1724] loss: 0.824, ave_loss: 0.744
[3]  [40/1724] loss: 0.597, ave_loss: 0.695
[4]  [60/1724] loss: 0.527, ave_loss: 0.653
[5]  [80/1724] loss: 0.670, ave_loss: 0.656
[6]  [100/1724] loss: 0.759, ave_loss: 0.674
[7]  [120/1724] loss: 0.758, ave_loss: 0.686
[8]  [140/1724] loss: 0.628, ave_loss: 0.678
[9]  [160/1724] loss: 0.460, ave_loss: 0.654
[10]  [180/1724] loss: 0.881, ave_loss: 0.677
[11]  [200/1724] loss: 1.070, ave_loss: 0.713
[12]  [220/1724] loss: 0.895, ave_loss: 0.728
[13]  [240/1724] loss: 0.774, ave_loss: 0.731
[14]  [260/1724] loss: 0.921, ave_loss: 0.745
[15]  [280/1724] loss: 0.836, ave_loss: 0.751
[16]  [300/1724] loss: 0.948, ave_loss: 0.763
[17]  [320/1724] loss: 0.793, ave_loss: 0.765
[18]  [340/1724] loss: 0.876, ave_loss: 0.771
[19]  [360/1724] loss: 0.935, ave_loss: 0.780
[20]  [380/1724] loss: 0.899, ave_loss: 0.786
[21]  [400/1724] loss: 0.963, ave_loss: 0.794
[22]  [420/1724] loss: 0.951, ave_loss: 0.801
[23]  [440/1724] loss: 0.706, ave_loss: 0.797
[24]  [460/1724] loss: 0.994, ave_loss: 0.805
[25]  [480/1724] loss: 0.649, ave_loss: 0.799
[26]  [500/1724] loss: 0.787, ave_loss: 0.799
[27]  [520/1724] loss: 0.729, ave_loss: 0.796
[28]  [540/1724] loss: 0.635, ave_loss: 0.790
[29]  [560/1724] loss: 0.665, ave_loss: 0.786
[30]  [580/1724] loss: 0.784, ave_loss: 0.786
[31]  [600/1724] loss: 0.587, ave_loss: 0.779
[32]  [620/1724] loss: 0.632, ave_loss: 0.775
[33]  [640/1724] loss: 0.613, ave_loss: 0.770
[34]  [660/1724] loss: 0.691, ave_loss: 0.768
[35]  [680/1724] loss: 0.564, ave_loss: 0.762
[36]  [700/1724] loss: 0.769, ave_loss: 0.762
[37]  [720/1724] loss: 0.510, ave_loss: 0.755
[38]  [740/1724] loss: 0.843, ave_loss: 0.758
[39]  [760/1724] loss: 0.974, ave_loss: 0.763
[40]  [780/1724] loss: 0.834, ave_loss: 0.765
[41]  [800/1724] loss: 0.872, ave_loss: 0.767
[42]  [820/1724] loss: 0.668, ave_loss: 0.765
[43]  [840/1724] loss: 0.672, ave_loss: 0.763
[44]  [860/1724] loss: 0.851, ave_loss: 0.765
[45]  [880/1724] loss: 0.541, ave_loss: 0.760
[46]  [900/1724] loss: 0.638, ave_loss: 0.757
[47]  [920/1724] loss: 0.471, ave_loss: 0.751
[48]  [940/1724] loss: 0.854, ave_loss: 0.753
[49]  [960/1724] loss: 0.572, ave_loss: 0.750
[50]  [980/1724] loss: 0.608, ave_loss: 0.747
[51]  [1000/1724] loss: 0.499, ave_loss: 0.742
[52]  [1020/1724] loss: 1.085, ave_loss: 0.749
[53]  [1040/1724] loss: 0.650, ave_loss: 0.747
[54]  [1060/1724] loss: 0.544, ave_loss: 0.743
[55]  [1080/1724] loss: 0.535, ave_loss: 0.739
[56]  [1100/1724] loss: 0.621, ave_loss: 0.737
[57]  [1120/1724] loss: 0.544, ave_loss: 0.734
[58]  [1140/1724] loss: 0.606, ave_loss: 0.731
[59]  [1160/1724] loss: 0.758, ave_loss: 0.732
[60]  [1180/1724] loss: 0.795, ave_loss: 0.733
[61]  [1200/1724] loss: 0.496, ave_loss: 0.729
[62]  [1220/1724] loss: 0.601, ave_loss: 0.727
[63]  [1240/1724] loss: 0.621, ave_loss: 0.725
[64]  [1260/1724] loss: 0.850, ave_loss: 0.727
[65]  [1280/1724] loss: 0.840, ave_loss: 0.729
[66]  [1300/1724] loss: 0.688, ave_loss: 0.728
[67]  [1320/1724] loss: 0.639, ave_loss: 0.727
[68]  [1340/1724] loss: 0.699, ave_loss: 0.727
[69]  [1360/1724] loss: 0.832, ave_loss: 0.728
[70]  [1380/1724] loss: 0.836, ave_loss: 0.730
[71]  [1400/1724] loss: 0.566, ave_loss: 0.727
[72]  [1420/1724] loss: 0.502, ave_loss: 0.724
[73]  [1440/1724] loss: 0.604, ave_loss: 0.723
[74]  [1460/1724] loss: 0.543, ave_loss: 0.720
[75]  [1480/1724] loss: 0.398, ave_loss: 0.716
[76]  [1500/1724] loss: 0.751, ave_loss: 0.716
[77]  [1520/1724] loss: 0.526, ave_loss: 0.714
[78]  [1540/1724] loss: 0.660, ave_loss: 0.713
[79]  [1560/1724] loss: 0.764, ave_loss: 0.714
[80]  [1580/1724] loss: 0.545, ave_loss: 0.712
[81]  [1600/1724] loss: 0.727, ave_loss: 0.712
[82]  [1620/1724] loss: 0.532, ave_loss: 0.710
[83]  [1640/1724] loss: 0.623, ave_loss: 0.709
[84]  [1660/1724] loss: 0.696, ave_loss: 0.709
[85]  [1680/1724] loss: 0.629, ave_loss: 0.708
[86]  [1700/1724] loss: 0.605, ave_loss: 0.706
[87]  [1720/1724] loss: 0.434, ave_loss: 0.703
[88]  [1740/1724] loss: 0.815, ave_loss: 0.705

Finished Training finishing at 2021-08-29 19:01:40.383540
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 7.046e-01
Validation Loss: 8.479e+04
Validation ROC: 0.4461
Saving model
23.895591647331788 epochs left to go

Training Epoch 5.104408352668213/30 starting at 2021-08-29 19:03:52.074974
[1]  [0/1724] loss: 1.001, ave_loss: 1.001
[2]  [20/1724] loss: 0.578, ave_loss: 0.790
[3]  [40/1724] loss: 0.623, ave_loss: 0.734
[4]  [60/1724] loss: 0.838, ave_loss: 0.760
[5]  [80/1724] loss: 0.673, ave_loss: 0.743
[6]  [100/1724] loss: 0.653, ave_loss: 0.728
[7]  [120/1724] loss: 0.672, ave_loss: 0.720
[8]  [140/1724] loss: 0.613, ave_loss: 0.706
[9]  [160/1724] loss: 0.705, ave_loss: 0.706
[10]  [180/1724] loss: 0.684, ave_loss: 0.704
[11]  [200/1724] loss: 0.554, ave_loss: 0.690
[12]  [220/1724] loss: 0.540, ave_loss: 0.678
[13]  [240/1724] loss: 0.669, ave_loss: 0.677
[14]  [260/1724] loss: 0.519, ave_loss: 0.666
[15]  [280/1724] loss: 0.640, ave_loss: 0.664
[16]  [300/1724] loss: 0.433, ave_loss: 0.650
[17]  [320/1724] loss: 0.489, ave_loss: 0.640
[18]  [340/1724] loss: 0.975, ave_loss: 0.659
[19]  [360/1724] loss: 0.593, ave_loss: 0.655
[20]  [380/1724] loss: 0.556, ave_loss: 0.650
[21]  [400/1724] loss: 0.636, ave_loss: 0.650
[22]  [420/1724] loss: 0.867, ave_loss: 0.660
[23]  [440/1724] loss: 0.773, ave_loss: 0.665
[24]  [460/1724] loss: 0.641, ave_loss: 0.664
[25]  [480/1724] loss: 0.734, ave_loss: 0.666
[26]  [500/1724] loss: 0.893, ave_loss: 0.675
[27]  [520/1724] loss: 0.769, ave_loss: 0.679
[28]  [540/1724] loss: 0.752, ave_loss: 0.681
[29]  [560/1724] loss: 0.920, ave_loss: 0.690
[30]  [580/1724] loss: 0.679, ave_loss: 0.689
[31]  [600/1724] loss: 0.685, ave_loss: 0.689
[32]  [620/1724] loss: 0.646, ave_loss: 0.688
[33]  [640/1724] loss: 0.835, ave_loss: 0.692
[34]  [660/1724] loss: 0.630, ave_loss: 0.690
[35]  [680/1724] loss: 0.487, ave_loss: 0.685
[36]  [700/1724] loss: 0.592, ave_loss: 0.682
[37]  [720/1724] loss: 0.731, ave_loss: 0.683
[38]  [740/1724] loss: 0.631, ave_loss: 0.682
[39]  [760/1724] loss: 0.647, ave_loss: 0.681
[40]  [780/1724] loss: 0.596, ave_loss: 0.679
[41]  [800/1724] loss: 0.717, ave_loss: 0.680
[42]  [820/1724] loss: 0.580, ave_loss: 0.677
[43]  [840/1724] loss: 0.721, ave_loss: 0.678
[44]  [860/1724] loss: 0.423, ave_loss: 0.673
[45]  [880/1724] loss: 0.594, ave_loss: 0.671
[46]  [900/1724] loss: 0.565, ave_loss: 0.669
[47]  [920/1724] loss: 0.623, ave_loss: 0.668
[48]  [940/1724] loss: 0.625, ave_loss: 0.667
[49]  [960/1724] loss: 0.915, ave_loss: 0.672
[50]  [980/1724] loss: 0.820, ave_loss: 0.675
[51]  [1000/1724] loss: 0.462, ave_loss: 0.671
[52]  [1020/1724] loss: 0.591, ave_loss: 0.669
[53]  [1040/1724] loss: 0.557, ave_loss: 0.667
[54]  [1060/1724] loss: 0.851, ave_loss: 0.670
[55]  [1080/1724] loss: 0.508, ave_loss: 0.667
[56]  [1100/1724] loss: 0.692, ave_loss: 0.668
[57]  [1120/1724] loss: 0.532, ave_loss: 0.665
[58]  [1140/1724] loss: 0.945, ave_loss: 0.670
[59]  [1160/1724] loss: 0.855, ave_loss: 0.673
[60]  [1180/1724] loss: 0.437, ave_loss: 0.669
[61]  [1200/1724] loss: 0.467, ave_loss: 0.666
[62]  [1220/1724] loss: 0.554, ave_loss: 0.664
[63]  [1240/1724] loss: 0.443, ave_loss: 0.661
[64]  [1260/1724] loss: 0.635, ave_loss: 0.660
[65]  [1280/1724] loss: 0.943, ave_loss: 0.665
[66]  [1300/1724] loss: 0.532, ave_loss: 0.663
[67]  [1320/1724] loss: 0.748, ave_loss: 0.664
[68]  [1340/1724] loss: 0.533, ave_loss: 0.662
[69]  [1360/1724] loss: 0.530, ave_loss: 0.660
[70]  [1380/1724] loss: 0.584, ave_loss: 0.659
[71]  [1400/1724] loss: 0.607, ave_loss: 0.658
[72]  [1420/1724] loss: 0.657, ave_loss: 0.658
[73]  [1440/1724] loss: 0.554, ave_loss: 0.657
[74]  [1460/1724] loss: 0.530, ave_loss: 0.655
[75]  [1480/1724] loss: 0.653, ave_loss: 0.655
[76]  [1500/1724] loss: 0.718, ave_loss: 0.656
[77]  [1520/1724] loss: 0.535, ave_loss: 0.654
[78]  [1540/1724] loss: 0.735, ave_loss: 0.655
[79]  [1560/1724] loss: 0.462, ave_loss: 0.653
[80]  [1580/1724] loss: 0.523, ave_loss: 0.651
[81]  [1600/1724] loss: 0.734, ave_loss: 0.652
[82]  [1620/1724] loss: 0.653, ave_loss: 0.652
[83]  [1640/1724] loss: 0.750, ave_loss: 0.654
[84]  [1660/1724] loss: 0.621, ave_loss: 0.653
[85]  [1680/1724] loss: 0.754, ave_loss: 0.654
[86]  [1700/1724] loss: 0.788, ave_loss: 0.656
[87]  [1720/1724] loss: 0.720, ave_loss: 0.657
[88]  [1740/1724] loss: 0.698, ave_loss: 0.657

Finished Training finishing at 2021-08-29 19:06:54.030080
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.572e-01
Validation Loss: 8.713e+04
Validation ROC: 0.5108
Saving model
22.874709976798144 epochs left to go

Training Epoch 6.125290023201856/30 starting at 2021-08-29 19:09:40.896391
[1]  [0/1724] loss: 0.634, ave_loss: 0.634
[2]  [20/1724] loss: 0.748, ave_loss: 0.691
[3]  [40/1724] loss: 0.633, ave_loss: 0.672
[4]  [60/1724] loss: 0.555, ave_loss: 0.643
[5]  [80/1724] loss: 0.734, ave_loss: 0.661
[6]  [100/1724] loss: 0.601, ave_loss: 0.651
[7]  [120/1724] loss: 0.503, ave_loss: 0.630
[8]  [140/1724] loss: 0.458, ave_loss: 0.608
[9]  [160/1724] loss: 0.605, ave_loss: 0.608
[10]  [180/1724] loss: 0.831, ave_loss: 0.630
[11]  [200/1724] loss: 0.657, ave_loss: 0.633
[12]  [220/1724] loss: 0.708, ave_loss: 0.639
[13]  [240/1724] loss: 0.555, ave_loss: 0.632
[14]  [260/1724] loss: 0.748, ave_loss: 0.641
[15]  [280/1724] loss: 0.597, ave_loss: 0.638
[16]  [300/1724] loss: 0.678, ave_loss: 0.640
[17]  [320/1724] loss: 0.374, ave_loss: 0.625
[18]  [340/1724] loss: 0.596, ave_loss: 0.623
[19]  [360/1724] loss: 0.532, ave_loss: 0.618
[20]  [380/1724] loss: 0.441, ave_loss: 0.609
[21]  [400/1724] loss: 0.643, ave_loss: 0.611
[22]  [420/1724] loss: 0.648, ave_loss: 0.613
[23]  [440/1724] loss: 0.707, ave_loss: 0.617
[24]  [460/1724] loss: 0.618, ave_loss: 0.617
[25]  [480/1724] loss: 0.690, ave_loss: 0.620
[26]  [500/1724] loss: 0.730, ave_loss: 0.624
[27]  [520/1724] loss: 0.622, ave_loss: 0.624
[28]  [540/1724] loss: 0.725, ave_loss: 0.628
[29]  [560/1724] loss: 0.701, ave_loss: 0.630
[30]  [580/1724] loss: 0.604, ave_loss: 0.629
[31]  [600/1724] loss: 0.835, ave_loss: 0.636
[32]  [620/1724] loss: 0.443, ave_loss: 0.630
[33]  [640/1724] loss: 0.590, ave_loss: 0.629
[34]  [660/1724] loss: 0.593, ave_loss: 0.628
[35]  [680/1724] loss: 0.616, ave_loss: 0.627
[36]  [700/1724] loss: 0.752, ave_loss: 0.631
[37]  [720/1724] loss: 0.621, ave_loss: 0.630
[38]  [740/1724] loss: 0.656, ave_loss: 0.631
[39]  [760/1724] loss: 0.860, ave_loss: 0.637
[40]  [780/1724] loss: 0.660, ave_loss: 0.638
[41]  [800/1724] loss: 0.660, ave_loss: 0.638
[42]  [820/1724] loss: 0.837, ave_loss: 0.643
[43]  [840/1724] loss: 0.717, ave_loss: 0.645
[44]  [860/1724] loss: 0.531, ave_loss: 0.642
[45]  [880/1724] loss: 0.792, ave_loss: 0.645
[46]  [900/1724] loss: 0.661, ave_loss: 0.646
[47]  [920/1724] loss: 0.575, ave_loss: 0.644
[48]  [940/1724] loss: 0.677, ave_loss: 0.645
[49]  [960/1724] loss: 0.744, ave_loss: 0.647
[50]  [980/1724] loss: 0.679, ave_loss: 0.648
[51]  [1000/1724] loss: 0.539, ave_loss: 0.645
[52]  [1020/1724] loss: 0.571, ave_loss: 0.644
[53]  [1040/1724] loss: 0.498, ave_loss: 0.641
[54]  [1060/1724] loss: 0.591, ave_loss: 0.640
[55]  [1080/1724] loss: 0.580, ave_loss: 0.639
[56]  [1100/1724] loss: 0.765, ave_loss: 0.641
[57]  [1120/1724] loss: 0.605, ave_loss: 0.641
[58]  [1140/1724] loss: 0.602, ave_loss: 0.640
[59]  [1160/1724] loss: 0.634, ave_loss: 0.640
[60]  [1180/1724] loss: 0.509, ave_loss: 0.638
[61]  [1200/1724] loss: 0.529, ave_loss: 0.636
[62]  [1220/1724] loss: 0.688, ave_loss: 0.637
[63]  [1240/1724] loss: 0.538, ave_loss: 0.635
[64]  [1260/1724] loss: 0.659, ave_loss: 0.636
[65]  [1280/1724] loss: 0.598, ave_loss: 0.635
[66]  [1300/1724] loss: 0.733, ave_loss: 0.637
[67]  [1320/1724] loss: 0.563, ave_loss: 0.636
[68]  [1340/1724] loss: 0.652, ave_loss: 0.636
[69]  [1360/1724] loss: 0.577, ave_loss: 0.635
[70]  [1380/1724] loss: 0.640, ave_loss: 0.635
[71]  [1400/1724] loss: 0.453, ave_loss: 0.632
[72]  [1420/1724] loss: 0.945, ave_loss: 0.637
[73]  [1440/1724] loss: 0.695, ave_loss: 0.638
[74]  [1460/1724] loss: 0.755, ave_loss: 0.639
[75]  [1480/1724] loss: 0.609, ave_loss: 0.639
[76]  [1500/1724] loss: 0.599, ave_loss: 0.638
[77]  [1520/1724] loss: 0.621, ave_loss: 0.638
[78]  [1540/1724] loss: 0.531, ave_loss: 0.637
[79]  [1560/1724] loss: 0.424, ave_loss: 0.634
[80]  [1580/1724] loss: 0.688, ave_loss: 0.635
[81]  [1600/1724] loss: 0.648, ave_loss: 0.635
[82]  [1620/1724] loss: 0.554, ave_loss: 0.634
[83]  [1640/1724] loss: 0.684, ave_loss: 0.634
[84]  [1660/1724] loss: 0.585, ave_loss: 0.634
[85]  [1680/1724] loss: 0.638, ave_loss: 0.634
[86]  [1700/1724] loss: 0.611, ave_loss: 0.634
[87]  [1720/1724] loss: 0.890, ave_loss: 0.637
[88]  [1740/1724] loss: 0.970, ave_loss: 0.640

Finished Training finishing at 2021-08-29 19:13:30.694138
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.403e-01
Validation Loss: 8.218e+04
Validation ROC: 0.5111
Saving model
21.8538283062645 epochs left to go

Training Epoch 7.146171693735499/30 starting at 2021-08-29 19:15:46.383441
[1]  [0/1724] loss: 0.920, ave_loss: 0.920
[2]  [20/1724] loss: 0.553, ave_loss: 0.737
[3]  [40/1724] loss: 0.585, ave_loss: 0.686
[4]  [60/1724] loss: 0.412, ave_loss: 0.618
[5]  [80/1724] loss: 0.490, ave_loss: 0.592
[6]  [100/1724] loss: 0.544, ave_loss: 0.584
[7]  [120/1724] loss: 0.474, ave_loss: 0.568
[8]  [140/1724] loss: 0.740, ave_loss: 0.590
[9]  [160/1724] loss: 0.651, ave_loss: 0.597
[10]  [180/1724] loss: 0.751, ave_loss: 0.612
[11]  [200/1724] loss: 0.800, ave_loss: 0.629
[12]  [220/1724] loss: 0.606, ave_loss: 0.627
[13]  [240/1724] loss: 0.645, ave_loss: 0.629
[14]  [260/1724] loss: 0.769, ave_loss: 0.639
[15]  [280/1724] loss: 0.774, ave_loss: 0.648
[16]  [300/1724] loss: 0.462, ave_loss: 0.636
[17]  [320/1724] loss: 0.670, ave_loss: 0.638
[18]  [340/1724] loss: 0.661, ave_loss: 0.639
[19]  [360/1724] loss: 0.782, ave_loss: 0.647
[20]  [380/1724] loss: 0.573, ave_loss: 0.643
[21]  [400/1724] loss: 0.568, ave_loss: 0.640
[22]  [420/1724] loss: 0.668, ave_loss: 0.641
[23]  [440/1724] loss: 0.594, ave_loss: 0.639
[24]  [460/1724] loss: 0.462, ave_loss: 0.631
[25]  [480/1724] loss: 0.907, ave_loss: 0.643
[26]  [500/1724] loss: 0.712, ave_loss: 0.645
[27]  [520/1724] loss: 0.520, ave_loss: 0.641
[28]  [540/1724] loss: 0.568, ave_loss: 0.638
[29]  [560/1724] loss: 0.653, ave_loss: 0.638
[30]  [580/1724] loss: 0.553, ave_loss: 0.636
[31]  [600/1724] loss: 0.700, ave_loss: 0.638
[32]  [620/1724] loss: 0.654, ave_loss: 0.638
[33]  [640/1724] loss: 0.758, ave_loss: 0.642
[34]  [660/1724] loss: 0.622, ave_loss: 0.641
[35]  [680/1724] loss: 0.498, ave_loss: 0.637
[36]  [700/1724] loss: 0.736, ave_loss: 0.640
[37]  [720/1724] loss: 0.571, ave_loss: 0.638
[38]  [740/1724] loss: 0.520, ave_loss: 0.635
[39]  [760/1724] loss: 0.312, ave_loss: 0.627
[40]  [780/1724] loss: 0.508, ave_loss: 0.624
[41]  [800/1724] loss: 0.559, ave_loss: 0.622
[42]  [820/1724] loss: 0.688, ave_loss: 0.624
[43]  [840/1724] loss: 0.825, ave_loss: 0.628
[44]  [860/1724] loss: 0.587, ave_loss: 0.627
[45]  [880/1724] loss: 0.560, ave_loss: 0.626
[46]  [900/1724] loss: 0.557, ave_loss: 0.624
[47]  [920/1724] loss: 0.554, ave_loss: 0.623
[48]  [940/1724] loss: 0.558, ave_loss: 0.622
[49]  [960/1724] loss: 0.889, ave_loss: 0.627
[50]  [980/1724] loss: 0.634, ave_loss: 0.627
[51]  [1000/1724] loss: 0.815, ave_loss: 0.631
[52]  [1020/1724] loss: 0.786, ave_loss: 0.634
[53]  [1040/1724] loss: 0.782, ave_loss: 0.637
[54]  [1060/1724] loss: 0.576, ave_loss: 0.635
[55]  [1080/1724] loss: 0.687, ave_loss: 0.636
[56]  [1100/1724] loss: 0.582, ave_loss: 0.635
[57]  [1120/1724] loss: 0.931, ave_loss: 0.641
[58]  [1140/1724] loss: 0.578, ave_loss: 0.640
[59]  [1160/1724] loss: 0.670, ave_loss: 0.640
[60]  [1180/1724] loss: 0.529, ave_loss: 0.638
[61]  [1200/1724] loss: 0.630, ave_loss: 0.638
[62]  [1220/1724] loss: 0.624, ave_loss: 0.638
[63]  [1240/1724] loss: 0.641, ave_loss: 0.638
[64]  [1260/1724] loss: 0.504, ave_loss: 0.636
[65]  [1280/1724] loss: 0.684, ave_loss: 0.637
[66]  [1300/1724] loss: 0.699, ave_loss: 0.638
[67]  [1320/1724] loss: 0.804, ave_loss: 0.640
[68]  [1340/1724] loss: 0.804, ave_loss: 0.642
[69]  [1360/1724] loss: 0.624, ave_loss: 0.642
[70]  [1380/1724] loss: 0.425, ave_loss: 0.639
[71]  [1400/1724] loss: 0.640, ave_loss: 0.639
[72]  [1420/1724] loss: 0.528, ave_loss: 0.637
[73]  [1440/1724] loss: 0.548, ave_loss: 0.636
[74]  [1460/1724] loss: 0.537, ave_loss: 0.635
[75]  [1480/1724] loss: 0.530, ave_loss: 0.634
[76]  [1500/1724] loss: 0.618, ave_loss: 0.633
[77]  [1520/1724] loss: 0.771, ave_loss: 0.635
[78]  [1540/1724] loss: 0.826, ave_loss: 0.638
[79]  [1560/1724] loss: 0.552, ave_loss: 0.636
[80]  [1580/1724] loss: 0.437, ave_loss: 0.634
[81]  [1600/1724] loss: 0.624, ave_loss: 0.634
[82]  [1620/1724] loss: 0.562, ave_loss: 0.633
[83]  [1640/1724] loss: 0.511, ave_loss: 0.631
[84]  [1660/1724] loss: 0.670, ave_loss: 0.632
[85]  [1680/1724] loss: 0.583, ave_loss: 0.631
[86]  [1700/1724] loss: 0.821, ave_loss: 0.634
[87]  [1720/1724] loss: 0.429, ave_loss: 0.631
[88]  [1740/1724] loss: 0.750, ave_loss: 0.633

Finished Training finishing at 2021-08-29 19:19:23.919440
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.326e-01
Validation Loss: 8.202e+04
Validation ROC: 0.5262
Saving model
20.832946635730856 epochs left to go

Training Epoch 8.167053364269142/30 starting at 2021-08-29 19:21:52.082719
[1]  [0/1724] loss: 0.756, ave_loss: 0.756
[2]  [20/1724] loss: 0.658, ave_loss: 0.707
[3]  [40/1724] loss: 0.578, ave_loss: 0.664
[4]  [60/1724] loss: 0.424, ave_loss: 0.604
[5]  [80/1724] loss: 0.775, ave_loss: 0.638
[6]  [100/1724] loss: 0.731, ave_loss: 0.654
[7]  [120/1724] loss: 0.560, ave_loss: 0.640
[8]  [140/1724] loss: 0.414, ave_loss: 0.612
[9]  [160/1724] loss: 0.805, ave_loss: 0.634
[10]  [180/1724] loss: 0.674, ave_loss: 0.638
[11]  [200/1724] loss: 0.540, ave_loss: 0.629
[12]  [220/1724] loss: 0.604, ave_loss: 0.627
[13]  [240/1724] loss: 0.536, ave_loss: 0.620
[14]  [260/1724] loss: 0.455, ave_loss: 0.608
[15]  [280/1724] loss: 0.744, ave_loss: 0.617
[16]  [300/1724] loss: 0.409, ave_loss: 0.604
[17]  [320/1724] loss: 0.678, ave_loss: 0.608
[18]  [340/1724] loss: 0.774, ave_loss: 0.618
[19]  [360/1724] loss: 0.865, ave_loss: 0.631
[20]  [380/1724] loss: 0.663, ave_loss: 0.632
[21]  [400/1724] loss: 0.686, ave_loss: 0.635
[22]  [420/1724] loss: 0.706, ave_loss: 0.638
[23]  [440/1724] loss: 0.526, ave_loss: 0.633
[24]  [460/1724] loss: 0.609, ave_loss: 0.632
[25]  [480/1724] loss: 0.597, ave_loss: 0.631
[26]  [500/1724] loss: 0.739, ave_loss: 0.635
[27]  [520/1724] loss: 0.642, ave_loss: 0.635
[28]  [540/1724] loss: 0.577, ave_loss: 0.633
[29]  [560/1724] loss: 0.607, ave_loss: 0.632
[30]  [580/1724] loss: 0.452, ave_loss: 0.626
[31]  [600/1724] loss: 0.592, ave_loss: 0.625
[32]  [620/1724] loss: 0.596, ave_loss: 0.624
[33]  [640/1724] loss: 0.501, ave_loss: 0.620
[34]  [660/1724] loss: 0.490, ave_loss: 0.617
[35]  [680/1724] loss: 0.663, ave_loss: 0.618
[36]  [700/1724] loss: 0.598, ave_loss: 0.617
[37]  [720/1724] loss: 0.488, ave_loss: 0.614
[38]  [740/1724] loss: 0.629, ave_loss: 0.614
[39]  [760/1724] loss: 0.417, ave_loss: 0.609
[40]  [780/1724] loss: 0.607, ave_loss: 0.609
[41]  [800/1724] loss: 0.615, ave_loss: 0.609
[42]  [820/1724] loss: 0.522, ave_loss: 0.607
[43]  [840/1724] loss: 0.463, ave_loss: 0.604
[44]  [860/1724] loss: 0.519, ave_loss: 0.602
[45]  [880/1724] loss: 0.561, ave_loss: 0.601
[46]  [900/1724] loss: 0.621, ave_loss: 0.601
[47]  [920/1724] loss: 0.907, ave_loss: 0.608
[48]  [940/1724] loss: 0.545, ave_loss: 0.607
[49]  [960/1724] loss: 0.547, ave_loss: 0.605
[50]  [980/1724] loss: 0.422, ave_loss: 0.602
[51]  [1000/1724] loss: 0.673, ave_loss: 0.603
[52]  [1020/1724] loss: 0.437, ave_loss: 0.600
[53]  [1040/1724] loss: 0.528, ave_loss: 0.599
[54]  [1060/1724] loss: 0.679, ave_loss: 0.600
[55]  [1080/1724] loss: 0.530, ave_loss: 0.599
[56]  [1100/1724] loss: 0.701, ave_loss: 0.601
[57]  [1120/1724] loss: 0.522, ave_loss: 0.599
[58]  [1140/1724] loss: 0.621, ave_loss: 0.600
[59]  [1160/1724] loss: 0.897, ave_loss: 0.605
[60]  [1180/1724] loss: 0.606, ave_loss: 0.605
[61]  [1200/1724] loss: 0.790, ave_loss: 0.608
[62]  [1220/1724] loss: 0.926, ave_loss: 0.613
[63]  [1240/1724] loss: 0.809, ave_loss: 0.616
[64]  [1260/1724] loss: 0.421, ave_loss: 0.613
[65]  [1280/1724] loss: 0.565, ave_loss: 0.612
[66]  [1300/1724] loss: 0.717, ave_loss: 0.614
[67]  [1320/1724] loss: 0.621, ave_loss: 0.614
[68]  [1340/1724] loss: 0.526, ave_loss: 0.613
[69]  [1360/1724] loss: 0.702, ave_loss: 0.614
[70]  [1380/1724] loss: 0.556, ave_loss: 0.613
[71]  [1400/1724] loss: 0.500, ave_loss: 0.612
[72]  [1420/1724] loss: 0.660, ave_loss: 0.612
[73]  [1440/1724] loss: 0.465, ave_loss: 0.610
[74]  [1460/1724] loss: 0.457, ave_loss: 0.608
[75]  [1480/1724] loss: 0.668, ave_loss: 0.609
[76]  [1500/1724] loss: 0.579, ave_loss: 0.609
[77]  [1520/1724] loss: 0.641, ave_loss: 0.609
[78]  [1540/1724] loss: 0.907, ave_loss: 0.613
[79]  [1560/1724] loss: 0.755, ave_loss: 0.615
[80]  [1580/1724] loss: 0.422, ave_loss: 0.612
[81]  [1600/1724] loss: 0.574, ave_loss: 0.612
[82]  [1620/1724] loss: 0.362, ave_loss: 0.609
[83]  [1640/1724] loss: 0.721, ave_loss: 0.610
[84]  [1660/1724] loss: 0.691, ave_loss: 0.611
[85]  [1680/1724] loss: 0.508, ave_loss: 0.610
[86]  [1700/1724] loss: 0.798, ave_loss: 0.612
[87]  [1720/1724] loss: 0.497, ave_loss: 0.611
[88]  [1740/1724] loss: 0.535, ave_loss: 0.610

Finished Training finishing at 2021-08-29 19:25:24.740385
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.097e-01
Validation Loss: 8.664e+04
Validation ROC: 0.5708
Saving model
19.812064965197216 epochs left to go

Training Epoch 9.187935034802784/30 starting at 2021-08-29 19:27:45.341616
[1]  [0/1724] loss: 0.770, ave_loss: 0.770
[2]  [20/1724] loss: 0.465, ave_loss: 0.618
[3]  [40/1724] loss: 0.572, ave_loss: 0.602
[4]  [60/1724] loss: 0.572, ave_loss: 0.595
[5]  [80/1724] loss: 0.529, ave_loss: 0.582
[6]  [100/1724] loss: 0.663, ave_loss: 0.595
[7]  [120/1724] loss: 0.505, ave_loss: 0.582
[8]  [140/1724] loss: 0.638, ave_loss: 0.589
[9]  [160/1724] loss: 0.614, ave_loss: 0.592
[10]  [180/1724] loss: 0.509, ave_loss: 0.584
[11]  [200/1724] loss: 0.627, ave_loss: 0.588
[12]  [220/1724] loss: 0.484, ave_loss: 0.579
[13]  [240/1724] loss: 0.466, ave_loss: 0.570
[14]  [260/1724] loss: 0.563, ave_loss: 0.570
[15]  [280/1724] loss: 0.563, ave_loss: 0.569
[16]  [300/1724] loss: 0.601, ave_loss: 0.571
[17]  [320/1724] loss: 0.636, ave_loss: 0.575
[18]  [340/1724] loss: 0.590, ave_loss: 0.576
[19]  [360/1724] loss: 0.609, ave_loss: 0.578
[20]  [380/1724] loss: 0.623, ave_loss: 0.580
[21]  [400/1724] loss: 0.476, ave_loss: 0.575
[22]  [420/1724] loss: 0.751, ave_loss: 0.583
[23]  [440/1724] loss: 0.722, ave_loss: 0.589
[24]  [460/1724] loss: 0.620, ave_loss: 0.590
[25]  [480/1724] loss: 0.717, ave_loss: 0.595
[26]  [500/1724] loss: 0.487, ave_loss: 0.591
[27]  [520/1724] loss: 0.834, ave_loss: 0.600
[28]  [540/1724] loss: 0.624, ave_loss: 0.601
[29]  [560/1724] loss: 0.450, ave_loss: 0.596
[30]  [580/1724] loss: 0.591, ave_loss: 0.596
[31]  [600/1724] loss: 0.661, ave_loss: 0.598
[32]  [620/1724] loss: 0.664, ave_loss: 0.600
[33]  [640/1724] loss: 0.550, ave_loss: 0.598
[34]  [660/1724] loss: 0.596, ave_loss: 0.598
[35]  [680/1724] loss: 0.615, ave_loss: 0.599
[36]  [700/1724] loss: 0.429, ave_loss: 0.594
[37]  [720/1724] loss: 0.555, ave_loss: 0.593
[38]  [740/1724] loss: 0.661, ave_loss: 0.595
[39]  [760/1724] loss: 0.530, ave_loss: 0.593
[40]  [780/1724] loss: 0.895, ave_loss: 0.601
[41]  [800/1724] loss: 0.378, ave_loss: 0.595
[42]  [820/1724] loss: 0.560, ave_loss: 0.594
[43]  [840/1724] loss: 0.559, ave_loss: 0.594
[44]  [860/1724] loss: 0.503, ave_loss: 0.591
[45]  [880/1724] loss: 0.589, ave_loss: 0.591
[46]  [900/1724] loss: 0.857, ave_loss: 0.597
[47]  [920/1724] loss: 0.564, ave_loss: 0.596
[48]  [940/1724] loss: 0.654, ave_loss: 0.598
[49]  [960/1724] loss: 0.626, ave_loss: 0.598
[50]  [980/1724] loss: 0.529, ave_loss: 0.597
[51]  [1000/1724] loss: 0.508, ave_loss: 0.595
[52]  [1020/1724] loss: 0.717, ave_loss: 0.597
[53]  [1040/1724] loss: 0.671, ave_loss: 0.599
[54]  [1060/1724] loss: 0.692, ave_loss: 0.601
[55]  [1080/1724] loss: 0.479, ave_loss: 0.598
[56]  [1100/1724] loss: 0.515, ave_loss: 0.597
[57]  [1120/1724] loss: 0.592, ave_loss: 0.597
[58]  [1140/1724] loss: 0.522, ave_loss: 0.596
[59]  [1160/1724] loss: 0.567, ave_loss: 0.595
[60]  [1180/1724] loss: 0.777, ave_loss: 0.598
[61]  [1200/1724] loss: 0.728, ave_loss: 0.600
[62]  [1220/1724] loss: 0.516, ave_loss: 0.599
[63]  [1240/1724] loss: 0.614, ave_loss: 0.599
[64]  [1260/1724] loss: 0.449, ave_loss: 0.597
[65]  [1280/1724] loss: 0.702, ave_loss: 0.598
[66]  [1300/1724] loss: 0.570, ave_loss: 0.598
[67]  [1320/1724] loss: 0.568, ave_loss: 0.597
[68]  [1340/1724] loss: 0.775, ave_loss: 0.600
[69]  [1360/1724] loss: 0.591, ave_loss: 0.600
[70]  [1380/1724] loss: 0.553, ave_loss: 0.599
[71]  [1400/1724] loss: 0.729, ave_loss: 0.601
[72]  [1420/1724] loss: 0.739, ave_loss: 0.603
[73]  [1440/1724] loss: 0.490, ave_loss: 0.601
[74]  [1460/1724] loss: 0.549, ave_loss: 0.601
[75]  [1480/1724] loss: 0.455, ave_loss: 0.599
[76]  [1500/1724] loss: 0.504, ave_loss: 0.598
[77]  [1520/1724] loss: 0.525, ave_loss: 0.597
[78]  [1540/1724] loss: 0.787, ave_loss: 0.599
[79]  [1560/1724] loss: 0.482, ave_loss: 0.598
[80]  [1580/1724] loss: 0.747, ave_loss: 0.599
[81]  [1600/1724] loss: 0.640, ave_loss: 0.600
[82]  [1620/1724] loss: 0.450, ave_loss: 0.598
[83]  [1640/1724] loss: 0.527, ave_loss: 0.597
[84]  [1660/1724] loss: 0.592, ave_loss: 0.597
[85]  [1680/1724] loss: 0.534, ave_loss: 0.596
[86]  [1700/1724] loss: 0.734, ave_loss: 0.598
[87]  [1720/1724] loss: 0.810, ave_loss: 0.601
[88]  [1740/1724] loss: 0.606, ave_loss: 0.601

Finished Training finishing at 2021-08-29 19:31:07.209706
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.006e-01
Validation Loss: 9.161e+04
Validation ROC: 0.5983
Saving model
18.791183294663576 epochs left to go

Training Epoch 10.208816705336426/30 starting at 2021-08-29 19:33:37.148297
[1]  [0/1724] loss: 0.649, ave_loss: 0.649
[2]  [20/1724] loss: 0.656, ave_loss: 0.653
[3]  [40/1724] loss: 0.510, ave_loss: 0.605
[4]  [60/1724] loss: 0.637, ave_loss: 0.613
[5]  [80/1724] loss: 0.610, ave_loss: 0.613
[6]  [100/1724] loss: 0.537, ave_loss: 0.600
[7]  [120/1724] loss: 0.514, ave_loss: 0.588
[8]  [140/1724] loss: 0.465, ave_loss: 0.572
[9]  [160/1724] loss: 0.532, ave_loss: 0.568
[10]  [180/1724] loss: 0.601, ave_loss: 0.571
[11]  [200/1724] loss: 0.646, ave_loss: 0.578
[12]  [220/1724] loss: 0.581, ave_loss: 0.578
[13]  [240/1724] loss: 0.573, ave_loss: 0.578
[14]  [260/1724] loss: 0.463, ave_loss: 0.570
[15]  [280/1724] loss: 0.524, ave_loss: 0.567
[16]  [300/1724] loss: 0.571, ave_loss: 0.567
[17]  [320/1724] loss: 0.704, ave_loss: 0.575
[18]  [340/1724] loss: 0.815, ave_loss: 0.588
[19]  [360/1724] loss: 0.577, ave_loss: 0.588
[20]  [380/1724] loss: 0.773, ave_loss: 0.597
[21]  [400/1724] loss: 0.632, ave_loss: 0.599
[22]  [420/1724] loss: 0.783, ave_loss: 0.607
[23]  [440/1724] loss: 0.615, ave_loss: 0.607
[24]  [460/1724] loss: 0.492, ave_loss: 0.603
[25]  [480/1724] loss: 0.671, ave_loss: 0.605
[26]  [500/1724] loss: 0.495, ave_loss: 0.601
[27]  [520/1724] loss: 0.668, ave_loss: 0.604
[28]  [540/1724] loss: 0.724, ave_loss: 0.608
[29]  [560/1724] loss: 0.344, ave_loss: 0.599
[30]  [580/1724] loss: 0.366, ave_loss: 0.591
[31]  [600/1724] loss: 0.568, ave_loss: 0.590
[32]  [620/1724] loss: 0.458, ave_loss: 0.586
[33]  [640/1724] loss: 0.563, ave_loss: 0.585
[34]  [660/1724] loss: 0.521, ave_loss: 0.583
[35]  [680/1724] loss: 0.594, ave_loss: 0.584
[36]  [700/1724] loss: 0.613, ave_loss: 0.585
[37]  [720/1724] loss: 0.679, ave_loss: 0.587
[38]  [740/1724] loss: 0.616, ave_loss: 0.588
[39]  [760/1724] loss: 0.558, ave_loss: 0.587
[40]  [780/1724] loss: 0.511, ave_loss: 0.585
[41]  [800/1724] loss: 0.571, ave_loss: 0.585
[42]  [820/1724] loss: 0.466, ave_loss: 0.582
[43]  [840/1724] loss: 0.362, ave_loss: 0.577
[44]  [860/1724] loss: 0.407, ave_loss: 0.573
[45]  [880/1724] loss: 0.658, ave_loss: 0.575
[46]  [900/1724] loss: 0.660, ave_loss: 0.577
[47]  [920/1724] loss: 0.507, ave_loss: 0.575
[48]  [940/1724] loss: 0.593, ave_loss: 0.576
[49]  [960/1724] loss: 0.476, ave_loss: 0.574
[50]  [980/1724] loss: 0.542, ave_loss: 0.573
[51]  [1000/1724] loss: 0.539, ave_loss: 0.572
[52]  [1020/1724] loss: 0.673, ave_loss: 0.574
[53]  [1040/1724] loss: 0.572, ave_loss: 0.574
[54]  [1060/1724] loss: 0.734, ave_loss: 0.577
[55]  [1080/1724] loss: 0.621, ave_loss: 0.578
[56]  [1100/1724] loss: 0.680, ave_loss: 0.580
[57]  [1120/1724] loss: 0.652, ave_loss: 0.581
[58]  [1140/1724] loss: 0.630, ave_loss: 0.582
[59]  [1160/1724] loss: 0.638, ave_loss: 0.583
[60]  [1180/1724] loss: 0.618, ave_loss: 0.583
[61]  [1200/1724] loss: 0.486, ave_loss: 0.582
[62]  [1220/1724] loss: 0.496, ave_loss: 0.580
[63]  [1240/1724] loss: 0.660, ave_loss: 0.582
[64]  [1260/1724] loss: 0.636, ave_loss: 0.583
[65]  [1280/1724] loss: 0.672, ave_loss: 0.584
[66]  [1300/1724] loss: 0.599, ave_loss: 0.584
[67]  [1320/1724] loss: 0.575, ave_loss: 0.584
[68]  [1340/1724] loss: 0.631, ave_loss: 0.585
[69]  [1360/1724] loss: 0.461, ave_loss: 0.583
[70]  [1380/1724] loss: 0.587, ave_loss: 0.583
[71]  [1400/1724] loss: 0.694, ave_loss: 0.585
[72]  [1420/1724] loss: 0.607, ave_loss: 0.585
[73]  [1440/1724] loss: 0.695, ave_loss: 0.586
[74]  [1460/1724] loss: 0.354, ave_loss: 0.583
[75]  [1480/1724] loss: 0.406, ave_loss: 0.581
[76]  [1500/1724] loss: 0.608, ave_loss: 0.581
[77]  [1520/1724] loss: 0.481, ave_loss: 0.580
[78]  [1540/1724] loss: 0.427, ave_loss: 0.578
[79]  [1560/1724] loss: 0.542, ave_loss: 0.578
[80]  [1580/1724] loss: 0.419, ave_loss: 0.576
[81]  [1600/1724] loss: 0.502, ave_loss: 0.575
[82]  [1620/1724] loss: 0.584, ave_loss: 0.575
[83]  [1640/1724] loss: 0.664, ave_loss: 0.576
[84]  [1660/1724] loss: 0.714, ave_loss: 0.577
[85]  [1680/1724] loss: 0.688, ave_loss: 0.579
[86]  [1700/1724] loss: 0.818, ave_loss: 0.582
[87]  [1720/1724] loss: 0.572, ave_loss: 0.581
[88]  [1740/1724] loss: 0.487, ave_loss: 0.580

Finished Training finishing at 2021-08-29 19:37:23.084870
printing_out epoch  11.22969837587007 learning rate: 0.0005153561248318907
0.00036863495918228726
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.804e-01
Validation Loss: 9.557e+04
Validation ROC: 0.5980
No improvement, still saving model
17.770301624129928 epochs left to go

Training Epoch 11.22969837587007/30 starting at 2021-08-29 19:39:28.806954
[1]  [0/1724] loss: 0.640, ave_loss: 0.640
[2]  [20/1724] loss: 0.884, ave_loss: 0.762
[3]  [40/1724] loss: 0.567, ave_loss: 0.697
[4]  [60/1724] loss: 0.508, ave_loss: 0.650
[5]  [80/1724] loss: 0.507, ave_loss: 0.621
[6]  [100/1724] loss: 0.714, ave_loss: 0.637
[7]  [120/1724] loss: 0.576, ave_loss: 0.628
[8]  [140/1724] loss: 0.753, ave_loss: 0.644
[9]  [160/1724] loss: 0.531, ave_loss: 0.631
[10]  [180/1724] loss: 0.612, ave_loss: 0.629
[11]  [200/1724] loss: 0.559, ave_loss: 0.623
[12]  [220/1724] loss: 0.479, ave_loss: 0.611
[13]  [240/1724] loss: 0.492, ave_loss: 0.602
[14]  [260/1724] loss: 0.600, ave_loss: 0.602
[15]  [280/1724] loss: 0.571, ave_loss: 0.600
[16]  [300/1724] loss: 0.522, ave_loss: 0.595
[17]  [320/1724] loss: 0.730, ave_loss: 0.603
[18]  [340/1724] loss: 0.500, ave_loss: 0.597
[19]  [360/1724] loss: 0.329, ave_loss: 0.583
[20]  [380/1724] loss: 0.590, ave_loss: 0.583
[21]  [400/1724] loss: 0.477, ave_loss: 0.578
[22]  [420/1724] loss: 0.467, ave_loss: 0.573
[23]  [440/1724] loss: 0.517, ave_loss: 0.571
[24]  [460/1724] loss: 0.521, ave_loss: 0.569
[25]  [480/1724] loss: 0.592, ave_loss: 0.570
[26]  [500/1724] loss: 0.740, ave_loss: 0.576
[27]  [520/1724] loss: 0.306, ave_loss: 0.566
[28]  [540/1724] loss: 0.596, ave_loss: 0.567
[29]  [560/1724] loss: 0.503, ave_loss: 0.565
[30]  [580/1724] loss: 0.679, ave_loss: 0.569
[31]  [600/1724] loss: 0.558, ave_loss: 0.568
[32]  [620/1724] loss: 0.513, ave_loss: 0.567
[33]  [640/1724] loss: 0.504, ave_loss: 0.565
[34]  [660/1724] loss: 0.611, ave_loss: 0.566
[35]  [680/1724] loss: 0.679, ave_loss: 0.569
[36]  [700/1724] loss: 0.566, ave_loss: 0.569
[37]  [720/1724] loss: 0.664, ave_loss: 0.572
[38]  [740/1724] loss: 0.490, ave_loss: 0.570
[39]  [760/1724] loss: 0.517, ave_loss: 0.568
[40]  [780/1724] loss: 0.599, ave_loss: 0.569
[41]  [800/1724] loss: 0.443, ave_loss: 0.566
[42]  [820/1724] loss: 0.448, ave_loss: 0.563
[43]  [840/1724] loss: 0.475, ave_loss: 0.561
[44]  [860/1724] loss: 0.653, ave_loss: 0.563
[45]  [880/1724] loss: 0.557, ave_loss: 0.563
[46]  [900/1724] loss: 0.505, ave_loss: 0.562
[47]  [920/1724] loss: 0.618, ave_loss: 0.563
[48]  [940/1724] loss: 0.682, ave_loss: 0.565
[49]  [960/1724] loss: 0.516, ave_loss: 0.564
[50]  [980/1724] loss: 0.517, ave_loss: 0.564
[51]  [1000/1724] loss: 0.735, ave_loss: 0.567
[52]  [1020/1724] loss: 0.638, ave_loss: 0.568
[53]  [1040/1724] loss: 0.423, ave_loss: 0.566
[54]  [1060/1724] loss: 0.566, ave_loss: 0.566
[55]  [1080/1724] loss: 0.693, ave_loss: 0.568
[56]  [1100/1724] loss: 0.576, ave_loss: 0.568
[57]  [1120/1724] loss: 0.526, ave_loss: 0.567
[58]  [1140/1724] loss: 0.744, ave_loss: 0.570
[59]  [1160/1724] loss: 0.477, ave_loss: 0.569
[60]  [1180/1724] loss: 0.583, ave_loss: 0.569
[61]  [1200/1724] loss: 0.563, ave_loss: 0.569
[62]  [1220/1724] loss: 0.456, ave_loss: 0.567
[63]  [1240/1724] loss: 0.613, ave_loss: 0.568
[64]  [1260/1724] loss: 0.500, ave_loss: 0.567
[65]  [1280/1724] loss: 0.827, ave_loss: 0.571
[66]  [1300/1724] loss: 0.537, ave_loss: 0.570
[67]  [1320/1724] loss: 0.570, ave_loss: 0.570
[68]  [1340/1724] loss: 0.417, ave_loss: 0.568
[69]  [1360/1724] loss: 0.693, ave_loss: 0.570
[70]  [1380/1724] loss: 0.549, ave_loss: 0.569
[71]  [1400/1724] loss: 0.560, ave_loss: 0.569
[72]  [1420/1724] loss: 0.502, ave_loss: 0.568
[73]  [1440/1724] loss: 0.476, ave_loss: 0.567
[74]  [1460/1724] loss: 0.440, ave_loss: 0.565
[75]  [1480/1724] loss: 0.677, ave_loss: 0.567
[76]  [1500/1724] loss: 0.625, ave_loss: 0.568
[77]  [1520/1724] loss: 0.627, ave_loss: 0.568
[78]  [1540/1724] loss: 0.336, ave_loss: 0.565
[79]  [1560/1724] loss: 0.407, ave_loss: 0.563
[80]  [1580/1724] loss: 0.650, ave_loss: 0.564
[81]  [1600/1724] loss: 0.598, ave_loss: 0.565
[82]  [1620/1724] loss: 0.617, ave_loss: 0.566
[83]  [1640/1724] loss: 0.472, ave_loss: 0.564
[84]  [1660/1724] loss: 0.500, ave_loss: 0.564
[85]  [1680/1724] loss: 0.720, ave_loss: 0.565
[86]  [1700/1724] loss: 0.771, ave_loss: 0.568
[87]  [1720/1724] loss: 0.640, ave_loss: 0.569
[88]  [1740/1724] loss: 0.667, ave_loss: 0.570

Finished Training finishing at 2021-08-29 19:42:54.449758
printing_out epoch  12.250580046403712 learning rate: 0.0005153561248318907
0.0003575759104068186
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.698e-01
Validation Loss: 9.373e+04
Validation ROC: 0.6190
Saving model
16.749419953596288 epochs left to go

Training Epoch 12.250580046403712/30 starting at 2021-08-29 19:45:28.495182
[1]  [0/1724] loss: 0.584, ave_loss: 0.584
[2]  [20/1724] loss: 0.403, ave_loss: 0.493
[3]  [40/1724] loss: 0.418, ave_loss: 0.468
[4]  [60/1724] loss: 0.568, ave_loss: 0.493
[5]  [80/1724] loss: 0.420, ave_loss: 0.479
[6]  [100/1724] loss: 0.718, ave_loss: 0.518
[7]  [120/1724] loss: 0.591, ave_loss: 0.529
[8]  [140/1724] loss: 0.423, ave_loss: 0.516
[9]  [160/1724] loss: 0.508, ave_loss: 0.515
[10]  [180/1724] loss: 0.693, ave_loss: 0.533
[11]  [200/1724] loss: 0.703, ave_loss: 0.548
[12]  [220/1724] loss: 0.592, ave_loss: 0.552
[13]  [240/1724] loss: 0.537, ave_loss: 0.551
[14]  [260/1724] loss: 0.612, ave_loss: 0.555
[15]  [280/1724] loss: 0.487, ave_loss: 0.551
[16]  [300/1724] loss: 0.516, ave_loss: 0.548
[17]  [320/1724] loss: 0.606, ave_loss: 0.552
[18]  [340/1724] loss: 0.600, ave_loss: 0.554
[19]  [360/1724] loss: 0.661, ave_loss: 0.560
[20]  [380/1724] loss: 0.429, ave_loss: 0.554
[21]  [400/1724] loss: 0.516, ave_loss: 0.552
[22]  [420/1724] loss: 0.427, ave_loss: 0.546
[23]  [440/1724] loss: 0.384, ave_loss: 0.539
[24]  [460/1724] loss: 0.506, ave_loss: 0.538
[25]  [480/1724] loss: 0.517, ave_loss: 0.537
[26]  [500/1724] loss: 0.627, ave_loss: 0.540
[27]  [520/1724] loss: 0.571, ave_loss: 0.541
[28]  [540/1724] loss: 0.626, ave_loss: 0.544
[29]  [560/1724] loss: 0.560, ave_loss: 0.545
[30]  [580/1724] loss: 0.681, ave_loss: 0.549
[31]  [600/1724] loss: 0.680, ave_loss: 0.554
[32]  [620/1724] loss: 0.531, ave_loss: 0.553
[33]  [640/1724] loss: 0.553, ave_loss: 0.553
[34]  [660/1724] loss: 0.441, ave_loss: 0.550
[35]  [680/1724] loss: 0.506, ave_loss: 0.548
[36]  [700/1724] loss: 0.547, ave_loss: 0.548
[37]  [720/1724] loss: 0.495, ave_loss: 0.547
[38]  [740/1724] loss: 0.807, ave_loss: 0.554
[39]  [760/1724] loss: 0.613, ave_loss: 0.555
[40]  [780/1724] loss: 0.564, ave_loss: 0.556
[41]  [800/1724] loss: 0.455, ave_loss: 0.553
[42]  [820/1724] loss: 0.416, ave_loss: 0.550
[43]  [840/1724] loss: 0.537, ave_loss: 0.550
[44]  [860/1724] loss: 0.712, ave_loss: 0.553
[45]  [880/1724] loss: 0.578, ave_loss: 0.554
[46]  [900/1724] loss: 0.534, ave_loss: 0.553
[47]  [920/1724] loss: 0.545, ave_loss: 0.553
[48]  [940/1724] loss: 0.585, ave_loss: 0.554
[49]  [960/1724] loss: 0.568, ave_loss: 0.554
[50]  [980/1724] loss: 0.542, ave_loss: 0.554
[51]  [1000/1724] loss: 0.698, ave_loss: 0.557
[52]  [1020/1724] loss: 0.550, ave_loss: 0.557
[53]  [1040/1724] loss: 0.633, ave_loss: 0.558
[54]  [1060/1724] loss: 0.592, ave_loss: 0.559
[55]  [1080/1724] loss: 0.554, ave_loss: 0.559
[56]  [1100/1724] loss: 0.811, ave_loss: 0.563
[57]  [1120/1724] loss: 0.467, ave_loss: 0.561
[58]  [1140/1724] loss: 0.476, ave_loss: 0.560
[59]  [1160/1724] loss: 0.690, ave_loss: 0.562
[60]  [1180/1724] loss: 0.416, ave_loss: 0.560
[61]  [1200/1724] loss: 0.823, ave_loss: 0.564
[62]  [1220/1724] loss: 0.574, ave_loss: 0.564
[63]  [1240/1724] loss: 0.703, ave_loss: 0.566
[64]  [1260/1724] loss: 0.606, ave_loss: 0.567
[65]  [1280/1724] loss: 0.437, ave_loss: 0.565
[66]  [1300/1724] loss: 0.557, ave_loss: 0.565
[67]  [1320/1724] loss: 0.650, ave_loss: 0.566
[68]  [1340/1724] loss: 0.506, ave_loss: 0.565
[69]  [1360/1724] loss: 0.724, ave_loss: 0.568
[70]  [1380/1724] loss: 0.518, ave_loss: 0.567
[71]  [1400/1724] loss: 0.777, ave_loss: 0.570
[72]  [1420/1724] loss: 0.576, ave_loss: 0.570
[73]  [1440/1724] loss: 0.575, ave_loss: 0.570
[74]  [1460/1724] loss: 0.503, ave_loss: 0.569
[75]  [1480/1724] loss: 0.692, ave_loss: 0.571
[76]  [1500/1724] loss: 0.616, ave_loss: 0.571
[77]  [1520/1724] loss: 0.568, ave_loss: 0.571
[78]  [1540/1724] loss: 0.620, ave_loss: 0.572
[79]  [1560/1724] loss: 0.661, ave_loss: 0.573
[80]  [1580/1724] loss: 0.533, ave_loss: 0.572
[81]  [1600/1724] loss: 0.562, ave_loss: 0.572
[82]  [1620/1724] loss: 0.395, ave_loss: 0.570
[83]  [1640/1724] loss: 0.583, ave_loss: 0.570
[84]  [1660/1724] loss: 0.453, ave_loss: 0.569
[85]  [1680/1724] loss: 0.579, ave_loss: 0.569
[86]  [1700/1724] loss: 0.588, ave_loss: 0.569
[87]  [1720/1724] loss: 0.586, ave_loss: 0.569
[88]  [1740/1724] loss: 0.622, ave_loss: 0.570

Finished Training finishing at 2021-08-29 19:50:16.101801
printing_out epoch  13.271461716937354 learning rate: 0.0005153561248318907
0.00034684863309461403
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.701e-01
Validation Loss: 9.255e+04
Validation ROC: 0.6099
No improvement, still saving model
15.728538283062646 epochs left to go

Training Epoch 13.271461716937354/30 starting at 2021-08-29 19:52:29.741660
[1]  [0/1724] loss: 0.647, ave_loss: 0.647
[2]  [20/1724] loss: 0.533, ave_loss: 0.590
[3]  [40/1724] loss: 0.432, ave_loss: 0.537
[4]  [60/1724] loss: 0.574, ave_loss: 0.546
[5]  [80/1724] loss: 0.590, ave_loss: 0.555
[6]  [100/1724] loss: 0.494, ave_loss: 0.545
[7]  [120/1724] loss: 0.975, ave_loss: 0.606
[8]  [140/1724] loss: 0.633, ave_loss: 0.610
[9]  [160/1724] loss: 0.474, ave_loss: 0.595
[10]  [180/1724] loss: 0.584, ave_loss: 0.594
[11]  [200/1724] loss: 0.889, ave_loss: 0.620
[12]  [220/1724] loss: 0.611, ave_loss: 0.620
[13]  [240/1724] loss: 0.506, ave_loss: 0.611
[14]  [260/1724] loss: 0.599, ave_loss: 0.610
[15]  [280/1724] loss: 0.610, ave_loss: 0.610
[16]  [300/1724] loss: 0.488, ave_loss: 0.602
[17]  [320/1724] loss: 0.628, ave_loss: 0.604
[18]  [340/1724] loss: 0.454, ave_loss: 0.596
[19]  [360/1724] loss: 0.610, ave_loss: 0.596
[20]  [380/1724] loss: 0.564, ave_loss: 0.595
[21]  [400/1724] loss: 0.473, ave_loss: 0.589
[22]  [420/1724] loss: 0.603, ave_loss: 0.590
[23]  [440/1724] loss: 0.627, ave_loss: 0.591
[24]  [460/1724] loss: 0.654, ave_loss: 0.594
[25]  [480/1724] loss: 0.449, ave_loss: 0.588
[26]  [500/1724] loss: 0.596, ave_loss: 0.588
[27]  [520/1724] loss: 0.596, ave_loss: 0.589
[28]  [540/1724] loss: 0.415, ave_loss: 0.582
[29]  [560/1724] loss: 0.510, ave_loss: 0.580
[30]  [580/1724] loss: 0.733, ave_loss: 0.585
[31]  [600/1724] loss: 0.508, ave_loss: 0.583
[32]  [620/1724] loss: 0.460, ave_loss: 0.579
[33]  [640/1724] loss: 0.463, ave_loss: 0.575
[34]  [660/1724] loss: 0.575, ave_loss: 0.575
[35]  [680/1724] loss: 0.359, ave_loss: 0.569
[36]  [700/1724] loss: 0.482, ave_loss: 0.567
[37]  [720/1724] loss: 0.525, ave_loss: 0.565
[38]  [740/1724] loss: 0.693, ave_loss: 0.569
[39]  [760/1724] loss: 0.675, ave_loss: 0.572
[40]  [780/1724] loss: 0.519, ave_loss: 0.570
[41]  [800/1724] loss: 0.645, ave_loss: 0.572
[42]  [820/1724] loss: 0.407, ave_loss: 0.568
[43]  [840/1724] loss: 0.563, ave_loss: 0.568
[44]  [860/1724] loss: 0.362, ave_loss: 0.563
[45]  [880/1724] loss: 0.439, ave_loss: 0.561
[46]  [900/1724] loss: 0.598, ave_loss: 0.561
[47]  [920/1724] loss: 0.499, ave_loss: 0.560
[48]  [940/1724] loss: 0.794, ave_loss: 0.565
[49]  [960/1724] loss: 0.591, ave_loss: 0.565
[50]  [980/1724] loss: 0.660, ave_loss: 0.567
[51]  [1000/1724] loss: 0.488, ave_loss: 0.566
[52]  [1020/1724] loss: 0.487, ave_loss: 0.564
[53]  [1040/1724] loss: 0.549, ave_loss: 0.564
[54]  [1060/1724] loss: 0.459, ave_loss: 0.562
[55]  [1080/1724] loss: 0.448, ave_loss: 0.560
[56]  [1100/1724] loss: 0.738, ave_loss: 0.563
[57]  [1120/1724] loss: 0.449, ave_loss: 0.561
[58]  [1140/1724] loss: 0.643, ave_loss: 0.563
[59]  [1160/1724] loss: 0.456, ave_loss: 0.561
[60]  [1180/1724] loss: 0.669, ave_loss: 0.563
[61]  [1200/1724] loss: 0.566, ave_loss: 0.563
[62]  [1220/1724] loss: 0.483, ave_loss: 0.561
[63]  [1240/1724] loss: 0.626, ave_loss: 0.562
[64]  [1260/1724] loss: 0.489, ave_loss: 0.561
[65]  [1280/1724] loss: 0.620, ave_loss: 0.562
[66]  [1300/1724] loss: 0.650, ave_loss: 0.563
[67]  [1320/1724] loss: 0.713, ave_loss: 0.566
[68]  [1340/1724] loss: 0.563, ave_loss: 0.566
[69]  [1360/1724] loss: 0.558, ave_loss: 0.566
[70]  [1380/1724] loss: 0.576, ave_loss: 0.566
[71]  [1400/1724] loss: 0.747, ave_loss: 0.568
[72]  [1420/1724] loss: 0.735, ave_loss: 0.571
[73]  [1440/1724] loss: 0.494, ave_loss: 0.570
[74]  [1460/1724] loss: 0.566, ave_loss: 0.569
[75]  [1480/1724] loss: 0.598, ave_loss: 0.570
[76]  [1500/1724] loss: 0.534, ave_loss: 0.569
[77]  [1520/1724] loss: 0.489, ave_loss: 0.568
[78]  [1540/1724] loss: 0.509, ave_loss: 0.568
[79]  [1560/1724] loss: 0.654, ave_loss: 0.569
[80]  [1580/1724] loss: 0.555, ave_loss: 0.568
[81]  [1600/1724] loss: 0.597, ave_loss: 0.569
[82]  [1620/1724] loss: 0.549, ave_loss: 0.569
[83]  [1640/1724] loss: 0.679, ave_loss: 0.570
[84]  [1660/1724] loss: 0.588, ave_loss: 0.570
[85]  [1680/1724] loss: 0.550, ave_loss: 0.570
[86]  [1700/1724] loss: 0.569, ave_loss: 0.570
[87]  [1720/1724] loss: 0.640, ave_loss: 0.571
[88]  [1740/1724] loss: 0.768, ave_loss: 0.573

Finished Training finishing at 2021-08-29 19:55:45.699196
printing_out epoch  14.292343387470998 learning rate: 0.0005153561248318907
0.0003364431741017756
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.729e-01
Validation Loss: 9.395e+04
Validation ROC: 0.6165
No improvement, still saving model
14.707656612529002 epochs left to go

Training Epoch 14.292343387470998/30 starting at 2021-08-29 19:58:07.904797
[1]  [0/1724] loss: 0.727, ave_loss: 0.727
[2]  [20/1724] loss: 0.550, ave_loss: 0.638
[3]  [40/1724] loss: 0.506, ave_loss: 0.594
[4]  [60/1724] loss: 0.516, ave_loss: 0.574
[5]  [80/1724] loss: 0.499, ave_loss: 0.559
[6]  [100/1724] loss: 0.633, ave_loss: 0.572
[7]  [120/1724] loss: 0.700, ave_loss: 0.590
[8]  [140/1724] loss: 0.710, ave_loss: 0.605
[9]  [160/1724] loss: 0.602, ave_loss: 0.605
[10]  [180/1724] loss: 0.565, ave_loss: 0.601
[11]  [200/1724] loss: 0.652, ave_loss: 0.605
[12]  [220/1724] loss: 0.551, ave_loss: 0.601
[13]  [240/1724] loss: 0.616, ave_loss: 0.602
[14]  [260/1724] loss: 0.558, ave_loss: 0.599
[15]  [280/1724] loss: 0.694, ave_loss: 0.605
[16]  [300/1724] loss: 0.449, ave_loss: 0.595
[17]  [320/1724] loss: 0.541, ave_loss: 0.592
[18]  [340/1724] loss: 0.523, ave_loss: 0.588
[19]  [360/1724] loss: 0.513, ave_loss: 0.584
[20]  [380/1724] loss: 0.464, ave_loss: 0.578
[21]  [400/1724] loss: 0.599, ave_loss: 0.579
[22]  [420/1724] loss: 0.483, ave_loss: 0.575
[23]  [440/1724] loss: 0.578, ave_loss: 0.575
[24]  [460/1724] loss: 0.605, ave_loss: 0.576
[25]  [480/1724] loss: 0.494, ave_loss: 0.573
[26]  [500/1724] loss: 0.625, ave_loss: 0.575
[27]  [520/1724] loss: 0.509, ave_loss: 0.573
[28]  [540/1724] loss: 0.385, ave_loss: 0.566
[29]  [560/1724] loss: 0.561, ave_loss: 0.566
[30]  [580/1724] loss: 0.672, ave_loss: 0.569
[31]  [600/1724] loss: 0.409, ave_loss: 0.564
[32]  [620/1724] loss: 0.468, ave_loss: 0.561
[33]  [640/1724] loss: 0.532, ave_loss: 0.560
[34]  [660/1724] loss: 0.540, ave_loss: 0.560
[35]  [680/1724] loss: 0.590, ave_loss: 0.560
[36]  [700/1724] loss: 0.523, ave_loss: 0.559
[37]  [720/1724] loss: 0.724, ave_loss: 0.564
[38]  [740/1724] loss: 0.635, ave_loss: 0.566
[39]  [760/1724] loss: 0.591, ave_loss: 0.566
[40]  [780/1724] loss: 0.569, ave_loss: 0.566
[41]  [800/1724] loss: 0.612, ave_loss: 0.568
[42]  [820/1724] loss: 0.486, ave_loss: 0.566
[43]  [840/1724] loss: 0.416, ave_loss: 0.562
[44]  [860/1724] loss: 0.519, ave_loss: 0.561
[45]  [880/1724] loss: 0.726, ave_loss: 0.565
[46]  [900/1724] loss: 0.569, ave_loss: 0.565
[47]  [920/1724] loss: 0.815, ave_loss: 0.570
[48]  [940/1724] loss: 0.595, ave_loss: 0.571
[49]  [960/1724] loss: 0.629, ave_loss: 0.572
[50]  [980/1724] loss: 0.633, ave_loss: 0.573
[51]  [1000/1724] loss: 0.658, ave_loss: 0.575
[52]  [1020/1724] loss: 0.547, ave_loss: 0.574
[53]  [1040/1724] loss: 0.625, ave_loss: 0.575
[54]  [1060/1724] loss: 0.525, ave_loss: 0.574
[55]  [1080/1724] loss: 0.565, ave_loss: 0.574
[56]  [1100/1724] loss: 0.504, ave_loss: 0.573
[57]  [1120/1724] loss: 0.495, ave_loss: 0.571
[58]  [1140/1724] loss: 0.704, ave_loss: 0.574
[59]  [1160/1724] loss: 0.522, ave_loss: 0.573
[60]  [1180/1724] loss: 0.426, ave_loss: 0.570
[61]  [1200/1724] loss: 0.528, ave_loss: 0.570
[62]  [1220/1724] loss: 0.524, ave_loss: 0.569
[63]  [1240/1724] loss: 0.457, ave_loss: 0.567
[64]  [1260/1724] loss: 0.544, ave_loss: 0.567
[65]  [1280/1724] loss: 0.667, ave_loss: 0.568
[66]  [1300/1724] loss: 0.538, ave_loss: 0.568
[67]  [1320/1724] loss: 0.476, ave_loss: 0.567
[68]  [1340/1724] loss: 0.454, ave_loss: 0.565
[69]  [1360/1724] loss: 0.576, ave_loss: 0.565
[70]  [1380/1724] loss: 0.583, ave_loss: 0.565
[71]  [1400/1724] loss: 0.588, ave_loss: 0.566
[72]  [1420/1724] loss: 0.451, ave_loss: 0.564
[73]  [1440/1724] loss: 0.793, ave_loss: 0.567
[74]  [1460/1724] loss: 0.456, ave_loss: 0.566
[75]  [1480/1724] loss: 0.522, ave_loss: 0.565
[76]  [1500/1724] loss: 0.565, ave_loss: 0.565
[77]  [1520/1724] loss: 0.387, ave_loss: 0.563
[78]  [1540/1724] loss: 0.461, ave_loss: 0.562
[79]  [1560/1724] loss: 0.533, ave_loss: 0.561
[80]  [1580/1724] loss: 0.429, ave_loss: 0.560
[81]  [1600/1724] loss: 0.387, ave_loss: 0.557
[82]  [1620/1724] loss: 0.525, ave_loss: 0.557
[83]  [1640/1724] loss: 0.571, ave_loss: 0.557
[84]  [1660/1724] loss: 0.498, ave_loss: 0.556
[85]  [1680/1724] loss: 0.570, ave_loss: 0.557
[86]  [1700/1724] loss: 0.578, ave_loss: 0.557
[87]  [1720/1724] loss: 0.749, ave_loss: 0.559
[88]  [1740/1724] loss: 0.664, ave_loss: 0.560

Finished Training finishing at 2021-08-29 20:01:31.938233
printing_out epoch  15.31322505800464 learning rate: 0.0005153561248318907
0.0003263498788787223
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.603e-01
Validation Loss: 1.008e+05
Validation ROC: 0.6300
Saving model
13.68677494199536 epochs left to go

Training Epoch 15.31322505800464/30 starting at 2021-08-29 20:03:59.791997
[1]  [0/1724] loss: 1.072, ave_loss: 1.072
[2]  [20/1724] loss: 0.747, ave_loss: 0.909
[3]  [40/1724] loss: 0.577, ave_loss: 0.799
[4]  [60/1724] loss: 0.436, ave_loss: 0.708
[5]  [80/1724] loss: 0.520, ave_loss: 0.670
[6]  [100/1724] loss: 0.489, ave_loss: 0.640
[7]  [120/1724] loss: 0.665, ave_loss: 0.644
[8]  [140/1724] loss: 0.691, ave_loss: 0.650
[9]  [160/1724] loss: 0.476, ave_loss: 0.630
[10]  [180/1724] loss: 0.697, ave_loss: 0.637
[11]  [200/1724] loss: 0.440, ave_loss: 0.619
[12]  [220/1724] loss: 0.456, ave_loss: 0.606
[13]  [240/1724] loss: 0.831, ave_loss: 0.623
[14]  [260/1724] loss: 0.385, ave_loss: 0.606
[15]  [280/1724] loss: 0.741, ave_loss: 0.615
[16]  [300/1724] loss: 0.397, ave_loss: 0.601
[17]  [320/1724] loss: 0.576, ave_loss: 0.600
[18]  [340/1724] loss: 0.542, ave_loss: 0.597
[19]  [360/1724] loss: 0.498, ave_loss: 0.591
[20]  [380/1724] loss: 0.649, ave_loss: 0.594
[21]  [400/1724] loss: 0.547, ave_loss: 0.592
[22]  [420/1724] loss: 0.823, ave_loss: 0.603
[23]  [440/1724] loss: 0.449, ave_loss: 0.596
[24]  [460/1724] loss: 0.742, ave_loss: 0.602
[25]  [480/1724] loss: 0.676, ave_loss: 0.605
[26]  [500/1724] loss: 0.539, ave_loss: 0.602
[27]  [520/1724] loss: 0.610, ave_loss: 0.603
[28]  [540/1724] loss: 0.531, ave_loss: 0.600
[29]  [560/1724] loss: 0.643, ave_loss: 0.602
[30]  [580/1724] loss: 0.686, ave_loss: 0.604
[31]  [600/1724] loss: 0.526, ave_loss: 0.602
[32]  [620/1724] loss: 0.528, ave_loss: 0.600
[33]  [640/1724] loss: 0.421, ave_loss: 0.594
[34]  [660/1724] loss: 0.495, ave_loss: 0.591
[35]  [680/1724] loss: 0.494, ave_loss: 0.589
[36]  [700/1724] loss: 0.668, ave_loss: 0.591
[37]  [720/1724] loss: 0.611, ave_loss: 0.591
[38]  [740/1724] loss: 0.459, ave_loss: 0.588
[39]  [760/1724] loss: 0.470, ave_loss: 0.585
[40]  [780/1724] loss: 0.490, ave_loss: 0.582
[41]  [800/1724] loss: 0.513, ave_loss: 0.581
[42]  [820/1724] loss: 0.487, ave_loss: 0.578
[43]  [840/1724] loss: 0.535, ave_loss: 0.577
[44]  [860/1724] loss: 0.454, ave_loss: 0.575
[45]  [880/1724] loss: 0.505, ave_loss: 0.573
[46]  [900/1724] loss: 0.645, ave_loss: 0.575
[47]  [920/1724] loss: 0.406, ave_loss: 0.571
[48]  [940/1724] loss: 0.594, ave_loss: 0.572
[49]  [960/1724] loss: 0.448, ave_loss: 0.569
[50]  [980/1724] loss: 0.593, ave_loss: 0.570
[51]  [1000/1724] loss: 0.626, ave_loss: 0.571
[52]  [1020/1724] loss: 0.630, ave_loss: 0.572
[53]  [1040/1724] loss: 0.508, ave_loss: 0.571
[54]  [1060/1724] loss: 0.536, ave_loss: 0.570
[55]  [1080/1724] loss: 0.444, ave_loss: 0.568
[56]  [1100/1724] loss: 0.495, ave_loss: 0.566
[57]  [1120/1724] loss: 0.612, ave_loss: 0.567
[58]  [1140/1724] loss: 0.418, ave_loss: 0.565
[59]  [1160/1724] loss: 0.581, ave_loss: 0.565
[60]  [1180/1724] loss: 0.420, ave_loss: 0.562
[61]  [1200/1724] loss: 0.543, ave_loss: 0.562
[62]  [1220/1724] loss: 0.472, ave_loss: 0.561
[63]  [1240/1724] loss: 0.338, ave_loss: 0.557
[64]  [1260/1724] loss: 0.545, ave_loss: 0.557
[65]  [1280/1724] loss: 0.555, ave_loss: 0.557
[66]  [1300/1724] loss: 0.835, ave_loss: 0.561
[67]  [1320/1724] loss: 0.528, ave_loss: 0.561
[68]  [1340/1724] loss: 0.507, ave_loss: 0.560
[69]  [1360/1724] loss: 0.504, ave_loss: 0.559
[70]  [1380/1724] loss: 0.488, ave_loss: 0.558
[71]  [1400/1724] loss: 0.466, ave_loss: 0.557
[72]  [1420/1724] loss: 0.546, ave_loss: 0.557
[73]  [1440/1724] loss: 0.604, ave_loss: 0.557
[74]  [1460/1724] loss: 0.662, ave_loss: 0.559
[75]  [1480/1724] loss: 0.518, ave_loss: 0.558
[76]  [1500/1724] loss: 0.497, ave_loss: 0.557
[77]  [1520/1724] loss: 0.601, ave_loss: 0.558
[78]  [1540/1724] loss: 0.460, ave_loss: 0.557
[79]  [1560/1724] loss: 0.704, ave_loss: 0.558
[80]  [1580/1724] loss: 0.379, ave_loss: 0.556
[81]  [1600/1724] loss: 0.489, ave_loss: 0.555
[82]  [1620/1724] loss: 0.630, ave_loss: 0.556
[83]  [1640/1724] loss: 0.561, ave_loss: 0.556
[84]  [1660/1724] loss: 0.530, ave_loss: 0.556
[85]  [1680/1724] loss: 0.436, ave_loss: 0.555
[86]  [1700/1724] loss: 0.496, ave_loss: 0.554
[87]  [1720/1724] loss: 0.543, ave_loss: 0.554
[88]  [1740/1724] loss: 0.523, ave_loss: 0.553

Finished Training finishing at 2021-08-29 20:07:31.251405
printing_out epoch  16.334106728538284 learning rate: 0.0005153561248318907
0.00031655938251236066
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.535e-01
Validation Loss: 9.568e+04
Validation ROC: 0.6350
Saving model
12.665893271461716 epochs left to go

Training Epoch 16.334106728538284/30 starting at 2021-08-29 20:09:27.545498
[1]  [0/1724] loss: 0.594, ave_loss: 0.594
[2]  [20/1724] loss: 0.427, ave_loss: 0.511
[3]  [40/1724] loss: 0.540, ave_loss: 0.520
[4]  [60/1724] loss: 0.392, ave_loss: 0.488
[5]  [80/1724] loss: 0.492, ave_loss: 0.489
[6]  [100/1724] loss: 0.602, ave_loss: 0.508
[7]  [120/1724] loss: 0.703, ave_loss: 0.536
[8]  [140/1724] loss: 0.526, ave_loss: 0.535
[9]  [160/1724] loss: 0.575, ave_loss: 0.539
[10]  [180/1724] loss: 0.843, ave_loss: 0.569
[11]  [200/1724] loss: 0.508, ave_loss: 0.564
[12]  [220/1724] loss: 0.467, ave_loss: 0.556
[13]  [240/1724] loss: 0.437, ave_loss: 0.547
[14]  [260/1724] loss: 0.488, ave_loss: 0.542
[15]  [280/1724] loss: 0.707, ave_loss: 0.553
[16]  [300/1724] loss: 0.664, ave_loss: 0.560
[17]  [320/1724] loss: 0.625, ave_loss: 0.564
[18]  [340/1724] loss: 0.810, ave_loss: 0.578
[19]  [360/1724] loss: 0.522, ave_loss: 0.575
[20]  [380/1724] loss: 0.687, ave_loss: 0.580
[21]  [400/1724] loss: 0.542, ave_loss: 0.579
[22]  [420/1724] loss: 0.616, ave_loss: 0.580
[23]  [440/1724] loss: 0.438, ave_loss: 0.574
[24]  [460/1724] loss: 0.519, ave_loss: 0.572
[25]  [480/1724] loss: 0.597, ave_loss: 0.573
[26]  [500/1724] loss: 0.789, ave_loss: 0.581
[27]  [520/1724] loss: 0.700, ave_loss: 0.586
[28]  [540/1724] loss: 0.506, ave_loss: 0.583
[29]  [560/1724] loss: 0.535, ave_loss: 0.581
[30]  [580/1724] loss: 0.679, ave_loss: 0.584
[31]  [600/1724] loss: 0.460, ave_loss: 0.580
[32]  [620/1724] loss: 0.468, ave_loss: 0.577
[33]  [640/1724] loss: 0.412, ave_loss: 0.572
[34]  [660/1724] loss: 0.438, ave_loss: 0.568
[35]  [680/1724] loss: 0.854, ave_loss: 0.576
[36]  [700/1724] loss: 0.404, ave_loss: 0.571
[37]  [720/1724] loss: 0.592, ave_loss: 0.572
[38]  [740/1724] loss: 0.401, ave_loss: 0.567
[39]  [760/1724] loss: 0.781, ave_loss: 0.573
[40]  [780/1724] loss: 0.531, ave_loss: 0.572
[41]  [800/1724] loss: 0.625, ave_loss: 0.573
[42]  [820/1724] loss: 0.416, ave_loss: 0.569
[43]  [840/1724] loss: 0.511, ave_loss: 0.568
[44]  [860/1724] loss: 0.491, ave_loss: 0.566
[45]  [880/1724] loss: 0.602, ave_loss: 0.567
[46]  [900/1724] loss: 0.337, ave_loss: 0.562
[47]  [920/1724] loss: 0.586, ave_loss: 0.563
[48]  [940/1724] loss: 0.448, ave_loss: 0.560
[49]  [960/1724] loss: 0.674, ave_loss: 0.562
[50]  [980/1724] loss: 0.736, ave_loss: 0.566
[51]  [1000/1724] loss: 0.407, ave_loss: 0.563
[52]  [1020/1724] loss: 0.649, ave_loss: 0.564
[53]  [1040/1724] loss: 0.437, ave_loss: 0.562
[54]  [1060/1724] loss: 0.456, ave_loss: 0.560
[55]  [1080/1724] loss: 0.432, ave_loss: 0.558
[56]  [1100/1724] loss: 0.433, ave_loss: 0.556
[57]  [1120/1724] loss: 0.520, ave_loss: 0.555
[58]  [1140/1724] loss: 0.701, ave_loss: 0.557
[59]  [1160/1724] loss: 0.629, ave_loss: 0.559
[60]  [1180/1724] loss: 0.499, ave_loss: 0.558
[61]  [1200/1724] loss: 0.592, ave_loss: 0.558
[62]  [1220/1724] loss: 0.541, ave_loss: 0.558
[63]  [1240/1724] loss: 0.375, ave_loss: 0.555
[64]  [1260/1724] loss: 0.454, ave_loss: 0.553
[65]  [1280/1724] loss: 0.697, ave_loss: 0.556
[66]  [1300/1724] loss: 0.560, ave_loss: 0.556
[67]  [1320/1724] loss: 0.779, ave_loss: 0.559
[68]  [1340/1724] loss: 0.577, ave_loss: 0.559
[69]  [1360/1724] loss: 0.447, ave_loss: 0.558
[70]  [1380/1724] loss: 0.553, ave_loss: 0.558
[71]  [1400/1724] loss: 0.589, ave_loss: 0.558
[72]  [1420/1724] loss: 0.385, ave_loss: 0.556
[73]  [1440/1724] loss: 0.641, ave_loss: 0.557
[74]  [1460/1724] loss: 0.796, ave_loss: 0.560
[75]  [1480/1724] loss: 0.573, ave_loss: 0.560
[76]  [1500/1724] loss: 0.513, ave_loss: 0.560
[77]  [1520/1724] loss: 0.667, ave_loss: 0.561
[78]  [1540/1724] loss: 0.575, ave_loss: 0.561
[79]  [1560/1724] loss: 0.622, ave_loss: 0.562
[80]  [1580/1724] loss: 0.599, ave_loss: 0.562
[81]  [1600/1724] loss: 0.551, ave_loss: 0.562
[82]  [1620/1724] loss: 0.604, ave_loss: 0.563
[83]  [1640/1724] loss: 0.640, ave_loss: 0.564
[84]  [1660/1724] loss: 0.427, ave_loss: 0.562
[85]  [1680/1724] loss: 0.533, ave_loss: 0.562
[86]  [1700/1724] loss: 0.478, ave_loss: 0.561
[87]  [1720/1724] loss: 0.397, ave_loss: 0.559
[88]  [1740/1724] loss: 0.708, ave_loss: 0.561

Finished Training finishing at 2021-08-29 20:13:01.224604
printing_out epoch  17.354988399071924 learning rate: 0.0005153561248318907
0.00030706260103698985
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.606e-01
Validation Loss: 1.005e+05
Validation ROC: 0.6413
Saving model
11.645011600928076 epochs left to go

Training Epoch 17.354988399071924/30 starting at 2021-08-29 20:15:29.101282
[1]  [0/1724] loss: 0.503, ave_loss: 0.503
[2]  [20/1724] loss: 0.616, ave_loss: 0.560
[3]  [40/1724] loss: 0.495, ave_loss: 0.538
[4]  [60/1724] loss: 0.576, ave_loss: 0.547
[5]  [80/1724] loss: 0.626, ave_loss: 0.563
[6]  [100/1724] loss: 0.592, ave_loss: 0.568
[7]  [120/1724] loss: 0.759, ave_loss: 0.595
[8]  [140/1724] loss: 0.409, ave_loss: 0.572
[9]  [160/1724] loss: 0.616, ave_loss: 0.577
[10]  [180/1724] loss: 0.506, ave_loss: 0.570
[11]  [200/1724] loss: 0.508, ave_loss: 0.564
[12]  [220/1724] loss: 0.576, ave_loss: 0.565
[13]  [240/1724] loss: 0.699, ave_loss: 0.576
[14]  [260/1724] loss: 0.488, ave_loss: 0.569
[15]  [280/1724] loss: 0.501, ave_loss: 0.565
[16]  [300/1724] loss: 0.509, ave_loss: 0.561
[17]  [320/1724] loss: 0.519, ave_loss: 0.559
[18]  [340/1724] loss: 0.567, ave_loss: 0.559
[19]  [360/1724] loss: 0.561, ave_loss: 0.559
[20]  [380/1724] loss: 0.582, ave_loss: 0.560
[21]  [400/1724] loss: 0.441, ave_loss: 0.555
[22]  [420/1724] loss: 0.486, ave_loss: 0.552
[23]  [440/1724] loss: 0.689, ave_loss: 0.558
[24]  [460/1724] loss: 0.680, ave_loss: 0.563
[25]  [480/1724] loss: 0.441, ave_loss: 0.558
[26]  [500/1724] loss: 0.563, ave_loss: 0.558
[27]  [520/1724] loss: 0.609, ave_loss: 0.560
[28]  [540/1724] loss: 0.658, ave_loss: 0.563
[29]  [560/1724] loss: 0.406, ave_loss: 0.558
[30]  [580/1724] loss: 0.540, ave_loss: 0.557
[31]  [600/1724] loss: 0.478, ave_loss: 0.555
[32]  [620/1724] loss: 0.531, ave_loss: 0.554
[33]  [640/1724] loss: 0.468, ave_loss: 0.551
[34]  [660/1724] loss: 0.631, ave_loss: 0.554
[35]  [680/1724] loss: 0.504, ave_loss: 0.552
[36]  [700/1724] loss: 0.546, ave_loss: 0.552
[37]  [720/1724] loss: 0.454, ave_loss: 0.550
[38]  [740/1724] loss: 0.413, ave_loss: 0.546
[39]  [760/1724] loss: 0.490, ave_loss: 0.545
[40]  [780/1724] loss: 0.443, ave_loss: 0.542
[41]  [800/1724] loss: 0.561, ave_loss: 0.542
[42]  [820/1724] loss: 0.467, ave_loss: 0.541
[43]  [840/1724] loss: 0.502, ave_loss: 0.540
[44]  [860/1724] loss: 0.519, ave_loss: 0.539
[45]  [880/1724] loss: 0.459, ave_loss: 0.537
[46]  [900/1724] loss: 0.704, ave_loss: 0.541
[47]  [920/1724] loss: 0.564, ave_loss: 0.542
[48]  [940/1724] loss: 0.527, ave_loss: 0.541
[49]  [960/1724] loss: 0.572, ave_loss: 0.542
[50]  [980/1724] loss: 0.398, ave_loss: 0.539
[51]  [1000/1724] loss: 0.457, ave_loss: 0.537
[52]  [1020/1724] loss: 0.540, ave_loss: 0.537
[53]  [1040/1724] loss: 0.575, ave_loss: 0.538
[54]  [1060/1724] loss: 0.532, ave_loss: 0.538
[55]  [1080/1724] loss: 0.586, ave_loss: 0.539
[56]  [1100/1724] loss: 0.617, ave_loss: 0.540
[57]  [1120/1724] loss: 0.510, ave_loss: 0.540
[58]  [1140/1724] loss: 0.684, ave_loss: 0.542
[59]  [1160/1724] loss: 0.615, ave_loss: 0.544
[60]  [1180/1724] loss: 0.553, ave_loss: 0.544
[61]  [1200/1724] loss: 0.493, ave_loss: 0.543
[62]  [1220/1724] loss: 0.389, ave_loss: 0.540
[63]  [1240/1724] loss: 0.353, ave_loss: 0.537
[64]  [1260/1724] loss: 0.688, ave_loss: 0.540
[65]  [1280/1724] loss: 0.564, ave_loss: 0.540
[66]  [1300/1724] loss: 0.433, ave_loss: 0.539
[67]  [1320/1724] loss: 0.377, ave_loss: 0.536
[68]  [1340/1724] loss: 0.432, ave_loss: 0.535
[69]  [1360/1724] loss: 0.387, ave_loss: 0.532
[70]  [1380/1724] loss: 0.507, ave_loss: 0.532
[71]  [1400/1724] loss: 0.426, ave_loss: 0.531
[72]  [1420/1724] loss: 0.459, ave_loss: 0.530
[73]  [1440/1724] loss: 0.407, ave_loss: 0.528
[74]  [1460/1724] loss: 0.508, ave_loss: 0.528
[75]  [1480/1724] loss: 0.666, ave_loss: 0.529
[76]  [1500/1724] loss: 0.402, ave_loss: 0.528
[77]  [1520/1724] loss: 0.453, ave_loss: 0.527
[78]  [1540/1724] loss: 0.537, ave_loss: 0.527
[79]  [1560/1724] loss: 0.732, ave_loss: 0.530
[80]  [1580/1724] loss: 0.765, ave_loss: 0.532
[81]  [1600/1724] loss: 0.319, ave_loss: 0.530
[82]  [1620/1724] loss: 0.791, ave_loss: 0.533
[83]  [1640/1724] loss: 0.542, ave_loss: 0.533
[84]  [1660/1724] loss: 0.834, ave_loss: 0.537
[85]  [1680/1724] loss: 0.505, ave_loss: 0.536
[86]  [1700/1724] loss: 0.503, ave_loss: 0.536
[87]  [1720/1724] loss: 0.488, ave_loss: 0.535
[88]  [1740/1724] loss: 0.431, ave_loss: 0.534

Finished Training finishing at 2021-08-29 20:19:16.870877
printing_out epoch  18.37587006960557 learning rate: 0.0005153561248318907
0.00029785072300588016
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.342e-01
Validation Loss: 9.610e+04
Validation ROC: 0.6533
Saving model
10.624129930394432 epochs left to go

Training Epoch 18.37587006960557/30 starting at 2021-08-29 20:22:25.430072
[1]  [0/1724] loss: 0.690, ave_loss: 0.690
[2]  [20/1724] loss: 0.597, ave_loss: 0.643
[3]  [40/1724] loss: 0.561, ave_loss: 0.616
[4]  [60/1724] loss: 0.500, ave_loss: 0.587
[5]  [80/1724] loss: 0.536, ave_loss: 0.577
[6]  [100/1724] loss: 0.488, ave_loss: 0.562
[7]  [120/1724] loss: 0.497, ave_loss: 0.553
[8]  [140/1724] loss: 0.445, ave_loss: 0.539
[9]  [160/1724] loss: 0.432, ave_loss: 0.527
[10]  [180/1724] loss: 0.511, ave_loss: 0.526
[11]  [200/1724] loss: 0.501, ave_loss: 0.524
[12]  [220/1724] loss: 0.608, ave_loss: 0.531
[13]  [240/1724] loss: 0.589, ave_loss: 0.535
[14]  [260/1724] loss: 0.426, ave_loss: 0.527
[15]  [280/1724] loss: 0.541, ave_loss: 0.528
[16]  [300/1724] loss: 0.406, ave_loss: 0.521
[17]  [320/1724] loss: 0.594, ave_loss: 0.525
[18]  [340/1724] loss: 0.503, ave_loss: 0.524
[19]  [360/1724] loss: 0.448, ave_loss: 0.520
[20]  [380/1724] loss: 0.676, ave_loss: 0.527
[21]  [400/1724] loss: 0.853, ave_loss: 0.543
[22]  [420/1724] loss: 0.544, ave_loss: 0.543
[23]  [440/1724] loss: 0.560, ave_loss: 0.544
[24]  [460/1724] loss: 0.429, ave_loss: 0.539
[25]  [480/1724] loss: 0.519, ave_loss: 0.538
[26]  [500/1724] loss: 0.545, ave_loss: 0.538
[27]  [520/1724] loss: 0.568, ave_loss: 0.539
[28]  [540/1724] loss: 0.508, ave_loss: 0.538
[29]  [560/1724] loss: 0.616, ave_loss: 0.541
[30]  [580/1724] loss: 0.804, ave_loss: 0.550
[31]  [600/1724] loss: 0.370, ave_loss: 0.544
[32]  [620/1724] loss: 0.511, ave_loss: 0.543
[33]  [640/1724] loss: 0.620, ave_loss: 0.545
[34]  [660/1724] loss: 0.690, ave_loss: 0.550
[35]  [680/1724] loss: 0.594, ave_loss: 0.551
[36]  [700/1724] loss: 0.543, ave_loss: 0.551
[37]  [720/1724] loss: 0.659, ave_loss: 0.554
[38]  [740/1724] loss: 0.636, ave_loss: 0.556
[39]  [760/1724] loss: 0.472, ave_loss: 0.554
[40]  [780/1724] loss: 0.585, ave_loss: 0.554
[41]  [800/1724] loss: 0.554, ave_loss: 0.554
[42]  [820/1724] loss: 0.492, ave_loss: 0.553
[43]  [840/1724] loss: 0.406, ave_loss: 0.549
[44]  [860/1724] loss: 0.490, ave_loss: 0.548
[45]  [880/1724] loss: 0.552, ave_loss: 0.548
[46]  [900/1724] loss: 0.527, ave_loss: 0.548
[47]  [920/1724] loss: 0.690, ave_loss: 0.551
[48]  [940/1724] loss: 0.502, ave_loss: 0.550
[49]  [960/1724] loss: 0.535, ave_loss: 0.549
[50]  [980/1724] loss: 0.450, ave_loss: 0.547
[51]  [1000/1724] loss: 0.508, ave_loss: 0.547
[52]  [1020/1724] loss: 0.463, ave_loss: 0.545
[53]  [1040/1724] loss: 0.564, ave_loss: 0.545
[54]  [1060/1724] loss: 0.386, ave_loss: 0.542
[55]  [1080/1724] loss: 0.620, ave_loss: 0.544
[56]  [1100/1724] loss: 0.399, ave_loss: 0.541
[57]  [1120/1724] loss: 0.458, ave_loss: 0.540
[58]  [1140/1724] loss: 0.552, ave_loss: 0.540
[59]  [1160/1724] loss: 0.793, ave_loss: 0.544
[60]  [1180/1724] loss: 0.545, ave_loss: 0.544
[61]  [1200/1724] loss: 0.539, ave_loss: 0.544
[62]  [1220/1724] loss: 0.593, ave_loss: 0.545
[63]  [1240/1724] loss: 0.461, ave_loss: 0.544
[64]  [1260/1724] loss: 0.599, ave_loss: 0.545
[65]  [1280/1724] loss: 0.439, ave_loss: 0.543
[66]  [1300/1724] loss: 0.754, ave_loss: 0.546
[67]  [1320/1724] loss: 0.633, ave_loss: 0.547
[68]  [1340/1724] loss: 0.465, ave_loss: 0.546
[69]  [1360/1724] loss: 0.469, ave_loss: 0.545
[70]  [1380/1724] loss: 0.379, ave_loss: 0.543
[71]  [1400/1724] loss: 0.498, ave_loss: 0.542
[72]  [1420/1724] loss: 0.396, ave_loss: 0.540
[73]  [1440/1724] loss: 0.648, ave_loss: 0.541
[74]  [1460/1724] loss: 0.457, ave_loss: 0.540
[75]  [1480/1724] loss: 0.624, ave_loss: 0.541
[76]  [1500/1724] loss: 0.546, ave_loss: 0.542
[77]  [1520/1724] loss: 0.474, ave_loss: 0.541
[78]  [1540/1724] loss: 0.809, ave_loss: 0.544
[79]  [1560/1724] loss: 0.435, ave_loss: 0.543
[80]  [1580/1724] loss: 0.611, ave_loss: 0.544
[81]  [1600/1724] loss: 0.564, ave_loss: 0.544
[82]  [1620/1724] loss: 0.715, ave_loss: 0.546
[83]  [1640/1724] loss: 0.392, ave_loss: 0.544
[84]  [1660/1724] loss: 0.673, ave_loss: 0.546
[85]  [1680/1724] loss: 0.294, ave_loss: 0.543
[86]  [1700/1724] loss: 0.525, ave_loss: 0.542
[87]  [1720/1724] loss: 0.431, ave_loss: 0.541
[88]  [1740/1724] loss: 0.492, ave_loss: 0.541

Finished Training finishing at 2021-08-29 20:26:08.589206
printing_out epoch  19.396751740139212 learning rate: 0.0005153561248318907
0.00028891520131570374
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.406e-01
Validation Loss: 1.014e+05
Validation ROC: 0.6532
No improvement, still saving model
9.603248259860788 epochs left to go

Training Epoch 19.396751740139212/30 starting at 2021-08-29 20:28:48.086768
[1]  [0/1724] loss: 0.528, ave_loss: 0.528
[2]  [20/1724] loss: 0.508, ave_loss: 0.518
[3]  [40/1724] loss: 0.380, ave_loss: 0.472
[4]  [60/1724] loss: 0.559, ave_loss: 0.494
[5]  [80/1724] loss: 0.432, ave_loss: 0.481
[6]  [100/1724] loss: 0.305, ave_loss: 0.452
[7]  [120/1724] loss: 0.490, ave_loss: 0.457
[8]  [140/1724] loss: 0.565, ave_loss: 0.471
[9]  [160/1724] loss: 0.537, ave_loss: 0.478
[10]  [180/1724] loss: 0.679, ave_loss: 0.498
[11]  [200/1724] loss: 0.495, ave_loss: 0.498
[12]  [220/1724] loss: 0.637, ave_loss: 0.510
[13]  [240/1724] loss: 0.473, ave_loss: 0.507
[14]  [260/1724] loss: 0.483, ave_loss: 0.505
[15]  [280/1724] loss: 0.526, ave_loss: 0.506
[16]  [300/1724] loss: 0.495, ave_loss: 0.506
[17]  [320/1724] loss: 0.288, ave_loss: 0.493
[18]  [340/1724] loss: 0.461, ave_loss: 0.491
[19]  [360/1724] loss: 0.576, ave_loss: 0.496
[20]  [380/1724] loss: 0.356, ave_loss: 0.489
[21]  [400/1724] loss: 0.448, ave_loss: 0.487
[22]  [420/1724] loss: 0.433, ave_loss: 0.484
[23]  [440/1724] loss: 0.682, ave_loss: 0.493
[24]  [460/1724] loss: 0.861, ave_loss: 0.508
[25]  [480/1724] loss: 0.576, ave_loss: 0.511
[26]  [500/1724] loss: 0.456, ave_loss: 0.509
[27]  [520/1724] loss: 0.439, ave_loss: 0.506
[28]  [540/1724] loss: 0.441, ave_loss: 0.504
[29]  [560/1724] loss: 0.744, ave_loss: 0.512
[30]  [580/1724] loss: 0.411, ave_loss: 0.509
[31]  [600/1724] loss: 0.710, ave_loss: 0.515
[32]  [620/1724] loss: 0.619, ave_loss: 0.519
[33]  [640/1724] loss: 0.636, ave_loss: 0.522
[34]  [660/1724] loss: 0.687, ave_loss: 0.527
[35]  [680/1724] loss: 0.563, ave_loss: 0.528
[36]  [700/1724] loss: 0.694, ave_loss: 0.533
[37]  [720/1724] loss: 0.509, ave_loss: 0.532
[38]  [740/1724] loss: 0.390, ave_loss: 0.528
[39]  [760/1724] loss: 0.455, ave_loss: 0.526
[40]  [780/1724] loss: 0.656, ave_loss: 0.530
[41]  [800/1724] loss: 0.595, ave_loss: 0.531
[42]  [820/1724] loss: 0.531, ave_loss: 0.531
[43]  [840/1724] loss: 0.514, ave_loss: 0.531
[44]  [860/1724] loss: 0.799, ave_loss: 0.537
[45]  [880/1724] loss: 0.763, ave_loss: 0.542
[46]  [900/1724] loss: 0.666, ave_loss: 0.545
[47]  [920/1724] loss: 0.411, ave_loss: 0.542
[48]  [940/1724] loss: 0.480, ave_loss: 0.540
[49]  [960/1724] loss: 0.368, ave_loss: 0.537
[50]  [980/1724] loss: 0.459, ave_loss: 0.535
[51]  [1000/1724] loss: 0.627, ave_loss: 0.537
[52]  [1020/1724] loss: 0.606, ave_loss: 0.538
[53]  [1040/1724] loss: 0.636, ave_loss: 0.540
[54]  [1060/1724] loss: 0.400, ave_loss: 0.538
[55]  [1080/1724] loss: 0.634, ave_loss: 0.539
[56]  [1100/1724] loss: 0.666, ave_loss: 0.542
[57]  [1120/1724] loss: 0.614, ave_loss: 0.543
[58]  [1140/1724] loss: 0.612, ave_loss: 0.544
[59]  [1160/1724] loss: 0.535, ave_loss: 0.544
[60]  [1180/1724] loss: 0.604, ave_loss: 0.545
[61]  [1200/1724] loss: 0.552, ave_loss: 0.545
[62]  [1220/1724] loss: 0.353, ave_loss: 0.542
[63]  [1240/1724] loss: 0.572, ave_loss: 0.542
[64]  [1260/1724] loss: 0.672, ave_loss: 0.545
[65]  [1280/1724] loss: 0.480, ave_loss: 0.544
[66]  [1300/1724] loss: 0.426, ave_loss: 0.542
[67]  [1320/1724] loss: 0.581, ave_loss: 0.542
[68]  [1340/1724] loss: 0.598, ave_loss: 0.543
[69]  [1360/1724] loss: 0.579, ave_loss: 0.544
[70]  [1380/1724] loss: 0.537, ave_loss: 0.544
[71]  [1400/1724] loss: 0.501, ave_loss: 0.543
[72]  [1420/1724] loss: 0.402, ave_loss: 0.541
[73]  [1440/1724] loss: 0.394, ave_loss: 0.539
[74]  [1460/1724] loss: 0.304, ave_loss: 0.536
[75]  [1480/1724] loss: 0.441, ave_loss: 0.535
[76]  [1500/1724] loss: 0.526, ave_loss: 0.534
[77]  [1520/1724] loss: 0.418, ave_loss: 0.533
[78]  [1540/1724] loss: 0.703, ave_loss: 0.535
[79]  [1560/1724] loss: 0.636, ave_loss: 0.536
[80]  [1580/1724] loss: 0.638, ave_loss: 0.538
[81]  [1600/1724] loss: 0.366, ave_loss: 0.536
[82]  [1620/1724] loss: 0.676, ave_loss: 0.537
[83]  [1640/1724] loss: 0.631, ave_loss: 0.538
[84]  [1660/1724] loss: 0.480, ave_loss: 0.538
[85]  [1680/1724] loss: 0.621, ave_loss: 0.539
[86]  [1700/1724] loss: 0.474, ave_loss: 0.538
[87]  [1720/1724] loss: 0.756, ave_loss: 0.540
[88]  [1740/1724] loss: 0.477, ave_loss: 0.540

Finished Training finishing at 2021-08-29 20:33:00.626134
printing_out epoch  20.417633410672853 learning rate: 0.0005153561248318907
0.0002802477452762326
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.397e-01
Validation Loss: 1.056e+05
Validation ROC: 0.6291
No improvement, still saving model
8.582366589327147 epochs left to go

Training Epoch 20.417633410672853/30 starting at 2021-08-29 20:35:13.666105
[1]  [0/1724] loss: 0.573, ave_loss: 0.573
[2]  [20/1724] loss: 0.550, ave_loss: 0.562
[3]  [40/1724] loss: 0.565, ave_loss: 0.563
[4]  [60/1724] loss: 0.707, ave_loss: 0.599
[5]  [80/1724] loss: 0.456, ave_loss: 0.570
[6]  [100/1724] loss: 0.411, ave_loss: 0.544
[7]  [120/1724] loss: 0.645, ave_loss: 0.558
[8]  [140/1724] loss: 0.409, ave_loss: 0.539
[9]  [160/1724] loss: 0.584, ave_loss: 0.544
[10]  [180/1724] loss: 0.658, ave_loss: 0.556
[11]  [200/1724] loss: 0.452, ave_loss: 0.546
[12]  [220/1724] loss: 0.833, ave_loss: 0.570
[13]  [240/1724] loss: 0.854, ave_loss: 0.592
[14]  [260/1724] loss: 0.419, ave_loss: 0.580
[15]  [280/1724] loss: 0.616, ave_loss: 0.582
[16]  [300/1724] loss: 0.511, ave_loss: 0.578
[17]  [320/1724] loss: 0.621, ave_loss: 0.580
[18]  [340/1724] loss: 0.489, ave_loss: 0.575
[19]  [360/1724] loss: 0.637, ave_loss: 0.578
[20]  [380/1724] loss: 0.375, ave_loss: 0.568
[21]  [400/1724] loss: 0.442, ave_loss: 0.562
[22]  [420/1724] loss: 0.534, ave_loss: 0.561
[23]  [440/1724] loss: 0.556, ave_loss: 0.561
[24]  [460/1724] loss: 0.702, ave_loss: 0.567
[25]  [480/1724] loss: 0.471, ave_loss: 0.563
[26]  [500/1724] loss: 0.590, ave_loss: 0.564
[27]  [520/1724] loss: 0.451, ave_loss: 0.560
[28]  [540/1724] loss: 0.591, ave_loss: 0.561
[29]  [560/1724] loss: 0.563, ave_loss: 0.561
[30]  [580/1724] loss: 0.470, ave_loss: 0.558
[31]  [600/1724] loss: 0.591, ave_loss: 0.559
[32]  [620/1724] loss: 0.562, ave_loss: 0.559
[33]  [640/1724] loss: 0.514, ave_loss: 0.558
[34]  [660/1724] loss: 0.499, ave_loss: 0.556
[35]  [680/1724] loss: 0.536, ave_loss: 0.555
[36]  [700/1724] loss: 0.588, ave_loss: 0.556
[37]  [720/1724] loss: 0.555, ave_loss: 0.556
[38]  [740/1724] loss: 0.510, ave_loss: 0.555
[39]  [760/1724] loss: 0.624, ave_loss: 0.557
[40]  [780/1724] loss: 0.455, ave_loss: 0.554
[41]  [800/1724] loss: 0.506, ave_loss: 0.553
[42]  [820/1724] loss: 0.567, ave_loss: 0.553
[43]  [840/1724] loss: 0.405, ave_loss: 0.550
[44]  [860/1724] loss: 0.484, ave_loss: 0.548
[45]  [880/1724] loss: 0.504, ave_loss: 0.547
[46]  [900/1724] loss: 0.626, ave_loss: 0.549
[47]  [920/1724] loss: 0.507, ave_loss: 0.548
[48]  [940/1724] loss: 0.387, ave_loss: 0.545
[49]  [960/1724] loss: 0.498, ave_loss: 0.544
[50]  [980/1724] loss: 0.814, ave_loss: 0.549
[51]  [1000/1724] loss: 0.564, ave_loss: 0.550
[52]  [1020/1724] loss: 0.747, ave_loss: 0.553
[53]  [1040/1724] loss: 0.495, ave_loss: 0.552
[54]  [1060/1724] loss: 0.608, ave_loss: 0.553
[55]  [1080/1724] loss: 0.661, ave_loss: 0.555
[56]  [1100/1724] loss: 0.542, ave_loss: 0.555
[57]  [1120/1724] loss: 0.546, ave_loss: 0.555
[58]  [1140/1724] loss: 0.504, ave_loss: 0.554
[59]  [1160/1724] loss: 0.524, ave_loss: 0.553
[60]  [1180/1724] loss: 0.549, ave_loss: 0.553
[61]  [1200/1724] loss: 0.447, ave_loss: 0.552
[62]  [1220/1724] loss: 0.514, ave_loss: 0.551
[63]  [1240/1724] loss: 0.639, ave_loss: 0.552
[64]  [1260/1724] loss: 0.443, ave_loss: 0.551
[65]  [1280/1724] loss: 0.397, ave_loss: 0.548
[66]  [1300/1724] loss: 0.532, ave_loss: 0.548
[67]  [1320/1724] loss: 0.518, ave_loss: 0.548
[68]  [1340/1724] loss: 0.566, ave_loss: 0.548
[69]  [1360/1724] loss: 0.472, ave_loss: 0.547
[70]  [1380/1724] loss: 0.448, ave_loss: 0.545
[71]  [1400/1724] loss: 0.648, ave_loss: 0.547
[72]  [1420/1724] loss: 0.686, ave_loss: 0.549
[73]  [1440/1724] loss: 0.486, ave_loss: 0.548
[74]  [1460/1724] loss: 0.560, ave_loss: 0.548
[75]  [1480/1724] loss: 0.575, ave_loss: 0.548
[76]  [1500/1724] loss: 0.662, ave_loss: 0.550
[77]  [1520/1724] loss: 0.381, ave_loss: 0.548
[78]  [1540/1724] loss: 0.543, ave_loss: 0.548
[79]  [1560/1724] loss: 0.693, ave_loss: 0.550
[80]  [1580/1724] loss: 0.493, ave_loss: 0.549
[81]  [1600/1724] loss: 0.607, ave_loss: 0.550
[82]  [1620/1724] loss: 0.546, ave_loss: 0.550
[83]  [1640/1724] loss: 0.684, ave_loss: 0.551
[84]  [1660/1724] loss: 0.397, ave_loss: 0.549
[85]  [1680/1724] loss: 0.678, ave_loss: 0.551
[86]  [1700/1724] loss: 0.480, ave_loss: 0.550
[87]  [1720/1724] loss: 0.626, ave_loss: 0.551
[88]  [1740/1724] loss: 0.553, ave_loss: 0.551

Finished Training finishing at 2021-08-29 20:39:36.531038
printing_out epoch  21.438515081206496 learning rate: 0.0005153561248318907
0.00027184031291794565
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.509e-01
Validation Loss: 1.077e+05
Validation ROC: 0.6374
No improvement, still saving model
7.561484918793504 epochs left to go

Training Epoch 21.438515081206496/30 starting at 2021-08-29 20:42:21.593625
[1]  [0/1724] loss: 0.434, ave_loss: 0.434
[2]  [20/1724] loss: 0.455, ave_loss: 0.444
[3]  [40/1724] loss: 0.473, ave_loss: 0.454
[4]  [60/1724] loss: 0.496, ave_loss: 0.465
[5]  [80/1724] loss: 0.599, ave_loss: 0.491
[6]  [100/1724] loss: 0.448, ave_loss: 0.484
[7]  [120/1724] loss: 0.479, ave_loss: 0.484
[8]  [140/1724] loss: 0.572, ave_loss: 0.495
[9]  [160/1724] loss: 0.725, ave_loss: 0.520
[10]  [180/1724] loss: 0.579, ave_loss: 0.526
[11]  [200/1724] loss: 0.487, ave_loss: 0.523
[12]  [220/1724] loss: 0.536, ave_loss: 0.524
[13]  [240/1724] loss: 0.526, ave_loss: 0.524
[14]  [260/1724] loss: 0.477, ave_loss: 0.520
[15]  [280/1724] loss: 0.526, ave_loss: 0.521
[16]  [300/1724] loss: 0.404, ave_loss: 0.514
[17]  [320/1724] loss: 0.504, ave_loss: 0.513
[18]  [340/1724] loss: 0.479, ave_loss: 0.511
[19]  [360/1724] loss: 0.438, ave_loss: 0.507
[20]  [380/1724] loss: 0.593, ave_loss: 0.512
[21]  [400/1724] loss: 0.639, ave_loss: 0.518
[22]  [420/1724] loss: 0.509, ave_loss: 0.517
[23]  [440/1724] loss: 0.445, ave_loss: 0.514
[24]  [460/1724] loss: 0.540, ave_loss: 0.515
[25]  [480/1724] loss: 0.401, ave_loss: 0.511
[26]  [500/1724] loss: 0.536, ave_loss: 0.512
[27]  [520/1724] loss: 0.536, ave_loss: 0.513
[28]  [540/1724] loss: 0.549, ave_loss: 0.514
[29]  [560/1724] loss: 0.646, ave_loss: 0.518
[30]  [580/1724] loss: 0.355, ave_loss: 0.513
[31]  [600/1724] loss: 0.524, ave_loss: 0.513
[32]  [620/1724] loss: 0.444, ave_loss: 0.511
[33]  [640/1724] loss: 0.615, ave_loss: 0.514
[34]  [660/1724] loss: 0.545, ave_loss: 0.515
[35]  [680/1724] loss: 0.617, ave_loss: 0.518
[36]  [700/1724] loss: 0.486, ave_loss: 0.517
[37]  [720/1724] loss: 0.604, ave_loss: 0.520
[38]  [740/1724] loss: 0.531, ave_loss: 0.520
[39]  [760/1724] loss: 0.584, ave_loss: 0.521
[40]  [780/1724] loss: 0.408, ave_loss: 0.519
[41]  [800/1724] loss: 0.470, ave_loss: 0.517
[42]  [820/1724] loss: 0.414, ave_loss: 0.515
[43]  [840/1724] loss: 0.599, ave_loss: 0.517
[44]  [860/1724] loss: 0.569, ave_loss: 0.518
[45]  [880/1724] loss: 0.572, ave_loss: 0.519
[46]  [900/1724] loss: 0.576, ave_loss: 0.521
[47]  [920/1724] loss: 0.528, ave_loss: 0.521
[48]  [940/1724] loss: 0.464, ave_loss: 0.520
[49]  [960/1724] loss: 0.446, ave_loss: 0.518
[50]  [980/1724] loss: 0.527, ave_loss: 0.518
[51]  [1000/1724] loss: 0.718, ave_loss: 0.522
[52]  [1020/1724] loss: 0.420, ave_loss: 0.520
[53]  [1040/1724] loss: 0.594, ave_loss: 0.522
[54]  [1060/1724] loss: 0.344, ave_loss: 0.518
[55]  [1080/1724] loss: 0.651, ave_loss: 0.521
[56]  [1100/1724] loss: 0.460, ave_loss: 0.520
[57]  [1120/1724] loss: 0.475, ave_loss: 0.519
[58]  [1140/1724] loss: 0.381, ave_loss: 0.516
[59]  [1160/1724] loss: 0.510, ave_loss: 0.516
[60]  [1180/1724] loss: 0.424, ave_loss: 0.515
[61]  [1200/1724] loss: 0.393, ave_loss: 0.513
[62]  [1220/1724] loss: 0.605, ave_loss: 0.514
[63]  [1240/1724] loss: 0.568, ave_loss: 0.515
[64]  [1260/1724] loss: 0.451, ave_loss: 0.514
[65]  [1280/1724] loss: 0.516, ave_loss: 0.514
[66]  [1300/1724] loss: 0.443, ave_loss: 0.513
[67]  [1320/1724] loss: 0.531, ave_loss: 0.513
[68]  [1340/1724] loss: 0.483, ave_loss: 0.513
[69]  [1360/1724] loss: 0.462, ave_loss: 0.512
[70]  [1380/1724] loss: 0.477, ave_loss: 0.512
[71]  [1400/1724] loss: 0.408, ave_loss: 0.510
[72]  [1420/1724] loss: 0.544, ave_loss: 0.511
[73]  [1440/1724] loss: 0.531, ave_loss: 0.511
[74]  [1460/1724] loss: 0.638, ave_loss: 0.513
[75]  [1480/1724] loss: 0.583, ave_loss: 0.514
[76]  [1500/1724] loss: 0.469, ave_loss: 0.513
[77]  [1520/1724] loss: 0.618, ave_loss: 0.514
[78]  [1540/1724] loss: 0.548, ave_loss: 0.515
[79]  [1560/1724] loss: 0.370, ave_loss: 0.513
[80]  [1580/1724] loss: 0.568, ave_loss: 0.514
[81]  [1600/1724] loss: 0.352, ave_loss: 0.512
[82]  [1620/1724] loss: 0.619, ave_loss: 0.513
[83]  [1640/1724] loss: 0.524, ave_loss: 0.513
[84]  [1660/1724] loss: 0.730, ave_loss: 0.516
[85]  [1680/1724] loss: 0.396, ave_loss: 0.514
[86]  [1700/1724] loss: 0.436, ave_loss: 0.513
[87]  [1720/1724] loss: 0.544, ave_loss: 0.514
[88]  [1740/1724] loss: 0.447, ave_loss: 0.513

Finished Training finishing at 2021-08-29 20:46:10.185289
printing_out epoch  22.45939675174014 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.129e-01
Validation Loss: 1.016e+05
Validation ROC: 0.6496
No improvement, still saving model
6.54060324825986 epochs left to go

Training Epoch 22.45939675174014/30 starting at 2021-08-29 20:48:28.448155
[1]  [0/1724] loss: 0.416, ave_loss: 0.416
[2]  [20/1724] loss: 0.538, ave_loss: 0.477
[3]  [40/1724] loss: 0.401, ave_loss: 0.452
[4]  [60/1724] loss: 0.388, ave_loss: 0.436
[5]  [80/1724] loss: 0.691, ave_loss: 0.487
[6]  [100/1724] loss: 0.498, ave_loss: 0.489
[7]  [120/1724] loss: 0.566, ave_loss: 0.500
[8]  [140/1724] loss: 0.548, ave_loss: 0.506
[9]  [160/1724] loss: 0.438, ave_loss: 0.498
[10]  [180/1724] loss: 0.547, ave_loss: 0.503
[11]  [200/1724] loss: 0.507, ave_loss: 0.503
[12]  [220/1724] loss: 0.480, ave_loss: 0.502
[13]  [240/1724] loss: 0.437, ave_loss: 0.497
[14]  [260/1724] loss: 0.451, ave_loss: 0.493
[15]  [280/1724] loss: 0.576, ave_loss: 0.499
[16]  [300/1724] loss: 0.691, ave_loss: 0.511
[17]  [320/1724] loss: 0.541, ave_loss: 0.513
[18]  [340/1724] loss: 0.693, ave_loss: 0.523
[19]  [360/1724] loss: 0.568, ave_loss: 0.525
[20]  [380/1724] loss: 0.605, ave_loss: 0.529
[21]  [400/1724] loss: 0.524, ave_loss: 0.529
[22]  [420/1724] loss: 0.488, ave_loss: 0.527
[23]  [440/1724] loss: 0.649, ave_loss: 0.532
[24]  [460/1724] loss: 0.603, ave_loss: 0.535
[25]  [480/1724] loss: 0.537, ave_loss: 0.535
[26]  [500/1724] loss: 0.612, ave_loss: 0.538
[27]  [520/1724] loss: 0.441, ave_loss: 0.535
[28]  [540/1724] loss: 0.557, ave_loss: 0.535
[29]  [560/1724] loss: 0.566, ave_loss: 0.536
[30]  [580/1724] loss: 0.456, ave_loss: 0.534
[31]  [600/1724] loss: 0.575, ave_loss: 0.535
[32]  [620/1724] loss: 0.521, ave_loss: 0.535
[33]  [640/1724] loss: 0.389, ave_loss: 0.530
[34]  [660/1724] loss: 0.535, ave_loss: 0.530
[35]  [680/1724] loss: 0.537, ave_loss: 0.531
[36]  [700/1724] loss: 0.348, ave_loss: 0.525
[37]  [720/1724] loss: 0.448, ave_loss: 0.523
[38]  [740/1724] loss: 0.465, ave_loss: 0.522
[39]  [760/1724] loss: 0.575, ave_loss: 0.523
[40]  [780/1724] loss: 0.471, ave_loss: 0.522
[41]  [800/1724] loss: 0.862, ave_loss: 0.530
[42]  [820/1724] loss: 0.499, ave_loss: 0.529
[43]  [840/1724] loss: 0.414, ave_loss: 0.527
[44]  [860/1724] loss: 0.376, ave_loss: 0.523
[45]  [880/1724] loss: 0.419, ave_loss: 0.521
[46]  [900/1724] loss: 0.470, ave_loss: 0.520
[47]  [920/1724] loss: 0.414, ave_loss: 0.518
[48]  [940/1724] loss: 0.420, ave_loss: 0.516
[49]  [960/1724] loss: 0.400, ave_loss: 0.513
[50]  [980/1724] loss: 0.452, ave_loss: 0.512
[51]  [1000/1724] loss: 0.352, ave_loss: 0.509
[52]  [1020/1724] loss: 0.644, ave_loss: 0.512
[53]  [1040/1724] loss: 0.655, ave_loss: 0.514
[54]  [1060/1724] loss: 0.627, ave_loss: 0.516
[55]  [1080/1724] loss: 0.500, ave_loss: 0.516
[56]  [1100/1724] loss: 0.554, ave_loss: 0.517
[57]  [1120/1724] loss: 0.519, ave_loss: 0.517
[58]  [1140/1724] loss: 0.542, ave_loss: 0.517
[59]  [1160/1724] loss: 0.379, ave_loss: 0.515
[60]  [1180/1724] loss: 0.579, ave_loss: 0.516
[61]  [1200/1724] loss: 0.513, ave_loss: 0.516
[62]  [1220/1724] loss: 0.471, ave_loss: 0.515
[63]  [1240/1724] loss: 0.603, ave_loss: 0.517
[64]  [1260/1724] loss: 0.547, ave_loss: 0.517
[65]  [1280/1724] loss: 0.604, ave_loss: 0.518
[66]  [1300/1724] loss: 0.587, ave_loss: 0.519
[67]  [1320/1724] loss: 0.565, ave_loss: 0.520
[68]  [1340/1724] loss: 0.621, ave_loss: 0.522
[69]  [1360/1724] loss: 0.430, ave_loss: 0.520
[70]  [1380/1724] loss: 0.548, ave_loss: 0.521
[71]  [1400/1724] loss: 0.578, ave_loss: 0.521
[72]  [1420/1724] loss: 0.443, ave_loss: 0.520
[73]  [1440/1724] loss: 0.454, ave_loss: 0.519
[74]  [1460/1724] loss: 0.483, ave_loss: 0.519
[75]  [1480/1724] loss: 0.567, ave_loss: 0.520
[76]  [1500/1724] loss: 0.526, ave_loss: 0.520
[77]  [1520/1724] loss: 0.611, ave_loss: 0.521
[78]  [1540/1724] loss: 0.558, ave_loss: 0.521
[79]  [1560/1724] loss: 0.353, ave_loss: 0.519
[80]  [1580/1724] loss: 0.485, ave_loss: 0.519
[81]  [1600/1724] loss: 0.360, ave_loss: 0.517
[82]  [1620/1724] loss: 0.541, ave_loss: 0.517
[83]  [1640/1724] loss: 0.720, ave_loss: 0.520
[84]  [1660/1724] loss: 0.590, ave_loss: 0.520
[85]  [1680/1724] loss: 0.496, ave_loss: 0.520
[86]  [1700/1724] loss: 0.567, ave_loss: 0.521
[87]  [1720/1724] loss: 0.536, ave_loss: 0.521
[88]  [1740/1724] loss: 0.569, ave_loss: 0.521

Finished Training finishing at 2021-08-29 20:52:15.910825
printing_out epoch  23.48027842227378 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.214e-01
Validation Loss: 1.096e+05
Validation ROC: 0.6059
No improvement, still saving model
5.519721577726219 epochs left to go

Training Epoch 23.48027842227378/30 starting at 2021-08-29 20:54:30.974031
[1]  [0/1724] loss: 0.567, ave_loss: 0.567
[2]  [20/1724] loss: 0.561, ave_loss: 0.564
[3]  [40/1724] loss: 0.508, ave_loss: 0.545
[4]  [60/1724] loss: 0.392, ave_loss: 0.507
[5]  [80/1724] loss: 0.606, ave_loss: 0.527
[6]  [100/1724] loss: 0.360, ave_loss: 0.499
[7]  [120/1724] loss: 0.351, ave_loss: 0.478
[8]  [140/1724] loss: 0.530, ave_loss: 0.484
[9]  [160/1724] loss: 0.466, ave_loss: 0.482
[10]  [180/1724] loss: 0.331, ave_loss: 0.467
[11]  [200/1724] loss: 0.645, ave_loss: 0.483
[12]  [220/1724] loss: 0.535, ave_loss: 0.488
[13]  [240/1724] loss: 0.622, ave_loss: 0.498
[14]  [260/1724] loss: 0.430, ave_loss: 0.493
[15]  [280/1724] loss: 0.652, ave_loss: 0.504
[16]  [300/1724] loss: 0.580, ave_loss: 0.509
[17]  [320/1724] loss: 0.530, ave_loss: 0.510
[18]  [340/1724] loss: 0.470, ave_loss: 0.508
[19]  [360/1724] loss: 0.578, ave_loss: 0.511
[20]  [380/1724] loss: 0.436, ave_loss: 0.507
[21]  [400/1724] loss: 0.515, ave_loss: 0.508
[22]  [420/1724] loss: 0.596, ave_loss: 0.512
[23]  [440/1724] loss: 0.484, ave_loss: 0.511
[24]  [460/1724] loss: 0.571, ave_loss: 0.513
[25]  [480/1724] loss: 0.470, ave_loss: 0.511
[26]  [500/1724] loss: 0.571, ave_loss: 0.514
[27]  [520/1724] loss: 0.368, ave_loss: 0.508
[28]  [540/1724] loss: 0.564, ave_loss: 0.510
[29]  [560/1724] loss: 0.522, ave_loss: 0.511
[30]  [580/1724] loss: 0.424, ave_loss: 0.508
[31]  [600/1724] loss: 0.626, ave_loss: 0.512
[32]  [620/1724] loss: 0.489, ave_loss: 0.511
[33]  [640/1724] loss: 0.574, ave_loss: 0.513
[34]  [660/1724] loss: 0.451, ave_loss: 0.511
[35]  [680/1724] loss: 0.604, ave_loss: 0.514
[36]  [700/1724] loss: 0.388, ave_loss: 0.510
[37]  [720/1724] loss: 0.379, ave_loss: 0.507
[38]  [740/1724] loss: 0.459, ave_loss: 0.505
[39]  [760/1724] loss: 0.635, ave_loss: 0.509
[40]  [780/1724] loss: 0.439, ave_loss: 0.507
[41]  [800/1724] loss: 0.495, ave_loss: 0.507
[42]  [820/1724] loss: 0.724, ave_loss: 0.512
[43]  [840/1724] loss: 0.585, ave_loss: 0.513
[44]  [860/1724] loss: 0.445, ave_loss: 0.512
[45]  [880/1724] loss: 0.589, ave_loss: 0.514
[46]  [900/1724] loss: 0.621, ave_loss: 0.516
[47]  [920/1724] loss: 0.472, ave_loss: 0.515
[48]  [940/1724] loss: 0.512, ave_loss: 0.515
[49]  [960/1724] loss: 0.760, ave_loss: 0.520
[50]  [980/1724] loss: 0.428, ave_loss: 0.518
[51]  [1000/1724] loss: 0.528, ave_loss: 0.518
[52]  [1020/1724] loss: 0.521, ave_loss: 0.518
[53]  [1040/1724] loss: 0.510, ave_loss: 0.518
[54]  [1060/1724] loss: 0.463, ave_loss: 0.517
[55]  [1080/1724] loss: 0.441, ave_loss: 0.516
[56]  [1100/1724] loss: 0.659, ave_loss: 0.518
[57]  [1120/1724] loss: 0.406, ave_loss: 0.516
[58]  [1140/1724] loss: 0.281, ave_loss: 0.512
[59]  [1160/1724] loss: 0.440, ave_loss: 0.511
[60]  [1180/1724] loss: 0.466, ave_loss: 0.510
[61]  [1200/1724] loss: 0.584, ave_loss: 0.512
[62]  [1220/1724] loss: 0.703, ave_loss: 0.515
[63]  [1240/1724] loss: 0.495, ave_loss: 0.514
[64]  [1260/1724] loss: 0.493, ave_loss: 0.514
[65]  [1280/1724] loss: 0.352, ave_loss: 0.512
[66]  [1300/1724] loss: 0.510, ave_loss: 0.511
[67]  [1320/1724] loss: 0.499, ave_loss: 0.511
[68]  [1340/1724] loss: 0.489, ave_loss: 0.511
[69]  [1360/1724] loss: 0.478, ave_loss: 0.510
[70]  [1380/1724] loss: 0.627, ave_loss: 0.512
[71]  [1400/1724] loss: 0.443, ave_loss: 0.511
[72]  [1420/1724] loss: 0.629, ave_loss: 0.513
[73]  [1440/1724] loss: 0.494, ave_loss: 0.513
[74]  [1460/1724] loss: 0.468, ave_loss: 0.512
[75]  [1480/1724] loss: 0.434, ave_loss: 0.511
[76]  [1500/1724] loss: 0.424, ave_loss: 0.510
[77]  [1520/1724] loss: 0.506, ave_loss: 0.510
[78]  [1540/1724] loss: 0.485, ave_loss: 0.509
[79]  [1560/1724] loss: 0.374, ave_loss: 0.508
[80]  [1580/1724] loss: 0.591, ave_loss: 0.509
[81]  [1600/1724] loss: 0.408, ave_loss: 0.508
[82]  [1620/1724] loss: 0.631, ave_loss: 0.509
[83]  [1640/1724] loss: 0.522, ave_loss: 0.509
[84]  [1660/1724] loss: 0.426, ave_loss: 0.508
[85]  [1680/1724] loss: 0.571, ave_loss: 0.509
[86]  [1700/1724] loss: 0.475, ave_loss: 0.509
[87]  [1720/1724] loss: 0.590, ave_loss: 0.509
[88]  [1740/1724] loss: 0.487, ave_loss: 0.509

Finished Training finishing at 2021-08-29 20:58:06.627712
printing_out epoch  24.501160092807424 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.092e-01
Validation Loss: 1.102e+05
Validation ROC: 0.6253
No improvement, still saving model
4.4988399071925755 epochs left to go

Training Epoch 24.501160092807424/30 starting at 2021-08-29 21:00:20.576218
[1]  [0/1724] loss: 0.608, ave_loss: 0.608
[2]  [20/1724] loss: 0.513, ave_loss: 0.561
[3]  [40/1724] loss: 0.566, ave_loss: 0.563
[4]  [60/1724] loss: 0.537, ave_loss: 0.556
[5]  [80/1724] loss: 0.596, ave_loss: 0.564
[6]  [100/1724] loss: 0.476, ave_loss: 0.550
[7]  [120/1724] loss: 0.492, ave_loss: 0.541
[8]  [140/1724] loss: 0.627, ave_loss: 0.552
[9]  [160/1724] loss: 0.517, ave_loss: 0.548
[10]  [180/1724] loss: 0.466, ave_loss: 0.540
[11]  [200/1724] loss: 0.541, ave_loss: 0.540
[12]  [220/1724] loss: 0.445, ave_loss: 0.532
[13]  [240/1724] loss: 0.450, ave_loss: 0.526
[14]  [260/1724] loss: 0.525, ave_loss: 0.526
[15]  [280/1724] loss: 0.467, ave_loss: 0.522
[16]  [300/1724] loss: 0.510, ave_loss: 0.521
[17]  [320/1724] loss: 0.378, ave_loss: 0.513
[18]  [340/1724] loss: 0.458, ave_loss: 0.510
[19]  [360/1724] loss: 0.587, ave_loss: 0.514
[20]  [380/1724] loss: 0.556, ave_loss: 0.516
[21]  [400/1724] loss: 0.503, ave_loss: 0.515
[22]  [420/1724] loss: 0.507, ave_loss: 0.515
[23]  [440/1724] loss: 0.457, ave_loss: 0.512
[24]  [460/1724] loss: 0.647, ave_loss: 0.518
[25]  [480/1724] loss: 0.349, ave_loss: 0.511
[26]  [500/1724] loss: 0.524, ave_loss: 0.512
[27]  [520/1724] loss: 0.681, ave_loss: 0.518
[28]  [540/1724] loss: 0.701, ave_loss: 0.524
[29]  [560/1724] loss: 0.447, ave_loss: 0.522
[30]  [580/1724] loss: 0.472, ave_loss: 0.520
[31]  [600/1724] loss: 0.654, ave_loss: 0.524
[32]  [620/1724] loss: 0.520, ave_loss: 0.524
[33]  [640/1724] loss: 0.502, ave_loss: 0.524
[34]  [660/1724] loss: 0.466, ave_loss: 0.522
[35]  [680/1724] loss: 0.403, ave_loss: 0.519
[36]  [700/1724] loss: 0.572, ave_loss: 0.520
[37]  [720/1724] loss: 0.621, ave_loss: 0.523
[38]  [740/1724] loss: 0.377, ave_loss: 0.519
[39]  [760/1724] loss: 0.372, ave_loss: 0.515
[40]  [780/1724] loss: 0.620, ave_loss: 0.518
[41]  [800/1724] loss: 0.369, ave_loss: 0.514
[42]  [820/1724] loss: 0.597, ave_loss: 0.516
[43]  [840/1724] loss: 0.373, ave_loss: 0.513
[44]  [860/1724] loss: 0.507, ave_loss: 0.513
[45]  [880/1724] loss: 0.419, ave_loss: 0.511
[46]  [900/1724] loss: 0.464, ave_loss: 0.510
[47]  [920/1724] loss: 0.348, ave_loss: 0.506
[48]  [940/1724] loss: 0.686, ave_loss: 0.510
[49]  [960/1724] loss: 0.383, ave_loss: 0.507
[50]  [980/1724] loss: 0.512, ave_loss: 0.507
[51]  [1000/1724] loss: 0.595, ave_loss: 0.509
[52]  [1020/1724] loss: 0.497, ave_loss: 0.509
[53]  [1040/1724] loss: 0.659, ave_loss: 0.512
[54]  [1060/1724] loss: 0.489, ave_loss: 0.511
[55]  [1080/1724] loss: 0.506, ave_loss: 0.511
[56]  [1100/1724] loss: 0.451, ave_loss: 0.510
[57]  [1120/1724] loss: 0.603, ave_loss: 0.512
[58]  [1140/1724] loss: 0.595, ave_loss: 0.513
[59]  [1160/1724] loss: 0.602, ave_loss: 0.515
[60]  [1180/1724] loss: 0.497, ave_loss: 0.514
[61]  [1200/1724] loss: 0.468, ave_loss: 0.514
[62]  [1220/1724] loss: 0.434, ave_loss: 0.512
[63]  [1240/1724] loss: 0.501, ave_loss: 0.512
[64]  [1260/1724] loss: 0.588, ave_loss: 0.513
[65]  [1280/1724] loss: 0.562, ave_loss: 0.514
[66]  [1300/1724] loss: 0.417, ave_loss: 0.513
[67]  [1320/1724] loss: 0.560, ave_loss: 0.513
[68]  [1340/1724] loss: 0.419, ave_loss: 0.512
[69]  [1360/1724] loss: 0.569, ave_loss: 0.513
[70]  [1380/1724] loss: 0.432, ave_loss: 0.512
[71]  [1400/1724] loss: 0.499, ave_loss: 0.511
[72]  [1420/1724] loss: 0.477, ave_loss: 0.511
[73]  [1440/1724] loss: 0.509, ave_loss: 0.511
[74]  [1460/1724] loss: 0.517, ave_loss: 0.511
[75]  [1480/1724] loss: 0.333, ave_loss: 0.509
[76]  [1500/1724] loss: 0.478, ave_loss: 0.508
[77]  [1520/1724] loss: 0.508, ave_loss: 0.508
[78]  [1540/1724] loss: 0.466, ave_loss: 0.508
[79]  [1560/1724] loss: 0.493, ave_loss: 0.508
[80]  [1580/1724] loss: 0.616, ave_loss: 0.509
[81]  [1600/1724] loss: 0.450, ave_loss: 0.508
[82]  [1620/1724] loss: 0.459, ave_loss: 0.508
[83]  [1640/1724] loss: 0.527, ave_loss: 0.508
[84]  [1660/1724] loss: 0.483, ave_loss: 0.507
[85]  [1680/1724] loss: 0.594, ave_loss: 0.508
[86]  [1700/1724] loss: 0.342, ave_loss: 0.507
[87]  [1720/1724] loss: 0.419, ave_loss: 0.506
[88]  [1740/1724] loss: 0.397, ave_loss: 0.504

Finished Training finishing at 2021-08-29 21:03:53.594936
printing_out epoch  25.52204176334107 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.043e-01
Validation Loss: 1.008e+05
Validation ROC: 0.6342
No improvement, still saving model
3.4779582366589317 epochs left to go

Training Epoch 25.52204176334107/30 starting at 2021-08-29 21:05:59.784437
[1]  [0/1724] loss: 0.722, ave_loss: 0.722
[2]  [20/1724] loss: 0.510, ave_loss: 0.616
[3]  [40/1724] loss: 0.568, ave_loss: 0.600
[4]  [60/1724] loss: 0.396, ave_loss: 0.549
[5]  [80/1724] loss: 0.510, ave_loss: 0.541
[6]  [100/1724] loss: 0.412, ave_loss: 0.520
[7]  [120/1724] loss: 0.789, ave_loss: 0.558
[8]  [140/1724] loss: 0.370, ave_loss: 0.535
[9]  [160/1724] loss: 0.581, ave_loss: 0.540
[10]  [180/1724] loss: 0.677, ave_loss: 0.554
[11]  [200/1724] loss: 0.544, ave_loss: 0.553
[12]  [220/1724] loss: 0.730, ave_loss: 0.567
[13]  [240/1724] loss: 0.307, ave_loss: 0.547
[14]  [260/1724] loss: 0.515, ave_loss: 0.545
[15]  [280/1724] loss: 0.585, ave_loss: 0.548
[16]  [300/1724] loss: 0.618, ave_loss: 0.552
[17]  [320/1724] loss: 0.500, ave_loss: 0.549
[18]  [340/1724] loss: 0.535, ave_loss: 0.548
[19]  [360/1724] loss: 0.481, ave_loss: 0.545
[20]  [380/1724] loss: 0.428, ave_loss: 0.539
[21]  [400/1724] loss: 0.510, ave_loss: 0.538
[22]  [420/1724] loss: 0.548, ave_loss: 0.538
[23]  [440/1724] loss: 0.572, ave_loss: 0.539
[24]  [460/1724] loss: 0.401, ave_loss: 0.534
[25]  [480/1724] loss: 0.672, ave_loss: 0.539
[26]  [500/1724] loss: 0.445, ave_loss: 0.536
[27]  [520/1724] loss: 0.627, ave_loss: 0.539
[28]  [540/1724] loss: 0.547, ave_loss: 0.539
[29]  [560/1724] loss: 0.597, ave_loss: 0.541
[30]  [580/1724] loss: 0.464, ave_loss: 0.539
[31]  [600/1724] loss: 0.534, ave_loss: 0.539
[32]  [620/1724] loss: 0.563, ave_loss: 0.539
[33]  [640/1724] loss: 0.682, ave_loss: 0.544
[34]  [660/1724] loss: 0.566, ave_loss: 0.544
[35]  [680/1724] loss: 0.465, ave_loss: 0.542
[36]  [700/1724] loss: 0.564, ave_loss: 0.543
[37]  [720/1724] loss: 0.478, ave_loss: 0.541
[38]  [740/1724] loss: 0.378, ave_loss: 0.537
[39]  [760/1724] loss: 0.455, ave_loss: 0.535
[40]  [780/1724] loss: 0.514, ave_loss: 0.534
[41]  [800/1724] loss: 0.594, ave_loss: 0.535
[42]  [820/1724] loss: 0.666, ave_loss: 0.539
[43]  [840/1724] loss: 0.448, ave_loss: 0.536
[44]  [860/1724] loss: 0.556, ave_loss: 0.537
[45]  [880/1724] loss: 0.508, ave_loss: 0.536
[46]  [900/1724] loss: 0.359, ave_loss: 0.532
[47]  [920/1724] loss: 0.428, ave_loss: 0.530
[48]  [940/1724] loss: 0.472, ave_loss: 0.529
[49]  [960/1724] loss: 0.430, ave_loss: 0.527
[50]  [980/1724] loss: 0.379, ave_loss: 0.524
[51]  [1000/1724] loss: 0.675, ave_loss: 0.527
[52]  [1020/1724] loss: 0.416, ave_loss: 0.525
[53]  [1040/1724] loss: 0.433, ave_loss: 0.523
[54]  [1060/1724] loss: 0.314, ave_loss: 0.519
[55]  [1080/1724] loss: 0.417, ave_loss: 0.517
[56]  [1100/1724] loss: 0.534, ave_loss: 0.518
[57]  [1120/1724] loss: 0.528, ave_loss: 0.518
[58]  [1140/1724] loss: 0.653, ave_loss: 0.520
[59]  [1160/1724] loss: 0.625, ave_loss: 0.522
[60]  [1180/1724] loss: 0.626, ave_loss: 0.524
[61]  [1200/1724] loss: 0.532, ave_loss: 0.524
[62]  [1220/1724] loss: 0.589, ave_loss: 0.525
[63]  [1240/1724] loss: 0.461, ave_loss: 0.524
[64]  [1260/1724] loss: 0.555, ave_loss: 0.524
[65]  [1280/1724] loss: 0.529, ave_loss: 0.524
[66]  [1300/1724] loss: 0.350, ave_loss: 0.522
[67]  [1320/1724] loss: 0.451, ave_loss: 0.521
[68]  [1340/1724] loss: 0.440, ave_loss: 0.520
[69]  [1360/1724] loss: 0.447, ave_loss: 0.518
[70]  [1380/1724] loss: 0.457, ave_loss: 0.518
[71]  [1400/1724] loss: 0.502, ave_loss: 0.517
[72]  [1420/1724] loss: 0.591, ave_loss: 0.518
[73]  [1440/1724] loss: 0.428, ave_loss: 0.517
[74]  [1460/1724] loss: 0.410, ave_loss: 0.516
[75]  [1480/1724] loss: 0.319, ave_loss: 0.513
[76]  [1500/1724] loss: 0.262, ave_loss: 0.510
[77]  [1520/1724] loss: 0.571, ave_loss: 0.511
[78]  [1540/1724] loss: 0.702, ave_loss: 0.513
[79]  [1560/1724] loss: 0.530, ave_loss: 0.513
[80]  [1580/1724] loss: 0.419, ave_loss: 0.512
[81]  [1600/1724] loss: 0.621, ave_loss: 0.513
[82]  [1620/1724] loss: 0.475, ave_loss: 0.513
[83]  [1640/1724] loss: 0.442, ave_loss: 0.512
[84]  [1660/1724] loss: 0.477, ave_loss: 0.512
[85]  [1680/1724] loss: 0.650, ave_loss: 0.513
[86]  [1700/1724] loss: 0.618, ave_loss: 0.515
[87]  [1720/1724] loss: 0.539, ave_loss: 0.515
[88]  [1740/1724] loss: 0.582, ave_loss: 0.516

Finished Training finishing at 2021-08-29 21:09:52.035866
printing_out epoch  26.54292343387471 learning rate: 0.0003199956068772566
0.0003103957386709389
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.156e-01
Validation Loss: 9.449e+04
Validation ROC: 0.6311
No improvement, still saving model
2.4570765661252914 epochs left to go

Training Epoch 26.54292343387471/30 starting at 2021-08-29 21:11:55.805017
[1]  [0/1724] loss: 0.853, ave_loss: 0.853
[2]  [20/1724] loss: 0.591, ave_loss: 0.722
[3]  [40/1724] loss: 0.604, ave_loss: 0.683
[4]  [60/1724] loss: 0.311, ave_loss: 0.590
[5]  [80/1724] loss: 0.411, ave_loss: 0.554
[6]  [100/1724] loss: 0.610, ave_loss: 0.563
[7]  [120/1724] loss: 0.425, ave_loss: 0.544
[8]  [140/1724] loss: 0.535, ave_loss: 0.542
[9]  [160/1724] loss: 0.585, ave_loss: 0.547
[10]  [180/1724] loss: 0.616, ave_loss: 0.554
[11]  [200/1724] loss: 0.438, ave_loss: 0.544
[12]  [220/1724] loss: 0.384, ave_loss: 0.530
[13]  [240/1724] loss: 0.407, ave_loss: 0.521
[14]  [260/1724] loss: 0.477, ave_loss: 0.518
[15]  [280/1724] loss: 0.523, ave_loss: 0.518
[16]  [300/1724] loss: 0.450, ave_loss: 0.514
[17]  [320/1724] loss: 0.523, ave_loss: 0.514
[18]  [340/1724] loss: 0.436, ave_loss: 0.510
[19]  [360/1724] loss: 0.504, ave_loss: 0.510
[20]  [380/1724] loss: 0.454, ave_loss: 0.507
[21]  [400/1724] loss: 0.579, ave_loss: 0.510
[22]  [420/1724] loss: 0.459, ave_loss: 0.508
[23]  [440/1724] loss: 0.725, ave_loss: 0.517
[24]  [460/1724] loss: 0.367, ave_loss: 0.511
[25]  [480/1724] loss: 0.416, ave_loss: 0.507
[26]  [500/1724] loss: 0.375, ave_loss: 0.502
[27]  [520/1724] loss: 0.341, ave_loss: 0.496
[28]  [540/1724] loss: 0.506, ave_loss: 0.497
[29]  [560/1724] loss: 0.690, ave_loss: 0.503
[30]  [580/1724] loss: 0.408, ave_loss: 0.500
[31]  [600/1724] loss: 0.476, ave_loss: 0.499
[32]  [620/1724] loss: 0.353, ave_loss: 0.495
[33]  [640/1724] loss: 0.580, ave_loss: 0.497
[34]  [660/1724] loss: 0.639, ave_loss: 0.501
[35]  [680/1724] loss: 0.421, ave_loss: 0.499
[36]  [700/1724] loss: 0.571, ave_loss: 0.501
[37]  [720/1724] loss: 0.546, ave_loss: 0.502
[38]  [740/1724] loss: 0.551, ave_loss: 0.504
[39]  [760/1724] loss: 0.631, ave_loss: 0.507
[40]  [780/1724] loss: 0.498, ave_loss: 0.507
[41]  [800/1724] loss: 0.653, ave_loss: 0.510
[42]  [820/1724] loss: 0.648, ave_loss: 0.514
[43]  [840/1724] loss: 0.468, ave_loss: 0.513
[44]  [860/1724] loss: 0.409, ave_loss: 0.510
[45]  [880/1724] loss: 0.509, ave_loss: 0.510
[46]  [900/1724] loss: 0.551, ave_loss: 0.511
[47]  [920/1724] loss: 0.578, ave_loss: 0.512
[48]  [940/1724] loss: 0.289, ave_loss: 0.508
[49]  [960/1724] loss: 0.433, ave_loss: 0.506
[50]  [980/1724] loss: 0.371, ave_loss: 0.504
[51]  [1000/1724] loss: 0.588, ave_loss: 0.505
[52]  [1020/1724] loss: 0.364, ave_loss: 0.502
[53]  [1040/1724] loss: 0.493, ave_loss: 0.502
[54]  [1060/1724] loss: 0.357, ave_loss: 0.500
[55]  [1080/1724] loss: 0.489, ave_loss: 0.499
[56]  [1100/1724] loss: 0.538, ave_loss: 0.500
[57]  [1120/1724] loss: 0.576, ave_loss: 0.501
[58]  [1140/1724] loss: 0.432, ave_loss: 0.500
[59]  [1160/1724] loss: 0.520, ave_loss: 0.501
[60]  [1180/1724] loss: 0.496, ave_loss: 0.501
[61]  [1200/1724] loss: 0.757, ave_loss: 0.505
[62]  [1220/1724] loss: 0.431, ave_loss: 0.504
[63]  [1240/1724] loss: 0.340, ave_loss: 0.501
[64]  [1260/1724] loss: 0.449, ave_loss: 0.500
[65]  [1280/1724] loss: 0.533, ave_loss: 0.501
[66]  [1300/1724] loss: 0.600, ave_loss: 0.502
[67]  [1320/1724] loss: 0.404, ave_loss: 0.501
[68]  [1340/1724] loss: 0.488, ave_loss: 0.500
[69]  [1360/1724] loss: 0.529, ave_loss: 0.501
[70]  [1380/1724] loss: 0.560, ave_loss: 0.502
[71]  [1400/1724] loss: 0.703, ave_loss: 0.505
[72]  [1420/1724] loss: 0.484, ave_loss: 0.504
[73]  [1440/1724] loss: 0.421, ave_loss: 0.503
[74]  [1460/1724] loss: 0.500, ave_loss: 0.503
[75]  [1480/1724] loss: 0.500, ave_loss: 0.503
[76]  [1500/1724] loss: 0.469, ave_loss: 0.503
[77]  [1520/1724] loss: 0.466, ave_loss: 0.502
[78]  [1540/1724] loss: 0.608, ave_loss: 0.504
[79]  [1560/1724] loss: 0.332, ave_loss: 0.501
[80]  [1580/1724] loss: 0.491, ave_loss: 0.501
[81]  [1600/1724] loss: 0.361, ave_loss: 0.499
[82]  [1620/1724] loss: 0.583, ave_loss: 0.501
[83]  [1640/1724] loss: 0.512, ave_loss: 0.501
[84]  [1660/1724] loss: 0.491, ave_loss: 0.501
[85]  [1680/1724] loss: 0.550, ave_loss: 0.501
[86]  [1700/1724] loss: 0.449, ave_loss: 0.501
[87]  [1720/1724] loss: 0.449, ave_loss: 0.500
[88]  [1740/1724] loss: 0.595, ave_loss: 0.501

Finished Training finishing at 2021-08-29 21:15:35.166298
printing_out epoch  27.563805104408353 learning rate: 0.00029090509716114235
0.0002821779442463081
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.010e-01
Validation Loss: 1.008e+05
Validation ROC: 0.6315
No improvement, still saving model
1.4361948955916475 epochs left to go

Training Epoch 27.563805104408353/30 starting at 2021-08-29 21:18:05.964835
[1]  [0/1724] loss: 0.412, ave_loss: 0.412
[2]  [20/1724] loss: 0.712, ave_loss: 0.562
[3]  [40/1724] loss: 0.450, ave_loss: 0.525
[4]  [60/1724] loss: 0.489, ave_loss: 0.516
[5]  [80/1724] loss: 0.412, ave_loss: 0.495
[6]  [100/1724] loss: 0.669, ave_loss: 0.524
[7]  [120/1724] loss: 0.479, ave_loss: 0.518
[8]  [140/1724] loss: 0.429, ave_loss: 0.506
[9]  [160/1724] loss: 0.415, ave_loss: 0.496
[10]  [180/1724] loss: 0.580, ave_loss: 0.505
[11]  [200/1724] loss: 0.590, ave_loss: 0.512
[12]  [220/1724] loss: 0.610, ave_loss: 0.521
[13]  [240/1724] loss: 0.320, ave_loss: 0.505
[14]  [260/1724] loss: 0.600, ave_loss: 0.512
[15]  [280/1724] loss: 0.489, ave_loss: 0.510
[16]  [300/1724] loss: 0.454, ave_loss: 0.507
[17]  [320/1724] loss: 0.603, ave_loss: 0.513
[18]  [340/1724] loss: 0.584, ave_loss: 0.517
[19]  [360/1724] loss: 0.427, ave_loss: 0.512
[20]  [380/1724] loss: 0.462, ave_loss: 0.509
[21]  [400/1724] loss: 0.352, ave_loss: 0.502
[22]  [420/1724] loss: 0.429, ave_loss: 0.499
[23]  [440/1724] loss: 0.628, ave_loss: 0.504
[24]  [460/1724] loss: 0.383, ave_loss: 0.499
[25]  [480/1724] loss: 0.462, ave_loss: 0.498
[26]  [500/1724] loss: 0.496, ave_loss: 0.498
[27]  [520/1724] loss: 0.590, ave_loss: 0.501
[28]  [540/1724] loss: 0.429, ave_loss: 0.498
[29]  [560/1724] loss: 0.491, ave_loss: 0.498
[30]  [580/1724] loss: 0.626, ave_loss: 0.502
[31]  [600/1724] loss: 0.556, ave_loss: 0.504
[32]  [620/1724] loss: 0.698, ave_loss: 0.510
[33]  [640/1724] loss: 0.591, ave_loss: 0.513
[34]  [660/1724] loss: 0.616, ave_loss: 0.516
[35]  [680/1724] loss: 0.635, ave_loss: 0.519
[36]  [700/1724] loss: 0.344, ave_loss: 0.514
[37]  [720/1724] loss: 0.637, ave_loss: 0.518
[38]  [740/1724] loss: 0.609, ave_loss: 0.520
[39]  [760/1724] loss: 0.514, ave_loss: 0.520
[40]  [780/1724] loss: 0.424, ave_loss: 0.517
[41]  [800/1724] loss: 0.533, ave_loss: 0.518
[42]  [820/1724] loss: 0.559, ave_loss: 0.519
[43]  [840/1724] loss: 0.464, ave_loss: 0.517
[44]  [860/1724] loss: 0.483, ave_loss: 0.517
[45]  [880/1724] loss: 0.701, ave_loss: 0.521
[46]  [900/1724] loss: 0.501, ave_loss: 0.520
[47]  [920/1724] loss: 0.649, ave_loss: 0.523
[48]  [940/1724] loss: 0.398, ave_loss: 0.521
[49]  [960/1724] loss: 0.447, ave_loss: 0.519
[50]  [980/1724] loss: 0.498, ave_loss: 0.519
[51]  [1000/1724] loss: 0.596, ave_loss: 0.520
[52]  [1020/1724] loss: 0.618, ave_loss: 0.522
[53]  [1040/1724] loss: 0.525, ave_loss: 0.522
[54]  [1060/1724] loss: 0.414, ave_loss: 0.520
[55]  [1080/1724] loss: 0.432, ave_loss: 0.518
[56]  [1100/1724] loss: 0.482, ave_loss: 0.518
[57]  [1120/1724] loss: 0.496, ave_loss: 0.517
[58]  [1140/1724] loss: 0.456, ave_loss: 0.516
[59]  [1160/1724] loss: 0.421, ave_loss: 0.515
[60]  [1180/1724] loss: 0.368, ave_loss: 0.512
[61]  [1200/1724] loss: 0.373, ave_loss: 0.510
[62]  [1220/1724] loss: 0.558, ave_loss: 0.511
[63]  [1240/1724] loss: 0.381, ave_loss: 0.509
[64]  [1260/1724] loss: 0.461, ave_loss: 0.508
[65]  [1280/1724] loss: 0.478, ave_loss: 0.508
[66]  [1300/1724] loss: 0.566, ave_loss: 0.508
[67]  [1320/1724] loss: 0.456, ave_loss: 0.508
[68]  [1340/1724] loss: 0.377, ave_loss: 0.506
[69]  [1360/1724] loss: 0.497, ave_loss: 0.506
[70]  [1380/1724] loss: 0.414, ave_loss: 0.504
[71]  [1400/1724] loss: 0.669, ave_loss: 0.507
[72]  [1420/1724] loss: 0.387, ave_loss: 0.505
[73]  [1440/1724] loss: 0.482, ave_loss: 0.505
[74]  [1460/1724] loss: 0.535, ave_loss: 0.505
[75]  [1480/1724] loss: 0.588, ave_loss: 0.506
[76]  [1500/1724] loss: 0.539, ave_loss: 0.507
[77]  [1520/1724] loss: 0.565, ave_loss: 0.507
[78]  [1540/1724] loss: 0.648, ave_loss: 0.509
[79]  [1560/1724] loss: 0.437, ave_loss: 0.508
[80]  [1580/1724] loss: 0.376, ave_loss: 0.507
[81]  [1600/1724] loss: 0.467, ave_loss: 0.506
[82]  [1620/1724] loss: 0.638, ave_loss: 0.508
[83]  [1640/1724] loss: 0.480, ave_loss: 0.507
[84]  [1660/1724] loss: 0.350, ave_loss: 0.505
[85]  [1680/1724] loss: 0.485, ave_loss: 0.505
[86]  [1700/1724] loss: 0.530, ave_loss: 0.506
[87]  [1720/1724] loss: 0.506, ave_loss: 0.506
[88]  [1740/1724] loss: 0.530, ave_loss: 0.506

Finished Training finishing at 2021-08-29 21:21:44.257821
printing_out epoch  28.584686774941996 learning rate: 0.00026445917923740214
0.0002565254038602801
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.058e-01
Validation Loss: 9.930e+04
Validation ROC: 0.6240
No improvement, still saving model
0.4153132250580036 epochs left to go

Training Epoch 28.584686774941996/30 starting at 2021-08-29 21:24:04.974242
[1]  [0/1724] loss: 0.730, ave_loss: 0.730
[2]  [20/1724] loss: 0.492, ave_loss: 0.611
[3]  [40/1724] loss: 0.461, ave_loss: 0.561
[4]  [60/1724] loss: 0.557, ave_loss: 0.560
[5]  [80/1724] loss: 0.364, ave_loss: 0.521
[6]  [100/1724] loss: 0.460, ave_loss: 0.511
[7]  [120/1724] loss: 0.635, ave_loss: 0.529
[8]  [140/1724] loss: 0.441, ave_loss: 0.518
[9]  [160/1724] loss: 0.694, ave_loss: 0.537
[10]  [180/1724] loss: 0.494, ave_loss: 0.533
[11]  [200/1724] loss: 0.608, ave_loss: 0.540
[12]  [220/1724] loss: 0.535, ave_loss: 0.539
[13]  [240/1724] loss: 0.519, ave_loss: 0.538
[14]  [260/1724] loss: 0.639, ave_loss: 0.545
[15]  [280/1724] loss: 0.440, ave_loss: 0.538
[16]  [300/1724] loss: 0.468, ave_loss: 0.534
[17]  [320/1724] loss: 0.360, ave_loss: 0.523
[18]  [340/1724] loss: 0.492, ave_loss: 0.522
[19]  [360/1724] loss: 0.470, ave_loss: 0.519
[20]  [380/1724] loss: 0.484, ave_loss: 0.517
[21]  [400/1724] loss: 0.526, ave_loss: 0.518
[22]  [420/1724] loss: 0.331, ave_loss: 0.509
[23]  [440/1724] loss: 0.411, ave_loss: 0.505
[24]  [460/1724] loss: 0.449, ave_loss: 0.503
[25]  [480/1724] loss: 0.449, ave_loss: 0.500
[26]  [500/1724] loss: 0.524, ave_loss: 0.501
[27]  [520/1724] loss: 0.475, ave_loss: 0.500
[28]  [540/1724] loss: 0.388, ave_loss: 0.496
[29]  [560/1724] loss: 0.407, ave_loss: 0.493
[30]  [580/1724] loss: 0.656, ave_loss: 0.499
[31]  [600/1724] loss: 0.462, ave_loss: 0.497
[32]  [620/1724] loss: 0.498, ave_loss: 0.498
[33]  [640/1724] loss: 0.448, ave_loss: 0.496
[34]  [660/1724] loss: 0.429, ave_loss: 0.494
[35]  [680/1724] loss: 0.443, ave_loss: 0.493
[36]  [700/1724] loss: 0.476, ave_loss: 0.492
[37]  [720/1724] loss: 0.397, ave_loss: 0.490
[38]  [740/1724] loss: 0.380, ave_loss: 0.487
[39]  [760/1724] loss: 0.523, ave_loss: 0.488
[40]  [780/1724] loss: 0.479, ave_loss: 0.487
[41]  [800/1724] loss: 0.385, ave_loss: 0.485
[42]  [820/1724] loss: 0.594, ave_loss: 0.487
[43]  [840/1724] loss: 0.600, ave_loss: 0.490
[44]  [860/1724] loss: 0.512, ave_loss: 0.491
[45]  [880/1724] loss: 0.605, ave_loss: 0.493
[46]  [900/1724] loss: 0.371, ave_loss: 0.490
[47]  [920/1724] loss: 0.471, ave_loss: 0.490
[48]  [940/1724] loss: 0.633, ave_loss: 0.493
[49]  [960/1724] loss: 0.552, ave_loss: 0.494
[50]  [980/1724] loss: 0.332, ave_loss: 0.491
[51]  [1000/1724] loss: 0.453, ave_loss: 0.490
[52]  [1020/1724] loss: 0.414, ave_loss: 0.489
[53]  [1040/1724] loss: 0.378, ave_loss: 0.487
[54]  [1060/1724] loss: 0.519, ave_loss: 0.487
[55]  [1080/1724] loss: 0.547, ave_loss: 0.488
[56]  [1100/1724] loss: 0.379, ave_loss: 0.486
[57]  [1120/1724] loss: 0.418, ave_loss: 0.485
[58]  [1140/1724] loss: 0.406, ave_loss: 0.484
[59]  [1160/1724] loss: 0.525, ave_loss: 0.485
[60]  [1180/1724] loss: 0.543, ave_loss: 0.486
[61]  [1200/1724] loss: 0.631, ave_loss: 0.488
[62]  [1220/1724] loss: 0.371, ave_loss: 0.486
[63]  [1240/1724] loss: 0.548, ave_loss: 0.487
[64]  [1260/1724] loss: 0.434, ave_loss: 0.486
[65]  [1280/1724] loss: 0.566, ave_loss: 0.487
[66]  [1300/1724] loss: 0.470, ave_loss: 0.487
[67]  [1320/1724] loss: 0.501, ave_loss: 0.487
[68]  [1340/1724] loss: 0.547, ave_loss: 0.488
[69]  [1360/1724] loss: 0.390, ave_loss: 0.487
[70]  [1380/1724] loss: 0.516, ave_loss: 0.487
[71]  [1400/1724] loss: 0.478, ave_loss: 0.487
[72]  [1420/1724] loss: 0.621, ave_loss: 0.489
[73]  [1440/1724] loss: 0.430, ave_loss: 0.488
[74]  [1460/1724] loss: 0.376, ave_loss: 0.487
[75]  [1480/1724] loss: 0.379, ave_loss: 0.485
[76]  [1500/1724] loss: 0.405, ave_loss: 0.484
[77]  [1520/1724] loss: 0.353, ave_loss: 0.482
[78]  [1540/1724] loss: 0.534, ave_loss: 0.483
[79]  [1560/1724] loss: 0.605, ave_loss: 0.485
[80]  [1580/1724] loss: 0.484, ave_loss: 0.485
[81]  [1600/1724] loss: 0.486, ave_loss: 0.485
[82]  [1620/1724] loss: 0.560, ave_loss: 0.486
[83]  [1640/1724] loss: 0.478, ave_loss: 0.485
[84]  [1660/1724] loss: 0.603, ave_loss: 0.487
[85]  [1680/1724] loss: 0.464, ave_loss: 0.487
[86]  [1700/1724] loss: 0.545, ave_loss: 0.487
[87]  [1720/1724] loss: 0.537, ave_loss: 0.488
[88]  [1740/1724] loss: 0.526, ave_loss: 0.488

Finished Training finishing at 2021-08-29 21:27:59.917181
printing_out epoch  29.605568445475637 learning rate: 0.00024041743567036557
0.0002332049126002546
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.883e-01
Validation Loss: 9.755e+04
Validation ROC: 0.6166
No improvement, still saving model
saving results
