reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
Arguments:  Namespace(channels_spatial=['c16', 'c8', 'c4', 'c2'], channels_temporal=['c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15'], input_div=1.0, kernel_spatial=4, kernel_temporal=4, linear_sizes=[20, 5], no_scalars=False, subsampling=10, tcn_hidden=15, tcn_layers=20, tcn_type='c')
...done
Training on 1724 shots, testing on 857 shots
Classical convolution with channels  2 16
Classical convolution with channels  16 8
Classical convolution with channels  8 4
Classical convolution with channels  4 2
InputBlock parameters:  14 2 64 ['c16', 'c8', 'c4', 'c2'] 4 [20, 5] 0.08
TCN parameters:  19 1 ['c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15', 'c15'] 4 0.08
29 epochs left to go

Training Epoch 0/30 starting at 2021-08-30 21:44:05.439679
[1]  [0/1724] loss: 2.566, ave_loss: 2.566
[2]  [20/1724] loss: 2.478, ave_loss: 2.522
[3]  [40/1724] loss: 1.516, ave_loss: 2.187
[4]  [60/1724] loss: 0.978, ave_loss: 1.884
[5]  [80/1724] loss: 0.738, ave_loss: 1.655
[6]  [100/1724] loss: 1.033, ave_loss: 1.551
[7]  [120/1724] loss: 0.864, ave_loss: 1.453
[8]  [140/1724] loss: 0.651, ave_loss: 1.353
[9]  [160/1724] loss: 1.116, ave_loss: 1.327
[10]  [180/1724] loss: 0.783, ave_loss: 1.272
[11]  [200/1724] loss: 0.674, ave_loss: 1.218
[12]  [220/1724] loss: 2.037, ave_loss: 1.286
[13]  [240/1724] loss: 0.525, ave_loss: 1.228
[14]  [260/1724] loss: 0.604, ave_loss: 1.183
[15]  [280/1724] loss: 0.715, ave_loss: 1.152
[16]  [300/1724] loss: 0.460, ave_loss: 1.109
[17]  [320/1724] loss: 0.877, ave_loss: 1.095
[18]  [340/1724] loss: 0.512, ave_loss: 1.063
[19]  [360/1724] loss: 0.682, ave_loss: 1.043
[20]  [380/1724] loss: 0.694, ave_loss: 1.025
[21]  [400/1724] loss: 0.650, ave_loss: 1.007
[22]  [420/1724] loss: 0.634, ave_loss: 0.990
[23]  [440/1724] loss: 0.564, ave_loss: 0.972
[24]  [460/1724] loss: 0.692, ave_loss: 0.960
[25]  [480/1724] loss: 0.736, ave_loss: 0.951
[26]  [500/1724] loss: 0.622, ave_loss: 0.938
[27]  [520/1724] loss: 0.619, ave_loss: 0.927
[28]  [540/1724] loss: 0.666, ave_loss: 0.917
[29]  [560/1724] loss: 0.619, ave_loss: 0.907
[30]  [580/1724] loss: 0.610, ave_loss: 0.897
[31]  [600/1724] loss: 0.715, ave_loss: 0.891
[32]  [620/1724] loss: 0.683, ave_loss: 0.885
[33]  [640/1724] loss: 0.779, ave_loss: 0.882
[34]  [660/1724] loss: 0.557, ave_loss: 0.872
[35]  [680/1724] loss: 0.681, ave_loss: 0.867
[36]  [700/1724] loss: 0.491, ave_loss: 0.856
[37]  [720/1724] loss: 0.620, ave_loss: 0.850
[38]  [740/1724] loss: 0.708, ave_loss: 0.846
[39]  [760/1724] loss: 0.414, ave_loss: 0.835
[40]  [780/1724] loss: 0.619, ave_loss: 0.830
[41]  [800/1724] loss: 0.436, ave_loss: 0.820
[42]  [820/1724] loss: 0.618, ave_loss: 0.815
[43]  [840/1724] loss: 0.672, ave_loss: 0.812
[44]  [860/1724] loss: 0.837, ave_loss: 0.812
[45]  [880/1724] loss: 0.483, ave_loss: 0.805
[46]  [900/1724] loss: 0.466, ave_loss: 0.798
[47]  [920/1724] loss: 0.536, ave_loss: 0.792
[48]  [940/1724] loss: 0.571, ave_loss: 0.788
[49]  [960/1724] loss: 0.694, ave_loss: 0.786
[50]  [980/1724] loss: 0.421, ave_loss: 0.778
[51]  [1000/1724] loss: 0.674, ave_loss: 0.776
[52]  [1020/1724] loss: 0.423, ave_loss: 0.770
[53]  [1040/1724] loss: 0.479, ave_loss: 0.764
[54]  [1060/1724] loss: 0.585, ave_loss: 0.761
[55]  [1080/1724] loss: 0.686, ave_loss: 0.759
[56]  [1100/1724] loss: 0.574, ave_loss: 0.756
[57]  [1120/1724] loss: 0.487, ave_loss: 0.751
[58]  [1140/1724] loss: 0.488, ave_loss: 0.747
[59]  [1160/1724] loss: 0.462, ave_loss: 0.742
[60]  [1180/1724] loss: 0.594, ave_loss: 0.739
[61]  [1200/1724] loss: 0.451, ave_loss: 0.735
[62]  [1220/1724] loss: 0.575, ave_loss: 0.732
[63]  [1240/1724] loss: 0.509, ave_loss: 0.729
[64]  [1260/1724] loss: 0.409, ave_loss: 0.724
[65]  [1280/1724] loss: 0.446, ave_loss: 0.719
[66]  [1300/1724] loss: 0.390, ave_loss: 0.714
[67]  [1320/1724] loss: 0.413, ave_loss: 0.710
[68]  [1340/1724] loss: 0.570, ave_loss: 0.708
[69]  [1360/1724] loss: 0.673, ave_loss: 0.707
[70]  [1380/1724] loss: 0.566, ave_loss: 0.705
[71]  [1400/1724] loss: 0.586, ave_loss: 0.704
[72]  [1420/1724] loss: 0.478, ave_loss: 0.700
[73]  [1440/1724] loss: 0.655, ave_loss: 0.700
[74]  [1460/1724] loss: 0.545, ave_loss: 0.698
[75]  [1480/1724] loss: 0.398, ave_loss: 0.694
[76]  [1500/1724] loss: 0.478, ave_loss: 0.691
[77]  [1520/1724] loss: 0.545, ave_loss: 0.689
[78]  [1540/1724] loss: 0.569, ave_loss: 0.687
[79]  [1560/1724] loss: 0.433, ave_loss: 0.684
[80]  [1580/1724] loss: 0.417, ave_loss: 0.681
[81]  [1600/1724] loss: 0.545, ave_loss: 0.679
[82]  [1620/1724] loss: 0.595, ave_loss: 0.678
[83]  [1640/1724] loss: 0.468, ave_loss: 0.676
[84]  [1660/1724] loss: 0.416, ave_loss: 0.673
[85]  [1680/1724] loss: 0.560, ave_loss: 0.671
[86]  [1700/1724] loss: 0.420, ave_loss: 0.668
[87]  [1720/1724] loss: 0.586, ave_loss: 0.667
[88]  [1740/1724] loss: 0.601, ave_loss: 0.667

Finished Training finishing at 2021-08-30 21:49:19.272124
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.666e-01
Validation Loss: 7.435e+05
Validation ROC: 0.6632
Saving model
27.979118329466356 epochs left to go

Training Epoch 1.0208816705336428/30 starting at 2021-08-30 21:51:27.430678
[1]  [0/1724] loss: 0.683, ave_loss: 0.683
[2]  [20/1724] loss: 0.596, ave_loss: 0.640
[3]  [40/1724] loss: 0.525, ave_loss: 0.601
[4]  [60/1724] loss: 0.476, ave_loss: 0.570
[5]  [80/1724] loss: 0.378, ave_loss: 0.532
[6]  [100/1724] loss: 0.490, ave_loss: 0.525
[7]  [120/1724] loss: 0.454, ave_loss: 0.514
[8]  [140/1724] loss: 0.453, ave_loss: 0.507
[9]  [160/1724] loss: 0.367, ave_loss: 0.491
[10]  [180/1724] loss: 0.325, ave_loss: 0.475
[11]  [200/1724] loss: 0.367, ave_loss: 0.465
[12]  [220/1724] loss: 0.504, ave_loss: 0.468
[13]  [240/1724] loss: 0.382, ave_loss: 0.461
[14]  [260/1724] loss: 0.548, ave_loss: 0.468
[15]  [280/1724] loss: 0.607, ave_loss: 0.477
[16]  [300/1724] loss: 0.588, ave_loss: 0.484
[17]  [320/1724] loss: 0.481, ave_loss: 0.484
[18]  [340/1724] loss: 0.418, ave_loss: 0.480
[19]  [360/1724] loss: 0.536, ave_loss: 0.483
[20]  [380/1724] loss: 0.402, ave_loss: 0.479
[21]  [400/1724] loss: 0.582, ave_loss: 0.484
[22]  [420/1724] loss: 0.434, ave_loss: 0.482
[23]  [440/1724] loss: 0.573, ave_loss: 0.486
[24]  [460/1724] loss: 0.357, ave_loss: 0.480
[25]  [480/1724] loss: 0.561, ave_loss: 0.483
[26]  [500/1724] loss: 0.606, ave_loss: 0.488
[27]  [520/1724] loss: 0.631, ave_loss: 0.493
[28]  [540/1724] loss: 0.404, ave_loss: 0.490
[29]  [560/1724] loss: 0.588, ave_loss: 0.494
[30]  [580/1724] loss: 0.400, ave_loss: 0.490
[31]  [600/1724] loss: 0.319, ave_loss: 0.485
[32]  [620/1724] loss: 0.551, ave_loss: 0.487
[33]  [640/1724] loss: 0.473, ave_loss: 0.487
[34]  [660/1724] loss: 0.495, ave_loss: 0.487
[35]  [680/1724] loss: 0.438, ave_loss: 0.485
[36]  [700/1724] loss: 0.408, ave_loss: 0.483
[37]  [720/1724] loss: 0.549, ave_loss: 0.485
[38]  [740/1724] loss: 0.518, ave_loss: 0.486
[39]  [760/1724] loss: 0.468, ave_loss: 0.485
[40]  [780/1724] loss: 0.574, ave_loss: 0.488
[41]  [800/1724] loss: 0.546, ave_loss: 0.489
[42]  [820/1724] loss: 0.501, ave_loss: 0.489
[43]  [840/1724] loss: 0.460, ave_loss: 0.489
[44]  [860/1724] loss: 0.416, ave_loss: 0.487
[45]  [880/1724] loss: 0.586, ave_loss: 0.489
[46]  [900/1724] loss: 0.460, ave_loss: 0.489
[47]  [920/1724] loss: 0.459, ave_loss: 0.488
[48]  [940/1724] loss: 0.509, ave_loss: 0.488
[49]  [960/1724] loss: 0.503, ave_loss: 0.489
[50]  [980/1724] loss: 0.438, ave_loss: 0.488
[51]  [1000/1724] loss: 0.295, ave_loss: 0.484
[52]  [1020/1724] loss: 0.496, ave_loss: 0.484
[53]  [1040/1724] loss: 0.418, ave_loss: 0.483
[54]  [1060/1724] loss: 0.573, ave_loss: 0.485
[55]  [1080/1724] loss: 0.503, ave_loss: 0.485
[56]  [1100/1724] loss: 0.496, ave_loss: 0.485
[57]  [1120/1724] loss: 0.481, ave_loss: 0.485
[58]  [1140/1724] loss: 0.575, ave_loss: 0.487
[59]  [1160/1724] loss: 0.461, ave_loss: 0.486
[60]  [1180/1724] loss: 0.441, ave_loss: 0.485
[61]  [1200/1724] loss: 0.461, ave_loss: 0.485
[62]  [1220/1724] loss: 0.443, ave_loss: 0.484
[63]  [1240/1724] loss: 0.623, ave_loss: 0.487
[64]  [1260/1724] loss: 0.441, ave_loss: 0.486
[65]  [1280/1724] loss: 0.383, ave_loss: 0.484
[66]  [1300/1724] loss: 0.494, ave_loss: 0.484
[67]  [1320/1724] loss: 0.596, ave_loss: 0.486
[68]  [1340/1724] loss: 0.503, ave_loss: 0.486
[69]  [1360/1724] loss: 0.495, ave_loss: 0.486
[70]  [1380/1724] loss: 0.616, ave_loss: 0.488
[71]  [1400/1724] loss: 0.294, ave_loss: 0.486
[72]  [1420/1724] loss: 0.479, ave_loss: 0.485
[73]  [1440/1724] loss: 0.554, ave_loss: 0.486
[74]  [1460/1724] loss: 0.384, ave_loss: 0.485
[75]  [1480/1724] loss: 0.386, ave_loss: 0.484
[76]  [1500/1724] loss: 0.374, ave_loss: 0.482
[77]  [1520/1724] loss: 0.452, ave_loss: 0.482
[78]  [1540/1724] loss: 0.557, ave_loss: 0.483
[79]  [1560/1724] loss: 0.459, ave_loss: 0.482
[80]  [1580/1724] loss: 0.521, ave_loss: 0.483
[81]  [1600/1724] loss: 0.443, ave_loss: 0.482
[82]  [1620/1724] loss: 0.397, ave_loss: 0.481
[83]  [1640/1724] loss: 0.476, ave_loss: 0.481
[84]  [1660/1724] loss: 0.454, ave_loss: 0.481
[85]  [1680/1724] loss: 0.528, ave_loss: 0.482
[86]  [1700/1724] loss: 0.512, ave_loss: 0.482
[87]  [1720/1724] loss: 0.542, ave_loss: 0.483
[88]  [1740/1724] loss: 0.404, ave_loss: 0.482

Finished Training finishing at 2021-08-30 21:56:22.436302
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.818e-01
Validation Loss: 7.130e+05
Validation ROC: 0.7038
Saving model
26.958236658932716 epochs left to go

Training Epoch 2.0417633410672855/30 starting at 2021-08-30 21:58:23.652974
[1]  [0/1724] loss: 0.443, ave_loss: 0.443
[2]  [20/1724] loss: 0.444, ave_loss: 0.443
[3]  [40/1724] loss: 0.474, ave_loss: 0.453
[4]  [60/1724] loss: 0.530, ave_loss: 0.473
[5]  [80/1724] loss: 0.394, ave_loss: 0.457
[6]  [100/1724] loss: 0.569, ave_loss: 0.476
[7]  [120/1724] loss: 0.328, ave_loss: 0.455
[8]  [140/1724] loss: 0.542, ave_loss: 0.465
[9]  [160/1724] loss: 0.460, ave_loss: 0.465
[10]  [180/1724] loss: 0.411, ave_loss: 0.459
[11]  [200/1724] loss: 0.427, ave_loss: 0.456
[12]  [220/1724] loss: 0.460, ave_loss: 0.457
[13]  [240/1724] loss: 0.621, ave_loss: 0.469
[14]  [260/1724] loss: 0.394, ave_loss: 0.464
[15]  [280/1724] loss: 0.516, ave_loss: 0.467
[16]  [300/1724] loss: 0.450, ave_loss: 0.466
[17]  [320/1724] loss: 0.338, ave_loss: 0.459
[18]  [340/1724] loss: 0.537, ave_loss: 0.463
[19]  [360/1724] loss: 0.507, ave_loss: 0.465
[20]  [380/1724] loss: 0.474, ave_loss: 0.466
[21]  [400/1724] loss: 0.482, ave_loss: 0.467
[22]  [420/1724] loss: 0.457, ave_loss: 0.466
[23]  [440/1724] loss: 0.319, ave_loss: 0.460
[24]  [460/1724] loss: 0.445, ave_loss: 0.459
[25]  [480/1724] loss: 0.411, ave_loss: 0.457
[26]  [500/1724] loss: 0.636, ave_loss: 0.464
[27]  [520/1724] loss: 0.480, ave_loss: 0.465
[28]  [540/1724] loss: 0.417, ave_loss: 0.463
[29]  [560/1724] loss: 0.525, ave_loss: 0.465
[30]  [580/1724] loss: 0.397, ave_loss: 0.463
[31]  [600/1724] loss: 0.430, ave_loss: 0.462
[32]  [620/1724] loss: 0.535, ave_loss: 0.464
[33]  [640/1724] loss: 0.398, ave_loss: 0.462
[34]  [660/1724] loss: 0.406, ave_loss: 0.461
[35]  [680/1724] loss: 0.420, ave_loss: 0.459
[36]  [700/1724] loss: 0.474, ave_loss: 0.460
[37]  [720/1724] loss: 0.322, ave_loss: 0.456
[38]  [740/1724] loss: 0.397, ave_loss: 0.454
[39]  [760/1724] loss: 0.378, ave_loss: 0.453
[40]  [780/1724] loss: 0.407, ave_loss: 0.451
[41]  [800/1724] loss: 0.671, ave_loss: 0.457
[42]  [820/1724] loss: 0.406, ave_loss: 0.456
[43]  [840/1724] loss: 0.383, ave_loss: 0.454
[44]  [860/1724] loss: 0.714, ave_loss: 0.460
[45]  [880/1724] loss: 0.407, ave_loss: 0.459
[46]  [900/1724] loss: 0.293, ave_loss: 0.455
[47]  [920/1724] loss: 0.418, ave_loss: 0.454
[48]  [940/1724] loss: 0.405, ave_loss: 0.453
[49]  [960/1724] loss: 0.445, ave_loss: 0.453
[50]  [980/1724] loss: 0.389, ave_loss: 0.452
[51]  [1000/1724] loss: 0.452, ave_loss: 0.452
[52]  [1020/1724] loss: 0.632, ave_loss: 0.455
[53]  [1040/1724] loss: 0.446, ave_loss: 0.455
[54]  [1060/1724] loss: 0.429, ave_loss: 0.455
[55]  [1080/1724] loss: 0.495, ave_loss: 0.455
[56]  [1100/1724] loss: 0.425, ave_loss: 0.455
[57]  [1120/1724] loss: 0.386, ave_loss: 0.454
[58]  [1140/1724] loss: 0.510, ave_loss: 0.455
[59]  [1160/1724] loss: 0.350, ave_loss: 0.453
[60]  [1180/1724] loss: 0.349, ave_loss: 0.451
[61]  [1200/1724] loss: 0.468, ave_loss: 0.451
[62]  [1220/1724] loss: 0.553, ave_loss: 0.453
[63]  [1240/1724] loss: 0.386, ave_loss: 0.452
[64]  [1260/1724] loss: 0.452, ave_loss: 0.452
[65]  [1280/1724] loss: 0.438, ave_loss: 0.452
[66]  [1300/1724] loss: 0.615, ave_loss: 0.454
[67]  [1320/1724] loss: 0.411, ave_loss: 0.453
[68]  [1340/1724] loss: 0.615, ave_loss: 0.456
[69]  [1360/1724] loss: 0.459, ave_loss: 0.456
[70]  [1380/1724] loss: 0.460, ave_loss: 0.456
[71]  [1400/1724] loss: 0.543, ave_loss: 0.457
[72]  [1420/1724] loss: 0.588, ave_loss: 0.459
[73]  [1440/1724] loss: 0.440, ave_loss: 0.459
[74]  [1460/1724] loss: 0.472, ave_loss: 0.459
[75]  [1480/1724] loss: 0.371, ave_loss: 0.458
[76]  [1500/1724] loss: 0.407, ave_loss: 0.457
[77]  [1520/1724] loss: 0.578, ave_loss: 0.459
[78]  [1540/1724] loss: 0.300, ave_loss: 0.457
[79]  [1560/1724] loss: 0.646, ave_loss: 0.459
[80]  [1580/1724] loss: 0.529, ave_loss: 0.460
[81]  [1600/1724] loss: 0.525, ave_loss: 0.461
[82]  [1620/1724] loss: 0.280, ave_loss: 0.458
[83]  [1640/1724] loss: 0.423, ave_loss: 0.458
[84]  [1660/1724] loss: 0.534, ave_loss: 0.459
[85]  [1680/1724] loss: 0.622, ave_loss: 0.461
[86]  [1700/1724] loss: 0.435, ave_loss: 0.461
[87]  [1720/1724] loss: 0.482, ave_loss: 0.461
[88]  [1740/1724] loss: 0.515, ave_loss: 0.461

Finished Training finishing at 2021-08-30 22:03:17.771935
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.615e-01
Validation Loss: 4.103e+05
Validation ROC: 0.7135
Saving model
25.937354988399072 epochs left to go

Training Epoch 3.062645011600928/30 starting at 2021-08-30 22:05:02.092281
[1]  [0/1724] loss: 0.473, ave_loss: 0.473
[2]  [20/1724] loss: 0.456, ave_loss: 0.464
[3]  [40/1724] loss: 0.496, ave_loss: 0.475
[4]  [60/1724] loss: 0.547, ave_loss: 0.493
[5]  [80/1724] loss: 0.546, ave_loss: 0.504
[6]  [100/1724] loss: 0.483, ave_loss: 0.500
[7]  [120/1724] loss: 0.520, ave_loss: 0.503
[8]  [140/1724] loss: 0.423, ave_loss: 0.493
[9]  [160/1724] loss: 0.541, ave_loss: 0.498
[10]  [180/1724] loss: 0.395, ave_loss: 0.488
[11]  [200/1724] loss: 0.507, ave_loss: 0.490
[12]  [220/1724] loss: 0.507, ave_loss: 0.491
[13]  [240/1724] loss: 0.559, ave_loss: 0.496
[14]  [260/1724] loss: 0.424, ave_loss: 0.491
[15]  [280/1724] loss: 0.433, ave_loss: 0.487
[16]  [300/1724] loss: 0.343, ave_loss: 0.478
[17]  [320/1724] loss: 0.532, ave_loss: 0.481
[18]  [340/1724] loss: 0.492, ave_loss: 0.482
[19]  [360/1724] loss: 0.409, ave_loss: 0.478
[20]  [380/1724] loss: 0.468, ave_loss: 0.478
[21]  [400/1724] loss: 0.516, ave_loss: 0.480
[22]  [420/1724] loss: 0.471, ave_loss: 0.479
[23]  [440/1724] loss: 0.397, ave_loss: 0.476
[24]  [460/1724] loss: 0.497, ave_loss: 0.476
[25]  [480/1724] loss: 0.432, ave_loss: 0.475
[26]  [500/1724] loss: 0.546, ave_loss: 0.477
[27]  [520/1724] loss: 0.315, ave_loss: 0.471
[28]  [540/1724] loss: 0.541, ave_loss: 0.474
[29]  [560/1724] loss: 0.609, ave_loss: 0.479
[30]  [580/1724] loss: 0.473, ave_loss: 0.478
[31]  [600/1724] loss: 0.379, ave_loss: 0.475
[32]  [620/1724] loss: 0.388, ave_loss: 0.472
[33]  [640/1724] loss: 0.419, ave_loss: 0.471
[34]  [660/1724] loss: 0.483, ave_loss: 0.471
[35]  [680/1724] loss: 0.366, ave_loss: 0.468
[36]  [700/1724] loss: 0.443, ave_loss: 0.467
[37]  [720/1724] loss: 0.568, ave_loss: 0.470
[38]  [740/1724] loss: 0.480, ave_loss: 0.470
[39]  [760/1724] loss: 0.380, ave_loss: 0.468
[40]  [780/1724] loss: 0.548, ave_loss: 0.470
[41]  [800/1724] loss: 0.407, ave_loss: 0.469
[42]  [820/1724] loss: 0.346, ave_loss: 0.466
[43]  [840/1724] loss: 0.316, ave_loss: 0.462
[44]  [860/1724] loss: 0.487, ave_loss: 0.463
[45]  [880/1724] loss: 0.449, ave_loss: 0.462
[46]  [900/1724] loss: 0.356, ave_loss: 0.460
[47]  [920/1724] loss: 0.528, ave_loss: 0.462
[48]  [940/1724] loss: 0.479, ave_loss: 0.462
[49]  [960/1724] loss: 0.389, ave_loss: 0.460
[50]  [980/1724] loss: 0.332, ave_loss: 0.458
[51]  [1000/1724] loss: 0.482, ave_loss: 0.458
[52]  [1020/1724] loss: 0.446, ave_loss: 0.458
[53]  [1040/1724] loss: 0.408, ave_loss: 0.457
[54]  [1060/1724] loss: 0.339, ave_loss: 0.455
[55]  [1080/1724] loss: 0.403, ave_loss: 0.454
[56]  [1100/1724] loss: 0.412, ave_loss: 0.453
[57]  [1120/1724] loss: 0.466, ave_loss: 0.454
[58]  [1140/1724] loss: 0.273, ave_loss: 0.450
[59]  [1160/1724] loss: 0.356, ave_loss: 0.449
[60]  [1180/1724] loss: 0.446, ave_loss: 0.449
[61]  [1200/1724] loss: 0.408, ave_loss: 0.448
[62]  [1220/1724] loss: 0.524, ave_loss: 0.449
[63]  [1240/1724] loss: 0.399, ave_loss: 0.449
[64]  [1260/1724] loss: 0.501, ave_loss: 0.449
[65]  [1280/1724] loss: 0.339, ave_loss: 0.448
[66]  [1300/1724] loss: 0.658, ave_loss: 0.451
[67]  [1320/1724] loss: 0.292, ave_loss: 0.448
[68]  [1340/1724] loss: 0.374, ave_loss: 0.447
[69]  [1360/1724] loss: 0.397, ave_loss: 0.447
[70]  [1380/1724] loss: 0.382, ave_loss: 0.446
[71]  [1400/1724] loss: 0.314, ave_loss: 0.444
[72]  [1420/1724] loss: 0.523, ave_loss: 0.445
[73]  [1440/1724] loss: 0.389, ave_loss: 0.444
[74]  [1460/1724] loss: 0.418, ave_loss: 0.444
[75]  [1480/1724] loss: 0.561, ave_loss: 0.445
[76]  [1500/1724] loss: 0.457, ave_loss: 0.446
[77]  [1520/1724] loss: 0.534, ave_loss: 0.447
[78]  [1540/1724] loss: 0.367, ave_loss: 0.446
[79]  [1560/1724] loss: 0.481, ave_loss: 0.446
[80]  [1580/1724] loss: 0.310, ave_loss: 0.444
[81]  [1600/1724] loss: 0.287, ave_loss: 0.443
[82]  [1620/1724] loss: 0.596, ave_loss: 0.444
[83]  [1640/1724] loss: 0.434, ave_loss: 0.444
[84]  [1660/1724] loss: 0.348, ave_loss: 0.443
[85]  [1680/1724] loss: 0.378, ave_loss: 0.442
[86]  [1700/1724] loss: 0.378, ave_loss: 0.442
[87]  [1720/1724] loss: 0.319, ave_loss: 0.440
[88]  [1740/1724] loss: 0.347, ave_loss: 0.439

Finished Training finishing at 2021-08-30 22:09:40.545744
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.391e-01
Validation Loss: 3.147e+05
Validation ROC: 0.7384
Saving model
24.916473317865428 epochs left to go

Training Epoch 4.083526682134571/30 starting at 2021-08-30 22:11:44.353673
[1]  [0/1724] loss: 0.456, ave_loss: 0.456
[2]  [20/1724] loss: 0.464, ave_loss: 0.460
[3]  [40/1724] loss: 0.308, ave_loss: 0.409
[4]  [60/1724] loss: 0.480, ave_loss: 0.427
[5]  [80/1724] loss: 0.325, ave_loss: 0.407
[6]  [100/1724] loss: 0.390, ave_loss: 0.404
[7]  [120/1724] loss: 0.422, ave_loss: 0.407
[8]  [140/1724] loss: 0.622, ave_loss: 0.434
[9]  [160/1724] loss: 0.415, ave_loss: 0.431
[10]  [180/1724] loss: 0.351, ave_loss: 0.423
[11]  [200/1724] loss: 0.413, ave_loss: 0.423
[12]  [220/1724] loss: 0.500, ave_loss: 0.429
[13]  [240/1724] loss: 0.455, ave_loss: 0.431
[14]  [260/1724] loss: 0.424, ave_loss: 0.431
[15]  [280/1724] loss: 0.497, ave_loss: 0.435
[16]  [300/1724] loss: 0.399, ave_loss: 0.433
[17]  [320/1724] loss: 0.422, ave_loss: 0.432
[18]  [340/1724] loss: 0.388, ave_loss: 0.430
[19]  [360/1724] loss: 0.336, ave_loss: 0.425
[20]  [380/1724] loss: 0.628, ave_loss: 0.435
[21]  [400/1724] loss: 0.582, ave_loss: 0.442
[22]  [420/1724] loss: 0.356, ave_loss: 0.438
[23]  [440/1724] loss: 0.566, ave_loss: 0.443
[24]  [460/1724] loss: 0.501, ave_loss: 0.446
[25]  [480/1724] loss: 0.501, ave_loss: 0.448
[26]  [500/1724] loss: 0.411, ave_loss: 0.447
[27]  [520/1724] loss: 0.424, ave_loss: 0.446
[28]  [540/1724] loss: 0.344, ave_loss: 0.442
[29]  [560/1724] loss: 0.462, ave_loss: 0.443
[30]  [580/1724] loss: 0.473, ave_loss: 0.444
[31]  [600/1724] loss: 0.455, ave_loss: 0.444
[32]  [620/1724] loss: 0.294, ave_loss: 0.440
[33]  [640/1724] loss: 0.353, ave_loss: 0.437
[34]  [660/1724] loss: 0.414, ave_loss: 0.436
[35]  [680/1724] loss: 0.369, ave_loss: 0.434
[36]  [700/1724] loss: 0.392, ave_loss: 0.433
[37]  [720/1724] loss: 0.378, ave_loss: 0.432
[38]  [740/1724] loss: 0.345, ave_loss: 0.429
[39]  [760/1724] loss: 0.459, ave_loss: 0.430
[40]  [780/1724] loss: 0.607, ave_loss: 0.435
[41]  [800/1724] loss: 0.352, ave_loss: 0.433
[42]  [820/1724] loss: 0.391, ave_loss: 0.432
[43]  [840/1724] loss: 0.562, ave_loss: 0.435
[44]  [860/1724] loss: 0.388, ave_loss: 0.434
[45]  [880/1724] loss: 0.359, ave_loss: 0.432
[46]  [900/1724] loss: 0.421, ave_loss: 0.432
[47]  [920/1724] loss: 0.269, ave_loss: 0.428
[48]  [940/1724] loss: 0.437, ave_loss: 0.428
[49]  [960/1724] loss: 0.408, ave_loss: 0.428
[50]  [980/1724] loss: 0.455, ave_loss: 0.428
[51]  [1000/1724] loss: 0.293, ave_loss: 0.426
[52]  [1020/1724] loss: 0.584, ave_loss: 0.429
[53]  [1040/1724] loss: 0.614, ave_loss: 0.432
[54]  [1060/1724] loss: 0.440, ave_loss: 0.432
[55]  [1080/1724] loss: 0.436, ave_loss: 0.433
[56]  [1100/1724] loss: 0.346, ave_loss: 0.431
[57]  [1120/1724] loss: 0.366, ave_loss: 0.430
[58]  [1140/1724] loss: 0.365, ave_loss: 0.429
[59]  [1160/1724] loss: 0.405, ave_loss: 0.428
[60]  [1180/1724] loss: 0.508, ave_loss: 0.430
[61]  [1200/1724] loss: 0.515, ave_loss: 0.431
[62]  [1220/1724] loss: 0.421, ave_loss: 0.431
[63]  [1240/1724] loss: 0.386, ave_loss: 0.430
[64]  [1260/1724] loss: 0.418, ave_loss: 0.430
[65]  [1280/1724] loss: 0.381, ave_loss: 0.429
[66]  [1300/1724] loss: 0.593, ave_loss: 0.432
[67]  [1320/1724] loss: 0.498, ave_loss: 0.433
[68]  [1340/1724] loss: 0.395, ave_loss: 0.432
[69]  [1360/1724] loss: 0.550, ave_loss: 0.434
[70]  [1380/1724] loss: 0.411, ave_loss: 0.434
[71]  [1400/1724] loss: 0.476, ave_loss: 0.434
[72]  [1420/1724] loss: 0.327, ave_loss: 0.433
[73]  [1440/1724] loss: 0.359, ave_loss: 0.432
[74]  [1460/1724] loss: 0.443, ave_loss: 0.432
[75]  [1480/1724] loss: 0.237, ave_loss: 0.429
[76]  [1500/1724] loss: 0.298, ave_loss: 0.427
[77]  [1520/1724] loss: 0.369, ave_loss: 0.427
[78]  [1540/1724] loss: 0.380, ave_loss: 0.426
[79]  [1560/1724] loss: 0.465, ave_loss: 0.427
[80]  [1580/1724] loss: 0.602, ave_loss: 0.429
[81]  [1600/1724] loss: 0.397, ave_loss: 0.428
[82]  [1620/1724] loss: 0.378, ave_loss: 0.428
[83]  [1640/1724] loss: 0.374, ave_loss: 0.427
[84]  [1660/1724] loss: 0.447, ave_loss: 0.427
[85]  [1680/1724] loss: 0.354, ave_loss: 0.427
[86]  [1700/1724] loss: 0.346, ave_loss: 0.426
[87]  [1720/1724] loss: 0.416, ave_loss: 0.425
[88]  [1740/1724] loss: 0.493, ave_loss: 0.426

Finished Training finishing at 2021-08-30 22:16:23.291733
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.263e-01
Validation Loss: 2.622e+05
Validation ROC: 0.7410
Saving model
23.895591647331788 epochs left to go

Training Epoch 5.104408352668213/30 starting at 2021-08-30 22:18:18.085046
[1]  [0/1724] loss: 0.570, ave_loss: 0.570
[2]  [20/1724] loss: 0.343, ave_loss: 0.456
[3]  [40/1724] loss: 0.513, ave_loss: 0.475
[4]  [60/1724] loss: 0.385, ave_loss: 0.453
[5]  [80/1724] loss: 0.548, ave_loss: 0.472
[6]  [100/1724] loss: 0.344, ave_loss: 0.450
[7]  [120/1724] loss: 0.309, ave_loss: 0.430
[8]  [140/1724] loss: 0.354, ave_loss: 0.421
[9]  [160/1724] loss: 0.322, ave_loss: 0.410
[10]  [180/1724] loss: 0.399, ave_loss: 0.409
[11]  [200/1724] loss: 0.281, ave_loss: 0.397
[12]  [220/1724] loss: 0.192, ave_loss: 0.380
[13]  [240/1724] loss: 0.488, ave_loss: 0.388
[14]  [260/1724] loss: 0.400, ave_loss: 0.389
[15]  [280/1724] loss: 0.310, ave_loss: 0.384
[16]  [300/1724] loss: 0.298, ave_loss: 0.379
[17]  [320/1724] loss: 0.487, ave_loss: 0.385
[18]  [340/1724] loss: 0.526, ave_loss: 0.393
[19]  [360/1724] loss: 0.451, ave_loss: 0.396
[20]  [380/1724] loss: 0.341, ave_loss: 0.393
[21]  [400/1724] loss: 0.415, ave_loss: 0.394
[22]  [420/1724] loss: 0.435, ave_loss: 0.396
[23]  [440/1724] loss: 0.341, ave_loss: 0.394
[24]  [460/1724] loss: 0.440, ave_loss: 0.395
[25]  [480/1724] loss: 0.350, ave_loss: 0.394
[26]  [500/1724] loss: 0.709, ave_loss: 0.406
[27]  [520/1724] loss: 0.350, ave_loss: 0.404
[28]  [540/1724] loss: 0.707, ave_loss: 0.415
[29]  [560/1724] loss: 0.466, ave_loss: 0.416
[30]  [580/1724] loss: 0.400, ave_loss: 0.416
[31]  [600/1724] loss: 0.435, ave_loss: 0.416
[32]  [620/1724] loss: 0.367, ave_loss: 0.415
[33]  [640/1724] loss: 0.415, ave_loss: 0.415
[34]  [660/1724] loss: 0.455, ave_loss: 0.416
[35]  [680/1724] loss: 0.419, ave_loss: 0.416
[36]  [700/1724] loss: 0.503, ave_loss: 0.419
[37]  [720/1724] loss: 0.412, ave_loss: 0.418
[38]  [740/1724] loss: 0.469, ave_loss: 0.420
[39]  [760/1724] loss: 0.506, ave_loss: 0.422
[40]  [780/1724] loss: 0.487, ave_loss: 0.424
[41]  [800/1724] loss: 0.402, ave_loss: 0.423
[42]  [820/1724] loss: 0.467, ave_loss: 0.424
[43]  [840/1724] loss: 0.392, ave_loss: 0.423
[44]  [860/1724] loss: 0.366, ave_loss: 0.422
[45]  [880/1724] loss: 0.405, ave_loss: 0.422
[46]  [900/1724] loss: 0.617, ave_loss: 0.426
[47]  [920/1724] loss: 0.420, ave_loss: 0.426
[48]  [940/1724] loss: 0.321, ave_loss: 0.424
[49]  [960/1724] loss: 0.702, ave_loss: 0.429
[50]  [980/1724] loss: 0.525, ave_loss: 0.431
[51]  [1000/1724] loss: 0.466, ave_loss: 0.432
[52]  [1020/1724] loss: 0.351, ave_loss: 0.430
[53]  [1040/1724] loss: 0.372, ave_loss: 0.429
[54]  [1060/1724] loss: 0.339, ave_loss: 0.428
[55]  [1080/1724] loss: 0.307, ave_loss: 0.425
[56]  [1100/1724] loss: 0.548, ave_loss: 0.428
[57]  [1120/1724] loss: 0.435, ave_loss: 0.428
[58]  [1140/1724] loss: 0.486, ave_loss: 0.429
[59]  [1160/1724] loss: 0.474, ave_loss: 0.429
[60]  [1180/1724] loss: 0.323, ave_loss: 0.428
[61]  [1200/1724] loss: 0.364, ave_loss: 0.427
[62]  [1220/1724] loss: 0.469, ave_loss: 0.427
[63]  [1240/1724] loss: 0.344, ave_loss: 0.426
[64]  [1260/1724] loss: 0.403, ave_loss: 0.426
[65]  [1280/1724] loss: 0.610, ave_loss: 0.428
[66]  [1300/1724] loss: 0.517, ave_loss: 0.430
[67]  [1320/1724] loss: 0.498, ave_loss: 0.431
[68]  [1340/1724] loss: 0.424, ave_loss: 0.431
[69]  [1360/1724] loss: 0.300, ave_loss: 0.429
[70]  [1380/1724] loss: 0.394, ave_loss: 0.428
[71]  [1400/1724] loss: 0.448, ave_loss: 0.429
[72]  [1420/1724] loss: 0.378, ave_loss: 0.428
[73]  [1440/1724] loss: 0.377, ave_loss: 0.427
[74]  [1460/1724] loss: 0.387, ave_loss: 0.427
[75]  [1480/1724] loss: 0.418, ave_loss: 0.427
[76]  [1500/1724] loss: 0.494, ave_loss: 0.427
[77]  [1520/1724] loss: 0.270, ave_loss: 0.425
[78]  [1540/1724] loss: 0.385, ave_loss: 0.425
[79]  [1560/1724] loss: 0.428, ave_loss: 0.425
[80]  [1580/1724] loss: 0.391, ave_loss: 0.424
[81]  [1600/1724] loss: 0.525, ave_loss: 0.426
[82]  [1620/1724] loss: 0.487, ave_loss: 0.426
[83]  [1640/1724] loss: 0.551, ave_loss: 0.428
[84]  [1660/1724] loss: 0.466, ave_loss: 0.428
[85]  [1680/1724] loss: 0.324, ave_loss: 0.427
[86]  [1700/1724] loss: 0.348, ave_loss: 0.426
[87]  [1720/1724] loss: 0.448, ave_loss: 0.427
[88]  [1740/1724] loss: 0.530, ave_loss: 0.428

Finished Training finishing at 2021-08-30 22:22:57.221877
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.277e-01
Validation Loss: 2.786e+05
Validation ROC: 0.7381
No improvement, still saving model
22.874709976798144 epochs left to go

Training Epoch 6.125290023201856/30 starting at 2021-08-30 22:24:36.901010
[1]  [0/1724] loss: 0.522, ave_loss: 0.522
[2]  [20/1724] loss: 0.277, ave_loss: 0.400
[3]  [40/1724] loss: 0.256, ave_loss: 0.352
[4]  [60/1724] loss: 0.484, ave_loss: 0.385
[5]  [80/1724] loss: 0.391, ave_loss: 0.386
[6]  [100/1724] loss: 0.422, ave_loss: 0.392
[7]  [120/1724] loss: 0.453, ave_loss: 0.401
[8]  [140/1724] loss: 0.387, ave_loss: 0.399
[9]  [160/1724] loss: 0.405, ave_loss: 0.400
[10]  [180/1724] loss: 0.476, ave_loss: 0.407
[11]  [200/1724] loss: 0.447, ave_loss: 0.411
[12]  [220/1724] loss: 0.321, ave_loss: 0.404
[13]  [240/1724] loss: 0.332, ave_loss: 0.398
[14]  [260/1724] loss: 0.373, ave_loss: 0.396
[15]  [280/1724] loss: 0.422, ave_loss: 0.398
[16]  [300/1724] loss: 0.413, ave_loss: 0.399
[17]  [320/1724] loss: 0.325, ave_loss: 0.395
[18]  [340/1724] loss: 0.367, ave_loss: 0.393
[19]  [360/1724] loss: 0.571, ave_loss: 0.402
[20]  [380/1724] loss: 0.387, ave_loss: 0.402
[21]  [400/1724] loss: 0.358, ave_loss: 0.400
[22]  [420/1724] loss: 0.412, ave_loss: 0.400
[23]  [440/1724] loss: 0.453, ave_loss: 0.402
[24]  [460/1724] loss: 0.609, ave_loss: 0.411
[25]  [480/1724] loss: 0.293, ave_loss: 0.406
[26]  [500/1724] loss: 0.379, ave_loss: 0.405
[27]  [520/1724] loss: 0.419, ave_loss: 0.406
[28]  [540/1724] loss: 0.423, ave_loss: 0.406
[29]  [560/1724] loss: 0.543, ave_loss: 0.411
[30]  [580/1724] loss: 0.552, ave_loss: 0.416
[31]  [600/1724] loss: 0.519, ave_loss: 0.419
[32]  [620/1724] loss: 0.458, ave_loss: 0.420
[33]  [640/1724] loss: 0.446, ave_loss: 0.421
[34]  [660/1724] loss: 0.409, ave_loss: 0.421
[35]  [680/1724] loss: 0.429, ave_loss: 0.421
[36]  [700/1724] loss: 0.363, ave_loss: 0.419
[37]  [720/1724] loss: 0.416, ave_loss: 0.419
[38]  [740/1724] loss: 0.546, ave_loss: 0.423
[39]  [760/1724] loss: 0.368, ave_loss: 0.421
[40]  [780/1724] loss: 0.534, ave_loss: 0.424
[41]  [800/1724] loss: 0.354, ave_loss: 0.422
[42]  [820/1724] loss: 0.460, ave_loss: 0.423
[43]  [840/1724] loss: 0.352, ave_loss: 0.422
[44]  [860/1724] loss: 0.458, ave_loss: 0.422
[45]  [880/1724] loss: 0.518, ave_loss: 0.424
[46]  [900/1724] loss: 0.515, ave_loss: 0.426
[47]  [920/1724] loss: 0.382, ave_loss: 0.426
[48]  [940/1724] loss: 0.549, ave_loss: 0.428
[49]  [960/1724] loss: 0.449, ave_loss: 0.429
[50]  [980/1724] loss: 0.404, ave_loss: 0.428
[51]  [1000/1724] loss: 0.371, ave_loss: 0.427
[52]  [1020/1724] loss: 0.320, ave_loss: 0.425
[53]  [1040/1724] loss: 0.284, ave_loss: 0.422
[54]  [1060/1724] loss: 0.445, ave_loss: 0.423
[55]  [1080/1724] loss: 0.475, ave_loss: 0.424
[56]  [1100/1724] loss: 0.403, ave_loss: 0.423
[57]  [1120/1724] loss: 0.475, ave_loss: 0.424
[58]  [1140/1724] loss: 0.461, ave_loss: 0.425
[59]  [1160/1724] loss: 0.450, ave_loss: 0.425
[60]  [1180/1724] loss: 0.545, ave_loss: 0.427
[61]  [1200/1724] loss: 0.353, ave_loss: 0.426
[62]  [1220/1724] loss: 0.302, ave_loss: 0.424
[63]  [1240/1724] loss: 0.348, ave_loss: 0.423
[64]  [1260/1724] loss: 0.500, ave_loss: 0.424
[65]  [1280/1724] loss: 0.383, ave_loss: 0.423
[66]  [1300/1724] loss: 0.479, ave_loss: 0.424
[67]  [1320/1724] loss: 0.318, ave_loss: 0.423
[68]  [1340/1724] loss: 0.413, ave_loss: 0.422
[69]  [1360/1724] loss: 0.399, ave_loss: 0.422
[70]  [1380/1724] loss: 0.412, ave_loss: 0.422
[71]  [1400/1724] loss: 0.420, ave_loss: 0.422
[72]  [1420/1724] loss: 0.566, ave_loss: 0.424
[73]  [1440/1724] loss: 0.489, ave_loss: 0.425
[74]  [1460/1724] loss: 0.593, ave_loss: 0.427
[75]  [1480/1724] loss: 0.559, ave_loss: 0.429
[76]  [1500/1724] loss: 0.399, ave_loss: 0.428
[77]  [1520/1724] loss: 0.303, ave_loss: 0.427
[78]  [1540/1724] loss: 0.419, ave_loss: 0.427
[79]  [1560/1724] loss: 0.228, ave_loss: 0.424
[80]  [1580/1724] loss: 0.571, ave_loss: 0.426
[81]  [1600/1724] loss: 0.405, ave_loss: 0.426
[82]  [1620/1724] loss: 0.411, ave_loss: 0.426
[83]  [1640/1724] loss: 0.418, ave_loss: 0.426
[84]  [1660/1724] loss: 0.245, ave_loss: 0.423
[85]  [1680/1724] loss: 0.490, ave_loss: 0.424
[86]  [1700/1724] loss: 0.528, ave_loss: 0.425
[87]  [1720/1724] loss: 0.664, ave_loss: 0.428
[88]  [1740/1724] loss: 0.734, ave_loss: 0.432

Finished Training finishing at 2021-08-30 22:29:19.145185
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.316e-01
Validation Loss: 2.485e+05
Validation ROC: 0.7569
Saving model
21.8538283062645 epochs left to go

Training Epoch 7.146171693735499/30 starting at 2021-08-30 22:31:22.588618
[1]  [0/1724] loss: 0.438, ave_loss: 0.438
[2]  [20/1724] loss: 0.379, ave_loss: 0.409
[3]  [40/1724] loss: 0.506, ave_loss: 0.441
[4]  [60/1724] loss: 0.438, ave_loss: 0.440
[5]  [80/1724] loss: 0.269, ave_loss: 0.406
[6]  [100/1724] loss: 0.462, ave_loss: 0.415
[7]  [120/1724] loss: 0.216, ave_loss: 0.387
[8]  [140/1724] loss: 0.379, ave_loss: 0.386
[9]  [160/1724] loss: 0.533, ave_loss: 0.402
[10]  [180/1724] loss: 0.423, ave_loss: 0.404
[11]  [200/1724] loss: 0.384, ave_loss: 0.402
[12]  [220/1724] loss: 0.547, ave_loss: 0.415
[13]  [240/1724] loss: 0.458, ave_loss: 0.418
[14]  [260/1724] loss: 0.459, ave_loss: 0.421
[15]  [280/1724] loss: 0.451, ave_loss: 0.423
[16]  [300/1724] loss: 0.472, ave_loss: 0.426
[17]  [320/1724] loss: 0.571, ave_loss: 0.434
[18]  [340/1724] loss: 0.495, ave_loss: 0.438
[19]  [360/1724] loss: 0.456, ave_loss: 0.439
[20]  [380/1724] loss: 0.515, ave_loss: 0.443
[21]  [400/1724] loss: 0.383, ave_loss: 0.440
[22]  [420/1724] loss: 0.448, ave_loss: 0.440
[23]  [440/1724] loss: 0.445, ave_loss: 0.440
[24]  [460/1724] loss: 0.351, ave_loss: 0.437
[25]  [480/1724] loss: 0.399, ave_loss: 0.435
[26]  [500/1724] loss: 0.359, ave_loss: 0.432
[27]  [520/1724] loss: 0.323, ave_loss: 0.428
[28]  [540/1724] loss: 0.373, ave_loss: 0.426
[29]  [560/1724] loss: 0.394, ave_loss: 0.425
[30]  [580/1724] loss: 0.389, ave_loss: 0.424
[31]  [600/1724] loss: 0.489, ave_loss: 0.426
[32]  [620/1724] loss: 0.357, ave_loss: 0.424
[33]  [640/1724] loss: 0.461, ave_loss: 0.425
[34]  [660/1724] loss: 0.331, ave_loss: 0.422
[35]  [680/1724] loss: 0.366, ave_loss: 0.421
[36]  [700/1724] loss: 0.469, ave_loss: 0.422
[37]  [720/1724] loss: 0.420, ave_loss: 0.422
[38]  [740/1724] loss: 0.370, ave_loss: 0.421
[39]  [760/1724] loss: 0.221, ave_loss: 0.415
[40]  [780/1724] loss: 0.352, ave_loss: 0.414
[41]  [800/1724] loss: 0.307, ave_loss: 0.411
[42]  [820/1724] loss: 0.346, ave_loss: 0.410
[43]  [840/1724] loss: 0.427, ave_loss: 0.410
[44]  [860/1724] loss: 0.349, ave_loss: 0.409
[45]  [880/1724] loss: 0.466, ave_loss: 0.410
[46]  [900/1724] loss: 0.317, ave_loss: 0.408
[47]  [920/1724] loss: 0.522, ave_loss: 0.410
[48]  [940/1724] loss: 0.296, ave_loss: 0.408
[49]  [960/1724] loss: 0.578, ave_loss: 0.411
[50]  [980/1724] loss: 0.237, ave_loss: 0.408
[51]  [1000/1724] loss: 0.395, ave_loss: 0.408
[52]  [1020/1724] loss: 0.526, ave_loss: 0.410
[53]  [1040/1724] loss: 0.342, ave_loss: 0.409
[54]  [1060/1724] loss: 0.418, ave_loss: 0.409
[55]  [1080/1724] loss: 0.526, ave_loss: 0.411
[56]  [1100/1724] loss: 0.250, ave_loss: 0.408
[57]  [1120/1724] loss: 0.873, ave_loss: 0.416
[58]  [1140/1724] loss: 0.462, ave_loss: 0.417
[59]  [1160/1724] loss: 0.352, ave_loss: 0.416
[60]  [1180/1724] loss: 0.308, ave_loss: 0.414
[61]  [1200/1724] loss: 0.343, ave_loss: 0.413
[62]  [1220/1724] loss: 0.399, ave_loss: 0.413
[63]  [1240/1724] loss: 0.424, ave_loss: 0.413
[64]  [1260/1724] loss: 0.419, ave_loss: 0.413
[65]  [1280/1724] loss: 0.473, ave_loss: 0.414
[66]  [1300/1724] loss: 0.571, ave_loss: 0.416
[67]  [1320/1724] loss: 0.455, ave_loss: 0.417
[68]  [1340/1724] loss: 0.372, ave_loss: 0.416
[69]  [1360/1724] loss: 0.241, ave_loss: 0.414
[70]  [1380/1724] loss: 0.308, ave_loss: 0.412
[71]  [1400/1724] loss: 0.515, ave_loss: 0.414
[72]  [1420/1724] loss: 0.514, ave_loss: 0.415
[73]  [1440/1724] loss: 0.356, ave_loss: 0.414
[74]  [1460/1724] loss: 0.448, ave_loss: 0.415
[75]  [1480/1724] loss: 0.386, ave_loss: 0.414
[76]  [1500/1724] loss: 0.373, ave_loss: 0.414
[77]  [1520/1724] loss: 0.495, ave_loss: 0.415
[78]  [1540/1724] loss: 0.407, ave_loss: 0.415
[79]  [1560/1724] loss: 0.421, ave_loss: 0.415
[80]  [1580/1724] loss: 0.296, ave_loss: 0.413
[81]  [1600/1724] loss: 0.448, ave_loss: 0.414
[82]  [1620/1724] loss: 0.333, ave_loss: 0.413
[83]  [1640/1724] loss: 0.450, ave_loss: 0.413
[84]  [1660/1724] loss: 0.409, ave_loss: 0.413
[85]  [1680/1724] loss: 0.302, ave_loss: 0.412
[86]  [1700/1724] loss: 0.438, ave_loss: 0.412
[87]  [1720/1724] loss: 0.372, ave_loss: 0.412
[88]  [1740/1724] loss: 0.400, ave_loss: 0.412

Finished Training finishing at 2021-08-30 22:36:18.286918
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.115e-01
Validation Loss: 2.529e+05
Validation ROC: 0.7645
Saving model
20.832946635730856 epochs left to go

Training Epoch 8.167053364269142/30 starting at 2021-08-30 22:37:53.716533
[1]  [0/1724] loss: 0.285, ave_loss: 0.285
[2]  [20/1724] loss: 0.454, ave_loss: 0.369
[3]  [40/1724] loss: 0.346, ave_loss: 0.361
[4]  [60/1724] loss: 0.315, ave_loss: 0.350
[5]  [80/1724] loss: 0.409, ave_loss: 0.362
[6]  [100/1724] loss: 0.531, ave_loss: 0.390
[7]  [120/1724] loss: 0.328, ave_loss: 0.381
[8]  [140/1724] loss: 0.432, ave_loss: 0.387
[9]  [160/1724] loss: 0.396, ave_loss: 0.388
[10]  [180/1724] loss: 0.305, ave_loss: 0.380
[11]  [200/1724] loss: 0.285, ave_loss: 0.371
[12]  [220/1724] loss: 0.489, ave_loss: 0.381
[13]  [240/1724] loss: 0.304, ave_loss: 0.375
[14]  [260/1724] loss: 0.403, ave_loss: 0.377
[15]  [280/1724] loss: 0.394, ave_loss: 0.378
[16]  [300/1724] loss: 0.328, ave_loss: 0.375
[17]  [320/1724] loss: 0.347, ave_loss: 0.374
[18]  [340/1724] loss: 0.500, ave_loss: 0.381
[19]  [360/1724] loss: 0.406, ave_loss: 0.382
[20]  [380/1724] loss: 0.414, ave_loss: 0.384
[21]  [400/1724] loss: 0.410, ave_loss: 0.385
[22]  [420/1724] loss: 0.363, ave_loss: 0.384
[23]  [440/1724] loss: 0.435, ave_loss: 0.386
[24]  [460/1724] loss: 0.376, ave_loss: 0.386
[25]  [480/1724] loss: 0.370, ave_loss: 0.385
[26]  [500/1724] loss: 0.392, ave_loss: 0.385
[27]  [520/1724] loss: 0.508, ave_loss: 0.390
[28]  [540/1724] loss: 0.345, ave_loss: 0.388
[29]  [560/1724] loss: 0.329, ave_loss: 0.386
[30]  [580/1724] loss: 0.302, ave_loss: 0.383
[31]  [600/1724] loss: 0.512, ave_loss: 0.388
[32]  [620/1724] loss: 0.372, ave_loss: 0.387
[33]  [640/1724] loss: 0.493, ave_loss: 0.390
[34]  [660/1724] loss: 0.354, ave_loss: 0.389
[35]  [680/1724] loss: 0.327, ave_loss: 0.387
[36]  [700/1724] loss: 0.266, ave_loss: 0.384
[37]  [720/1724] loss: 0.285, ave_loss: 0.381
[38]  [740/1724] loss: 0.336, ave_loss: 0.380
[39]  [760/1724] loss: 0.230, ave_loss: 0.376
[40]  [780/1724] loss: 0.480, ave_loss: 0.379
[41]  [800/1724] loss: 0.405, ave_loss: 0.380
[42]  [820/1724] loss: 0.356, ave_loss: 0.379
[43]  [840/1724] loss: 0.297, ave_loss: 0.377
[44]  [860/1724] loss: 0.311, ave_loss: 0.376
[45]  [880/1724] loss: 0.385, ave_loss: 0.376
[46]  [900/1724] loss: 0.259, ave_loss: 0.373
[47]  [920/1724] loss: 0.472, ave_loss: 0.375
[48]  [940/1724] loss: 0.250, ave_loss: 0.373
[49]  [960/1724] loss: 0.400, ave_loss: 0.373
[50]  [980/1724] loss: 0.583, ave_loss: 0.378
[51]  [1000/1724] loss: 0.509, ave_loss: 0.380
[52]  [1020/1724] loss: 0.353, ave_loss: 0.380
[53]  [1040/1724] loss: 0.418, ave_loss: 0.380
[54]  [1060/1724] loss: 0.459, ave_loss: 0.382
[55]  [1080/1724] loss: 0.264, ave_loss: 0.380
[56]  [1100/1724] loss: 0.326, ave_loss: 0.379
[57]  [1120/1724] loss: 0.390, ave_loss: 0.379
[58]  [1140/1724] loss: 0.491, ave_loss: 0.381
[59]  [1160/1724] loss: 0.786, ave_loss: 0.388
[60]  [1180/1724] loss: 0.491, ave_loss: 0.389
[61]  [1200/1724] loss: 0.445, ave_loss: 0.390
[62]  [1220/1724] loss: 0.601, ave_loss: 0.394
[63]  [1240/1724] loss: 0.445, ave_loss: 0.395
[64]  [1260/1724] loss: 0.336, ave_loss: 0.394
[65]  [1280/1724] loss: 0.321, ave_loss: 0.392
[66]  [1300/1724] loss: 0.605, ave_loss: 0.396
[67]  [1320/1724] loss: 0.478, ave_loss: 0.397
[68]  [1340/1724] loss: 0.582, ave_loss: 0.400
[69]  [1360/1724] loss: 0.491, ave_loss: 0.401
[70]  [1380/1724] loss: 0.386, ave_loss: 0.401
[71]  [1400/1724] loss: 0.422, ave_loss: 0.401
[72]  [1420/1724] loss: 0.327, ave_loss: 0.400
[73]  [1440/1724] loss: 0.340, ave_loss: 0.399
[74]  [1460/1724] loss: 0.417, ave_loss: 0.399
[75]  [1480/1724] loss: 0.420, ave_loss: 0.400
[76]  [1500/1724] loss: 0.463, ave_loss: 0.401
[77]  [1520/1724] loss: 0.401, ave_loss: 0.401
[78]  [1540/1724] loss: 0.508, ave_loss: 0.402
[79]  [1560/1724] loss: 0.632, ave_loss: 0.405
[80]  [1580/1724] loss: 0.366, ave_loss: 0.404
[81]  [1600/1724] loss: 0.278, ave_loss: 0.403
[82]  [1620/1724] loss: 0.314, ave_loss: 0.402
[83]  [1640/1724] loss: 0.500, ave_loss: 0.403
[84]  [1660/1724] loss: 0.504, ave_loss: 0.404
[85]  [1680/1724] loss: 0.423, ave_loss: 0.404
[86]  [1700/1724] loss: 0.430, ave_loss: 0.405
[87]  [1720/1724] loss: 0.279, ave_loss: 0.403
[88]  [1740/1724] loss: 0.467, ave_loss: 0.404

Finished Training finishing at 2021-08-30 22:42:53.385022
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.039e-01
Validation Loss: 2.327e+05
Validation ROC: 0.7666
Saving model
19.812064965197216 epochs left to go

Training Epoch 9.187935034802784/30 starting at 2021-08-30 22:44:55.515936
[1]  [0/1724] loss: 0.523, ave_loss: 0.523
[2]  [20/1724] loss: 0.362, ave_loss: 0.442
[3]  [40/1724] loss: 0.416, ave_loss: 0.434
[4]  [60/1724] loss: 0.428, ave_loss: 0.432
[5]  [80/1724] loss: 0.351, ave_loss: 0.416
[6]  [100/1724] loss: 0.286, ave_loss: 0.394
[7]  [120/1724] loss: 0.367, ave_loss: 0.390
[8]  [140/1724] loss: 0.349, ave_loss: 0.385
[9]  [160/1724] loss: 0.615, ave_loss: 0.411
[10]  [180/1724] loss: 0.236, ave_loss: 0.393
[11]  [200/1724] loss: 0.402, ave_loss: 0.394
[12]  [220/1724] loss: 0.264, ave_loss: 0.383
[13]  [240/1724] loss: 0.321, ave_loss: 0.379
[14]  [260/1724] loss: 0.574, ave_loss: 0.392
[15]  [280/1724] loss: 0.607, ave_loss: 0.407
[16]  [300/1724] loss: 0.315, ave_loss: 0.401
[17]  [320/1724] loss: 0.481, ave_loss: 0.406
[18]  [340/1724] loss: 0.479, ave_loss: 0.410
[19]  [360/1724] loss: 0.429, ave_loss: 0.411
[20]  [380/1724] loss: 0.567, ave_loss: 0.419
[21]  [400/1724] loss: 0.335, ave_loss: 0.415
[22]  [420/1724] loss: 0.280, ave_loss: 0.409
[23]  [440/1724] loss: 0.456, ave_loss: 0.411
[24]  [460/1724] loss: 0.421, ave_loss: 0.411
[25]  [480/1724] loss: 0.440, ave_loss: 0.412
[26]  [500/1724] loss: 0.406, ave_loss: 0.412
[27]  [520/1724] loss: 0.546, ave_loss: 0.417
[28]  [540/1724] loss: 0.443, ave_loss: 0.418
[29]  [560/1724] loss: 0.378, ave_loss: 0.417
[30]  [580/1724] loss: 0.433, ave_loss: 0.417
[31]  [600/1724] loss: 0.368, ave_loss: 0.416
[32]  [620/1724] loss: 0.532, ave_loss: 0.419
[33]  [640/1724] loss: 0.566, ave_loss: 0.424
[34]  [660/1724] loss: 0.463, ave_loss: 0.425
[35]  [680/1724] loss: 0.440, ave_loss: 0.425
[36]  [700/1724] loss: 0.252, ave_loss: 0.420
[37]  [720/1724] loss: 0.283, ave_loss: 0.417
[38]  [740/1724] loss: 0.500, ave_loss: 0.419
[39]  [760/1724] loss: 0.598, ave_loss: 0.423
[40]  [780/1724] loss: 0.446, ave_loss: 0.424
[41]  [800/1724] loss: 0.269, ave_loss: 0.420
[42]  [820/1724] loss: 0.367, ave_loss: 0.419
[43]  [840/1724] loss: 0.349, ave_loss: 0.417
[44]  [860/1724] loss: 0.375, ave_loss: 0.416
[45]  [880/1724] loss: 0.411, ave_loss: 0.416
[46]  [900/1724] loss: 0.513, ave_loss: 0.418
[47]  [920/1724] loss: 0.389, ave_loss: 0.418
[48]  [940/1724] loss: 0.432, ave_loss: 0.418
[49]  [960/1724] loss: 0.427, ave_loss: 0.418
[50]  [980/1724] loss: 0.374, ave_loss: 0.417
[51]  [1000/1724] loss: 0.381, ave_loss: 0.417
[52]  [1020/1724] loss: 0.474, ave_loss: 0.418
[53]  [1040/1724] loss: 0.334, ave_loss: 0.416
[54]  [1060/1724] loss: 0.457, ave_loss: 0.417
[55]  [1080/1724] loss: 0.375, ave_loss: 0.416
[56]  [1100/1724] loss: 0.281, ave_loss: 0.414
[57]  [1120/1724] loss: 0.412, ave_loss: 0.414
[58]  [1140/1724] loss: 0.326, ave_loss: 0.412
[59]  [1160/1724] loss: 0.444, ave_loss: 0.413
[60]  [1180/1724] loss: 0.507, ave_loss: 0.414
[61]  [1200/1724] loss: 0.556, ave_loss: 0.417
[62]  [1220/1724] loss: 0.275, ave_loss: 0.414
[63]  [1240/1724] loss: 0.397, ave_loss: 0.414
[64]  [1260/1724] loss: 0.324, ave_loss: 0.413
[65]  [1280/1724] loss: 0.466, ave_loss: 0.413
[66]  [1300/1724] loss: 0.388, ave_loss: 0.413
[67]  [1320/1724] loss: 0.369, ave_loss: 0.412
[68]  [1340/1724] loss: 0.460, ave_loss: 0.413
[69]  [1360/1724] loss: 0.461, ave_loss: 0.414
[70]  [1380/1724] loss: 0.366, ave_loss: 0.413
[71]  [1400/1724] loss: 0.489, ave_loss: 0.414
[72]  [1420/1724] loss: 0.376, ave_loss: 0.414
[73]  [1440/1724] loss: 0.344, ave_loss: 0.413
[74]  [1460/1724] loss: 0.375, ave_loss: 0.412
[75]  [1480/1724] loss: 0.367, ave_loss: 0.412
[76]  [1500/1724] loss: 0.365, ave_loss: 0.411
[77]  [1520/1724] loss: 0.408, ave_loss: 0.411
[78]  [1540/1724] loss: 0.313, ave_loss: 0.410
[79]  [1560/1724] loss: 0.376, ave_loss: 0.409
[80]  [1580/1724] loss: 0.353, ave_loss: 0.409
[81]  [1600/1724] loss: 0.487, ave_loss: 0.410
[82]  [1620/1724] loss: 0.368, ave_loss: 0.409
[83]  [1640/1724] loss: 0.308, ave_loss: 0.408
[84]  [1660/1724] loss: 0.394, ave_loss: 0.408
[85]  [1680/1724] loss: 0.348, ave_loss: 0.407
[86]  [1700/1724] loss: 0.390, ave_loss: 0.407
[87]  [1720/1724] loss: 0.539, ave_loss: 0.408
[88]  [1740/1724] loss: 0.444, ave_loss: 0.409

Finished Training finishing at 2021-08-30 22:49:49.394484
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.087e-01
Validation Loss: 2.493e+05
Validation ROC: 0.7781
Saving model
18.791183294663576 epochs left to go

Training Epoch 10.208816705336426/30 starting at 2021-08-30 22:51:22.317825
[1]  [0/1724] loss: 0.544, ave_loss: 0.544
[2]  [20/1724] loss: 0.408, ave_loss: 0.476
[3]  [40/1724] loss: 0.451, ave_loss: 0.468
[4]  [60/1724] loss: 0.627, ave_loss: 0.507
[5]  [80/1724] loss: 0.322, ave_loss: 0.470
[6]  [100/1724] loss: 0.307, ave_loss: 0.443
[7]  [120/1724] loss: 0.418, ave_loss: 0.440
[8]  [140/1724] loss: 0.278, ave_loss: 0.419
[9]  [160/1724] loss: 0.399, ave_loss: 0.417
[10]  [180/1724] loss: 0.526, ave_loss: 0.428
[11]  [200/1724] loss: 0.318, ave_loss: 0.418
[12]  [220/1724] loss: 0.313, ave_loss: 0.409
[13]  [240/1724] loss: 0.355, ave_loss: 0.405
[14]  [260/1724] loss: 0.361, ave_loss: 0.402
[15]  [280/1724] loss: 0.326, ave_loss: 0.397
[16]  [300/1724] loss: 0.335, ave_loss: 0.393
[17]  [320/1724] loss: 0.466, ave_loss: 0.397
[18]  [340/1724] loss: 0.432, ave_loss: 0.399
[19]  [360/1724] loss: 0.353, ave_loss: 0.397
[20]  [380/1724] loss: 0.324, ave_loss: 0.393
[21]  [400/1724] loss: 0.478, ave_loss: 0.397
[22]  [420/1724] loss: 0.295, ave_loss: 0.393
[23]  [440/1724] loss: 0.363, ave_loss: 0.391
[24]  [460/1724] loss: 0.354, ave_loss: 0.390
[25]  [480/1724] loss: 0.480, ave_loss: 0.393
[26]  [500/1724] loss: 0.339, ave_loss: 0.391
[27]  [520/1724] loss: 0.582, ave_loss: 0.398
[28]  [540/1724] loss: 0.450, ave_loss: 0.400
[29]  [560/1724] loss: 0.412, ave_loss: 0.401
[30]  [580/1724] loss: 0.286, ave_loss: 0.397
[31]  [600/1724] loss: 0.320, ave_loss: 0.394
[32]  [620/1724] loss: 0.409, ave_loss: 0.395
[33]  [640/1724] loss: 0.534, ave_loss: 0.399
[34]  [660/1724] loss: 0.388, ave_loss: 0.399
[35]  [680/1724] loss: 0.468, ave_loss: 0.401
[36]  [700/1724] loss: 0.441, ave_loss: 0.402
[37]  [720/1724] loss: 0.487, ave_loss: 0.404
[38]  [740/1724] loss: 0.309, ave_loss: 0.402
[39]  [760/1724] loss: 0.446, ave_loss: 0.403
[40]  [780/1724] loss: 0.401, ave_loss: 0.403
[41]  [800/1724] loss: 0.424, ave_loss: 0.403
[42]  [820/1724] loss: 0.528, ave_loss: 0.406
[43]  [840/1724] loss: 0.186, ave_loss: 0.401
[44]  [860/1724] loss: 0.215, ave_loss: 0.397
[45]  [880/1724] loss: 0.375, ave_loss: 0.396
[46]  [900/1724] loss: 0.303, ave_loss: 0.394
[47]  [920/1724] loss: 0.429, ave_loss: 0.395
[48]  [940/1724] loss: 0.436, ave_loss: 0.396
[49]  [960/1724] loss: 0.331, ave_loss: 0.395
[50]  [980/1724] loss: 0.499, ave_loss: 0.397
[51]  [1000/1724] loss: 0.232, ave_loss: 0.393
[52]  [1020/1724] loss: 0.507, ave_loss: 0.396
[53]  [1040/1724] loss: 0.338, ave_loss: 0.395
[54]  [1060/1724] loss: 0.579, ave_loss: 0.398
[55]  [1080/1724] loss: 0.486, ave_loss: 0.400
[56]  [1100/1724] loss: 0.315, ave_loss: 0.398
[57]  [1120/1724] loss: 0.372, ave_loss: 0.398
[58]  [1140/1724] loss: 0.447, ave_loss: 0.398
[59]  [1160/1724] loss: 0.435, ave_loss: 0.399
[60]  [1180/1724] loss: 0.471, ave_loss: 0.400
[61]  [1200/1724] loss: 0.259, ave_loss: 0.398
[62]  [1220/1724] loss: 0.329, ave_loss: 0.397
[63]  [1240/1724] loss: 0.478, ave_loss: 0.398
[64]  [1260/1724] loss: 0.606, ave_loss: 0.401
[65]  [1280/1724] loss: 0.402, ave_loss: 0.401
[66]  [1300/1724] loss: 0.353, ave_loss: 0.401
[67]  [1320/1724] loss: 0.354, ave_loss: 0.400
[68]  [1340/1724] loss: 0.470, ave_loss: 0.401
[69]  [1360/1724] loss: 0.388, ave_loss: 0.401
[70]  [1380/1724] loss: 0.494, ave_loss: 0.402
[71]  [1400/1724] loss: 0.427, ave_loss: 0.402
[72]  [1420/1724] loss: 0.369, ave_loss: 0.402
[73]  [1440/1724] loss: 0.423, ave_loss: 0.402
[74]  [1460/1724] loss: 0.363, ave_loss: 0.402
[75]  [1480/1724] loss: 0.334, ave_loss: 0.401
[76]  [1500/1724] loss: 0.385, ave_loss: 0.401
[77]  [1520/1724] loss: 0.377, ave_loss: 0.400
[78]  [1540/1724] loss: 0.295, ave_loss: 0.399
[79]  [1560/1724] loss: 0.443, ave_loss: 0.400
[80]  [1580/1724] loss: 0.302, ave_loss: 0.398
[81]  [1600/1724] loss: 0.448, ave_loss: 0.399
[82]  [1620/1724] loss: 0.472, ave_loss: 0.400
[83]  [1640/1724] loss: 0.544, ave_loss: 0.402
[84]  [1660/1724] loss: 0.442, ave_loss: 0.402
[85]  [1680/1724] loss: 0.326, ave_loss: 0.401
[86]  [1700/1724] loss: 0.354, ave_loss: 0.401
[87]  [1720/1724] loss: 0.401, ave_loss: 0.401
[88]  [1740/1724] loss: 0.391, ave_loss: 0.400

Finished Training finishing at 2021-08-30 22:56:15.767997
printing_out epoch  11.22969837587007 learning rate: 0.0005153561248318907
0.00036863495918228726
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.005e-01
Validation Loss: 2.009e+05
Validation ROC: 0.7745
No improvement, still saving model
17.770301624129928 epochs left to go

Training Epoch 11.22969837587007/30 starting at 2021-08-30 22:58:14.299240
[1]  [0/1724] loss: 0.456, ave_loss: 0.456
[2]  [20/1724] loss: 0.378, ave_loss: 0.417
[3]  [40/1724] loss: 0.288, ave_loss: 0.374
[4]  [60/1724] loss: 0.354, ave_loss: 0.369
[5]  [80/1724] loss: 0.328, ave_loss: 0.361
[6]  [100/1724] loss: 0.460, ave_loss: 0.377
[7]  [120/1724] loss: 0.330, ave_loss: 0.371
[8]  [140/1724] loss: 0.620, ave_loss: 0.402
[9]  [160/1724] loss: 0.317, ave_loss: 0.392
[10]  [180/1724] loss: 0.440, ave_loss: 0.397
[11]  [200/1724] loss: 0.369, ave_loss: 0.394
[12]  [220/1724] loss: 0.287, ave_loss: 0.386
[13]  [240/1724] loss: 0.336, ave_loss: 0.382
[14]  [260/1724] loss: 0.421, ave_loss: 0.385
[15]  [280/1724] loss: 0.355, ave_loss: 0.383
[16]  [300/1724] loss: 0.334, ave_loss: 0.380
[17]  [320/1724] loss: 0.440, ave_loss: 0.383
[18]  [340/1724] loss: 0.445, ave_loss: 0.387
[19]  [360/1724] loss: 0.283, ave_loss: 0.381
[20]  [380/1724] loss: 0.268, ave_loss: 0.375
[21]  [400/1724] loss: 0.240, ave_loss: 0.369
[22]  [420/1724] loss: 0.365, ave_loss: 0.369
[23]  [440/1724] loss: 0.511, ave_loss: 0.375
[24]  [460/1724] loss: 0.329, ave_loss: 0.373
[25]  [480/1724] loss: 0.277, ave_loss: 0.369
[26]  [500/1724] loss: 0.370, ave_loss: 0.369
[27]  [520/1724] loss: 0.266, ave_loss: 0.365
[28]  [540/1724] loss: 0.358, ave_loss: 0.365
[29]  [560/1724] loss: 0.419, ave_loss: 0.367
[30]  [580/1724] loss: 0.408, ave_loss: 0.368
[31]  [600/1724] loss: 0.398, ave_loss: 0.369
[32]  [620/1724] loss: 0.249, ave_loss: 0.366
[33]  [640/1724] loss: 0.320, ave_loss: 0.364
[34]  [660/1724] loss: 0.432, ave_loss: 0.366
[35]  [680/1724] loss: 0.297, ave_loss: 0.364
[36]  [700/1724] loss: 0.620, ave_loss: 0.371
[37]  [720/1724] loss: 0.484, ave_loss: 0.374
[38]  [740/1724] loss: 0.422, ave_loss: 0.376
[39]  [760/1724] loss: 0.283, ave_loss: 0.373
[40]  [780/1724] loss: 0.213, ave_loss: 0.369
[41]  [800/1724] loss: 0.276, ave_loss: 0.367
[42]  [820/1724] loss: 0.387, ave_loss: 0.367
[43]  [840/1724] loss: 0.387, ave_loss: 0.368
[44]  [860/1724] loss: 0.505, ave_loss: 0.371
[45]  [880/1724] loss: 0.486, ave_loss: 0.374
[46]  [900/1724] loss: 0.354, ave_loss: 0.373
[47]  [920/1724] loss: 0.419, ave_loss: 0.374
[48]  [940/1724] loss: 0.339, ave_loss: 0.373
[49]  [960/1724] loss: 0.342, ave_loss: 0.373
[50]  [980/1724] loss: 0.360, ave_loss: 0.372
[51]  [1000/1724] loss: 0.451, ave_loss: 0.374
[52]  [1020/1724] loss: 0.351, ave_loss: 0.374
[53]  [1040/1724] loss: 0.427, ave_loss: 0.375
[54]  [1060/1724] loss: 0.348, ave_loss: 0.374
[55]  [1080/1724] loss: 0.422, ave_loss: 0.375
[56]  [1100/1724] loss: 0.348, ave_loss: 0.374
[57]  [1120/1724] loss: 0.362, ave_loss: 0.374
[58]  [1140/1724] loss: 0.467, ave_loss: 0.376
[59]  [1160/1724] loss: 0.308, ave_loss: 0.375
[60]  [1180/1724] loss: 0.271, ave_loss: 0.373
[61]  [1200/1724] loss: 0.329, ave_loss: 0.372
[62]  [1220/1724] loss: 0.223, ave_loss: 0.370
[63]  [1240/1724] loss: 0.370, ave_loss: 0.370
[64]  [1260/1724] loss: 0.365, ave_loss: 0.370
[65]  [1280/1724] loss: 0.479, ave_loss: 0.371
[66]  [1300/1724] loss: 0.290, ave_loss: 0.370
[67]  [1320/1724] loss: 0.360, ave_loss: 0.370
[68]  [1340/1724] loss: 0.310, ave_loss: 0.369
[69]  [1360/1724] loss: 0.254, ave_loss: 0.368
[70]  [1380/1724] loss: 0.383, ave_loss: 0.368
[71]  [1400/1724] loss: 0.358, ave_loss: 0.368
[72]  [1420/1724] loss: 0.370, ave_loss: 0.368
[73]  [1440/1724] loss: 0.465, ave_loss: 0.369
[74]  [1460/1724] loss: 0.289, ave_loss: 0.368
[75]  [1480/1724] loss: 0.268, ave_loss: 0.367
[76]  [1500/1724] loss: 0.435, ave_loss: 0.367
[77]  [1520/1724] loss: 0.462, ave_loss: 0.369
[78]  [1540/1724] loss: 0.328, ave_loss: 0.368
[79]  [1560/1724] loss: 0.481, ave_loss: 0.370
[80]  [1580/1724] loss: 0.506, ave_loss: 0.371
[81]  [1600/1724] loss: 0.583, ave_loss: 0.374
[82]  [1620/1724] loss: 0.469, ave_loss: 0.375
[83]  [1640/1724] loss: 0.419, ave_loss: 0.376
[84]  [1660/1724] loss: 0.295, ave_loss: 0.375
[85]  [1680/1724] loss: 0.337, ave_loss: 0.374
[86]  [1700/1724] loss: 0.481, ave_loss: 0.375
[87]  [1720/1724] loss: 0.548, ave_loss: 0.377
[88]  [1740/1724] loss: 0.364, ave_loss: 0.377

Finished Training finishing at 2021-08-30 23:02:54.537177
printing_out epoch  12.250580046403712 learning rate: 0.0005153561248318907
0.0003575759104068186
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.773e-01
Validation Loss: 1.643e+05
Validation ROC: 0.7777
No improvement, still saving model
16.749419953596288 epochs left to go

Training Epoch 12.250580046403712/30 starting at 2021-08-30 23:04:35.274496
[1]  [0/1724] loss: 0.687, ave_loss: 0.687
[2]  [20/1724] loss: 0.332, ave_loss: 0.510
[3]  [40/1724] loss: 0.374, ave_loss: 0.464
[4]  [60/1724] loss: 0.269, ave_loss: 0.416
[5]  [80/1724] loss: 0.356, ave_loss: 0.404
[6]  [100/1724] loss: 0.512, ave_loss: 0.422
[7]  [120/1724] loss: 0.234, ave_loss: 0.395
[8]  [140/1724] loss: 0.336, ave_loss: 0.387
[9]  [160/1724] loss: 0.457, ave_loss: 0.395
[10]  [180/1724] loss: 0.537, ave_loss: 0.409
[11]  [200/1724] loss: 0.436, ave_loss: 0.412
[12]  [220/1724] loss: 0.422, ave_loss: 0.413
[13]  [240/1724] loss: 0.410, ave_loss: 0.412
[14]  [260/1724] loss: 0.305, ave_loss: 0.405
[15]  [280/1724] loss: 0.452, ave_loss: 0.408
[16]  [300/1724] loss: 0.321, ave_loss: 0.402
[17]  [320/1724] loss: 0.416, ave_loss: 0.403
[18]  [340/1724] loss: 0.349, ave_loss: 0.400
[19]  [360/1724] loss: 0.503, ave_loss: 0.406
[20]  [380/1724] loss: 0.290, ave_loss: 0.400
[21]  [400/1724] loss: 0.393, ave_loss: 0.400
[22]  [420/1724] loss: 0.298, ave_loss: 0.395
[23]  [440/1724] loss: 0.253, ave_loss: 0.389
[24]  [460/1724] loss: 0.266, ave_loss: 0.384
[25]  [480/1724] loss: 0.290, ave_loss: 0.380
[26]  [500/1724] loss: 0.527, ave_loss: 0.386
[27]  [520/1724] loss: 0.386, ave_loss: 0.386
[28]  [540/1724] loss: 0.516, ave_loss: 0.390
[29]  [560/1724] loss: 0.330, ave_loss: 0.388
[30]  [580/1724] loss: 0.616, ave_loss: 0.396
[31]  [600/1724] loss: 0.507, ave_loss: 0.399
[32]  [620/1724] loss: 0.332, ave_loss: 0.397
[33]  [640/1724] loss: 0.412, ave_loss: 0.398
[34]  [660/1724] loss: 0.157, ave_loss: 0.391
[35]  [680/1724] loss: 0.288, ave_loss: 0.388
[36]  [700/1724] loss: 0.417, ave_loss: 0.388
[37]  [720/1724] loss: 0.282, ave_loss: 0.386
[38]  [740/1724] loss: 0.597, ave_loss: 0.391
[39]  [760/1724] loss: 0.422, ave_loss: 0.392
[40]  [780/1724] loss: 0.413, ave_loss: 0.392
[41]  [800/1724] loss: 0.317, ave_loss: 0.391
[42]  [820/1724] loss: 0.371, ave_loss: 0.390
[43]  [840/1724] loss: 0.325, ave_loss: 0.389
[44]  [860/1724] loss: 0.520, ave_loss: 0.392
[45]  [880/1724] loss: 0.399, ave_loss: 0.392
[46]  [900/1724] loss: 0.428, ave_loss: 0.393
[47]  [920/1724] loss: 0.390, ave_loss: 0.393
[48]  [940/1724] loss: 0.427, ave_loss: 0.393
[49]  [960/1724] loss: 0.459, ave_loss: 0.395
[50]  [980/1724] loss: 0.335, ave_loss: 0.393
[51]  [1000/1724] loss: 0.378, ave_loss: 0.393
[52]  [1020/1724] loss: 0.356, ave_loss: 0.392
[53]  [1040/1724] loss: 0.440, ave_loss: 0.393
[54]  [1060/1724] loss: 0.402, ave_loss: 0.393
[55]  [1080/1724] loss: 0.387, ave_loss: 0.393
[56]  [1100/1724] loss: 0.439, ave_loss: 0.394
[57]  [1120/1724] loss: 0.394, ave_loss: 0.394
[58]  [1140/1724] loss: 0.359, ave_loss: 0.394
[59]  [1160/1724] loss: 0.270, ave_loss: 0.391
[60]  [1180/1724] loss: 0.272, ave_loss: 0.389
[61]  [1200/1724] loss: 0.309, ave_loss: 0.388
[62]  [1220/1724] loss: 0.349, ave_loss: 0.387
[63]  [1240/1724] loss: 0.435, ave_loss: 0.388
[64]  [1260/1724] loss: 0.414, ave_loss: 0.389
[65]  [1280/1724] loss: 0.319, ave_loss: 0.388
[66]  [1300/1724] loss: 0.344, ave_loss: 0.387
[67]  [1320/1724] loss: 0.440, ave_loss: 0.388
[68]  [1340/1724] loss: 0.445, ave_loss: 0.389
[69]  [1360/1724] loss: 0.392, ave_loss: 0.389
[70]  [1380/1724] loss: 0.380, ave_loss: 0.388
[71]  [1400/1724] loss: 0.503, ave_loss: 0.390
[72]  [1420/1724] loss: 0.392, ave_loss: 0.390
[73]  [1440/1724] loss: 0.435, ave_loss: 0.391
[74]  [1460/1724] loss: 0.315, ave_loss: 0.390
[75]  [1480/1724] loss: 0.554, ave_loss: 0.392
[76]  [1500/1724] loss: 0.306, ave_loss: 0.391
[77]  [1520/1724] loss: 0.317, ave_loss: 0.390
[78]  [1540/1724] loss: 0.271, ave_loss: 0.388
[79]  [1560/1724] loss: 0.279, ave_loss: 0.387
[80]  [1580/1724] loss: 0.428, ave_loss: 0.387
[81]  [1600/1724] loss: 0.288, ave_loss: 0.386
[82]  [1620/1724] loss: 0.314, ave_loss: 0.385
[83]  [1640/1724] loss: 0.287, ave_loss: 0.384
[84]  [1660/1724] loss: 0.393, ave_loss: 0.384
[85]  [1680/1724] loss: 0.464, ave_loss: 0.385
[86]  [1700/1724] loss: 0.454, ave_loss: 0.386
[87]  [1720/1724] loss: 0.425, ave_loss: 0.386
[88]  [1740/1724] loss: 0.549, ave_loss: 0.388

Finished Training finishing at 2021-08-30 23:09:12.490975
printing_out epoch  13.271461716937354 learning rate: 0.0005153561248318907
0.00034684863309461403
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.883e-01
Validation Loss: 1.853e+05
Validation ROC: 0.7779
No improvement, still saving model
15.728538283062646 epochs left to go

Training Epoch 13.271461716937354/30 starting at 2021-08-30 23:10:50.875418
[1]  [0/1724] loss: 0.468, ave_loss: 0.468
[2]  [20/1724] loss: 0.181, ave_loss: 0.324
[3]  [40/1724] loss: 0.206, ave_loss: 0.285
[4]  [60/1724] loss: 0.446, ave_loss: 0.325
[5]  [80/1724] loss: 0.367, ave_loss: 0.334
[6]  [100/1724] loss: 0.421, ave_loss: 0.348
[7]  [120/1724] loss: 0.489, ave_loss: 0.368
[8]  [140/1724] loss: 0.525, ave_loss: 0.388
[9]  [160/1724] loss: 0.438, ave_loss: 0.394
[10]  [180/1724] loss: 0.439, ave_loss: 0.398
[11]  [200/1724] loss: 0.614, ave_loss: 0.418
[12]  [220/1724] loss: 0.430, ave_loss: 0.419
[13]  [240/1724] loss: 0.299, ave_loss: 0.409
[14]  [260/1724] loss: 0.424, ave_loss: 0.411
[15]  [280/1724] loss: 0.290, ave_loss: 0.402
[16]  [300/1724] loss: 0.356, ave_loss: 0.400
[17]  [320/1724] loss: 0.494, ave_loss: 0.405
[18]  [340/1724] loss: 0.344, ave_loss: 0.402
[19]  [360/1724] loss: 0.415, ave_loss: 0.402
[20]  [380/1724] loss: 0.393, ave_loss: 0.402
[21]  [400/1724] loss: 0.255, ave_loss: 0.395
[22]  [420/1724] loss: 0.383, ave_loss: 0.394
[23]  [440/1724] loss: 0.312, ave_loss: 0.391
[24]  [460/1724] loss: 0.456, ave_loss: 0.393
[25]  [480/1724] loss: 0.422, ave_loss: 0.395
[26]  [500/1724] loss: 0.383, ave_loss: 0.394
[27]  [520/1724] loss: 0.568, ave_loss: 0.401
[28]  [540/1724] loss: 0.325, ave_loss: 0.398
[29]  [560/1724] loss: 0.397, ave_loss: 0.398
[30]  [580/1724] loss: 0.528, ave_loss: 0.402
[31]  [600/1724] loss: 0.497, ave_loss: 0.405
[32]  [620/1724] loss: 0.267, ave_loss: 0.401
[33]  [640/1724] loss: 0.335, ave_loss: 0.399
[34]  [660/1724] loss: 0.475, ave_loss: 0.401
[35]  [680/1724] loss: 0.253, ave_loss: 0.397
[36]  [700/1724] loss: 0.375, ave_loss: 0.396
[37]  [720/1724] loss: 0.510, ave_loss: 0.399
[38]  [740/1724] loss: 0.414, ave_loss: 0.400
[39]  [760/1724] loss: 0.443, ave_loss: 0.401
[40]  [780/1724] loss: 0.284, ave_loss: 0.398
[41]  [800/1724] loss: 0.368, ave_loss: 0.397
[42]  [820/1724] loss: 0.323, ave_loss: 0.395
[43]  [840/1724] loss: 0.443, ave_loss: 0.397
[44]  [860/1724] loss: 0.316, ave_loss: 0.395
[45]  [880/1724] loss: 0.354, ave_loss: 0.394
[46]  [900/1724] loss: 0.354, ave_loss: 0.393
[47]  [920/1724] loss: 0.247, ave_loss: 0.390
[48]  [940/1724] loss: 0.416, ave_loss: 0.390
[49]  [960/1724] loss: 0.536, ave_loss: 0.393
[50]  [980/1724] loss: 0.379, ave_loss: 0.393
[51]  [1000/1724] loss: 0.355, ave_loss: 0.392
[52]  [1020/1724] loss: 0.444, ave_loss: 0.393
[53]  [1040/1724] loss: 0.398, ave_loss: 0.393
[54]  [1060/1724] loss: 0.322, ave_loss: 0.392
[55]  [1080/1724] loss: 0.357, ave_loss: 0.391
[56]  [1100/1724] loss: 0.620, ave_loss: 0.396
[57]  [1120/1724] loss: 0.289, ave_loss: 0.394
[58]  [1140/1724] loss: 0.437, ave_loss: 0.394
[59]  [1160/1724] loss: 0.241, ave_loss: 0.392
[60]  [1180/1724] loss: 0.429, ave_loss: 0.392
[61]  [1200/1724] loss: 0.466, ave_loss: 0.394
[62]  [1220/1724] loss: 0.318, ave_loss: 0.392
[63]  [1240/1724] loss: 0.386, ave_loss: 0.392
[64]  [1260/1724] loss: 0.379, ave_loss: 0.392
[65]  [1280/1724] loss: 0.401, ave_loss: 0.392
[66]  [1300/1724] loss: 0.494, ave_loss: 0.394
[67]  [1320/1724] loss: 0.544, ave_loss: 0.396
[68]  [1340/1724] loss: 0.393, ave_loss: 0.396
[69]  [1360/1724] loss: 0.284, ave_loss: 0.394
[70]  [1380/1724] loss: 0.299, ave_loss: 0.393
[71]  [1400/1724] loss: 0.311, ave_loss: 0.392
[72]  [1420/1724] loss: 0.371, ave_loss: 0.392
[73]  [1440/1724] loss: 0.270, ave_loss: 0.390
[74]  [1460/1724] loss: 0.455, ave_loss: 0.391
[75]  [1480/1724] loss: 0.503, ave_loss: 0.392
[76]  [1500/1724] loss: 0.228, ave_loss: 0.390
[77]  [1520/1724] loss: 0.293, ave_loss: 0.389
[78]  [1540/1724] loss: 0.433, ave_loss: 0.389
[79]  [1560/1724] loss: 0.480, ave_loss: 0.391
[80]  [1580/1724] loss: 0.303, ave_loss: 0.389
[81]  [1600/1724] loss: 0.419, ave_loss: 0.390
[82]  [1620/1724] loss: 0.279, ave_loss: 0.389
[83]  [1640/1724] loss: 0.416, ave_loss: 0.389
[84]  [1660/1724] loss: 0.442, ave_loss: 0.389
[85]  [1680/1724] loss: 0.357, ave_loss: 0.389
[86]  [1700/1724] loss: 0.325, ave_loss: 0.388
[87]  [1720/1724] loss: 0.387, ave_loss: 0.388
[88]  [1740/1724] loss: 0.530, ave_loss: 0.390

Finished Training finishing at 2021-08-30 23:15:43.494546
printing_out epoch  14.292343387470998 learning rate: 0.0005153561248318907
0.0003364431741017756
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.899e-01
Validation Loss: 1.503e+05
Validation ROC: 0.7835
Saving model
14.707656612529002 epochs left to go

Training Epoch 14.292343387470998/30 starting at 2021-08-30 23:17:38.040325
[1]  [0/1724] loss: 0.555, ave_loss: 0.555
[2]  [20/1724] loss: 0.423, ave_loss: 0.489
[3]  [40/1724] loss: 0.389, ave_loss: 0.456
[4]  [60/1724] loss: 0.363, ave_loss: 0.432
[5]  [80/1724] loss: 0.409, ave_loss: 0.428
[6]  [100/1724] loss: 0.451, ave_loss: 0.432
[7]  [120/1724] loss: 0.474, ave_loss: 0.438
[8]  [140/1724] loss: 0.183, ave_loss: 0.406
[9]  [160/1724] loss: 0.368, ave_loss: 0.401
[10]  [180/1724] loss: 0.339, ave_loss: 0.395
[11]  [200/1724] loss: 0.359, ave_loss: 0.392
[12]  [220/1724] loss: 0.410, ave_loss: 0.393
[13]  [240/1724] loss: 0.399, ave_loss: 0.394
[14]  [260/1724] loss: 0.362, ave_loss: 0.392
[15]  [280/1724] loss: 0.294, ave_loss: 0.385
[16]  [300/1724] loss: 0.350, ave_loss: 0.383
[17]  [320/1724] loss: 0.306, ave_loss: 0.378
[18]  [340/1724] loss: 0.378, ave_loss: 0.378
[19]  [360/1724] loss: 0.428, ave_loss: 0.381
[20]  [380/1724] loss: 0.332, ave_loss: 0.378
[21]  [400/1724] loss: 0.449, ave_loss: 0.382
[22]  [420/1724] loss: 0.366, ave_loss: 0.381
[23]  [440/1724] loss: 0.385, ave_loss: 0.381
[24]  [460/1724] loss: 0.321, ave_loss: 0.379
[25]  [480/1724] loss: 0.436, ave_loss: 0.381
[26]  [500/1724] loss: 0.519, ave_loss: 0.386
[27]  [520/1724] loss: 0.370, ave_loss: 0.386
[28]  [540/1724] loss: 0.244, ave_loss: 0.381
[29]  [560/1724] loss: 0.419, ave_loss: 0.382
[30]  [580/1724] loss: 0.429, ave_loss: 0.384
[31]  [600/1724] loss: 0.246, ave_loss: 0.379
[32]  [620/1724] loss: 0.238, ave_loss: 0.375
[33]  [640/1724] loss: 0.379, ave_loss: 0.375
[34]  [660/1724] loss: 0.320, ave_loss: 0.373
[35]  [680/1724] loss: 0.294, ave_loss: 0.371
[36]  [700/1724] loss: 0.433, ave_loss: 0.373
[37]  [720/1724] loss: 0.490, ave_loss: 0.376
[38]  [740/1724] loss: 0.491, ave_loss: 0.379
[39]  [760/1724] loss: 0.316, ave_loss: 0.377
[40]  [780/1724] loss: 0.380, ave_loss: 0.377
[41]  [800/1724] loss: 0.587, ave_loss: 0.382
[42]  [820/1724] loss: 0.379, ave_loss: 0.382
[43]  [840/1724] loss: 0.308, ave_loss: 0.381
[44]  [860/1724] loss: 0.464, ave_loss: 0.383
[45]  [880/1724] loss: 0.522, ave_loss: 0.386
[46]  [900/1724] loss: 0.262, ave_loss: 0.383
[47]  [920/1724] loss: 0.641, ave_loss: 0.388
[48]  [940/1724] loss: 0.475, ave_loss: 0.390
[49]  [960/1724] loss: 0.408, ave_loss: 0.391
[50]  [980/1724] loss: 0.378, ave_loss: 0.390
[51]  [1000/1724] loss: 0.323, ave_loss: 0.389
[52]  [1020/1724] loss: 0.273, ave_loss: 0.387
[53]  [1040/1724] loss: 0.285, ave_loss: 0.385
[54]  [1060/1724] loss: 0.344, ave_loss: 0.384
[55]  [1080/1724] loss: 0.277, ave_loss: 0.382
[56]  [1100/1724] loss: 0.397, ave_loss: 0.382
[57]  [1120/1724] loss: 0.279, ave_loss: 0.381
[58]  [1140/1724] loss: 0.474, ave_loss: 0.382
[59]  [1160/1724] loss: 0.269, ave_loss: 0.380
[60]  [1180/1724] loss: 0.299, ave_loss: 0.379
[61]  [1200/1724] loss: 0.417, ave_loss: 0.380
[62]  [1220/1724] loss: 0.353, ave_loss: 0.379
[63]  [1240/1724] loss: 0.325, ave_loss: 0.378
[64]  [1260/1724] loss: 0.369, ave_loss: 0.378
[65]  [1280/1724] loss: 0.412, ave_loss: 0.379
[66]  [1300/1724] loss: 0.466, ave_loss: 0.380
[67]  [1320/1724] loss: 0.447, ave_loss: 0.381
[68]  [1340/1724] loss: 0.347, ave_loss: 0.380
[69]  [1360/1724] loss: 0.386, ave_loss: 0.381
[70]  [1380/1724] loss: 0.471, ave_loss: 0.382
[71]  [1400/1724] loss: 0.348, ave_loss: 0.381
[72]  [1420/1724] loss: 0.295, ave_loss: 0.380
[73]  [1440/1724] loss: 0.406, ave_loss: 0.381
[74]  [1460/1724] loss: 0.351, ave_loss: 0.380
[75]  [1480/1724] loss: 0.338, ave_loss: 0.380
[76]  [1500/1724] loss: 0.404, ave_loss: 0.380
[77]  [1520/1724] loss: 0.290, ave_loss: 0.379
[78]  [1540/1724] loss: 0.347, ave_loss: 0.378
[79]  [1560/1724] loss: 0.288, ave_loss: 0.377
[80]  [1580/1724] loss: 0.320, ave_loss: 0.376
[81]  [1600/1724] loss: 0.285, ave_loss: 0.375
[82]  [1620/1724] loss: 0.360, ave_loss: 0.375
[83]  [1640/1724] loss: 0.340, ave_loss: 0.375
[84]  [1660/1724] loss: 0.300, ave_loss: 0.374
[85]  [1680/1724] loss: 0.307, ave_loss: 0.373
[86]  [1700/1724] loss: 0.389, ave_loss: 0.373
[87]  [1720/1724] loss: 0.333, ave_loss: 0.373
[88]  [1740/1724] loss: 0.482, ave_loss: 0.374

Finished Training finishing at 2021-08-30 23:22:33.597748
printing_out epoch  15.31322505800464 learning rate: 0.0005153561248318907
0.0003263498788787223
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.740e-01
Validation Loss: 1.541e+05
Validation ROC: 0.7881
Saving model
13.68677494199536 epochs left to go

Training Epoch 15.31322505800464/30 starting at 2021-08-30 23:24:32.392408
[1]  [0/1724] loss: 0.617, ave_loss: 0.617
[2]  [20/1724] loss: 0.390, ave_loss: 0.504
[3]  [40/1724] loss: 0.413, ave_loss: 0.473
[4]  [60/1724] loss: 0.497, ave_loss: 0.479
[5]  [80/1724] loss: 0.442, ave_loss: 0.472
[6]  [100/1724] loss: 0.379, ave_loss: 0.456
[7]  [120/1724] loss: 0.487, ave_loss: 0.461
[8]  [140/1724] loss: 0.337, ave_loss: 0.445
[9]  [160/1724] loss: 0.248, ave_loss: 0.423
[10]  [180/1724] loss: 0.319, ave_loss: 0.413
[11]  [200/1724] loss: 0.380, ave_loss: 0.410
[12]  [220/1724] loss: 0.188, ave_loss: 0.392
[13]  [240/1724] loss: 0.528, ave_loss: 0.402
[14]  [260/1724] loss: 0.352, ave_loss: 0.398
[15]  [280/1724] loss: 0.462, ave_loss: 0.403
[16]  [300/1724] loss: 0.220, ave_loss: 0.391
[17]  [320/1724] loss: 0.260, ave_loss: 0.384
[18]  [340/1724] loss: 0.397, ave_loss: 0.384
[19]  [360/1724] loss: 0.381, ave_loss: 0.384
[20]  [380/1724] loss: 0.417, ave_loss: 0.386
[21]  [400/1724] loss: 0.356, ave_loss: 0.384
[22]  [420/1724] loss: 0.405, ave_loss: 0.385
[23]  [440/1724] loss: 0.319, ave_loss: 0.382
[24]  [460/1724] loss: 0.517, ave_loss: 0.388
[25]  [480/1724] loss: 0.468, ave_loss: 0.391
[26]  [500/1724] loss: 0.348, ave_loss: 0.390
[27]  [520/1724] loss: 0.479, ave_loss: 0.393
[28]  [540/1724] loss: 0.383, ave_loss: 0.392
[29]  [560/1724] loss: 0.389, ave_loss: 0.392
[30]  [580/1724] loss: 0.595, ave_loss: 0.399
[31]  [600/1724] loss: 0.333, ave_loss: 0.397
[32]  [620/1724] loss: 0.323, ave_loss: 0.395
[33]  [640/1724] loss: 0.343, ave_loss: 0.393
[34]  [660/1724] loss: 0.401, ave_loss: 0.393
[35]  [680/1724] loss: 0.413, ave_loss: 0.394
[36]  [700/1724] loss: 0.282, ave_loss: 0.391
[37]  [720/1724] loss: 0.391, ave_loss: 0.391
[38]  [740/1724] loss: 0.326, ave_loss: 0.389
[39]  [760/1724] loss: 0.380, ave_loss: 0.389
[40]  [780/1724] loss: 0.541, ave_loss: 0.393
[41]  [800/1724] loss: 0.365, ave_loss: 0.392
[42]  [820/1724] loss: 0.393, ave_loss: 0.392
[43]  [840/1724] loss: 0.372, ave_loss: 0.392
[44]  [860/1724] loss: 0.300, ave_loss: 0.389
[45]  [880/1724] loss: 0.366, ave_loss: 0.389
[46]  [900/1724] loss: 0.469, ave_loss: 0.391
[47]  [920/1724] loss: 0.341, ave_loss: 0.390
[48]  [940/1724] loss: 0.437, ave_loss: 0.391
[49]  [960/1724] loss: 0.393, ave_loss: 0.391
[50]  [980/1724] loss: 0.460, ave_loss: 0.392
[51]  [1000/1724] loss: 0.511, ave_loss: 0.394
[52]  [1020/1724] loss: 0.252, ave_loss: 0.392
[53]  [1040/1724] loss: 0.256, ave_loss: 0.389
[54]  [1060/1724] loss: 0.389, ave_loss: 0.389
[55]  [1080/1724] loss: 0.353, ave_loss: 0.388
[56]  [1100/1724] loss: 0.444, ave_loss: 0.389
[57]  [1120/1724] loss: 0.528, ave_loss: 0.392
[58]  [1140/1724] loss: 0.319, ave_loss: 0.391
[59]  [1160/1724] loss: 0.317, ave_loss: 0.389
[60]  [1180/1724] loss: 0.276, ave_loss: 0.387
[61]  [1200/1724] loss: 0.361, ave_loss: 0.387
[62]  [1220/1724] loss: 0.262, ave_loss: 0.385
[63]  [1240/1724] loss: 0.220, ave_loss: 0.382
[64]  [1260/1724] loss: 0.337, ave_loss: 0.382
[65]  [1280/1724] loss: 0.327, ave_loss: 0.381
[66]  [1300/1724] loss: 0.489, ave_loss: 0.383
[67]  [1320/1724] loss: 0.417, ave_loss: 0.383
[68]  [1340/1724] loss: 0.327, ave_loss: 0.382
[69]  [1360/1724] loss: 0.319, ave_loss: 0.381
[70]  [1380/1724] loss: 0.306, ave_loss: 0.380
[71]  [1400/1724] loss: 0.319, ave_loss: 0.379
[72]  [1420/1724] loss: 0.377, ave_loss: 0.379
[73]  [1440/1724] loss: 0.404, ave_loss: 0.380
[74]  [1460/1724] loss: 0.414, ave_loss: 0.380
[75]  [1480/1724] loss: 0.346, ave_loss: 0.380
[76]  [1500/1724] loss: 0.395, ave_loss: 0.380
[77]  [1520/1724] loss: 0.283, ave_loss: 0.379
[78]  [1540/1724] loss: 0.336, ave_loss: 0.378
[79]  [1560/1724] loss: 0.291, ave_loss: 0.377
[80]  [1580/1724] loss: 0.338, ave_loss: 0.376
[81]  [1600/1724] loss: 0.318, ave_loss: 0.376
[82]  [1620/1724] loss: 0.452, ave_loss: 0.377
[83]  [1640/1724] loss: 0.332, ave_loss: 0.376
[84]  [1660/1724] loss: 0.282, ave_loss: 0.375
[85]  [1680/1724] loss: 0.402, ave_loss: 0.375
[86]  [1700/1724] loss: 0.338, ave_loss: 0.375
[87]  [1720/1724] loss: 0.424, ave_loss: 0.375
[88]  [1740/1724] loss: 0.365, ave_loss: 0.375

Finished Training finishing at 2021-08-30 23:29:20.243190
printing_out epoch  16.334106728538284 learning rate: 0.0005153561248318907
0.00031655938251236066
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.753e-01
Validation Loss: 1.640e+05
Validation ROC: 0.7856
No improvement, still saving model
12.665893271461716 epochs left to go

Training Epoch 16.334106728538284/30 starting at 2021-08-30 23:31:19.050734
[1]  [0/1724] loss: 0.540, ave_loss: 0.540
[2]  [20/1724] loss: 0.212, ave_loss: 0.376
[3]  [40/1724] loss: 0.474, ave_loss: 0.409
[4]  [60/1724] loss: 0.365, ave_loss: 0.398
[5]  [80/1724] loss: 0.339, ave_loss: 0.386
[6]  [100/1724] loss: 0.442, ave_loss: 0.395
[7]  [120/1724] loss: 0.400, ave_loss: 0.396
[8]  [140/1724] loss: 0.272, ave_loss: 0.380
[9]  [160/1724] loss: 0.469, ave_loss: 0.390
[10]  [180/1724] loss: 0.381, ave_loss: 0.389
[11]  [200/1724] loss: 0.419, ave_loss: 0.392
[12]  [220/1724] loss: 0.385, ave_loss: 0.391
[13]  [240/1724] loss: 0.383, ave_loss: 0.391
[14]  [260/1724] loss: 0.341, ave_loss: 0.387
[15]  [280/1724] loss: 0.394, ave_loss: 0.388
[16]  [300/1724] loss: 0.490, ave_loss: 0.394
[17]  [320/1724] loss: 0.428, ave_loss: 0.396
[18]  [340/1724] loss: 0.545, ave_loss: 0.404
[19]  [360/1724] loss: 0.301, ave_loss: 0.399
[20]  [380/1724] loss: 0.434, ave_loss: 0.401
[21]  [400/1724] loss: 0.376, ave_loss: 0.399
[22]  [420/1724] loss: 0.307, ave_loss: 0.395
[23]  [440/1724] loss: 0.302, ave_loss: 0.391
[24]  [460/1724] loss: 0.290, ave_loss: 0.387
[25]  [480/1724] loss: 0.478, ave_loss: 0.391
[26]  [500/1724] loss: 0.520, ave_loss: 0.396
[27]  [520/1724] loss: 0.444, ave_loss: 0.397
[28]  [540/1724] loss: 0.304, ave_loss: 0.394
[29]  [560/1724] loss: 0.305, ave_loss: 0.391
[30]  [580/1724] loss: 0.517, ave_loss: 0.395
[31]  [600/1724] loss: 0.292, ave_loss: 0.392
[32]  [620/1724] loss: 0.334, ave_loss: 0.390
[33]  [640/1724] loss: 0.396, ave_loss: 0.390
[34]  [660/1724] loss: 0.336, ave_loss: 0.389
[35]  [680/1724] loss: 0.508, ave_loss: 0.392
[36]  [700/1724] loss: 0.251, ave_loss: 0.388
[37]  [720/1724] loss: 0.264, ave_loss: 0.385
[38]  [740/1724] loss: 0.264, ave_loss: 0.382
[39]  [760/1724] loss: 0.329, ave_loss: 0.380
[40]  [780/1724] loss: 0.318, ave_loss: 0.379
[41]  [800/1724] loss: 0.393, ave_loss: 0.379
[42]  [820/1724] loss: 0.312, ave_loss: 0.377
[43]  [840/1724] loss: 0.275, ave_loss: 0.375
[44]  [860/1724] loss: 0.416, ave_loss: 0.376
[45]  [880/1724] loss: 0.319, ave_loss: 0.375
[46]  [900/1724] loss: 0.225, ave_loss: 0.372
[47]  [920/1724] loss: 0.447, ave_loss: 0.373
[48]  [940/1724] loss: 0.325, ave_loss: 0.372
[49]  [960/1724] loss: 0.389, ave_loss: 0.372
[50]  [980/1724] loss: 0.294, ave_loss: 0.371
[51]  [1000/1724] loss: 0.343, ave_loss: 0.370
[52]  [1020/1724] loss: 0.452, ave_loss: 0.372
[53]  [1040/1724] loss: 0.239, ave_loss: 0.369
[54]  [1060/1724] loss: 0.346, ave_loss: 0.369
[55]  [1080/1724] loss: 0.394, ave_loss: 0.369
[56]  [1100/1724] loss: 0.189, ave_loss: 0.366
[57]  [1120/1724] loss: 0.408, ave_loss: 0.367
[58]  [1140/1724] loss: 0.274, ave_loss: 0.365
[59]  [1160/1724] loss: 0.294, ave_loss: 0.364
[60]  [1180/1724] loss: 0.302, ave_loss: 0.363
[61]  [1200/1724] loss: 0.318, ave_loss: 0.362
[62]  [1220/1724] loss: 0.265, ave_loss: 0.361
[63]  [1240/1724] loss: 0.244, ave_loss: 0.359
[64]  [1260/1724] loss: 0.281, ave_loss: 0.358
[65]  [1280/1724] loss: 0.443, ave_loss: 0.359
[66]  [1300/1724] loss: 0.284, ave_loss: 0.358
[67]  [1320/1724] loss: 0.534, ave_loss: 0.361
[68]  [1340/1724] loss: 0.276, ave_loss: 0.359
[69]  [1360/1724] loss: 0.261, ave_loss: 0.358
[70]  [1380/1724] loss: 0.519, ave_loss: 0.360
[71]  [1400/1724] loss: 0.508, ave_loss: 0.362
[72]  [1420/1724] loss: 0.226, ave_loss: 0.360
[73]  [1440/1724] loss: 0.486, ave_loss: 0.362
[74]  [1460/1724] loss: 0.545, ave_loss: 0.365
[75]  [1480/1724] loss: 0.388, ave_loss: 0.365
[76]  [1500/1724] loss: 0.412, ave_loss: 0.366
[77]  [1520/1724] loss: 0.400, ave_loss: 0.366
[78]  [1540/1724] loss: 0.278, ave_loss: 0.365
[79]  [1560/1724] loss: 0.334, ave_loss: 0.364
[80]  [1580/1724] loss: 0.385, ave_loss: 0.365
[81]  [1600/1724] loss: 0.433, ave_loss: 0.366
[82]  [1620/1724] loss: 0.342, ave_loss: 0.365
[83]  [1640/1724] loss: 0.290, ave_loss: 0.364
[84]  [1660/1724] loss: 0.362, ave_loss: 0.364
[85]  [1680/1724] loss: 0.469, ave_loss: 0.366
[86]  [1700/1724] loss: 0.409, ave_loss: 0.366
[87]  [1720/1724] loss: 0.315, ave_loss: 0.365
[88]  [1740/1724] loss: 0.404, ave_loss: 0.366

Finished Training finishing at 2021-08-30 23:35:55.919115
printing_out epoch  17.354988399071924 learning rate: 0.0005153561248318907
0.00030706260103698985
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.659e-01
Validation Loss: 1.602e+05
Validation ROC: 0.7880
No improvement, still saving model
11.645011600928076 epochs left to go

Training Epoch 17.354988399071924/30 starting at 2021-08-30 23:37:45.496637
[1]  [0/1724] loss: 0.406, ave_loss: 0.406
[2]  [20/1724] loss: 0.368, ave_loss: 0.387
[3]  [40/1724] loss: 0.389, ave_loss: 0.388
[4]  [60/1724] loss: 0.411, ave_loss: 0.394
[5]  [80/1724] loss: 0.390, ave_loss: 0.393
[6]  [100/1724] loss: 0.492, ave_loss: 0.409
[7]  [120/1724] loss: 0.381, ave_loss: 0.405
[8]  [140/1724] loss: 0.356, ave_loss: 0.399
[9]  [160/1724] loss: 0.370, ave_loss: 0.396
[10]  [180/1724] loss: 0.304, ave_loss: 0.387
[11]  [200/1724] loss: 0.243, ave_loss: 0.374
[12]  [220/1724] loss: 0.385, ave_loss: 0.374
[13]  [240/1724] loss: 0.523, ave_loss: 0.386
[14]  [260/1724] loss: 0.322, ave_loss: 0.381
[15]  [280/1724] loss: 0.407, ave_loss: 0.383
[16]  [300/1724] loss: 0.356, ave_loss: 0.381
[17]  [320/1724] loss: 0.358, ave_loss: 0.380
[18]  [340/1724] loss: 0.335, ave_loss: 0.377
[19]  [360/1724] loss: 0.458, ave_loss: 0.382
[20]  [380/1724] loss: 0.387, ave_loss: 0.382
[21]  [400/1724] loss: 0.357, ave_loss: 0.381
[22]  [420/1724] loss: 0.252, ave_loss: 0.375
[23]  [440/1724] loss: 0.409, ave_loss: 0.376
[24]  [460/1724] loss: 0.451, ave_loss: 0.380
[25]  [480/1724] loss: 0.449, ave_loss: 0.382
[26]  [500/1724] loss: 0.345, ave_loss: 0.381
[27]  [520/1724] loss: 0.427, ave_loss: 0.383
[28]  [540/1724] loss: 0.384, ave_loss: 0.383
[29]  [560/1724] loss: 0.281, ave_loss: 0.379
[30]  [580/1724] loss: 0.413, ave_loss: 0.380
[31]  [600/1724] loss: 0.347, ave_loss: 0.379
[32]  [620/1724] loss: 0.343, ave_loss: 0.378
[33]  [640/1724] loss: 0.380, ave_loss: 0.378
[34]  [660/1724] loss: 0.460, ave_loss: 0.381
[35]  [680/1724] loss: 0.284, ave_loss: 0.378
[36]  [700/1724] loss: 0.407, ave_loss: 0.379
[37]  [720/1724] loss: 0.259, ave_loss: 0.375
[38]  [740/1724] loss: 0.509, ave_loss: 0.379
[39]  [760/1724] loss: 0.290, ave_loss: 0.377
[40]  [780/1724] loss: 0.362, ave_loss: 0.376
[41]  [800/1724] loss: 0.277, ave_loss: 0.374
[42]  [820/1724] loss: 0.315, ave_loss: 0.372
[43]  [840/1724] loss: 0.306, ave_loss: 0.371
[44]  [860/1724] loss: 0.384, ave_loss: 0.371
[45]  [880/1724] loss: 0.295, ave_loss: 0.369
[46]  [900/1724] loss: 0.625, ave_loss: 0.375
[47]  [920/1724] loss: 0.431, ave_loss: 0.376
[48]  [940/1724] loss: 0.334, ave_loss: 0.375
[49]  [960/1724] loss: 0.390, ave_loss: 0.376
[50]  [980/1724] loss: 0.336, ave_loss: 0.375
[51]  [1000/1724] loss: 0.368, ave_loss: 0.375
[52]  [1020/1724] loss: 0.298, ave_loss: 0.373
[53]  [1040/1724] loss: 0.337, ave_loss: 0.373
[54]  [1060/1724] loss: 0.213, ave_loss: 0.370
[55]  [1080/1724] loss: 0.326, ave_loss: 0.369
[56]  [1100/1724] loss: 0.454, ave_loss: 0.370
[57]  [1120/1724] loss: 0.373, ave_loss: 0.370
[58]  [1140/1724] loss: 0.704, ave_loss: 0.376
[59]  [1160/1724] loss: 0.502, ave_loss: 0.378
[60]  [1180/1724] loss: 0.478, ave_loss: 0.380
[61]  [1200/1724] loss: 0.313, ave_loss: 0.379
[62]  [1220/1724] loss: 0.238, ave_loss: 0.377
[63]  [1240/1724] loss: 0.252, ave_loss: 0.375
[64]  [1260/1724] loss: 0.382, ave_loss: 0.375
[65]  [1280/1724] loss: 0.449, ave_loss: 0.376
[66]  [1300/1724] loss: 0.275, ave_loss: 0.374
[67]  [1320/1724] loss: 0.315, ave_loss: 0.373
[68]  [1340/1724] loss: 0.256, ave_loss: 0.372
[69]  [1360/1724] loss: 0.324, ave_loss: 0.371
[70]  [1380/1724] loss: 0.278, ave_loss: 0.370
[71]  [1400/1724] loss: 0.268, ave_loss: 0.368
[72]  [1420/1724] loss: 0.340, ave_loss: 0.368
[73]  [1440/1724] loss: 0.288, ave_loss: 0.367
[74]  [1460/1724] loss: 0.316, ave_loss: 0.366
[75]  [1480/1724] loss: 0.308, ave_loss: 0.365
[76]  [1500/1724] loss: 0.280, ave_loss: 0.364
[77]  [1520/1724] loss: 0.297, ave_loss: 0.363
[78]  [1540/1724] loss: 0.380, ave_loss: 0.364
[79]  [1560/1724] loss: 0.378, ave_loss: 0.364
[80]  [1580/1724] loss: 0.546, ave_loss: 0.366
[81]  [1600/1724] loss: 0.178, ave_loss: 0.364
[82]  [1620/1724] loss: 0.579, ave_loss: 0.366
[83]  [1640/1724] loss: 0.330, ave_loss: 0.366
[84]  [1660/1724] loss: 0.605, ave_loss: 0.369
[85]  [1680/1724] loss: 0.327, ave_loss: 0.368
[86]  [1700/1724] loss: 0.363, ave_loss: 0.368
[87]  [1720/1724] loss: 0.300, ave_loss: 0.367
[88]  [1740/1724] loss: 0.263, ave_loss: 0.366

Finished Training finishing at 2021-08-30 23:42:50.633710
printing_out epoch  18.37587006960557 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.662e-01
Validation Loss: 1.302e+05
Validation ROC: 0.7806
No improvement, still saving model
10.624129930394432 epochs left to go

Training Epoch 18.37587006960557/30 starting at 2021-08-30 23:44:36.558905
[1]  [0/1724] loss: 0.598, ave_loss: 0.598
[2]  [20/1724] loss: 0.448, ave_loss: 0.523
[3]  [40/1724] loss: 0.352, ave_loss: 0.466
[4]  [60/1724] loss: 0.354, ave_loss: 0.438
[5]  [80/1724] loss: 0.331, ave_loss: 0.417
[6]  [100/1724] loss: 0.393, ave_loss: 0.413
[7]  [120/1724] loss: 0.473, ave_loss: 0.421
[8]  [140/1724] loss: 0.356, ave_loss: 0.413
[9]  [160/1724] loss: 0.307, ave_loss: 0.401
[10]  [180/1724] loss: 0.365, ave_loss: 0.398
[11]  [200/1724] loss: 0.341, ave_loss: 0.393
[12]  [220/1724] loss: 0.416, ave_loss: 0.395
[13]  [240/1724] loss: 0.318, ave_loss: 0.389
[14]  [260/1724] loss: 0.279, ave_loss: 0.381
[15]  [280/1724] loss: 0.375, ave_loss: 0.380
[16]  [300/1724] loss: 0.257, ave_loss: 0.373
[17]  [320/1724] loss: 0.339, ave_loss: 0.371
[18]  [340/1724] loss: 0.365, ave_loss: 0.370
[19]  [360/1724] loss: 0.251, ave_loss: 0.364
[20]  [380/1724] loss: 0.465, ave_loss: 0.369
[21]  [400/1724] loss: 0.610, ave_loss: 0.381
[22]  [420/1724] loss: 0.384, ave_loss: 0.381
[23]  [440/1724] loss: 0.637, ave_loss: 0.392
[24]  [460/1724] loss: 0.282, ave_loss: 0.387
[25]  [480/1724] loss: 0.299, ave_loss: 0.384
[26]  [500/1724] loss: 0.410, ave_loss: 0.385
[27]  [520/1724] loss: 0.558, ave_loss: 0.391
[28]  [540/1724] loss: 0.348, ave_loss: 0.390
[29]  [560/1724] loss: 0.328, ave_loss: 0.388
[30]  [580/1724] loss: 0.515, ave_loss: 0.392
[31]  [600/1724] loss: 0.264, ave_loss: 0.388
[32]  [620/1724] loss: 0.306, ave_loss: 0.385
[33]  [640/1724] loss: 0.458, ave_loss: 0.387
[34]  [660/1724] loss: 0.284, ave_loss: 0.384
[35]  [680/1724] loss: 0.399, ave_loss: 0.385
[36]  [700/1724] loss: 0.313, ave_loss: 0.383
[37]  [720/1724] loss: 0.386, ave_loss: 0.383
[38]  [740/1724] loss: 0.274, ave_loss: 0.380
[39]  [760/1724] loss: 0.345, ave_loss: 0.379
[40]  [780/1724] loss: 0.358, ave_loss: 0.379
[41]  [800/1724] loss: 0.396, ave_loss: 0.379
[42]  [820/1724] loss: 0.218, ave_loss: 0.375
[43]  [840/1724] loss: 0.213, ave_loss: 0.371
[44]  [860/1724] loss: 0.266, ave_loss: 0.369
[45]  [880/1724] loss: 0.457, ave_loss: 0.371
[46]  [900/1724] loss: 0.294, ave_loss: 0.369
[47]  [920/1724] loss: 0.483, ave_loss: 0.372
[48]  [940/1724] loss: 0.332, ave_loss: 0.371
[49]  [960/1724] loss: 0.366, ave_loss: 0.371
[50]  [980/1724] loss: 0.241, ave_loss: 0.368
[51]  [1000/1724] loss: 0.441, ave_loss: 0.370
[52]  [1020/1724] loss: 0.340, ave_loss: 0.369
[53]  [1040/1724] loss: 0.486, ave_loss: 0.371
[54]  [1060/1724] loss: 0.216, ave_loss: 0.368
[55]  [1080/1724] loss: 0.353, ave_loss: 0.368
[56]  [1100/1724] loss: 0.290, ave_loss: 0.367
[57]  [1120/1724] loss: 0.328, ave_loss: 0.366
[58]  [1140/1724] loss: 0.390, ave_loss: 0.366
[59]  [1160/1724] loss: 0.588, ave_loss: 0.370
[60]  [1180/1724] loss: 0.294, ave_loss: 0.369
[61]  [1200/1724] loss: 0.426, ave_loss: 0.370
[62]  [1220/1724] loss: 0.360, ave_loss: 0.370
[63]  [1240/1724] loss: 0.269, ave_loss: 0.368
[64]  [1260/1724] loss: 0.415, ave_loss: 0.369
[65]  [1280/1724] loss: 0.276, ave_loss: 0.367
[66]  [1300/1724] loss: 0.537, ave_loss: 0.370
[67]  [1320/1724] loss: 0.391, ave_loss: 0.370
[68]  [1340/1724] loss: 0.217, ave_loss: 0.368
[69]  [1360/1724] loss: 0.339, ave_loss: 0.368
[70]  [1380/1724] loss: 0.219, ave_loss: 0.365
[71]  [1400/1724] loss: 0.383, ave_loss: 0.366
[72]  [1420/1724] loss: 0.296, ave_loss: 0.365
[73]  [1440/1724] loss: 0.332, ave_loss: 0.364
[74]  [1460/1724] loss: 0.278, ave_loss: 0.363
[75]  [1480/1724] loss: 0.300, ave_loss: 0.362
[76]  [1500/1724] loss: 0.386, ave_loss: 0.363
[77]  [1520/1724] loss: 0.341, ave_loss: 0.362
[78]  [1540/1724] loss: 0.455, ave_loss: 0.364
[79]  [1560/1724] loss: 0.249, ave_loss: 0.362
[80]  [1580/1724] loss: 0.414, ave_loss: 0.363
[81]  [1600/1724] loss: 0.524, ave_loss: 0.365
[82]  [1620/1724] loss: 0.407, ave_loss: 0.365
[83]  [1640/1724] loss: 0.304, ave_loss: 0.365
[84]  [1660/1724] loss: 0.625, ave_loss: 0.368
[85]  [1680/1724] loss: 0.303, ave_loss: 0.367
[86]  [1700/1724] loss: 0.288, ave_loss: 0.366
[87]  [1720/1724] loss: 0.330, ave_loss: 0.366
[88]  [1740/1724] loss: 0.310, ave_loss: 0.365

Finished Training finishing at 2021-08-30 23:49:20.470099
printing_out epoch  19.396751740139212 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.649e-01
Validation Loss: 1.313e+05
Validation ROC: 0.7883
Saving model
9.603248259860788 epochs left to go

Training Epoch 19.396751740139212/30 starting at 2021-08-30 23:51:22.551142
[1]  [0/1724] loss: 0.386, ave_loss: 0.386
[2]  [20/1724] loss: 0.375, ave_loss: 0.381
[3]  [40/1724] loss: 0.308, ave_loss: 0.356
[4]  [60/1724] loss: 0.507, ave_loss: 0.394
[5]  [80/1724] loss: 0.169, ave_loss: 0.349
[6]  [100/1724] loss: 0.249, ave_loss: 0.332
[7]  [120/1724] loss: 0.389, ave_loss: 0.340
[8]  [140/1724] loss: 0.406, ave_loss: 0.349
[9]  [160/1724] loss: 0.347, ave_loss: 0.348
[10]  [180/1724] loss: 0.357, ave_loss: 0.349
[11]  [200/1724] loss: 0.425, ave_loss: 0.356
[12]  [220/1724] loss: 0.315, ave_loss: 0.353
[13]  [240/1724] loss: 0.344, ave_loss: 0.352
[14]  [260/1724] loss: 0.256, ave_loss: 0.345
[15]  [280/1724] loss: 0.391, ave_loss: 0.348
[16]  [300/1724] loss: 0.435, ave_loss: 0.354
[17]  [320/1724] loss: 0.183, ave_loss: 0.344
[18]  [340/1724] loss: 0.283, ave_loss: 0.340
[19]  [360/1724] loss: 0.524, ave_loss: 0.350
[20]  [380/1724] loss: 0.368, ave_loss: 0.351
[21]  [400/1724] loss: 0.330, ave_loss: 0.350
[22]  [420/1724] loss: 0.285, ave_loss: 0.347
[23]  [440/1724] loss: 0.348, ave_loss: 0.347
[24]  [460/1724] loss: 0.595, ave_loss: 0.357
[25]  [480/1724] loss: 0.418, ave_loss: 0.360
[26]  [500/1724] loss: 0.324, ave_loss: 0.358
[27]  [520/1724] loss: 0.274, ave_loss: 0.355
[28]  [540/1724] loss: 0.342, ave_loss: 0.355
[29]  [560/1724] loss: 0.385, ave_loss: 0.356
[30]  [580/1724] loss: 0.330, ave_loss: 0.355
[31]  [600/1724] loss: 0.498, ave_loss: 0.360
[32]  [620/1724] loss: 0.469, ave_loss: 0.363
[33]  [640/1724] loss: 0.424, ave_loss: 0.365
[34]  [660/1724] loss: 0.261, ave_loss: 0.362
[35]  [680/1724] loss: 0.337, ave_loss: 0.361
[36]  [700/1724] loss: 0.310, ave_loss: 0.360
[37]  [720/1724] loss: 0.272, ave_loss: 0.357
[38]  [740/1724] loss: 0.358, ave_loss: 0.357
[39]  [760/1724] loss: 0.328, ave_loss: 0.357
[40]  [780/1724] loss: 0.291, ave_loss: 0.355
[41]  [800/1724] loss: 0.408, ave_loss: 0.356
[42]  [820/1724] loss: 0.348, ave_loss: 0.356
[43]  [840/1724] loss: 0.370, ave_loss: 0.356
[44]  [860/1724] loss: 0.619, ave_loss: 0.362
[45]  [880/1724] loss: 0.308, ave_loss: 0.361
[46]  [900/1724] loss: 0.509, ave_loss: 0.364
[47]  [920/1724] loss: 0.399, ave_loss: 0.365
[48]  [940/1724] loss: 0.349, ave_loss: 0.365
[49]  [960/1724] loss: 0.224, ave_loss: 0.362
[50]  [980/1724] loss: 0.308, ave_loss: 0.361
[51]  [1000/1724] loss: 0.315, ave_loss: 0.360
[52]  [1020/1724] loss: 0.347, ave_loss: 0.360
[53]  [1040/1724] loss: 0.394, ave_loss: 0.360
[54]  [1060/1724] loss: 0.388, ave_loss: 0.361
[55]  [1080/1724] loss: 0.334, ave_loss: 0.360
[56]  [1100/1724] loss: 0.412, ave_loss: 0.361
[57]  [1120/1724] loss: 0.456, ave_loss: 0.363
[58]  [1140/1724] loss: 0.339, ave_loss: 0.362
[59]  [1160/1724] loss: 0.425, ave_loss: 0.364
[60]  [1180/1724] loss: 0.352, ave_loss: 0.363
[61]  [1200/1724] loss: 0.366, ave_loss: 0.363
[62]  [1220/1724] loss: 0.230, ave_loss: 0.361
[63]  [1240/1724] loss: 0.313, ave_loss: 0.360
[64]  [1260/1724] loss: 0.397, ave_loss: 0.361
[65]  [1280/1724] loss: 0.381, ave_loss: 0.361
[66]  [1300/1724] loss: 0.263, ave_loss: 0.360
[67]  [1320/1724] loss: 0.281, ave_loss: 0.359
[68]  [1340/1724] loss: 0.502, ave_loss: 0.361
[69]  [1360/1724] loss: 0.429, ave_loss: 0.362
[70]  [1380/1724] loss: 0.325, ave_loss: 0.361
[71]  [1400/1724] loss: 0.300, ave_loss: 0.360
[72]  [1420/1724] loss: 0.285, ave_loss: 0.359
[73]  [1440/1724] loss: 0.287, ave_loss: 0.358
[74]  [1460/1724] loss: 0.187, ave_loss: 0.356
[75]  [1480/1724] loss: 0.325, ave_loss: 0.356
[76]  [1500/1724] loss: 0.313, ave_loss: 0.355
[77]  [1520/1724] loss: 0.326, ave_loss: 0.355
[78]  [1540/1724] loss: 0.409, ave_loss: 0.355
[79]  [1560/1724] loss: 0.560, ave_loss: 0.358
[80]  [1580/1724] loss: 0.404, ave_loss: 0.359
[81]  [1600/1724] loss: 0.261, ave_loss: 0.357
[82]  [1620/1724] loss: 0.240, ave_loss: 0.356
[83]  [1640/1724] loss: 0.463, ave_loss: 0.357
[84]  [1660/1724] loss: 0.293, ave_loss: 0.356
[85]  [1680/1724] loss: 0.363, ave_loss: 0.357
[86]  [1700/1724] loss: 0.492, ave_loss: 0.358
[87]  [1720/1724] loss: 0.615, ave_loss: 0.361
[88]  [1740/1724] loss: 0.475, ave_loss: 0.362

Finished Training finishing at 2021-08-30 23:55:56.218335
printing_out epoch  20.417633410672853 learning rate: 0.00042591415275362865
0.00040074262632588915
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.624e-01
Validation Loss: 1.025e+05
Validation ROC: 0.7893
Saving model
8.582366589327147 epochs left to go

Training Epoch 20.417633410672853/30 starting at 2021-08-30 23:58:03.907313
[1]  [0/1724] loss: 0.370, ave_loss: 0.370
[2]  [20/1724] loss: 0.303, ave_loss: 0.336
[3]  [40/1724] loss: 0.367, ave_loss: 0.347
[4]  [60/1724] loss: 0.479, ave_loss: 0.380
[5]  [80/1724] loss: 0.261, ave_loss: 0.356
[6]  [100/1724] loss: 0.385, ave_loss: 0.361
[7]  [120/1724] loss: 0.432, ave_loss: 0.371
[8]  [140/1724] loss: 0.200, ave_loss: 0.350
[9]  [160/1724] loss: 0.540, ave_loss: 0.371
[10]  [180/1724] loss: 0.298, ave_loss: 0.364
[11]  [200/1724] loss: 0.273, ave_loss: 0.355
[12]  [220/1724] loss: 0.415, ave_loss: 0.360
[13]  [240/1724] loss: 0.462, ave_loss: 0.368
[14]  [260/1724] loss: 0.345, ave_loss: 0.366
[15]  [280/1724] loss: 0.314, ave_loss: 0.363
[16]  [300/1724] loss: 0.272, ave_loss: 0.357
[17]  [320/1724] loss: 0.360, ave_loss: 0.357
[18]  [340/1724] loss: 0.385, ave_loss: 0.359
[19]  [360/1724] loss: 0.331, ave_loss: 0.357
[20]  [380/1724] loss: 0.319, ave_loss: 0.356
[21]  [400/1724] loss: 0.252, ave_loss: 0.351
[22]  [420/1724] loss: 0.422, ave_loss: 0.354
[23]  [440/1724] loss: 0.457, ave_loss: 0.358
[24]  [460/1724] loss: 0.368, ave_loss: 0.359
[25]  [480/1724] loss: 0.282, ave_loss: 0.356
[26]  [500/1724] loss: 0.327, ave_loss: 0.355
[27]  [520/1724] loss: 0.364, ave_loss: 0.355
[28]  [540/1724] loss: 0.463, ave_loss: 0.359
[29]  [560/1724] loss: 0.461, ave_loss: 0.362
[30]  [580/1724] loss: 0.303, ave_loss: 0.360
[31]  [600/1724] loss: 0.351, ave_loss: 0.360
[32]  [620/1724] loss: 0.440, ave_loss: 0.363
[33]  [640/1724] loss: 0.377, ave_loss: 0.363
[34]  [660/1724] loss: 0.290, ave_loss: 0.361
[35]  [680/1724] loss: 0.312, ave_loss: 0.359
[36]  [700/1724] loss: 0.337, ave_loss: 0.359
[37]  [720/1724] loss: 0.308, ave_loss: 0.357
[38]  [740/1724] loss: 0.323, ave_loss: 0.357
[39]  [760/1724] loss: 0.491, ave_loss: 0.360
[40]  [780/1724] loss: 0.352, ave_loss: 0.360
[41]  [800/1724] loss: 0.310, ave_loss: 0.359
[42]  [820/1724] loss: 0.364, ave_loss: 0.359
[43]  [840/1724] loss: 0.275, ave_loss: 0.357
[44]  [860/1724] loss: 0.278, ave_loss: 0.355
[45]  [880/1724] loss: 0.360, ave_loss: 0.355
[46]  [900/1724] loss: 0.445, ave_loss: 0.357
[47]  [920/1724] loss: 0.338, ave_loss: 0.357
[48]  [940/1724] loss: 0.292, ave_loss: 0.355
[49]  [960/1724] loss: 0.380, ave_loss: 0.356
[50]  [980/1724] loss: 0.559, ave_loss: 0.360
[51]  [1000/1724] loss: 0.343, ave_loss: 0.360
[52]  [1020/1724] loss: 0.529, ave_loss: 0.363
[53]  [1040/1724] loss: 0.254, ave_loss: 0.361
[54]  [1060/1724] loss: 0.582, ave_loss: 0.365
[55]  [1080/1724] loss: 0.321, ave_loss: 0.364
[56]  [1100/1724] loss: 0.403, ave_loss: 0.365
[57]  [1120/1724] loss: 0.418, ave_loss: 0.366
[58]  [1140/1724] loss: 0.375, ave_loss: 0.366
[59]  [1160/1724] loss: 0.348, ave_loss: 0.366
[60]  [1180/1724] loss: 0.434, ave_loss: 0.367
[61]  [1200/1724] loss: 0.339, ave_loss: 0.366
[62]  [1220/1724] loss: 0.280, ave_loss: 0.365
[63]  [1240/1724] loss: 0.382, ave_loss: 0.365
[64]  [1260/1724] loss: 0.420, ave_loss: 0.366
[65]  [1280/1724] loss: 0.260, ave_loss: 0.364
[66]  [1300/1724] loss: 0.416, ave_loss: 0.365
[67]  [1320/1724] loss: 0.401, ave_loss: 0.366
[68]  [1340/1724] loss: 0.356, ave_loss: 0.366
[69]  [1360/1724] loss: 0.270, ave_loss: 0.364
[70]  [1380/1724] loss: 0.269, ave_loss: 0.363
[71]  [1400/1724] loss: 0.465, ave_loss: 0.364
[72]  [1420/1724] loss: 0.314, ave_loss: 0.364
[73]  [1440/1724] loss: 0.368, ave_loss: 0.364
[74]  [1460/1724] loss: 0.283, ave_loss: 0.363
[75]  [1480/1724] loss: 0.364, ave_loss: 0.363
[76]  [1500/1724] loss: 0.421, ave_loss: 0.363
[77]  [1520/1724] loss: 0.371, ave_loss: 0.363
[78]  [1540/1724] loss: 0.364, ave_loss: 0.363
[79]  [1560/1724] loss: 0.386, ave_loss: 0.364
[80]  [1580/1724] loss: 0.245, ave_loss: 0.362
[81]  [1600/1724] loss: 0.468, ave_loss: 0.364
[82]  [1620/1724] loss: 0.364, ave_loss: 0.364
[83]  [1640/1724] loss: 0.375, ave_loss: 0.364
[84]  [1660/1724] loss: 0.403, ave_loss: 0.364
[85]  [1680/1724] loss: 0.389, ave_loss: 0.364
[86]  [1700/1724] loss: 0.257, ave_loss: 0.363
[87]  [1720/1724] loss: 0.295, ave_loss: 0.362
[88]  [1740/1724] loss: 0.389, ave_loss: 0.363

Finished Training finishing at 2021-08-31 00:02:47.166315
printing_out epoch  21.438515081206496 learning rate: 0.00042591415275362865
0.00038872034753611247
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.627e-01
Validation Loss: 1.050e+05
Validation ROC: 0.7986
Saving model
7.561484918793504 epochs left to go

Training Epoch 21.438515081206496/30 starting at 2021-08-31 00:04:23.572494
[1]  [0/1724] loss: 0.329, ave_loss: 0.329
[2]  [20/1724] loss: 0.406, ave_loss: 0.367
[3]  [40/1724] loss: 0.301, ave_loss: 0.345
[4]  [60/1724] loss: 0.460, ave_loss: 0.374
[5]  [80/1724] loss: 0.427, ave_loss: 0.385
[6]  [100/1724] loss: 0.362, ave_loss: 0.381
[7]  [120/1724] loss: 0.172, ave_loss: 0.351
[8]  [140/1724] loss: 0.326, ave_loss: 0.348
[9]  [160/1724] loss: 0.354, ave_loss: 0.349
[10]  [180/1724] loss: 0.537, ave_loss: 0.367
[11]  [200/1724] loss: 0.247, ave_loss: 0.357
[12]  [220/1724] loss: 0.264, ave_loss: 0.349
[13]  [240/1724] loss: 0.338, ave_loss: 0.348
[14]  [260/1724] loss: 0.198, ave_loss: 0.337
[15]  [280/1724] loss: 0.315, ave_loss: 0.336
[16]  [300/1724] loss: 0.328, ave_loss: 0.335
[17]  [320/1724] loss: 0.292, ave_loss: 0.333
[18]  [340/1724] loss: 0.382, ave_loss: 0.335
[19]  [360/1724] loss: 0.331, ave_loss: 0.335
[20]  [380/1724] loss: 0.485, ave_loss: 0.343
[21]  [400/1724] loss: 0.442, ave_loss: 0.347
[22]  [420/1724] loss: 0.429, ave_loss: 0.351
[23]  [440/1724] loss: 0.360, ave_loss: 0.352
[24]  [460/1724] loss: 0.373, ave_loss: 0.352
[25]  [480/1724] loss: 0.309, ave_loss: 0.351
[26]  [500/1724] loss: 0.367, ave_loss: 0.351
[27]  [520/1724] loss: 0.398, ave_loss: 0.353
[28]  [540/1724] loss: 0.316, ave_loss: 0.352
[29]  [560/1724] loss: 0.357, ave_loss: 0.352
[30]  [580/1724] loss: 0.503, ave_loss: 0.357
[31]  [600/1724] loss: 0.374, ave_loss: 0.358
[32]  [620/1724] loss: 0.274, ave_loss: 0.355
[33]  [640/1724] loss: 0.260, ave_loss: 0.352
[34]  [660/1724] loss: 0.347, ave_loss: 0.352
[35]  [680/1724] loss: 0.209, ave_loss: 0.348
[36]  [700/1724] loss: 0.485, ave_loss: 0.352
[37]  [720/1724] loss: 0.360, ave_loss: 0.352
[38]  [740/1724] loss: 0.254, ave_loss: 0.349
[39]  [760/1724] loss: 0.257, ave_loss: 0.347
[40]  [780/1724] loss: 0.228, ave_loss: 0.344
[41]  [800/1724] loss: 0.319, ave_loss: 0.343
[42]  [820/1724] loss: 0.264, ave_loss: 0.341
[43]  [840/1724] loss: 0.491, ave_loss: 0.345
[44]  [860/1724] loss: 0.392, ave_loss: 0.346
[45]  [880/1724] loss: 0.550, ave_loss: 0.351
[46]  [900/1724] loss: 0.437, ave_loss: 0.352
[47]  [920/1724] loss: 0.517, ave_loss: 0.356
[48]  [940/1724] loss: 0.257, ave_loss: 0.354
[49]  [960/1724] loss: 0.272, ave_loss: 0.352
[50]  [980/1724] loss: 0.348, ave_loss: 0.352
[51]  [1000/1724] loss: 0.603, ave_loss: 0.357
[52]  [1020/1724] loss: 0.423, ave_loss: 0.358
[53]  [1040/1724] loss: 0.521, ave_loss: 0.361
[54]  [1060/1724] loss: 0.161, ave_loss: 0.358
[55]  [1080/1724] loss: 0.370, ave_loss: 0.358
[56]  [1100/1724] loss: 0.253, ave_loss: 0.356
[57]  [1120/1724] loss: 0.241, ave_loss: 0.354
[58]  [1140/1724] loss: 0.392, ave_loss: 0.355
[59]  [1160/1724] loss: 0.271, ave_loss: 0.353
[60]  [1180/1724] loss: 0.294, ave_loss: 0.352
[61]  [1200/1724] loss: 0.284, ave_loss: 0.351
[62]  [1220/1724] loss: 0.365, ave_loss: 0.351
[63]  [1240/1724] loss: 0.317, ave_loss: 0.351
[64]  [1260/1724] loss: 0.268, ave_loss: 0.350
[65]  [1280/1724] loss: 0.282, ave_loss: 0.348
[66]  [1300/1724] loss: 0.385, ave_loss: 0.349
[67]  [1320/1724] loss: 0.493, ave_loss: 0.351
[68]  [1340/1724] loss: 0.358, ave_loss: 0.351
[69]  [1360/1724] loss: 0.371, ave_loss: 0.352
[70]  [1380/1724] loss: 0.287, ave_loss: 0.351
[71]  [1400/1724] loss: 0.385, ave_loss: 0.351
[72]  [1420/1724] loss: 0.377, ave_loss: 0.351
[73]  [1440/1724] loss: 0.305, ave_loss: 0.351
[74]  [1460/1724] loss: 0.369, ave_loss: 0.351
[75]  [1480/1724] loss: 0.299, ave_loss: 0.350
[76]  [1500/1724] loss: 0.344, ave_loss: 0.350
[77]  [1520/1724] loss: 0.311, ave_loss: 0.350
[78]  [1540/1724] loss: 0.285, ave_loss: 0.349
[79]  [1560/1724] loss: 0.155, ave_loss: 0.347
[80]  [1580/1724] loss: 0.492, ave_loss: 0.348
[81]  [1600/1724] loss: 0.225, ave_loss: 0.347
[82]  [1620/1724] loss: 0.459, ave_loss: 0.348
[83]  [1640/1724] loss: 0.421, ave_loss: 0.349
[84]  [1660/1724] loss: 0.467, ave_loss: 0.350
[85]  [1680/1724] loss: 0.333, ave_loss: 0.350
[86]  [1700/1724] loss: 0.240, ave_loss: 0.349
[87]  [1720/1724] loss: 0.307, ave_loss: 0.349
[88]  [1740/1724] loss: 0.346, ave_loss: 0.348

Finished Training finishing at 2021-08-31 00:09:11.477873
printing_out epoch  22.45939675174014 learning rate: 0.00042591415275362865
0.0003770587371100291
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.485e-01
Validation Loss: 1.154e+05
Validation ROC: 0.7942
No improvement, still saving model
6.54060324825986 epochs left to go

Training Epoch 22.45939675174014/30 starting at 2021-08-31 00:10:48.663554
[1]  [0/1724] loss: 0.303, ave_loss: 0.303
[2]  [20/1724] loss: 0.390, ave_loss: 0.346
[3]  [40/1724] loss: 0.303, ave_loss: 0.332
[4]  [60/1724] loss: 0.274, ave_loss: 0.318
[5]  [80/1724] loss: 0.471, ave_loss: 0.348
[6]  [100/1724] loss: 0.277, ave_loss: 0.336
[7]  [120/1724] loss: 0.407, ave_loss: 0.346
[8]  [140/1724] loss: 0.306, ave_loss: 0.341
[9]  [160/1724] loss: 0.178, ave_loss: 0.323
[10]  [180/1724] loss: 0.491, ave_loss: 0.340
[11]  [200/1724] loss: 0.450, ave_loss: 0.350
[12]  [220/1724] loss: 0.276, ave_loss: 0.344
[13]  [240/1724] loss: 0.237, ave_loss: 0.336
[14]  [260/1724] loss: 0.409, ave_loss: 0.341
[15]  [280/1724] loss: 0.340, ave_loss: 0.341
[16]  [300/1724] loss: 0.490, ave_loss: 0.350
[17]  [320/1724] loss: 0.200, ave_loss: 0.341
[18]  [340/1724] loss: 0.447, ave_loss: 0.347
[19]  [360/1724] loss: 0.459, ave_loss: 0.353
[20]  [380/1724] loss: 0.501, ave_loss: 0.360
[21]  [400/1724] loss: 0.340, ave_loss: 0.359
[22]  [420/1724] loss: 0.346, ave_loss: 0.359
[23]  [440/1724] loss: 0.454, ave_loss: 0.363
[24]  [460/1724] loss: 0.438, ave_loss: 0.366
[25]  [480/1724] loss: 0.316, ave_loss: 0.364
[26]  [500/1724] loss: 0.446, ave_loss: 0.367
[27]  [520/1724] loss: 0.374, ave_loss: 0.368
[28]  [540/1724] loss: 0.408, ave_loss: 0.369
[29]  [560/1724] loss: 0.368, ave_loss: 0.369
[30]  [580/1724] loss: 0.331, ave_loss: 0.368
[31]  [600/1724] loss: 0.358, ave_loss: 0.367
[32]  [620/1724] loss: 0.439, ave_loss: 0.370
[33]  [640/1724] loss: 0.257, ave_loss: 0.366
[34]  [660/1724] loss: 0.371, ave_loss: 0.366
[35]  [680/1724] loss: 0.360, ave_loss: 0.366
[36]  [700/1724] loss: 0.237, ave_loss: 0.363
[37]  [720/1724] loss: 0.278, ave_loss: 0.360
[38]  [740/1724] loss: 0.392, ave_loss: 0.361
[39]  [760/1724] loss: 0.362, ave_loss: 0.361
[40]  [780/1724] loss: 0.287, ave_loss: 0.359
[41]  [800/1724] loss: 0.543, ave_loss: 0.364
[42]  [820/1724] loss: 0.469, ave_loss: 0.366
[43]  [840/1724] loss: 0.333, ave_loss: 0.366
[44]  [860/1724] loss: 0.301, ave_loss: 0.364
[45]  [880/1724] loss: 0.284, ave_loss: 0.362
[46]  [900/1724] loss: 0.416, ave_loss: 0.363
[47]  [920/1724] loss: 0.218, ave_loss: 0.360
[48]  [940/1724] loss: 0.426, ave_loss: 0.362
[49]  [960/1724] loss: 0.328, ave_loss: 0.361
[50]  [980/1724] loss: 0.376, ave_loss: 0.361
[51]  [1000/1724] loss: 0.357, ave_loss: 0.361
[52]  [1020/1724] loss: 0.457, ave_loss: 0.363
[53]  [1040/1724] loss: 0.371, ave_loss: 0.363
[54]  [1060/1724] loss: 0.420, ave_loss: 0.364
[55]  [1080/1724] loss: 0.383, ave_loss: 0.365
[56]  [1100/1724] loss: 0.427, ave_loss: 0.366
[57]  [1120/1724] loss: 0.364, ave_loss: 0.366
[58]  [1140/1724] loss: 0.412, ave_loss: 0.367
[59]  [1160/1724] loss: 0.306, ave_loss: 0.366
[60]  [1180/1724] loss: 0.478, ave_loss: 0.367
[61]  [1200/1724] loss: 0.354, ave_loss: 0.367
[62]  [1220/1724] loss: 0.267, ave_loss: 0.366
[63]  [1240/1724] loss: 0.389, ave_loss: 0.366
[64]  [1260/1724] loss: 0.240, ave_loss: 0.364
[65]  [1280/1724] loss: 0.364, ave_loss: 0.364
[66]  [1300/1724] loss: 0.425, ave_loss: 0.365
[67]  [1320/1724] loss: 0.334, ave_loss: 0.364
[68]  [1340/1724] loss: 0.434, ave_loss: 0.365
[69]  [1360/1724] loss: 0.334, ave_loss: 0.365
[70]  [1380/1724] loss: 0.396, ave_loss: 0.365
[71]  [1400/1724] loss: 0.395, ave_loss: 0.366
[72]  [1420/1724] loss: 0.358, ave_loss: 0.366
[73]  [1440/1724] loss: 0.322, ave_loss: 0.365
[74]  [1460/1724] loss: 0.446, ave_loss: 0.366
[75]  [1480/1724] loss: 0.542, ave_loss: 0.369
[76]  [1500/1724] loss: 0.302, ave_loss: 0.368
[77]  [1520/1724] loss: 0.429, ave_loss: 0.368
[78]  [1540/1724] loss: 0.283, ave_loss: 0.367
[79]  [1560/1724] loss: 0.349, ave_loss: 0.367
[80]  [1580/1724] loss: 0.328, ave_loss: 0.367
[81]  [1600/1724] loss: 0.197, ave_loss: 0.365
[82]  [1620/1724] loss: 0.342, ave_loss: 0.364
[83]  [1640/1724] loss: 0.262, ave_loss: 0.363
[84]  [1660/1724] loss: 0.357, ave_loss: 0.363
[85]  [1680/1724] loss: 0.324, ave_loss: 0.363
[86]  [1700/1724] loss: 0.356, ave_loss: 0.362
[87]  [1720/1724] loss: 0.354, ave_loss: 0.362
[88]  [1740/1724] loss: 0.285, ave_loss: 0.361

Finished Training finishing at 2021-08-31 00:15:25.630828
printing_out epoch  23.48027842227378 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.615e-01
Validation Loss: 9.490e+04
Validation ROC: 0.7910
No improvement, still saving model
5.519721577726219 epochs left to go

Training Epoch 23.48027842227378/30 starting at 2021-08-31 00:16:49.967153
[1]  [0/1724] loss: 0.572, ave_loss: 0.572
[2]  [20/1724] loss: 0.497, ave_loss: 0.535
[3]  [40/1724] loss: 0.309, ave_loss: 0.459
[4]  [60/1724] loss: 0.347, ave_loss: 0.431
[5]  [80/1724] loss: 0.353, ave_loss: 0.416
[6]  [100/1724] loss: 0.297, ave_loss: 0.396
[7]  [120/1724] loss: 0.208, ave_loss: 0.369
[8]  [140/1724] loss: 0.428, ave_loss: 0.376
[9]  [160/1724] loss: 0.301, ave_loss: 0.368
[10]  [180/1724] loss: 0.150, ave_loss: 0.346
[11]  [200/1724] loss: 0.369, ave_loss: 0.348
[12]  [220/1724] loss: 0.268, ave_loss: 0.341
[13]  [240/1724] loss: 0.411, ave_loss: 0.347
[14]  [260/1724] loss: 0.267, ave_loss: 0.341
[15]  [280/1724] loss: 0.436, ave_loss: 0.347
[16]  [300/1724] loss: 0.310, ave_loss: 0.345
[17]  [320/1724] loss: 0.386, ave_loss: 0.348
[18]  [340/1724] loss: 0.370, ave_loss: 0.349
[19]  [360/1724] loss: 0.478, ave_loss: 0.356
[20]  [380/1724] loss: 0.244, ave_loss: 0.350
[21]  [400/1724] loss: 0.332, ave_loss: 0.349
[22]  [420/1724] loss: 0.370, ave_loss: 0.350
[23]  [440/1724] loss: 0.257, ave_loss: 0.346
[24]  [460/1724] loss: 0.507, ave_loss: 0.353
[25]  [480/1724] loss: 0.367, ave_loss: 0.353
[26]  [500/1724] loss: 0.422, ave_loss: 0.356
[27]  [520/1724] loss: 0.283, ave_loss: 0.353
[28]  [540/1724] loss: 0.439, ave_loss: 0.356
[29]  [560/1724] loss: 0.361, ave_loss: 0.357
[30]  [580/1724] loss: 0.214, ave_loss: 0.352
[31]  [600/1724] loss: 0.442, ave_loss: 0.355
[32]  [620/1724] loss: 0.264, ave_loss: 0.352
[33]  [640/1724] loss: 0.433, ave_loss: 0.354
[34]  [660/1724] loss: 0.298, ave_loss: 0.353
[35]  [680/1724] loss: 0.374, ave_loss: 0.353
[36]  [700/1724] loss: 0.216, ave_loss: 0.349
[37]  [720/1724] loss: 0.240, ave_loss: 0.347
[38]  [740/1724] loss: 0.379, ave_loss: 0.347
[39]  [760/1724] loss: 0.578, ave_loss: 0.353
[40]  [780/1724] loss: 0.270, ave_loss: 0.351
[41]  [800/1724] loss: 0.459, ave_loss: 0.354
[42]  [820/1724] loss: 0.415, ave_loss: 0.355
[43]  [840/1724] loss: 0.289, ave_loss: 0.354
[44]  [860/1724] loss: 0.307, ave_loss: 0.353
[45]  [880/1724] loss: 0.490, ave_loss: 0.356
[46]  [900/1724] loss: 0.389, ave_loss: 0.356
[47]  [920/1724] loss: 0.327, ave_loss: 0.356
[48]  [940/1724] loss: 0.260, ave_loss: 0.354
[49]  [960/1724] loss: 0.530, ave_loss: 0.357
[50]  [980/1724] loss: 0.248, ave_loss: 0.355
[51]  [1000/1724] loss: 0.356, ave_loss: 0.355
[52]  [1020/1724] loss: 0.321, ave_loss: 0.355
[53]  [1040/1724] loss: 0.345, ave_loss: 0.354
[54]  [1060/1724] loss: 0.370, ave_loss: 0.355
[55]  [1080/1724] loss: 0.282, ave_loss: 0.353
[56]  [1100/1724] loss: 0.499, ave_loss: 0.356
[57]  [1120/1724] loss: 0.278, ave_loss: 0.355
[58]  [1140/1724] loss: 0.263, ave_loss: 0.353
[59]  [1160/1724] loss: 0.326, ave_loss: 0.353
[60]  [1180/1724] loss: 0.277, ave_loss: 0.351
[61]  [1200/1724] loss: 0.473, ave_loss: 0.353
[62]  [1220/1724] loss: 0.548, ave_loss: 0.356
[63]  [1240/1724] loss: 0.301, ave_loss: 0.356
[64]  [1260/1724] loss: 0.349, ave_loss: 0.355
[65]  [1280/1724] loss: 0.247, ave_loss: 0.354
[66]  [1300/1724] loss: 0.337, ave_loss: 0.354
[67]  [1320/1724] loss: 0.356, ave_loss: 0.354
[68]  [1340/1724] loss: 0.310, ave_loss: 0.353
[69]  [1360/1724] loss: 0.315, ave_loss: 0.352
[70]  [1380/1724] loss: 0.365, ave_loss: 0.353
[71]  [1400/1724] loss: 0.237, ave_loss: 0.351
[72]  [1420/1724] loss: 0.408, ave_loss: 0.352
[73]  [1440/1724] loss: 0.266, ave_loss: 0.351
[74]  [1460/1724] loss: 0.287, ave_loss: 0.350
[75]  [1480/1724] loss: 0.309, ave_loss: 0.349
[76]  [1500/1724] loss: 0.368, ave_loss: 0.349
[77]  [1520/1724] loss: 0.437, ave_loss: 0.351
[78]  [1540/1724] loss: 0.323, ave_loss: 0.350
[79]  [1560/1724] loss: 0.250, ave_loss: 0.349
[80]  [1580/1724] loss: 0.481, ave_loss: 0.351
[81]  [1600/1724] loss: 0.278, ave_loss: 0.350
[82]  [1620/1724] loss: 0.394, ave_loss: 0.350
[83]  [1640/1724] loss: 0.297, ave_loss: 0.350
[84]  [1660/1724] loss: 0.319, ave_loss: 0.349
[85]  [1680/1724] loss: 0.277, ave_loss: 0.348
[86]  [1700/1724] loss: 0.260, ave_loss: 0.347
[87]  [1720/1724] loss: 0.363, ave_loss: 0.348
[88]  [1740/1724] loss: 0.313, ave_loss: 0.347

Finished Training finishing at 2021-08-31 00:21:42.312183
printing_out epoch  24.501160092807424 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.471e-01
Validation Loss: 1.063e+05
Validation ROC: 0.7929
No improvement, still saving model
4.4988399071925755 epochs left to go

Training Epoch 24.501160092807424/30 starting at 2021-08-31 00:23:16.174204
[1]  [0/1724] loss: 0.509, ave_loss: 0.509
[2]  [20/1724] loss: 0.233, ave_loss: 0.371
[3]  [40/1724] loss: 0.368, ave_loss: 0.370
[4]  [60/1724] loss: 0.339, ave_loss: 0.362
[5]  [80/1724] loss: 0.465, ave_loss: 0.383
[6]  [100/1724] loss: 0.383, ave_loss: 0.383
[7]  [120/1724] loss: 0.212, ave_loss: 0.359
[8]  [140/1724] loss: 0.460, ave_loss: 0.371
[9]  [160/1724] loss: 0.372, ave_loss: 0.371
[10]  [180/1724] loss: 0.332, ave_loss: 0.367
[11]  [200/1724] loss: 0.344, ave_loss: 0.365
[12]  [220/1724] loss: 0.342, ave_loss: 0.363
[13]  [240/1724] loss: 0.465, ave_loss: 0.371
[14]  [260/1724] loss: 0.251, ave_loss: 0.363
[15]  [280/1724] loss: 0.287, ave_loss: 0.358
[16]  [300/1724] loss: 0.382, ave_loss: 0.359
[17]  [320/1724] loss: 0.280, ave_loss: 0.354
[18]  [340/1724] loss: 0.280, ave_loss: 0.350
[19]  [360/1724] loss: 0.389, ave_loss: 0.352
[20]  [380/1724] loss: 0.474, ave_loss: 0.358
[21]  [400/1724] loss: 0.213, ave_loss: 0.351
[22]  [420/1724] loss: 0.361, ave_loss: 0.352
[23]  [440/1724] loss: 0.292, ave_loss: 0.349
[24]  [460/1724] loss: 0.531, ave_loss: 0.357
[25]  [480/1724] loss: 0.175, ave_loss: 0.350
[26]  [500/1724] loss: 0.279, ave_loss: 0.347
[27]  [520/1724] loss: 0.473, ave_loss: 0.352
[28]  [540/1724] loss: 0.458, ave_loss: 0.355
[29]  [560/1724] loss: 0.333, ave_loss: 0.355
[30]  [580/1724] loss: 0.289, ave_loss: 0.352
[31]  [600/1724] loss: 0.357, ave_loss: 0.353
[32]  [620/1724] loss: 0.436, ave_loss: 0.355
[33]  [640/1724] loss: 0.431, ave_loss: 0.357
[34]  [660/1724] loss: 0.256, ave_loss: 0.354
[35]  [680/1724] loss: 0.368, ave_loss: 0.355
[36]  [700/1724] loss: 0.411, ave_loss: 0.356
[37]  [720/1724] loss: 0.470, ave_loss: 0.359
[38]  [740/1724] loss: 0.289, ave_loss: 0.358
[39]  [760/1724] loss: 0.266, ave_loss: 0.355
[40]  [780/1724] loss: 0.457, ave_loss: 0.358
[41]  [800/1724] loss: 0.231, ave_loss: 0.355
[42]  [820/1724] loss: 0.372, ave_loss: 0.355
[43]  [840/1724] loss: 0.292, ave_loss: 0.354
[44]  [860/1724] loss: 0.316, ave_loss: 0.353
[45]  [880/1724] loss: 0.317, ave_loss: 0.352
[46]  [900/1724] loss: 0.280, ave_loss: 0.350
[47]  [920/1724] loss: 0.227, ave_loss: 0.348
[48]  [940/1724] loss: 0.571, ave_loss: 0.352
[49]  [960/1724] loss: 0.269, ave_loss: 0.351
[50]  [980/1724] loss: 0.388, ave_loss: 0.351
[51]  [1000/1724] loss: 0.476, ave_loss: 0.354
[52]  [1020/1724] loss: 0.356, ave_loss: 0.354
[53]  [1040/1724] loss: 0.441, ave_loss: 0.356
[54]  [1060/1724] loss: 0.321, ave_loss: 0.355
[55]  [1080/1724] loss: 0.315, ave_loss: 0.354
[56]  [1100/1724] loss: 0.354, ave_loss: 0.354
[57]  [1120/1724] loss: 0.382, ave_loss: 0.355
[58]  [1140/1724] loss: 0.396, ave_loss: 0.355
[59]  [1160/1724] loss: 0.396, ave_loss: 0.356
[60]  [1180/1724] loss: 0.241, ave_loss: 0.354
[61]  [1200/1724] loss: 0.441, ave_loss: 0.356
[62]  [1220/1724] loss: 0.260, ave_loss: 0.354
[63]  [1240/1724] loss: 0.402, ave_loss: 0.355
[64]  [1260/1724] loss: 0.317, ave_loss: 0.354
[65]  [1280/1724] loss: 0.361, ave_loss: 0.354
[66]  [1300/1724] loss: 0.355, ave_loss: 0.354
[67]  [1320/1724] loss: 0.274, ave_loss: 0.353
[68]  [1340/1724] loss: 0.365, ave_loss: 0.353
[69]  [1360/1724] loss: 0.401, ave_loss: 0.354
[70]  [1380/1724] loss: 0.311, ave_loss: 0.353
[71]  [1400/1724] loss: 0.323, ave_loss: 0.353
[72]  [1420/1724] loss: 0.278, ave_loss: 0.352
[73]  [1440/1724] loss: 0.238, ave_loss: 0.350
[74]  [1460/1724] loss: 0.442, ave_loss: 0.352
[75]  [1480/1724] loss: 0.333, ave_loss: 0.351
[76]  [1500/1724] loss: 0.297, ave_loss: 0.351
[77]  [1520/1724] loss: 0.428, ave_loss: 0.352
[78]  [1540/1724] loss: 0.421, ave_loss: 0.353
[79]  [1560/1724] loss: 0.298, ave_loss: 0.352
[80]  [1580/1724] loss: 0.477, ave_loss: 0.353
[81]  [1600/1724] loss: 0.268, ave_loss: 0.352
[82]  [1620/1724] loss: 0.253, ave_loss: 0.351
[83]  [1640/1724] loss: 0.352, ave_loss: 0.351
[84]  [1660/1724] loss: 0.327, ave_loss: 0.351
[85]  [1680/1724] loss: 0.334, ave_loss: 0.351
[86]  [1700/1724] loss: 0.221, ave_loss: 0.349
[87]  [1720/1724] loss: 0.331, ave_loss: 0.349
[88]  [1740/1724] loss: 0.240, ave_loss: 0.348

Finished Training finishing at 2021-08-31 00:27:40.169402
printing_out epoch  25.52204176334107 learning rate: 0.0003199956068772566
0.0003103957386709389
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.478e-01
Validation Loss: 8.859e+04
Validation ROC: 0.7938
No improvement, still saving model
3.4779582366589317 epochs left to go

Training Epoch 25.52204176334107/30 starting at 2021-08-31 00:29:32.094539
[1]  [0/1724] loss: 0.462, ave_loss: 0.462
[2]  [20/1724] loss: 0.400, ave_loss: 0.431
[3]  [40/1724] loss: 0.482, ave_loss: 0.448
[4]  [60/1724] loss: 0.270, ave_loss: 0.403
[5]  [80/1724] loss: 0.338, ave_loss: 0.390
[6]  [100/1724] loss: 0.390, ave_loss: 0.390
[7]  [120/1724] loss: 0.410, ave_loss: 0.393
[8]  [140/1724] loss: 0.282, ave_loss: 0.379
[9]  [160/1724] loss: 0.346, ave_loss: 0.375
[10]  [180/1724] loss: 0.482, ave_loss: 0.386
[11]  [200/1724] loss: 0.345, ave_loss: 0.382
[12]  [220/1724] loss: 0.310, ave_loss: 0.376
[13]  [240/1724] loss: 0.251, ave_loss: 0.367
[14]  [260/1724] loss: 0.452, ave_loss: 0.373
[15]  [280/1724] loss: 0.515, ave_loss: 0.382
[16]  [300/1724] loss: 0.581, ave_loss: 0.395
[17]  [320/1724] loss: 0.278, ave_loss: 0.388
[18]  [340/1724] loss: 0.415, ave_loss: 0.389
[19]  [360/1724] loss: 0.313, ave_loss: 0.385
[20]  [380/1724] loss: 0.329, ave_loss: 0.383
[21]  [400/1724] loss: 0.423, ave_loss: 0.384
[22]  [420/1724] loss: 0.308, ave_loss: 0.381
[23]  [440/1724] loss: 0.396, ave_loss: 0.382
[24]  [460/1724] loss: 0.232, ave_loss: 0.375
[25]  [480/1724] loss: 0.310, ave_loss: 0.373
[26]  [500/1724] loss: 0.258, ave_loss: 0.368
[27]  [520/1724] loss: 0.455, ave_loss: 0.372
[28]  [540/1724] loss: 0.413, ave_loss: 0.373
[29]  [560/1724] loss: 0.528, ave_loss: 0.378
[30]  [580/1724] loss: 0.507, ave_loss: 0.383
[31]  [600/1724] loss: 0.414, ave_loss: 0.384
[32]  [620/1724] loss: 0.373, ave_loss: 0.383
[33]  [640/1724] loss: 0.356, ave_loss: 0.383
[34]  [660/1724] loss: 0.231, ave_loss: 0.378
[35]  [680/1724] loss: 0.369, ave_loss: 0.378
[36]  [700/1724] loss: 0.298, ave_loss: 0.376
[37]  [720/1724] loss: 0.302, ave_loss: 0.374
[38]  [740/1724] loss: 0.246, ave_loss: 0.370
[39]  [760/1724] loss: 0.251, ave_loss: 0.367
[40]  [780/1724] loss: 0.332, ave_loss: 0.366
[41]  [800/1724] loss: 0.470, ave_loss: 0.369
[42]  [820/1724] loss: 0.455, ave_loss: 0.371
[43]  [840/1724] loss: 0.311, ave_loss: 0.370
[44]  [860/1724] loss: 0.412, ave_loss: 0.371
[45]  [880/1724] loss: 0.407, ave_loss: 0.371
[46]  [900/1724] loss: 0.327, ave_loss: 0.370
[47]  [920/1724] loss: 0.360, ave_loss: 0.370
[48]  [940/1724] loss: 0.368, ave_loss: 0.370
[49]  [960/1724] loss: 0.319, ave_loss: 0.369
[50]  [980/1724] loss: 0.245, ave_loss: 0.367
[51]  [1000/1724] loss: 0.309, ave_loss: 0.365
[52]  [1020/1724] loss: 0.401, ave_loss: 0.366
[53]  [1040/1724] loss: 0.335, ave_loss: 0.366
[54]  [1060/1724] loss: 0.245, ave_loss: 0.363
[55]  [1080/1724] loss: 0.265, ave_loss: 0.362
[56]  [1100/1724] loss: 0.412, ave_loss: 0.362
[57]  [1120/1724] loss: 0.355, ave_loss: 0.362
[58]  [1140/1724] loss: 0.458, ave_loss: 0.364
[59]  [1160/1724] loss: 0.352, ave_loss: 0.364
[60]  [1180/1724] loss: 0.618, ave_loss: 0.368
[61]  [1200/1724] loss: 0.407, ave_loss: 0.369
[62]  [1220/1724] loss: 0.463, ave_loss: 0.370
[63]  [1240/1724] loss: 0.345, ave_loss: 0.370
[64]  [1260/1724] loss: 0.392, ave_loss: 0.370
[65]  [1280/1724] loss: 0.334, ave_loss: 0.370
[66]  [1300/1724] loss: 0.242, ave_loss: 0.368
[67]  [1320/1724] loss: 0.329, ave_loss: 0.367
[68]  [1340/1724] loss: 0.370, ave_loss: 0.367
[69]  [1360/1724] loss: 0.326, ave_loss: 0.367
[70]  [1380/1724] loss: 0.295, ave_loss: 0.365
[71]  [1400/1724] loss: 0.249, ave_loss: 0.364
[72]  [1420/1724] loss: 0.313, ave_loss: 0.363
[73]  [1440/1724] loss: 0.322, ave_loss: 0.363
[74]  [1460/1724] loss: 0.319, ave_loss: 0.362
[75]  [1480/1724] loss: 0.298, ave_loss: 0.361
[76]  [1500/1724] loss: 0.191, ave_loss: 0.359
[77]  [1520/1724] loss: 0.296, ave_loss: 0.358
[78]  [1540/1724] loss: 0.563, ave_loss: 0.361
[79]  [1560/1724] loss: 0.363, ave_loss: 0.361
[80]  [1580/1724] loss: 0.294, ave_loss: 0.360
[81]  [1600/1724] loss: 0.336, ave_loss: 0.360
[82]  [1620/1724] loss: 0.271, ave_loss: 0.359
[83]  [1640/1724] loss: 0.411, ave_loss: 0.359
[84]  [1660/1724] loss: 0.626, ave_loss: 0.362
[85]  [1680/1724] loss: 0.412, ave_loss: 0.363
[86]  [1700/1724] loss: 0.291, ave_loss: 0.362
[87]  [1720/1724] loss: 0.299, ave_loss: 0.361
[88]  [1740/1724] loss: 0.480, ave_loss: 0.363

Finished Training finishing at 2021-08-31 00:34:07.201629
printing_out epoch  26.54292343387471 learning rate: 0.00029090509716114235
0.0002821779442463081
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.627e-01
Validation Loss: 8.167e+04
Validation ROC: 0.8004
Saving model
2.4570765661252914 epochs left to go

Training Epoch 26.54292343387471/30 starting at 2021-08-31 00:35:53.086877
[1]  [0/1724] loss: 0.706, ave_loss: 0.706
[2]  [20/1724] loss: 0.303, ave_loss: 0.504
[3]  [40/1724] loss: 0.395, ave_loss: 0.468
[4]  [60/1724] loss: 0.232, ave_loss: 0.409
[5]  [80/1724] loss: 0.324, ave_loss: 0.392
[6]  [100/1724] loss: 0.321, ave_loss: 0.380
[7]  [120/1724] loss: 0.224, ave_loss: 0.358
[8]  [140/1724] loss: 0.350, ave_loss: 0.357
[9]  [160/1724] loss: 0.292, ave_loss: 0.350
[10]  [180/1724] loss: 0.366, ave_loss: 0.351
[11]  [200/1724] loss: 0.374, ave_loss: 0.353
[12]  [220/1724] loss: 0.229, ave_loss: 0.343
[13]  [240/1724] loss: 0.370, ave_loss: 0.345
[14]  [260/1724] loss: 0.336, ave_loss: 0.344
[15]  [280/1724] loss: 0.282, ave_loss: 0.340
[16]  [300/1724] loss: 0.268, ave_loss: 0.336
[17]  [320/1724] loss: 0.346, ave_loss: 0.336
[18]  [340/1724] loss: 0.334, ave_loss: 0.336
[19]  [360/1724] loss: 0.269, ave_loss: 0.333
[20]  [380/1724] loss: 0.296, ave_loss: 0.331
[21]  [400/1724] loss: 0.363, ave_loss: 0.332
[22]  [420/1724] loss: 0.416, ave_loss: 0.336
[23]  [440/1724] loss: 0.653, ave_loss: 0.350
[24]  [460/1724] loss: 0.295, ave_loss: 0.348
[25]  [480/1724] loss: 0.279, ave_loss: 0.345
[26]  [500/1724] loss: 0.396, ave_loss: 0.347
[27]  [520/1724] loss: 0.192, ave_loss: 0.341
[28]  [540/1724] loss: 0.410, ave_loss: 0.344
[29]  [560/1724] loss: 0.469, ave_loss: 0.348
[30]  [580/1724] loss: 0.291, ave_loss: 0.346
[31]  [600/1724] loss: 0.226, ave_loss: 0.342
[32]  [620/1724] loss: 0.208, ave_loss: 0.338
[33]  [640/1724] loss: 0.477, ave_loss: 0.342
[34]  [660/1724] loss: 0.446, ave_loss: 0.345
[35]  [680/1724] loss: 0.331, ave_loss: 0.345
[36]  [700/1724] loss: 0.402, ave_loss: 0.346
[37]  [720/1724] loss: 0.349, ave_loss: 0.346
[38]  [740/1724] loss: 0.377, ave_loss: 0.347
[39]  [760/1724] loss: 0.485, ave_loss: 0.351
[40]  [780/1724] loss: 0.236, ave_loss: 0.348
[41]  [800/1724] loss: 0.391, ave_loss: 0.349
[42]  [820/1724] loss: 0.438, ave_loss: 0.351
[43]  [840/1724] loss: 0.298, ave_loss: 0.350
[44]  [860/1724] loss: 0.302, ave_loss: 0.349
[45]  [880/1724] loss: 0.218, ave_loss: 0.346
[46]  [900/1724] loss: 0.449, ave_loss: 0.348
[47]  [920/1724] loss: 0.354, ave_loss: 0.348
[48]  [940/1724] loss: 0.226, ave_loss: 0.346
[49]  [960/1724] loss: 0.390, ave_loss: 0.347
[50]  [980/1724] loss: 0.246, ave_loss: 0.345
[51]  [1000/1724] loss: 0.420, ave_loss: 0.346
[52]  [1020/1724] loss: 0.282, ave_loss: 0.345
[53]  [1040/1724] loss: 0.409, ave_loss: 0.346
[54]  [1060/1724] loss: 0.189, ave_loss: 0.343
[55]  [1080/1724] loss: 0.340, ave_loss: 0.343
[56]  [1100/1724] loss: 0.542, ave_loss: 0.347
[57]  [1120/1724] loss: 0.203, ave_loss: 0.344
[58]  [1140/1724] loss: 0.278, ave_loss: 0.343
[59]  [1160/1724] loss: 0.308, ave_loss: 0.342
[60]  [1180/1724] loss: 0.187, ave_loss: 0.340
[61]  [1200/1724] loss: 0.575, ave_loss: 0.344
[62]  [1220/1724] loss: 0.293, ave_loss: 0.343
[63]  [1240/1724] loss: 0.218, ave_loss: 0.341
[64]  [1260/1724] loss: 0.383, ave_loss: 0.342
[65]  [1280/1724] loss: 0.292, ave_loss: 0.341
[66]  [1300/1724] loss: 0.363, ave_loss: 0.341
[67]  [1320/1724] loss: 0.260, ave_loss: 0.340
[68]  [1340/1724] loss: 0.395, ave_loss: 0.341
[69]  [1360/1724] loss: 0.341, ave_loss: 0.341
[70]  [1380/1724] loss: 0.322, ave_loss: 0.340
[71]  [1400/1724] loss: 0.364, ave_loss: 0.341
[72]  [1420/1724] loss: 0.396, ave_loss: 0.342
[73]  [1440/1724] loss: 0.301, ave_loss: 0.341
[74]  [1460/1724] loss: 0.375, ave_loss: 0.341
[75]  [1480/1724] loss: 0.383, ave_loss: 0.342
[76]  [1500/1724] loss: 0.351, ave_loss: 0.342
[77]  [1520/1724] loss: 0.337, ave_loss: 0.342
[78]  [1540/1724] loss: 0.349, ave_loss: 0.342
[79]  [1560/1724] loss: 0.301, ave_loss: 0.342
[80]  [1580/1724] loss: 0.434, ave_loss: 0.343
[81]  [1600/1724] loss: 0.261, ave_loss: 0.342
[82]  [1620/1724] loss: 0.392, ave_loss: 0.342
[83]  [1640/1724] loss: 0.252, ave_loss: 0.341
[84]  [1660/1724] loss: 0.312, ave_loss: 0.341
[85]  [1680/1724] loss: 0.401, ave_loss: 0.342
[86]  [1700/1724] loss: 0.321, ave_loss: 0.341
[87]  [1720/1724] loss: 0.347, ave_loss: 0.341
[88]  [1740/1724] loss: 0.343, ave_loss: 0.341

Finished Training finishing at 2021-08-31 00:40:20.472837
printing_out epoch  27.563805104408353 learning rate: 0.00029090509716114235
0.00027371260591891884
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.415e-01
Validation Loss: 8.991e+04
Validation ROC: 0.7987
No improvement, still saving model
1.4361948955916475 epochs left to go

Training Epoch 27.563805104408353/30 starting at 2021-08-31 00:42:00.605267
[1]  [0/1724] loss: 0.195, ave_loss: 0.195
[2]  [20/1724] loss: 0.478, ave_loss: 0.337
[3]  [40/1724] loss: 0.294, ave_loss: 0.323
[4]  [60/1724] loss: 0.368, ave_loss: 0.334
[5]  [80/1724] loss: 0.116, ave_loss: 0.290
[6]  [100/1724] loss: 0.486, ave_loss: 0.323
[7]  [120/1724] loss: 0.378, ave_loss: 0.331
[8]  [140/1724] loss: 0.263, ave_loss: 0.322
[9]  [160/1724] loss: 0.270, ave_loss: 0.317
[10]  [180/1724] loss: 0.316, ave_loss: 0.317
[11]  [200/1724] loss: 0.620, ave_loss: 0.344
[12]  [220/1724] loss: 0.430, ave_loss: 0.351
[13]  [240/1724] loss: 0.293, ave_loss: 0.347
[14]  [260/1724] loss: 0.443, ave_loss: 0.354
[15]  [280/1724] loss: 0.347, ave_loss: 0.353
[16]  [300/1724] loss: 0.308, ave_loss: 0.350
[17]  [320/1724] loss: 0.475, ave_loss: 0.358
[18]  [340/1724] loss: 0.268, ave_loss: 0.353
[19]  [360/1724] loss: 0.364, ave_loss: 0.353
[20]  [380/1724] loss: 0.329, ave_loss: 0.352
[21]  [400/1724] loss: 0.301, ave_loss: 0.350
[22]  [420/1724] loss: 0.241, ave_loss: 0.345
[23]  [440/1724] loss: 0.286, ave_loss: 0.342
[24]  [460/1724] loss: 0.423, ave_loss: 0.345
[25]  [480/1724] loss: 0.189, ave_loss: 0.339
[26]  [500/1724] loss: 0.335, ave_loss: 0.339
[27]  [520/1724] loss: 0.332, ave_loss: 0.339
[28]  [540/1724] loss: 0.313, ave_loss: 0.338
[29]  [560/1724] loss: 0.267, ave_loss: 0.335
[30]  [580/1724] loss: 0.320, ave_loss: 0.335
[31]  [600/1724] loss: 0.364, ave_loss: 0.336
[32]  [620/1724] loss: 0.468, ave_loss: 0.340
[33]  [640/1724] loss: 0.178, ave_loss: 0.335
[34]  [660/1724] loss: 0.353, ave_loss: 0.336
[35]  [680/1724] loss: 0.443, ave_loss: 0.339
[36]  [700/1724] loss: 0.166, ave_loss: 0.334
[37]  [720/1724] loss: 0.449, ave_loss: 0.337
[38]  [740/1724] loss: 0.435, ave_loss: 0.340
[39]  [760/1724] loss: 0.270, ave_loss: 0.338
[40]  [780/1724] loss: 0.248, ave_loss: 0.336
[41]  [800/1724] loss: 0.312, ave_loss: 0.335
[42]  [820/1724] loss: 0.430, ave_loss: 0.337
[43]  [840/1724] loss: 0.291, ave_loss: 0.336
[44]  [860/1724] loss: 0.361, ave_loss: 0.337
[45]  [880/1724] loss: 0.435, ave_loss: 0.339
[46]  [900/1724] loss: 0.305, ave_loss: 0.338
[47]  [920/1724] loss: 0.286, ave_loss: 0.337
[48]  [940/1724] loss: 0.249, ave_loss: 0.335
[49]  [960/1724] loss: 0.487, ave_loss: 0.338
[50]  [980/1724] loss: 0.350, ave_loss: 0.339
[51]  [1000/1724] loss: 0.421, ave_loss: 0.340
[52]  [1020/1724] loss: 0.445, ave_loss: 0.342
[53]  [1040/1724] loss: 0.309, ave_loss: 0.342
[54]  [1060/1724] loss: 0.228, ave_loss: 0.339
[55]  [1080/1724] loss: 0.259, ave_loss: 0.338
[56]  [1100/1724] loss: 0.379, ave_loss: 0.339
[57]  [1120/1724] loss: 0.244, ave_loss: 0.337
[58]  [1140/1724] loss: 0.302, ave_loss: 0.336
[59]  [1160/1724] loss: 0.213, ave_loss: 0.334
[60]  [1180/1724] loss: 0.356, ave_loss: 0.335
[61]  [1200/1724] loss: 0.273, ave_loss: 0.334
[62]  [1220/1724] loss: 0.437, ave_loss: 0.335
[63]  [1240/1724] loss: 0.202, ave_loss: 0.333
[64]  [1260/1724] loss: 0.335, ave_loss: 0.333
[65]  [1280/1724] loss: 0.402, ave_loss: 0.334
[66]  [1300/1724] loss: 0.247, ave_loss: 0.333
[67]  [1320/1724] loss: 0.339, ave_loss: 0.333
[68]  [1340/1724] loss: 0.225, ave_loss: 0.332
[69]  [1360/1724] loss: 0.263, ave_loss: 0.331
[70]  [1380/1724] loss: 0.271, ave_loss: 0.330
[71]  [1400/1724] loss: 0.512, ave_loss: 0.332
[72]  [1420/1724] loss: 0.263, ave_loss: 0.331
[73]  [1440/1724] loss: 0.359, ave_loss: 0.332
[74]  [1460/1724] loss: 0.441, ave_loss: 0.333
[75]  [1480/1724] loss: 0.368, ave_loss: 0.334
[76]  [1500/1724] loss: 0.375, ave_loss: 0.334
[77]  [1520/1724] loss: 0.524, ave_loss: 0.337
[78]  [1540/1724] loss: 0.464, ave_loss: 0.338
[79]  [1560/1724] loss: 0.307, ave_loss: 0.338
[80]  [1580/1724] loss: 0.315, ave_loss: 0.338
[81]  [1600/1724] loss: 0.262, ave_loss: 0.337
[82]  [1620/1724] loss: 0.422, ave_loss: 0.338
[83]  [1640/1724] loss: 0.305, ave_loss: 0.337
[84]  [1660/1724] loss: 0.321, ave_loss: 0.337
[85]  [1680/1724] loss: 0.329, ave_loss: 0.337
[86]  [1700/1724] loss: 0.437, ave_loss: 0.338
[87]  [1720/1724] loss: 0.396, ave_loss: 0.339
[88]  [1740/1724] loss: 0.318, ave_loss: 0.339

Finished Training finishing at 2021-08-31 00:46:34.005511
printing_out epoch  28.584686774941996 learning rate: 0.00026445917923740214
0.0002565254038602801
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.386e-01
Validation Loss: 7.378e+04
Validation ROC: 0.7951
No improvement, still saving model
0.4153132250580036 epochs left to go

Training Epoch 28.584686774941996/30 starting at 2021-08-31 00:48:11.148056
[1]  [0/1724] loss: 0.427, ave_loss: 0.427
[2]  [20/1724] loss: 0.369, ave_loss: 0.398
[3]  [40/1724] loss: 0.390, ave_loss: 0.395
[4]  [60/1724] loss: 0.285, ave_loss: 0.368
[5]  [80/1724] loss: 0.243, ave_loss: 0.343
[6]  [100/1724] loss: 0.202, ave_loss: 0.320
[7]  [120/1724] loss: 0.340, ave_loss: 0.322
[8]  [140/1724] loss: 0.238, ave_loss: 0.312
[9]  [160/1724] loss: 0.461, ave_loss: 0.328
[10]  [180/1724] loss: 0.284, ave_loss: 0.324
[11]  [200/1724] loss: 0.434, ave_loss: 0.334
[12]  [220/1724] loss: 0.543, ave_loss: 0.351
[13]  [240/1724] loss: 0.411, ave_loss: 0.356
[14]  [260/1724] loss: 0.392, ave_loss: 0.359
[15]  [280/1724] loss: 0.196, ave_loss: 0.348
[16]  [300/1724] loss: 0.317, ave_loss: 0.346
[17]  [320/1724] loss: 0.233, ave_loss: 0.339
[18]  [340/1724] loss: 0.372, ave_loss: 0.341
[19]  [360/1724] loss: 0.329, ave_loss: 0.340
[20]  [380/1724] loss: 0.316, ave_loss: 0.339
[21]  [400/1724] loss: 0.411, ave_loss: 0.343
[22]  [420/1724] loss: 0.347, ave_loss: 0.343
[23]  [440/1724] loss: 0.297, ave_loss: 0.341
[24]  [460/1724] loss: 0.296, ave_loss: 0.339
[25]  [480/1724] loss: 0.211, ave_loss: 0.334
[26]  [500/1724] loss: 0.413, ave_loss: 0.337
[27]  [520/1724] loss: 0.379, ave_loss: 0.338
[28]  [540/1724] loss: 0.263, ave_loss: 0.336
[29]  [560/1724] loss: 0.242, ave_loss: 0.332
[30]  [580/1724] loss: 0.409, ave_loss: 0.335
[31]  [600/1724] loss: 0.260, ave_loss: 0.333
[32]  [620/1724] loss: 0.399, ave_loss: 0.335
[33]  [640/1724] loss: 0.374, ave_loss: 0.336
[34]  [660/1724] loss: 0.241, ave_loss: 0.333
[35]  [680/1724] loss: 0.357, ave_loss: 0.334
[36]  [700/1724] loss: 0.377, ave_loss: 0.335
[37]  [720/1724] loss: 0.277, ave_loss: 0.333
[38]  [740/1724] loss: 0.239, ave_loss: 0.331
[39]  [760/1724] loss: 0.329, ave_loss: 0.331
[40]  [780/1724] loss: 0.345, ave_loss: 0.331
[41]  [800/1724] loss: 0.242, ave_loss: 0.329
[42]  [820/1724] loss: 0.457, ave_loss: 0.332
[43]  [840/1724] loss: 0.481, ave_loss: 0.336
[44]  [860/1724] loss: 0.269, ave_loss: 0.334
[45]  [880/1724] loss: 0.377, ave_loss: 0.335
[46]  [900/1724] loss: 0.203, ave_loss: 0.332
[47]  [920/1724] loss: 0.371, ave_loss: 0.333
[48]  [940/1724] loss: 0.523, ave_loss: 0.337
[49]  [960/1724] loss: 0.258, ave_loss: 0.335
[50]  [980/1724] loss: 0.293, ave_loss: 0.334
[51]  [1000/1724] loss: 0.270, ave_loss: 0.333
[52]  [1020/1724] loss: 0.320, ave_loss: 0.333
[53]  [1040/1724] loss: 0.178, ave_loss: 0.330
[54]  [1060/1724] loss: 0.365, ave_loss: 0.331
[55]  [1080/1724] loss: 0.293, ave_loss: 0.330
[56]  [1100/1724] loss: 0.301, ave_loss: 0.329
[57]  [1120/1724] loss: 0.290, ave_loss: 0.329
[58]  [1140/1724] loss: 0.212, ave_loss: 0.327
[59]  [1160/1724] loss: 0.328, ave_loss: 0.327
[60]  [1180/1724] loss: 0.456, ave_loss: 0.329
[61]  [1200/1724] loss: 0.389, ave_loss: 0.330
[62]  [1220/1724] loss: 0.247, ave_loss: 0.329
[63]  [1240/1724] loss: 0.384, ave_loss: 0.329
[64]  [1260/1724] loss: 0.330, ave_loss: 0.329
[65]  [1280/1724] loss: 0.392, ave_loss: 0.330
[66]  [1300/1724] loss: 0.211, ave_loss: 0.329
[67]  [1320/1724] loss: 0.455, ave_loss: 0.331
[68]  [1340/1724] loss: 0.255, ave_loss: 0.329
[69]  [1360/1724] loss: 0.230, ave_loss: 0.328
[70]  [1380/1724] loss: 0.318, ave_loss: 0.328
[71]  [1400/1724] loss: 0.350, ave_loss: 0.328
[72]  [1420/1724] loss: 0.532, ave_loss: 0.331
[73]  [1440/1724] loss: 0.344, ave_loss: 0.331
[74]  [1460/1724] loss: 0.238, ave_loss: 0.330
[75]  [1480/1724] loss: 0.237, ave_loss: 0.329
[76]  [1500/1724] loss: 0.229, ave_loss: 0.327
[77]  [1520/1724] loss: 0.251, ave_loss: 0.326
[78]  [1540/1724] loss: 0.383, ave_loss: 0.327
[79]  [1560/1724] loss: 0.413, ave_loss: 0.328
[80]  [1580/1724] loss: 0.266, ave_loss: 0.327
[81]  [1600/1724] loss: 0.301, ave_loss: 0.327
[82]  [1620/1724] loss: 0.212, ave_loss: 0.326
[83]  [1640/1724] loss: 0.279, ave_loss: 0.325
[84]  [1660/1724] loss: 0.363, ave_loss: 0.326
[85]  [1680/1724] loss: 0.253, ave_loss: 0.325
[86]  [1700/1724] loss: 0.415, ave_loss: 0.326
[87]  [1720/1724] loss: 0.363, ave_loss: 0.326
[88]  [1740/1724] loss: 0.409, ave_loss: 0.327

Finished Training finishing at 2021-08-31 00:53:07.421756
printing_out epoch  29.605568445475637 learning rate: 0.00024041743567036557
0.0002332049126002546
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 3.271e-01
Validation Loss: 7.236e+04
Validation ROC: 0.7987
No improvement, still saving model
saving results
