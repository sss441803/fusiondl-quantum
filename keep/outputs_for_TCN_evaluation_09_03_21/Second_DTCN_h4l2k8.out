reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
Arguments:  Namespace(channels_spatial=['c16', 'c8', 'c4', 'c2'], channels_temporal=['c4', 'd4', 'd4'], input_div=1.0, kernel_spatial=4, kernel_temporal=8, linear_sizes=[20, 5], no_scalars=False, subsampling=10, tcn_hidden=4, tcn_layers=2, tcn_type='d')
...done
Training on 1724 shots, testing on 857 shots
Classical convolution with channels  2 16
Classical convolution with channels  16 8
Classical convolution with channels  8 4
Classical convolution with channels  4 2
InputBlock parameters:  14 2 64 ['c16', 'c8', 'c4', 'c2'] 4 [20, 5] 0.08
TCN parameters:  19 1 ['c4', 'd4', 'd4'] 8 0.08
29 epochs left to go

Training Epoch 0/30 starting at 2021-08-30 17:41:06.768440
[1]  [0/1724] loss: 2.600, ave_loss: 2.600
[2]  [20/1724] loss: 4.728, ave_loss: 3.664
[3]  [40/1724] loss: 3.530, ave_loss: 3.619
[4]  [60/1724] loss: 3.745, ave_loss: 3.651
[5]  [80/1724] loss: 3.700, ave_loss: 3.661
[6]  [100/1724] loss: 4.053, ave_loss: 3.726
[7]  [120/1724] loss: 2.705, ave_loss: 3.580
[8]  [140/1724] loss: 3.974, ave_loss: 3.629
[9]  [160/1724] loss: 2.801, ave_loss: 3.537
[10]  [180/1724] loss: 3.948, ave_loss: 3.578
[11]  [200/1724] loss: 3.935, ave_loss: 3.611
[12]  [220/1724] loss: 4.847, ave_loss: 3.714
[13]  [240/1724] loss: 3.441, ave_loss: 3.693
[14]  [260/1724] loss: 4.164, ave_loss: 3.726
[15]  [280/1724] loss: 3.431, ave_loss: 3.707
[16]  [300/1724] loss: 3.792, ave_loss: 3.712
[17]  [320/1724] loss: 2.131, ave_loss: 3.619
[18]  [340/1724] loss: 3.490, ave_loss: 3.612
[19]  [360/1724] loss: 2.280, ave_loss: 3.542
[20]  [380/1724] loss: 2.742, ave_loss: 3.502
[21]  [400/1724] loss: 2.535, ave_loss: 3.456
[22]  [420/1724] loss: 2.827, ave_loss: 3.427
[23]  [440/1724] loss: 2.764, ave_loss: 3.398
[24]  [460/1724] loss: 2.994, ave_loss: 3.382
[25]  [480/1724] loss: 2.982, ave_loss: 3.366
[26]  [500/1724] loss: 2.148, ave_loss: 3.319
[27]  [520/1724] loss: 3.298, ave_loss: 3.318
[28]  [540/1724] loss: 1.952, ave_loss: 3.269
[29]  [560/1724] loss: 5.945, ave_loss: 3.361
[30]  [580/1724] loss: 2.106, ave_loss: 3.320
[31]  [600/1724] loss: 2.527, ave_loss: 3.294
[32]  [620/1724] loss: 2.954, ave_loss: 3.283
[33]  [640/1724] loss: 3.663, ave_loss: 3.295
[34]  [660/1724] loss: 2.836, ave_loss: 3.281
[35]  [680/1724] loss: 2.334, ave_loss: 3.254
[36]  [700/1724] loss: 3.056, ave_loss: 3.249
[37]  [720/1724] loss: 8.164, ave_loss: 3.382
[38]  [740/1724] loss: 1.758, ave_loss: 3.339
[39]  [760/1724] loss: 3.173, ave_loss: 3.335
[40]  [780/1724] loss: 2.499, ave_loss: 3.314
[41]  [800/1724] loss: 3.105, ave_loss: 3.309
[42]  [820/1724] loss: 2.102, ave_loss: 3.280
[43]  [840/1724] loss: 2.312, ave_loss: 3.257
[44]  [860/1724] loss: 2.010, ave_loss: 3.229
[45]  [880/1724] loss: 2.367, ave_loss: 3.210
[46]  [900/1724] loss: 2.447, ave_loss: 3.193
[47]  [920/1724] loss: 2.088, ave_loss: 3.170
[48]  [940/1724] loss: 2.169, ave_loss: 3.149
[49]  [960/1724] loss: 2.121, ave_loss: 3.128
[50]  [980/1724] loss: 3.037, ave_loss: 3.126
[51]  [1000/1724] loss: 2.513, ave_loss: 3.114
[52]  [1020/1724] loss: 2.486, ave_loss: 3.102
[53]  [1040/1724] loss: 3.642, ave_loss: 3.112
[54]  [1060/1724] loss: 2.114, ave_loss: 3.094
[55]  [1080/1724] loss: 1.854, ave_loss: 3.071
[56]  [1100/1724] loss: 2.014, ave_loss: 3.052
[57]  [1120/1724] loss: 3.473, ave_loss: 3.060
[58]  [1140/1724] loss: 5.213, ave_loss: 3.097
[59]  [1160/1724] loss: 2.118, ave_loss: 3.080
[60]  [1180/1724] loss: 1.924, ave_loss: 3.061
[61]  [1200/1724] loss: 2.326, ave_loss: 3.049
[62]  [1220/1724] loss: 3.471, ave_loss: 3.056
[63]  [1240/1724] loss: 2.718, ave_loss: 3.050
[64]  [1260/1724] loss: 2.500, ave_loss: 3.042
[65]  [1280/1724] loss: 2.340, ave_loss: 3.031
[66]  [1300/1724] loss: 2.340, ave_loss: 3.021
[67]  [1320/1724] loss: 2.405, ave_loss: 3.011
[68]  [1340/1724] loss: 1.996, ave_loss: 2.996
[69]  [1360/1724] loss: 4.582, ave_loss: 3.019
[70]  [1380/1724] loss: 1.949, ave_loss: 3.004
[71]  [1400/1724] loss: 2.505, ave_loss: 2.997
[72]  [1420/1724] loss: 2.002, ave_loss: 2.983
[73]  [1440/1724] loss: 2.122, ave_loss: 2.971
[74]  [1460/1724] loss: 2.307, ave_loss: 2.962
[75]  [1480/1724] loss: 2.085, ave_loss: 2.951
[76]  [1500/1724] loss: 2.261, ave_loss: 2.942
[77]  [1520/1724] loss: 2.202, ave_loss: 2.932
[78]  [1540/1724] loss: 1.818, ave_loss: 2.918
[79]  [1560/1724] loss: 2.192, ave_loss: 2.909
[80]  [1580/1724] loss: 2.316, ave_loss: 2.901
[81]  [1600/1724] loss: 1.717, ave_loss: 2.887
[82]  [1620/1724] loss: 2.178, ave_loss: 2.878
[83]  [1640/1724] loss: 1.975, ave_loss: 2.867
[84]  [1660/1724] loss: 2.477, ave_loss: 2.862
[85]  [1680/1724] loss: 2.050, ave_loss: 2.853
[86]  [1700/1724] loss: 2.040, ave_loss: 2.843
[87]  [1720/1724] loss: 1.707, ave_loss: 2.830
[88]  [1740/1724] loss: 1.742, ave_loss: 2.818

Finished Training finishing at 2021-08-30 17:45:18.563375
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 2.818e+00
Validation Loss: 3.822e+03
Validation ROC: 0.3036
Saving model
27.979118329466356 epochs left to go

Training Epoch 1.0208816705336428/30 starting at 2021-08-30 17:47:57.210488
[1]  [0/1724] loss: 2.210, ave_loss: 2.210
[2]  [20/1724] loss: 2.036, ave_loss: 2.123
[3]  [40/1724] loss: 2.002, ave_loss: 2.083
[4]  [60/1724] loss: 1.940, ave_loss: 2.047
[5]  [80/1724] loss: 2.136, ave_loss: 2.065
[6]  [100/1724] loss: 1.778, ave_loss: 2.017
[7]  [120/1724] loss: 1.830, ave_loss: 1.990
[8]  [140/1724] loss: 2.441, ave_loss: 2.047
[9]  [160/1724] loss: 1.536, ave_loss: 1.990
[10]  [180/1724] loss: 2.117, ave_loss: 2.003
[11]  [200/1724] loss: 2.282, ave_loss: 2.028
[12]  [220/1724] loss: 1.771, ave_loss: 2.007
[13]  [240/1724] loss: 2.228, ave_loss: 2.024
[14]  [260/1724] loss: 1.527, ave_loss: 1.988
[15]  [280/1724] loss: 1.598, ave_loss: 1.962
[16]  [300/1724] loss: 1.982, ave_loss: 1.963
[17]  [320/1724] loss: 2.301, ave_loss: 1.983
[18]  [340/1724] loss: 2.025, ave_loss: 1.986
[19]  [360/1724] loss: 1.772, ave_loss: 1.974
[20]  [380/1724] loss: 1.540, ave_loss: 1.953
[21]  [400/1724] loss: 1.497, ave_loss: 1.931
[22]  [420/1724] loss: 1.453, ave_loss: 1.909
[23]  [440/1724] loss: 1.827, ave_loss: 1.906
[24]  [460/1724] loss: 1.853, ave_loss: 1.903
[25]  [480/1724] loss: 2.347, ave_loss: 1.921
[26]  [500/1724] loss: 2.023, ave_loss: 1.925
[27]  [520/1724] loss: 1.561, ave_loss: 1.912
[28]  [540/1724] loss: 1.379, ave_loss: 1.893
[29]  [560/1724] loss: 2.164, ave_loss: 1.902
[30]  [580/1724] loss: 1.650, ave_loss: 1.894
[31]  [600/1724] loss: 2.078, ave_loss: 1.899
[32]  [620/1724] loss: 1.803, ave_loss: 1.896
[33]  [640/1724] loss: 1.555, ave_loss: 1.886
[34]  [660/1724] loss: 5.379, ave_loss: 1.989
[35]  [680/1724] loss: 1.752, ave_loss: 1.982
[36]  [700/1724] loss: 1.910, ave_loss: 1.980
[37]  [720/1724] loss: 1.727, ave_loss: 1.973
[38]  [740/1724] loss: 1.779, ave_loss: 1.968
[39]  [760/1724] loss: 1.863, ave_loss: 1.965
[40]  [780/1724] loss: 1.538, ave_loss: 1.955
[41]  [800/1724] loss: 1.755, ave_loss: 1.950
[42]  [820/1724] loss: 1.384, ave_loss: 1.936
[43]  [840/1724] loss: 2.082, ave_loss: 1.940
[44]  [860/1724] loss: 1.838, ave_loss: 1.937
[45]  [880/1724] loss: 1.664, ave_loss: 1.931
[46]  [900/1724] loss: 2.286, ave_loss: 1.939
[47]  [920/1724] loss: 1.713, ave_loss: 1.934
[48]  [940/1724] loss: 2.344, ave_loss: 1.943
[49]  [960/1724] loss: 1.541, ave_loss: 1.935
[50]  [980/1724] loss: 2.153, ave_loss: 1.939
[51]  [1000/1724] loss: 2.199, ave_loss: 1.944
[52]  [1020/1724] loss: 1.986, ave_loss: 1.945
[53]  [1040/1724] loss: 2.458, ave_loss: 1.955
[54]  [1060/1724] loss: 1.523, ave_loss: 1.947
[55]  [1080/1724] loss: 1.465, ave_loss: 1.938
[56]  [1100/1724] loss: 1.502, ave_loss: 1.930
[57]  [1120/1724] loss: 1.769, ave_loss: 1.927
[58]  [1140/1724] loss: 1.654, ave_loss: 1.922
[59]  [1160/1724] loss: 1.895, ave_loss: 1.922
[60]  [1180/1724] loss: 1.449, ave_loss: 1.914
[61]  [1200/1724] loss: 2.022, ave_loss: 1.916
[62]  [1220/1724] loss: 1.934, ave_loss: 1.916
[63]  [1240/1724] loss: 1.993, ave_loss: 1.917
[64]  [1260/1724] loss: 1.748, ave_loss: 1.915
[65]  [1280/1724] loss: 1.796, ave_loss: 1.913
[66]  [1300/1724] loss: 0.979, ave_loss: 1.899
[67]  [1320/1724] loss: 0.821, ave_loss: 1.883
[68]  [1340/1724] loss: 1.704, ave_loss: 1.880
[69]  [1360/1724] loss: 1.593, ave_loss: 1.876
[70]  [1380/1724] loss: 2.038, ave_loss: 1.878
[71]  [1400/1724] loss: 1.609, ave_loss: 1.874
[72]  [1420/1724] loss: 1.633, ave_loss: 1.871
[73]  [1440/1724] loss: 1.684, ave_loss: 1.869
[74]  [1460/1724] loss: 2.012, ave_loss: 1.870
[75]  [1480/1724] loss: 1.506, ave_loss: 1.866
[76]  [1500/1724] loss: 1.961, ave_loss: 1.867
[77]  [1520/1724] loss: 1.659, ave_loss: 1.864
[78]  [1540/1724] loss: 1.926, ave_loss: 1.865
[79]  [1560/1724] loss: 1.538, ave_loss: 1.861
[80]  [1580/1724] loss: 2.121, ave_loss: 1.864
[81]  [1600/1724] loss: 1.587, ave_loss: 1.861
[82]  [1620/1724] loss: 1.641, ave_loss: 1.858
[83]  [1640/1724] loss: 1.545, ave_loss: 1.854
[84]  [1660/1724] loss: 1.601, ave_loss: 1.851
[85]  [1680/1724] loss: 1.539, ave_loss: 1.848
[86]  [1700/1724] loss: 1.355, ave_loss: 1.842
[87]  [1720/1724] loss: 4.431, ave_loss: 1.872
[88]  [1740/1724] loss: 1.979, ave_loss: 1.873

Finished Training finishing at 2021-08-30 17:50:53.225978
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 1.873e+00
Validation Loss: 3.431e+03
Validation ROC: 0.3546
Saving model
26.958236658932716 epochs left to go

Training Epoch 2.0417633410672855/30 starting at 2021-08-30 17:51:41.729952
[1]  [0/1724] loss: 1.776, ave_loss: 1.776
[2]  [20/1724] loss: 1.456, ave_loss: 1.616
[3]  [40/1724] loss: 2.106, ave_loss: 1.779
[4]  [60/1724] loss: 1.614, ave_loss: 1.738
[5]  [80/1724] loss: 1.254, ave_loss: 1.641
[6]  [100/1724] loss: 1.905, ave_loss: 1.685
[7]  [120/1724] loss: 1.331, ave_loss: 1.634
[8]  [140/1724] loss: 1.632, ave_loss: 1.634
[9]  [160/1724] loss: 1.585, ave_loss: 1.629
[10]  [180/1724] loss: 1.279, ave_loss: 1.594
[11]  [200/1724] loss: 1.752, ave_loss: 1.608
[12]  [220/1724] loss: 1.760, ave_loss: 1.621
[13]  [240/1724] loss: 1.791, ave_loss: 1.634
[14]  [260/1724] loss: 1.930, ave_loss: 1.655
[15]  [280/1724] loss: 1.793, ave_loss: 1.664
[16]  [300/1724] loss: 1.403, ave_loss: 1.648
[17]  [320/1724] loss: 2.000, ave_loss: 1.668
[18]  [340/1724] loss: 1.441, ave_loss: 1.656
[19]  [360/1724] loss: 1.134, ave_loss: 1.628
[20]  [380/1724] loss: 1.088, ave_loss: 1.601
[21]  [400/1724] loss: 1.813, ave_loss: 1.611
[22]  [420/1724] loss: 1.811, ave_loss: 1.620
[23]  [440/1724] loss: 1.695, ave_loss: 1.624
[24]  [460/1724] loss: 1.955, ave_loss: 1.638
[25]  [480/1724] loss: 1.395, ave_loss: 1.628
[26]  [500/1724] loss: 1.772, ave_loss: 1.633
[27]  [520/1724] loss: 1.659, ave_loss: 1.634
[28]  [540/1724] loss: 1.676, ave_loss: 1.636
[29]  [560/1724] loss: 1.123, ave_loss: 1.618
[30]  [580/1724] loss: 1.263, ave_loss: 1.606
[31]  [600/1724] loss: 1.528, ave_loss: 1.604
[32]  [620/1724] loss: 1.099, ave_loss: 1.588
[33]  [640/1724] loss: 1.610, ave_loss: 1.589
[34]  [660/1724] loss: 1.457, ave_loss: 1.585
[35]  [680/1724] loss: 1.410, ave_loss: 1.580
[36]  [700/1724] loss: 1.272, ave_loss: 1.571
[37]  [720/1724] loss: 1.778, ave_loss: 1.577
[38]  [740/1724] loss: 1.172, ave_loss: 1.566
[39]  [760/1724] loss: 1.575, ave_loss: 1.566
[40]  [780/1724] loss: 1.537, ave_loss: 1.566
[41]  [800/1724] loss: 2.607, ave_loss: 1.591
[42]  [820/1724] loss: 1.894, ave_loss: 1.598
[43]  [840/1724] loss: 1.790, ave_loss: 1.603
[44]  [860/1724] loss: 1.529, ave_loss: 1.601
[45]  [880/1724] loss: 1.724, ave_loss: 1.604
[46]  [900/1724] loss: 1.602, ave_loss: 1.604
[47]  [920/1724] loss: 1.529, ave_loss: 1.602
[48]  [940/1724] loss: 1.352, ave_loss: 1.597
[49]  [960/1724] loss: 1.577, ave_loss: 1.597
[50]  [980/1724] loss: 1.787, ave_loss: 1.600
[51]  [1000/1724] loss: 2.457, ave_loss: 1.617
[52]  [1020/1724] loss: 1.638, ave_loss: 1.618
[53]  [1040/1724] loss: 1.176, ave_loss: 1.609
[54]  [1060/1724] loss: 1.316, ave_loss: 1.604
[55]  [1080/1724] loss: 1.308, ave_loss: 1.598
[56]  [1100/1724] loss: 1.348, ave_loss: 1.594
[57]  [1120/1724] loss: 1.926, ave_loss: 1.600
[58]  [1140/1724] loss: 1.251, ave_loss: 1.594
[59]  [1160/1724] loss: 1.454, ave_loss: 1.591
[60]  [1180/1724] loss: 0.969, ave_loss: 1.581
[61]  [1200/1724] loss: 1.293, ave_loss: 1.576
[62]  [1220/1724] loss: 1.408, ave_loss: 1.574
[63]  [1240/1724] loss: 1.204, ave_loss: 1.568
[64]  [1260/1724] loss: 1.326, ave_loss: 1.564
[65]  [1280/1724] loss: 1.309, ave_loss: 1.560
[66]  [1300/1724] loss: 1.144, ave_loss: 1.554
[67]  [1320/1724] loss: 1.204, ave_loss: 1.548
[68]  [1340/1724] loss: 1.272, ave_loss: 1.544
[69]  [1360/1724] loss: 0.862, ave_loss: 1.535
[70]  [1380/1724] loss: 1.347, ave_loss: 1.532
[71]  [1400/1724] loss: 0.967, ave_loss: 1.524
[72]  [1420/1724] loss: 1.079, ave_loss: 1.518
[73]  [1440/1724] loss: 1.225, ave_loss: 1.514
[74]  [1460/1724] loss: 1.331, ave_loss: 1.511
[75]  [1480/1724] loss: 1.352, ave_loss: 1.509
[76]  [1500/1724] loss: 1.192, ave_loss: 1.505
[77]  [1520/1724] loss: 1.043, ave_loss: 1.499
[78]  [1540/1724] loss: 1.097, ave_loss: 1.494
[79]  [1560/1724] loss: 0.874, ave_loss: 1.486
[80]  [1580/1724] loss: 0.995, ave_loss: 1.480
[81]  [1600/1724] loss: 0.855, ave_loss: 1.472
[82]  [1620/1724] loss: 1.384, ave_loss: 1.471
[83]  [1640/1724] loss: 1.068, ave_loss: 1.466
[84]  [1660/1724] loss: 0.878, ave_loss: 1.459
[85]  [1680/1724] loss: 1.035, ave_loss: 1.454
[86]  [1700/1724] loss: 0.923, ave_loss: 1.448
[87]  [1720/1724] loss: 0.862, ave_loss: 1.441
[88]  [1740/1724] loss: 1.568, ave_loss: 1.443

Finished Training finishing at 2021-08-30 17:54:28.444568
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 1.443e+00
Validation Loss: 4.429e+04
Validation ROC: 0.2218
No improvement, still saving model
25.937354988399072 epochs left to go

Training Epoch 3.062645011600928/30 starting at 2021-08-30 17:55:22.208339
[1]  [0/1724] loss: 0.773, ave_loss: 0.773
[2]  [20/1724] loss: 0.849, ave_loss: 0.811
[3]  [40/1724] loss: 1.140, ave_loss: 0.921
[4]  [60/1724] loss: 0.761, ave_loss: 0.881
[5]  [80/1724] loss: 0.977, ave_loss: 0.900
[6]  [100/1724] loss: 1.030, ave_loss: 0.922
[7]  [120/1724] loss: 1.159, ave_loss: 0.956
[8]  [140/1724] loss: 0.983, ave_loss: 0.959
[9]  [160/1724] loss: 0.830, ave_loss: 0.945
[10]  [180/1724] loss: 0.941, ave_loss: 0.944
[11]  [200/1724] loss: 0.883, ave_loss: 0.939
[12]  [220/1724] loss: 0.768, ave_loss: 0.925
[13]  [240/1724] loss: 0.857, ave_loss: 0.919
[14]  [260/1724] loss: 0.934, ave_loss: 0.920
[15]  [280/1724] loss: 0.823, ave_loss: 0.914
[16]  [300/1724] loss: 0.841, ave_loss: 0.909
[17]  [320/1724] loss: 0.869, ave_loss: 0.907
[18]  [340/1724] loss: 1.101, ave_loss: 0.918
[19]  [360/1724] loss: 0.871, ave_loss: 0.915
[20]  [380/1724] loss: 0.828, ave_loss: 0.911
[21]  [400/1724] loss: 0.641, ave_loss: 0.898
[22]  [420/1724] loss: 0.837, ave_loss: 0.895
[23]  [440/1724] loss: 0.802, ave_loss: 0.891
[24]  [460/1724] loss: 0.846, ave_loss: 0.889
[25]  [480/1724] loss: 0.740, ave_loss: 0.883
[26]  [500/1724] loss: 0.577, ave_loss: 0.872
[27]  [520/1724] loss: 1.001, ave_loss: 0.876
[28]  [540/1724] loss: 1.582, ave_loss: 0.902
[29]  [560/1724] loss: 0.893, ave_loss: 0.901
[30]  [580/1724] loss: 0.974, ave_loss: 0.904
[31]  [600/1724] loss: 0.775, ave_loss: 0.900
[32]  [620/1724] loss: 0.762, ave_loss: 0.895
[33]  [640/1724] loss: 0.844, ave_loss: 0.894
[34]  [660/1724] loss: 0.895, ave_loss: 0.894
[35]  [680/1724] loss: 0.955, ave_loss: 0.896
[36]  [700/1724] loss: 0.889, ave_loss: 0.895
[37]  [720/1724] loss: 1.012, ave_loss: 0.898
[38]  [740/1724] loss: 0.665, ave_loss: 0.892
[39]  [760/1724] loss: 0.825, ave_loss: 0.891
[40]  [780/1724] loss: 0.818, ave_loss: 0.889
[41]  [800/1724] loss: 1.534, ave_loss: 0.905
[42]  [820/1724] loss: 0.727, ave_loss: 0.900
[43]  [840/1724] loss: 0.836, ave_loss: 0.899
[44]  [860/1724] loss: 0.600, ave_loss: 0.892
[45]  [880/1724] loss: 0.913, ave_loss: 0.892
[46]  [900/1724] loss: 0.721, ave_loss: 0.889
[47]  [920/1724] loss: 0.859, ave_loss: 0.888
[48]  [940/1724] loss: 0.737, ave_loss: 0.885
[49]  [960/1724] loss: 0.708, ave_loss: 0.881
[50]  [980/1724] loss: 0.953, ave_loss: 0.883
[51]  [1000/1724] loss: 0.585, ave_loss: 0.877
[52]  [1020/1724] loss: 0.732, ave_loss: 0.874
[53]  [1040/1724] loss: 0.752, ave_loss: 0.872
[54]  [1060/1724] loss: 0.751, ave_loss: 0.870
[55]  [1080/1724] loss: 0.922, ave_loss: 0.871
[56]  [1100/1724] loss: 0.809, ave_loss: 0.869
[57]  [1120/1724] loss: 0.656, ave_loss: 0.866
[58]  [1140/1724] loss: 0.634, ave_loss: 0.862
[59]  [1160/1724] loss: 0.923, ave_loss: 0.863
[60]  [1180/1724] loss: 0.662, ave_loss: 0.859
[61]  [1200/1724] loss: 0.709, ave_loss: 0.857
[62]  [1220/1724] loss: 0.751, ave_loss: 0.855
[63]  [1240/1724] loss: 0.720, ave_loss: 0.853
[64]  [1260/1724] loss: 0.604, ave_loss: 0.849
[65]  [1280/1724] loss: 0.775, ave_loss: 0.848
[66]  [1300/1724] loss: 1.004, ave_loss: 0.850
[67]  [1320/1724] loss: 0.720, ave_loss: 0.848
[68]  [1340/1724] loss: 0.673, ave_loss: 0.846
[69]  [1360/1724] loss: 0.818, ave_loss: 0.845
[70]  [1380/1724] loss: 0.707, ave_loss: 0.844
[71]  [1400/1724] loss: 0.698, ave_loss: 0.841
[72]  [1420/1724] loss: 0.763, ave_loss: 0.840
[73]  [1440/1724] loss: 0.669, ave_loss: 0.838
[74]  [1460/1724] loss: 0.735, ave_loss: 0.837
[75]  [1480/1724] loss: 0.933, ave_loss: 0.838
[76]  [1500/1724] loss: 0.685, ave_loss: 0.836
[77]  [1520/1724] loss: 0.742, ave_loss: 0.835
[78]  [1540/1724] loss: 0.822, ave_loss: 0.835
[79]  [1560/1724] loss: 0.858, ave_loss: 0.835
[80]  [1580/1724] loss: 0.718, ave_loss: 0.833
[81]  [1600/1724] loss: 0.683, ave_loss: 0.832
[82]  [1620/1724] loss: 0.735, ave_loss: 0.830
[83]  [1640/1724] loss: 1.906, ave_loss: 0.843
[84]  [1660/1724] loss: 0.681, ave_loss: 0.841
[85]  [1680/1724] loss: 0.760, ave_loss: 0.840
[86]  [1700/1724] loss: 0.757, ave_loss: 0.839
[87]  [1720/1724] loss: 0.735, ave_loss: 0.838
[88]  [1740/1724] loss: 1.077, ave_loss: 0.841

Finished Training finishing at 2021-08-30 17:58:18.197793
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 8.410e-01
Validation Loss: 1.772e+05
Validation ROC: 0.2305
No improvement, still saving model
24.916473317865428 epochs left to go

Training Epoch 4.083526682134571/30 starting at 2021-08-30 17:59:10.044436
[1]  [0/1724] loss: 0.929, ave_loss: 0.929
[2]  [20/1724] loss: 0.676, ave_loss: 0.803
[3]  [40/1724] loss: 0.654, ave_loss: 0.753
[4]  [60/1724] loss: 0.534, ave_loss: 0.698
[5]  [80/1724] loss: 0.839, ave_loss: 0.727
[6]  [100/1724] loss: 0.676, ave_loss: 0.718
[7]  [120/1724] loss: 1.117, ave_loss: 0.775
[8]  [140/1724] loss: 0.894, ave_loss: 0.790
[9]  [160/1724] loss: 0.678, ave_loss: 0.777
[10]  [180/1724] loss: 0.736, ave_loss: 0.773
[11]  [200/1724] loss: 0.961, ave_loss: 0.790
[12]  [220/1724] loss: 0.703, ave_loss: 0.783
[13]  [240/1724] loss: 0.697, ave_loss: 0.777
[14]  [260/1724] loss: 0.802, ave_loss: 0.778
[15]  [280/1724] loss: 0.828, ave_loss: 0.782
[16]  [300/1724] loss: 1.038, ave_loss: 0.798
[17]  [320/1724] loss: 0.735, ave_loss: 0.794
[18]  [340/1724] loss: 0.791, ave_loss: 0.794
[19]  [360/1724] loss: 0.808, ave_loss: 0.795
[20]  [380/1724] loss: 0.709, ave_loss: 0.790
[21]  [400/1724] loss: 0.833, ave_loss: 0.792
[22]  [420/1724] loss: 1.522, ave_loss: 0.825
[23]  [440/1724] loss: 0.656, ave_loss: 0.818
[24]  [460/1724] loss: 1.188, ave_loss: 0.834
[25]  [480/1724] loss: 0.779, ave_loss: 0.831
[26]  [500/1724] loss: 0.725, ave_loss: 0.827
[27]  [520/1724] loss: 0.725, ave_loss: 0.823
[28]  [540/1724] loss: 0.757, ave_loss: 0.821
[29]  [560/1724] loss: 0.838, ave_loss: 0.822
[30]  [580/1724] loss: 0.728, ave_loss: 0.819
[31]  [600/1724] loss: 0.679, ave_loss: 0.814
[32]  [620/1724] loss: 0.684, ave_loss: 0.810
[33]  [640/1724] loss: 0.681, ave_loss: 0.806
[34]  [660/1724] loss: 0.591, ave_loss: 0.800
[35]  [680/1724] loss: 0.793, ave_loss: 0.800
[36]  [700/1724] loss: 0.680, ave_loss: 0.796
[37]  [720/1724] loss: 0.624, ave_loss: 0.792
[38]  [740/1724] loss: 0.649, ave_loss: 0.788
[39]  [760/1724] loss: 0.777, ave_loss: 0.788
[40]  [780/1724] loss: 0.808, ave_loss: 0.788
[41]  [800/1724] loss: 0.715, ave_loss: 0.786
[42]  [820/1724] loss: 0.669, ave_loss: 0.784
[43]  [840/1724] loss: 0.787, ave_loss: 0.784
[44]  [860/1724] loss: 0.779, ave_loss: 0.784
[45]  [880/1724] loss: 0.631, ave_loss: 0.780
[46]  [900/1724] loss: 0.558, ave_loss: 0.775
[47]  [920/1724] loss: 0.470, ave_loss: 0.769
[48]  [940/1724] loss: 0.733, ave_loss: 0.768
[49]  [960/1724] loss: 0.552, ave_loss: 0.764
[50]  [980/1724] loss: 0.600, ave_loss: 0.760
[51]  [1000/1724] loss: 0.588, ave_loss: 0.757
[52]  [1020/1724] loss: 0.803, ave_loss: 0.758
[53]  [1040/1724] loss: 0.691, ave_loss: 0.757
[54]  [1060/1724] loss: 0.613, ave_loss: 0.754
[55]  [1080/1724] loss: 0.745, ave_loss: 0.754
[56]  [1100/1724] loss: 0.837, ave_loss: 0.755
[57]  [1120/1724] loss: 0.592, ave_loss: 0.752
[58]  [1140/1724] loss: 0.617, ave_loss: 0.750
[59]  [1160/1724] loss: 0.688, ave_loss: 0.749
[60]  [1180/1724] loss: 0.869, ave_loss: 0.751
[61]  [1200/1724] loss: 0.654, ave_loss: 0.749
[62]  [1220/1724] loss: 0.624, ave_loss: 0.747
[63]  [1240/1724] loss: 0.685, ave_loss: 0.746
[64]  [1260/1724] loss: 0.588, ave_loss: 0.744
[65]  [1280/1724] loss: 0.592, ave_loss: 0.742
[66]  [1300/1724] loss: 0.594, ave_loss: 0.739
[67]  [1320/1724] loss: 0.779, ave_loss: 0.740
[68]  [1340/1724] loss: 0.520, ave_loss: 0.737
[69]  [1360/1724] loss: 0.599, ave_loss: 0.735
[70]  [1380/1724] loss: 0.602, ave_loss: 0.733
[71]  [1400/1724] loss: 0.587, ave_loss: 0.731
[72]  [1420/1724] loss: 0.630, ave_loss: 0.729
[73]  [1440/1724] loss: 0.497, ave_loss: 0.726
[74]  [1460/1724] loss: 0.692, ave_loss: 0.726
[75]  [1480/1724] loss: 0.420, ave_loss: 0.722
[76]  [1500/1724] loss: 0.621, ave_loss: 0.720
[77]  [1520/1724] loss: 0.588, ave_loss: 0.719
[78]  [1540/1724] loss: 0.468, ave_loss: 0.715
[79]  [1560/1724] loss: 0.590, ave_loss: 0.714
[80]  [1580/1724] loss: 0.672, ave_loss: 0.713
[81]  [1600/1724] loss: 0.651, ave_loss: 0.713
[82]  [1620/1724] loss: 0.534, ave_loss: 0.710
[83]  [1640/1724] loss: 0.668, ave_loss: 0.710
[84]  [1660/1724] loss: 0.550, ave_loss: 0.708
[85]  [1680/1724] loss: 0.744, ave_loss: 0.708
[86]  [1700/1724] loss: 0.662, ave_loss: 0.708
[87]  [1720/1724] loss: 0.554, ave_loss: 0.706
[88]  [1740/1724] loss: 0.752, ave_loss: 0.707

Finished Training finishing at 2021-08-30 18:01:46.110090
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 7.066e-01
Validation Loss: 2.537e+05
Validation ROC: 0.4351
Saving model
23.895591647331788 epochs left to go

Training Epoch 5.104408352668213/30 starting at 2021-08-30 18:02:48.643748
[1]  [0/1724] loss: 0.693, ave_loss: 0.693
[2]  [20/1724] loss: 0.454, ave_loss: 0.574
[3]  [40/1724] loss: 0.740, ave_loss: 0.629
[4]  [60/1724] loss: 0.682, ave_loss: 0.642
[5]  [80/1724] loss: 0.605, ave_loss: 0.635
[6]  [100/1724] loss: 0.695, ave_loss: 0.645
[7]  [120/1724] loss: 0.589, ave_loss: 0.637
[8]  [140/1724] loss: 0.567, ave_loss: 0.628
[9]  [160/1724] loss: 0.620, ave_loss: 0.627
[10]  [180/1724] loss: 0.602, ave_loss: 0.625
[11]  [200/1724] loss: 0.716, ave_loss: 0.633
[12]  [220/1724] loss: 0.453, ave_loss: 0.618
[13]  [240/1724] loss: 0.613, ave_loss: 0.618
[14]  [260/1724] loss: 0.505, ave_loss: 0.610
[15]  [280/1724] loss: 0.501, ave_loss: 0.602
[16]  [300/1724] loss: 0.430, ave_loss: 0.591
[17]  [320/1724] loss: 0.597, ave_loss: 0.592
[18]  [340/1724] loss: 0.896, ave_loss: 0.609
[19]  [360/1724] loss: 0.628, ave_loss: 0.610
[20]  [380/1724] loss: 0.617, ave_loss: 0.610
[21]  [400/1724] loss: 0.639, ave_loss: 0.611
[22]  [420/1724] loss: 0.766, ave_loss: 0.618
[23]  [440/1724] loss: 0.665, ave_loss: 0.620
[24]  [460/1724] loss: 0.482, ave_loss: 0.615
[25]  [480/1724] loss: 0.733, ave_loss: 0.619
[26]  [500/1724] loss: 1.246, ave_loss: 0.643
[27]  [520/1724] loss: 0.675, ave_loss: 0.645
[28]  [540/1724] loss: 0.483, ave_loss: 0.639
[29]  [560/1724] loss: 0.685, ave_loss: 0.640
[30]  [580/1724] loss: 0.786, ave_loss: 0.645
[31]  [600/1724] loss: 0.639, ave_loss: 0.645
[32]  [620/1724] loss: 0.578, ave_loss: 0.643
[33]  [640/1724] loss: 0.612, ave_loss: 0.642
[34]  [660/1724] loss: 0.646, ave_loss: 0.642
[35]  [680/1724] loss: 0.475, ave_loss: 0.637
[36]  [700/1724] loss: 0.606, ave_loss: 0.637
[37]  [720/1724] loss: 0.617, ave_loss: 0.636
[38]  [740/1724] loss: 0.576, ave_loss: 0.634
[39]  [760/1724] loss: 0.719, ave_loss: 0.637
[40]  [780/1724] loss: 0.625, ave_loss: 0.636
[41]  [800/1724] loss: 0.548, ave_loss: 0.634
[42]  [820/1724] loss: 0.628, ave_loss: 0.634
[43]  [840/1724] loss: 0.595, ave_loss: 0.633
[44]  [860/1724] loss: 0.529, ave_loss: 0.631
[45]  [880/1724] loss: 0.388, ave_loss: 0.625
[46]  [900/1724] loss: 0.967, ave_loss: 0.633
[47]  [920/1724] loss: 0.666, ave_loss: 0.633
[48]  [940/1724] loss: 0.729, ave_loss: 0.635
[49]  [960/1724] loss: 0.637, ave_loss: 0.635
[50]  [980/1724] loss: 0.645, ave_loss: 0.636
[51]  [1000/1724] loss: 0.605, ave_loss: 0.635
[52]  [1020/1724] loss: 0.400, ave_loss: 0.631
[53]  [1040/1724] loss: 0.405, ave_loss: 0.626
[54]  [1060/1724] loss: 0.688, ave_loss: 0.627
[55]  [1080/1724] loss: 0.483, ave_loss: 0.625
[56]  [1100/1724] loss: 0.610, ave_loss: 0.625
[57]  [1120/1724] loss: 0.547, ave_loss: 0.623
[58]  [1140/1724] loss: 0.599, ave_loss: 0.623
[59]  [1160/1724] loss: 0.727, ave_loss: 0.625
[60]  [1180/1724] loss: 0.501, ave_loss: 0.622
[61]  [1200/1724] loss: 0.662, ave_loss: 0.623
[62]  [1220/1724] loss: 0.582, ave_loss: 0.622
[63]  [1240/1724] loss: 0.561, ave_loss: 0.621
[64]  [1260/1724] loss: 0.628, ave_loss: 0.622
[65]  [1280/1724] loss: 0.692, ave_loss: 0.623
[66]  [1300/1724] loss: 0.543, ave_loss: 0.621
[67]  [1320/1724] loss: 0.656, ave_loss: 0.622
[68]  [1340/1724] loss: 0.551, ave_loss: 0.621
[69]  [1360/1724] loss: 0.424, ave_loss: 0.618
[70]  [1380/1724] loss: 0.584, ave_loss: 0.618
[71]  [1400/1724] loss: 0.518, ave_loss: 0.616
[72]  [1420/1724] loss: 0.473, ave_loss: 0.614
[73]  [1440/1724] loss: 0.867, ave_loss: 0.618
[74]  [1460/1724] loss: 0.529, ave_loss: 0.616
[75]  [1480/1724] loss: 0.582, ave_loss: 0.616
[76]  [1500/1724] loss: 0.644, ave_loss: 0.616
[77]  [1520/1724] loss: 0.626, ave_loss: 0.616
[78]  [1540/1724] loss: 0.564, ave_loss: 0.616
[79]  [1560/1724] loss: 0.558, ave_loss: 0.615
[80]  [1580/1724] loss: 0.839, ave_loss: 0.618
[81]  [1600/1724] loss: 0.615, ave_loss: 0.618
[82]  [1620/1724] loss: 0.545, ave_loss: 0.617
[83]  [1640/1724] loss: 0.582, ave_loss: 0.617
[84]  [1660/1724] loss: 0.657, ave_loss: 0.617
[85]  [1680/1724] loss: 0.403, ave_loss: 0.614
[86]  [1700/1724] loss: 0.566, ave_loss: 0.614
[87]  [1720/1724] loss: 0.515, ave_loss: 0.613
[88]  [1740/1724] loss: 0.761, ave_loss: 0.614

Finished Training finishing at 2021-08-30 18:05:18.153161
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.145e-01
Validation Loss: 2.266e+05
Validation ROC: 0.5508
Saving model
22.874709976798144 epochs left to go

Training Epoch 6.125290023201856/30 starting at 2021-08-30 18:06:22.064789
[1]  [0/1724] loss: 0.738, ave_loss: 0.738
[2]  [20/1724] loss: 0.502, ave_loss: 0.620
[3]  [40/1724] loss: 0.502, ave_loss: 0.580
[4]  [60/1724] loss: 0.701, ave_loss: 0.610
[5]  [80/1724] loss: 0.725, ave_loss: 0.633
[6]  [100/1724] loss: 0.452, ave_loss: 0.603
[7]  [120/1724] loss: 0.698, ave_loss: 0.617
[8]  [140/1724] loss: 0.402, ave_loss: 0.590
[9]  [160/1724] loss: 0.670, ave_loss: 0.599
[10]  [180/1724] loss: 0.596, ave_loss: 0.598
[11]  [200/1724] loss: 0.484, ave_loss: 0.588
[12]  [220/1724] loss: 0.495, ave_loss: 0.580
[13]  [240/1724] loss: 0.511, ave_loss: 0.575
[14]  [260/1724] loss: 0.724, ave_loss: 0.586
[15]  [280/1724] loss: 0.687, ave_loss: 0.592
[16]  [300/1724] loss: 0.536, ave_loss: 0.589
[17]  [320/1724] loss: 0.421, ave_loss: 0.579
[18]  [340/1724] loss: 0.538, ave_loss: 0.577
[19]  [360/1724] loss: 0.583, ave_loss: 0.577
[20]  [380/1724] loss: 0.612, ave_loss: 0.579
[21]  [400/1724] loss: 0.441, ave_loss: 0.572
[22]  [420/1724] loss: 0.412, ave_loss: 0.565
[23]  [440/1724] loss: 0.671, ave_loss: 0.570
[24]  [460/1724] loss: 0.638, ave_loss: 0.572
[25]  [480/1724] loss: 0.475, ave_loss: 0.568
[26]  [500/1724] loss: 0.579, ave_loss: 0.569
[27]  [520/1724] loss: 0.435, ave_loss: 0.564
[28]  [540/1724] loss: 0.662, ave_loss: 0.567
[29]  [560/1724] loss: 0.624, ave_loss: 0.569
[30]  [580/1724] loss: 0.530, ave_loss: 0.568
[31]  [600/1724] loss: 0.611, ave_loss: 0.569
[32]  [620/1724] loss: 0.480, ave_loss: 0.567
[33]  [640/1724] loss: 0.475, ave_loss: 0.564
[34]  [660/1724] loss: 0.531, ave_loss: 0.563
[35]  [680/1724] loss: 0.467, ave_loss: 0.560
[36]  [700/1724] loss: 0.575, ave_loss: 0.561
[37]  [720/1724] loss: 0.561, ave_loss: 0.561
[38]  [740/1724] loss: 0.694, ave_loss: 0.564
[39]  [760/1724] loss: 0.482, ave_loss: 0.562
[40]  [780/1724] loss: 0.667, ave_loss: 0.565
[41]  [800/1724] loss: 0.502, ave_loss: 0.563
[42]  [820/1724] loss: 0.616, ave_loss: 0.564
[43]  [840/1724] loss: 0.533, ave_loss: 0.564
[44]  [860/1724] loss: 0.527, ave_loss: 0.563
[45]  [880/1724] loss: 0.582, ave_loss: 0.563
[46]  [900/1724] loss: 0.633, ave_loss: 0.565
[47]  [920/1724] loss: 0.529, ave_loss: 0.564
[48]  [940/1724] loss: 0.554, ave_loss: 0.564
[49]  [960/1724] loss: 0.653, ave_loss: 0.566
[50]  [980/1724] loss: 0.513, ave_loss: 0.565
[51]  [1000/1724] loss: 0.400, ave_loss: 0.561
[52]  [1020/1724] loss: 0.509, ave_loss: 0.560
[53]  [1040/1724] loss: 0.499, ave_loss: 0.559
[54]  [1060/1724] loss: 0.543, ave_loss: 0.559
[55]  [1080/1724] loss: 0.581, ave_loss: 0.559
[56]  [1100/1724] loss: 0.641, ave_loss: 0.561
[57]  [1120/1724] loss: 0.597, ave_loss: 0.561
[58]  [1140/1724] loss: 0.511, ave_loss: 0.560
[59]  [1160/1724] loss: 0.562, ave_loss: 0.560
[60]  [1180/1724] loss: 0.640, ave_loss: 0.562
[61]  [1200/1724] loss: 0.564, ave_loss: 0.562
[62]  [1220/1724] loss: 0.403, ave_loss: 0.559
[63]  [1240/1724] loss: 0.546, ave_loss: 0.559
[64]  [1260/1724] loss: 0.684, ave_loss: 0.561
[65]  [1280/1724] loss: 0.616, ave_loss: 0.562
[66]  [1300/1724] loss: 0.770, ave_loss: 0.565
[67]  [1320/1724] loss: 0.554, ave_loss: 0.565
[68]  [1340/1724] loss: 0.852, ave_loss: 0.569
[69]  [1360/1724] loss: 0.552, ave_loss: 0.569
[70]  [1380/1724] loss: 0.520, ave_loss: 0.568
[71]  [1400/1724] loss: 0.439, ave_loss: 0.566
[72]  [1420/1724] loss: 0.582, ave_loss: 0.567
[73]  [1440/1724] loss: 0.730, ave_loss: 0.569
[74]  [1460/1724] loss: 0.597, ave_loss: 0.569
[75]  [1480/1724] loss: 0.729, ave_loss: 0.571
[76]  [1500/1724] loss: 0.610, ave_loss: 0.572
[77]  [1520/1724] loss: 0.519, ave_loss: 0.571
[78]  [1540/1724] loss: 0.563, ave_loss: 0.571
[79]  [1560/1724] loss: 0.387, ave_loss: 0.569
[80]  [1580/1724] loss: 0.557, ave_loss: 0.569
[81]  [1600/1724] loss: 0.611, ave_loss: 0.569
[82]  [1620/1724] loss: 0.510, ave_loss: 0.568
[83]  [1640/1724] loss: 0.674, ave_loss: 0.570
[84]  [1660/1724] loss: 0.488, ave_loss: 0.569
[85]  [1680/1724] loss: 0.661, ave_loss: 0.570
[86]  [1700/1724] loss: 0.599, ave_loss: 0.570
[87]  [1720/1724] loss: 0.551, ave_loss: 0.570
[88]  [1740/1724] loss: 0.684, ave_loss: 0.571

Finished Training finishing at 2021-08-30 18:08:48.402076
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.711e-01
Validation Loss: 2.068e+05
Validation ROC: 0.5610
Saving model
21.8538283062645 epochs left to go

Training Epoch 7.146171693735499/30 starting at 2021-08-30 18:09:30.191991
[1]  [0/1724] loss: 0.548, ave_loss: 0.548
[2]  [20/1724] loss: 0.450, ave_loss: 0.499
[3]  [40/1724] loss: 0.603, ave_loss: 0.534
[4]  [60/1724] loss: 0.550, ave_loss: 0.538
[5]  [80/1724] loss: 0.551, ave_loss: 0.540
[6]  [100/1724] loss: 0.683, ave_loss: 0.564
[7]  [120/1724] loss: 0.319, ave_loss: 0.529
[8]  [140/1724] loss: 0.541, ave_loss: 0.531
[9]  [160/1724] loss: 0.753, ave_loss: 0.555
[10]  [180/1724] loss: 0.694, ave_loss: 0.569
[11]  [200/1724] loss: 0.496, ave_loss: 0.563
[12]  [220/1724] loss: 0.624, ave_loss: 0.568
[13]  [240/1724] loss: 0.569, ave_loss: 0.568
[14]  [260/1724] loss: 0.630, ave_loss: 0.572
[15]  [280/1724] loss: 0.750, ave_loss: 0.584
[16]  [300/1724] loss: 0.587, ave_loss: 0.584
[17]  [320/1724] loss: 0.756, ave_loss: 0.594
[18]  [340/1724] loss: 0.595, ave_loss: 0.594
[19]  [360/1724] loss: 0.524, ave_loss: 0.591
[20]  [380/1724] loss: 0.572, ave_loss: 0.590
[21]  [400/1724] loss: 0.470, ave_loss: 0.584
[22]  [420/1724] loss: 0.628, ave_loss: 0.586
[23]  [440/1724] loss: 0.591, ave_loss: 0.586
[24]  [460/1724] loss: 0.616, ave_loss: 0.588
[25]  [480/1724] loss: 0.588, ave_loss: 0.588
[26]  [500/1724] loss: 0.515, ave_loss: 0.585
[27]  [520/1724] loss: 0.496, ave_loss: 0.581
[28]  [540/1724] loss: 0.487, ave_loss: 0.578
[29]  [560/1724] loss: 0.434, ave_loss: 0.573
[30]  [580/1724] loss: 0.548, ave_loss: 0.572
[31]  [600/1724] loss: 0.655, ave_loss: 0.575
[32]  [620/1724] loss: 0.445, ave_loss: 0.571
[33]  [640/1724] loss: 0.705, ave_loss: 0.575
[34]  [660/1724] loss: 0.461, ave_loss: 0.572
[35]  [680/1724] loss: 0.512, ave_loss: 0.570
[36]  [700/1724] loss: 0.477, ave_loss: 0.567
[37]  [720/1724] loss: 0.513, ave_loss: 0.566
[38]  [740/1724] loss: 0.538, ave_loss: 0.565
[39]  [760/1724] loss: 0.432, ave_loss: 0.562
[40]  [780/1724] loss: 0.492, ave_loss: 0.560
[41]  [800/1724] loss: 0.543, ave_loss: 0.560
[42]  [820/1724] loss: 0.505, ave_loss: 0.558
[43]  [840/1724] loss: 0.540, ave_loss: 0.558
[44]  [860/1724] loss: 0.606, ave_loss: 0.559
[45]  [880/1724] loss: 0.607, ave_loss: 0.560
[46]  [900/1724] loss: 0.589, ave_loss: 0.561
[47]  [920/1724] loss: 0.498, ave_loss: 0.559
[48]  [940/1724] loss: 0.550, ave_loss: 0.559
[49]  [960/1724] loss: 0.721, ave_loss: 0.562
[50]  [980/1724] loss: 0.466, ave_loss: 0.560
[51]  [1000/1724] loss: 0.487, ave_loss: 0.559
[52]  [1020/1724] loss: 0.691, ave_loss: 0.562
[53]  [1040/1724] loss: 0.611, ave_loss: 0.562
[54]  [1060/1724] loss: 0.576, ave_loss: 0.563
[55]  [1080/1724] loss: 0.651, ave_loss: 0.564
[56]  [1100/1724] loss: 0.539, ave_loss: 0.564
[57]  [1120/1724] loss: 1.180, ave_loss: 0.575
[58]  [1140/1724] loss: 0.535, ave_loss: 0.574
[59]  [1160/1724] loss: 0.581, ave_loss: 0.574
[60]  [1180/1724] loss: 0.581, ave_loss: 0.574
[61]  [1200/1724] loss: 0.565, ave_loss: 0.574
[62]  [1220/1724] loss: 0.594, ave_loss: 0.574
[63]  [1240/1724] loss: 1.159, ave_loss: 0.584
[64]  [1260/1724] loss: 0.434, ave_loss: 0.581
[65]  [1280/1724] loss: 0.696, ave_loss: 0.583
[66]  [1300/1724] loss: 0.527, ave_loss: 0.582
[67]  [1320/1724] loss: 0.699, ave_loss: 0.584
[68]  [1340/1724] loss: 0.612, ave_loss: 0.584
[69]  [1360/1724] loss: 0.506, ave_loss: 0.583
[70]  [1380/1724] loss: 0.659, ave_loss: 0.584
[71]  [1400/1724] loss: 0.463, ave_loss: 0.583
[72]  [1420/1724] loss: 1.287, ave_loss: 0.592
[73]  [1440/1724] loss: 0.564, ave_loss: 0.592
[74]  [1460/1724] loss: 1.425, ave_loss: 0.603
[75]  [1480/1724] loss: 0.604, ave_loss: 0.603
[76]  [1500/1724] loss: 0.597, ave_loss: 0.603
[77]  [1520/1724] loss: 0.608, ave_loss: 0.603
[78]  [1540/1724] loss: 0.613, ave_loss: 0.603
[79]  [1560/1724] loss: 0.528, ave_loss: 0.602
[80]  [1580/1724] loss: 0.405, ave_loss: 0.600
[81]  [1600/1724] loss: 0.650, ave_loss: 0.601
[82]  [1620/1724] loss: 0.548, ave_loss: 0.600
[83]  [1640/1724] loss: 0.560, ave_loss: 0.599
[84]  [1660/1724] loss: 0.547, ave_loss: 0.599
[85]  [1680/1724] loss: 0.421, ave_loss: 0.597
[86]  [1700/1724] loss: 0.560, ave_loss: 0.596
[87]  [1720/1724] loss: 0.510, ave_loss: 0.595
[88]  [1740/1724] loss: 0.519, ave_loss: 0.594

Finished Training finishing at 2021-08-30 18:11:55.610984
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.945e-01
Validation Loss: 2.212e+05
Validation ROC: 0.6017
Saving model
20.832946635730856 epochs left to go

Training Epoch 8.167053364269142/30 starting at 2021-08-30 18:12:38.083458
[1]  [0/1724] loss: 0.423, ave_loss: 0.423
[2]  [20/1724] loss: 0.654, ave_loss: 0.538
[3]  [40/1724] loss: 0.520, ave_loss: 0.532
[4]  [60/1724] loss: 0.445, ave_loss: 0.510
[5]  [80/1724] loss: 0.580, ave_loss: 0.524
[6]  [100/1724] loss: 0.526, ave_loss: 0.525
[7]  [120/1724] loss: 0.475, ave_loss: 0.518
[8]  [140/1724] loss: 0.654, ave_loss: 0.535
[9]  [160/1724] loss: 0.612, ave_loss: 0.543
[10]  [180/1724] loss: 0.539, ave_loss: 0.543
[11]  [200/1724] loss: 0.391, ave_loss: 0.529
[12]  [220/1724] loss: 0.653, ave_loss: 0.539
[13]  [240/1724] loss: 0.556, ave_loss: 0.541
[14]  [260/1724] loss: 0.654, ave_loss: 0.549
[15]  [280/1724] loss: 0.479, ave_loss: 0.544
[16]  [300/1724] loss: 0.607, ave_loss: 0.548
[17]  [320/1724] loss: 0.512, ave_loss: 0.546
[18]  [340/1724] loss: 0.551, ave_loss: 0.546
[19]  [360/1724] loss: 0.522, ave_loss: 0.545
[20]  [380/1724] loss: 0.632, ave_loss: 0.549
[21]  [400/1724] loss: 0.881, ave_loss: 0.565
[22]  [420/1724] loss: 0.443, ave_loss: 0.559
[23]  [440/1724] loss: 0.652, ave_loss: 0.564
[24]  [460/1724] loss: 0.624, ave_loss: 0.566
[25]  [480/1724] loss: 0.468, ave_loss: 0.562
[26]  [500/1724] loss: 0.326, ave_loss: 0.553
[27]  [520/1724] loss: 0.594, ave_loss: 0.555
[28]  [540/1724] loss: 0.655, ave_loss: 0.558
[29]  [560/1724] loss: 0.471, ave_loss: 0.555
[30]  [580/1724] loss: 0.512, ave_loss: 0.554
[31]  [600/1724] loss: 0.613, ave_loss: 0.556
[32]  [620/1724] loss: 0.561, ave_loss: 0.556
[33]  [640/1724] loss: 0.896, ave_loss: 0.566
[34]  [660/1724] loss: 0.548, ave_loss: 0.566
[35]  [680/1724] loss: 0.635, ave_loss: 0.568
[36]  [700/1724] loss: 0.591, ave_loss: 0.568
[37]  [720/1724] loss: 0.530, ave_loss: 0.567
[38]  [740/1724] loss: 0.502, ave_loss: 0.565
[39]  [760/1724] loss: 0.327, ave_loss: 0.559
[40]  [780/1724] loss: 0.548, ave_loss: 0.559
[41]  [800/1724] loss: 0.525, ave_loss: 0.558
[42]  [820/1724] loss: 0.524, ave_loss: 0.557
[43]  [840/1724] loss: 0.511, ave_loss: 0.556
[44]  [860/1724] loss: 0.543, ave_loss: 0.556
[45]  [880/1724] loss: 0.522, ave_loss: 0.555
[46]  [900/1724] loss: 0.374, ave_loss: 0.551
[47]  [920/1724] loss: 0.578, ave_loss: 0.552
[48]  [940/1724] loss: 0.546, ave_loss: 0.552
[49]  [960/1724] loss: 0.606, ave_loss: 0.553
[50]  [980/1724] loss: 0.911, ave_loss: 0.560
[51]  [1000/1724] loss: 0.599, ave_loss: 0.561
[52]  [1020/1724] loss: 0.423, ave_loss: 0.558
[53]  [1040/1724] loss: 0.603, ave_loss: 0.559
[54]  [1060/1724] loss: 0.570, ave_loss: 0.559
[55]  [1080/1724] loss: 0.418, ave_loss: 0.557
[56]  [1100/1724] loss: 0.526, ave_loss: 0.556
[57]  [1120/1724] loss: 0.398, ave_loss: 0.553
[58]  [1140/1724] loss: 0.993, ave_loss: 0.561
[59]  [1160/1724] loss: 0.887, ave_loss: 0.566
[60]  [1180/1724] loss: 0.747, ave_loss: 0.569
[61]  [1200/1724] loss: 0.646, ave_loss: 0.571
[62]  [1220/1724] loss: 0.779, ave_loss: 0.574
[63]  [1240/1724] loss: 0.559, ave_loss: 0.574
[64]  [1260/1724] loss: 0.360, ave_loss: 0.571
[65]  [1280/1724] loss: 0.628, ave_loss: 0.571
[66]  [1300/1724] loss: 0.587, ave_loss: 0.572
[67]  [1320/1724] loss: 0.665, ave_loss: 0.573
[68]  [1340/1724] loss: 0.449, ave_loss: 0.571
[69]  [1360/1724] loss: 0.433, ave_loss: 0.569
[70]  [1380/1724] loss: 0.517, ave_loss: 0.568
[71]  [1400/1724] loss: 0.527, ave_loss: 0.568
[72]  [1420/1724] loss: 0.525, ave_loss: 0.567
[73]  [1440/1724] loss: 0.441, ave_loss: 0.566
[74]  [1460/1724] loss: 0.514, ave_loss: 0.565
[75]  [1480/1724] loss: 0.796, ave_loss: 0.568
[76]  [1500/1724] loss: 0.728, ave_loss: 0.570
[77]  [1520/1724] loss: 0.523, ave_loss: 0.569
[78]  [1540/1724] loss: 0.723, ave_loss: 0.571
[79]  [1560/1724] loss: 0.682, ave_loss: 0.573
[80]  [1580/1724] loss: 0.476, ave_loss: 0.572
[81]  [1600/1724] loss: 0.531, ave_loss: 0.571
[82]  [1620/1724] loss: 0.481, ave_loss: 0.570
[83]  [1640/1724] loss: 0.692, ave_loss: 0.571
[84]  [1660/1724] loss: 0.692, ave_loss: 0.573
[85]  [1680/1724] loss: 0.449, ave_loss: 0.571
[86]  [1700/1724] loss: 0.589, ave_loss: 0.572
[87]  [1720/1724] loss: 0.417, ave_loss: 0.570
[88]  [1740/1724] loss: 0.508, ave_loss: 0.569

Finished Training finishing at 2021-08-30 18:15:00.360970
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.691e-01
Validation Loss: 2.182e+05
Validation ROC: 0.6161
Saving model
19.812064965197216 epochs left to go

Training Epoch 9.187935034802784/30 starting at 2021-08-30 18:16:00.276781
[1]  [0/1724] loss: 0.756, ave_loss: 0.756
[2]  [20/1724] loss: 0.508, ave_loss: 0.632
[3]  [40/1724] loss: 0.869, ave_loss: 0.711
[4]  [60/1724] loss: 0.674, ave_loss: 0.702
[5]  [80/1724] loss: 0.467, ave_loss: 0.655
[6]  [100/1724] loss: 0.520, ave_loss: 0.632
[7]  [120/1724] loss: 0.775, ave_loss: 0.653
[8]  [140/1724] loss: 0.621, ave_loss: 0.649
[9]  [160/1724] loss: 0.841, ave_loss: 0.670
[10]  [180/1724] loss: 0.528, ave_loss: 0.656
[11]  [200/1724] loss: 0.471, ave_loss: 0.639
[12]  [220/1724] loss: 0.500, ave_loss: 0.628
[13]  [240/1724] loss: 0.683, ave_loss: 0.632
[14]  [260/1724] loss: 0.570, ave_loss: 0.627
[15]  [280/1724] loss: 0.586, ave_loss: 0.625
[16]  [300/1724] loss: 0.568, ave_loss: 0.621
[17]  [320/1724] loss: 0.590, ave_loss: 0.619
[18]  [340/1724] loss: 0.568, ave_loss: 0.616
[19]  [360/1724] loss: 0.621, ave_loss: 0.617
[20]  [380/1724] loss: 0.648, ave_loss: 0.618
[21]  [400/1724] loss: 0.635, ave_loss: 0.619
[22]  [420/1724] loss: 0.436, ave_loss: 0.611
[23]  [440/1724] loss: 0.474, ave_loss: 0.605
[24]  [460/1724] loss: 0.648, ave_loss: 0.607
[25]  [480/1724] loss: 0.598, ave_loss: 0.606
[26]  [500/1724] loss: 0.533, ave_loss: 0.603
[27]  [520/1724] loss: 0.715, ave_loss: 0.608
[28]  [540/1724] loss: 0.427, ave_loss: 0.601
[29]  [560/1724] loss: 0.488, ave_loss: 0.597
[30]  [580/1724] loss: 0.534, ave_loss: 0.595
[31]  [600/1724] loss: 0.532, ave_loss: 0.593
[32]  [620/1724] loss: 0.703, ave_loss: 0.596
[33]  [640/1724] loss: 0.510, ave_loss: 0.594
[34]  [660/1724] loss: 0.557, ave_loss: 0.593
[35]  [680/1724] loss: 0.563, ave_loss: 0.592
[36]  [700/1724] loss: 0.414, ave_loss: 0.587
[37]  [720/1724] loss: 0.463, ave_loss: 0.584
[38]  [740/1724] loss: 0.612, ave_loss: 0.584
[39]  [760/1724] loss: 0.657, ave_loss: 0.586
[40]  [780/1724] loss: 0.588, ave_loss: 0.586
[41]  [800/1724] loss: 0.391, ave_loss: 0.582
[42]  [820/1724] loss: 0.530, ave_loss: 0.580
[43]  [840/1724] loss: 0.563, ave_loss: 0.580
[44]  [860/1724] loss: 0.510, ave_loss: 0.578
[45]  [880/1724] loss: 0.623, ave_loss: 0.579
[46]  [900/1724] loss: 0.590, ave_loss: 0.580
[47]  [920/1724] loss: 0.657, ave_loss: 0.581
[48]  [940/1724] loss: 0.638, ave_loss: 0.582
[49]  [960/1724] loss: 0.566, ave_loss: 0.582
[50]  [980/1724] loss: 0.386, ave_loss: 0.578
[51]  [1000/1724] loss: 0.586, ave_loss: 0.578
[52]  [1020/1724] loss: 0.573, ave_loss: 0.578
[53]  [1040/1724] loss: 0.553, ave_loss: 0.578
[54]  [1060/1724] loss: 0.761, ave_loss: 0.581
[55]  [1080/1724] loss: 0.419, ave_loss: 0.578
[56]  [1100/1724] loss: 0.491, ave_loss: 0.577
[57]  [1120/1724] loss: 0.620, ave_loss: 0.577
[58]  [1140/1724] loss: 0.536, ave_loss: 0.577
[59]  [1160/1724] loss: 0.683, ave_loss: 0.578
[60]  [1180/1724] loss: 0.539, ave_loss: 0.578
[61]  [1200/1724] loss: 0.529, ave_loss: 0.577
[62]  [1220/1724] loss: 0.473, ave_loss: 0.575
[63]  [1240/1724] loss: 0.519, ave_loss: 0.574
[64]  [1260/1724] loss: 0.493, ave_loss: 0.573
[65]  [1280/1724] loss: 0.521, ave_loss: 0.572
[66]  [1300/1724] loss: 0.497, ave_loss: 0.571
[67]  [1320/1724] loss: 0.480, ave_loss: 0.570
[68]  [1340/1724] loss: 0.578, ave_loss: 0.570
[69]  [1360/1724] loss: 0.464, ave_loss: 0.568
[70]  [1380/1724] loss: 0.562, ave_loss: 0.568
[71]  [1400/1724] loss: 0.611, ave_loss: 0.569
[72]  [1420/1724] loss: 0.528, ave_loss: 0.568
[73]  [1440/1724] loss: 0.453, ave_loss: 0.567
[74]  [1460/1724] loss: 0.471, ave_loss: 0.565
[75]  [1480/1724] loss: 0.429, ave_loss: 0.564
[76]  [1500/1724] loss: 0.647, ave_loss: 0.565
[77]  [1520/1724] loss: 0.525, ave_loss: 0.564
[78]  [1540/1724] loss: 0.976, ave_loss: 0.570
[79]  [1560/1724] loss: 0.510, ave_loss: 0.569
[80]  [1580/1724] loss: 0.536, ave_loss: 0.568
[81]  [1600/1724] loss: 0.693, ave_loss: 0.570
[82]  [1620/1724] loss: 0.525, ave_loss: 0.569
[83]  [1640/1724] loss: 0.557, ave_loss: 0.569
[84]  [1660/1724] loss: 0.569, ave_loss: 0.569
[85]  [1680/1724] loss: 0.431, ave_loss: 0.568
[86]  [1700/1724] loss: 0.479, ave_loss: 0.567
[87]  [1720/1724] loss: 0.664, ave_loss: 0.568
[88]  [1740/1724] loss: 0.515, ave_loss: 0.567

Finished Training finishing at 2021-08-30 18:18:30.514661
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.670e-01
Validation Loss: 2.191e+05
Validation ROC: 0.6282
Saving model
18.791183294663576 epochs left to go

Training Epoch 10.208816705336426/30 starting at 2021-08-30 18:19:24.696224
[1]  [0/1724] loss: 0.620, ave_loss: 0.620
[2]  [20/1724] loss: 0.593, ave_loss: 0.606
[3]  [40/1724] loss: 0.549, ave_loss: 0.587
[4]  [60/1724] loss: 0.641, ave_loss: 0.600
[5]  [80/1724] loss: 0.736, ave_loss: 0.628
[6]  [100/1724] loss: 0.485, ave_loss: 0.604
[7]  [120/1724] loss: 0.640, ave_loss: 0.609
[8]  [140/1724] loss: 0.411, ave_loss: 0.584
[9]  [160/1724] loss: 0.530, ave_loss: 0.578
[10]  [180/1724] loss: 0.646, ave_loss: 0.585
[11]  [200/1724] loss: 0.453, ave_loss: 0.573
[12]  [220/1724] loss: 0.458, ave_loss: 0.563
[13]  [240/1724] loss: 0.472, ave_loss: 0.556
[14]  [260/1724] loss: 0.417, ave_loss: 0.547
[15]  [280/1724] loss: 0.350, ave_loss: 0.533
[16]  [300/1724] loss: 0.596, ave_loss: 0.537
[17]  [320/1724] loss: 0.613, ave_loss: 0.542
[18]  [340/1724] loss: 0.751, ave_loss: 0.553
[19]  [360/1724] loss: 0.715, ave_loss: 0.562
[20]  [380/1724] loss: 0.431, ave_loss: 0.555
[21]  [400/1724] loss: 0.823, ave_loss: 0.568
[22]  [420/1724] loss: 0.479, ave_loss: 0.564
[23]  [440/1724] loss: 0.646, ave_loss: 0.568
[24]  [460/1724] loss: 0.545, ave_loss: 0.567
[25]  [480/1724] loss: 0.619, ave_loss: 0.569
[26]  [500/1724] loss: 0.534, ave_loss: 0.567
[27]  [520/1724] loss: 0.687, ave_loss: 0.572
[28]  [540/1724] loss: 0.576, ave_loss: 0.572
[29]  [560/1724] loss: 0.446, ave_loss: 0.568
[30]  [580/1724] loss: 0.454, ave_loss: 0.564
[31]  [600/1724] loss: 0.504, ave_loss: 0.562
[32]  [620/1724] loss: 0.627, ave_loss: 0.564
[33]  [640/1724] loss: 0.552, ave_loss: 0.564
[34]  [660/1724] loss: 0.463, ave_loss: 0.561
[35]  [680/1724] loss: 0.559, ave_loss: 0.561
[36]  [700/1724] loss: 0.433, ave_loss: 0.557
[37]  [720/1724] loss: 0.559, ave_loss: 0.557
[38]  [740/1724] loss: 0.590, ave_loss: 0.558
[39]  [760/1724] loss: 0.514, ave_loss: 0.557
[40]  [780/1724] loss: 0.546, ave_loss: 0.557
[41]  [800/1724] loss: 0.535, ave_loss: 0.556
[42]  [820/1724] loss: 0.538, ave_loss: 0.556
[43]  [840/1724] loss: 0.522, ave_loss: 0.555
[44]  [860/1724] loss: 0.371, ave_loss: 0.551
[45]  [880/1724] loss: 0.575, ave_loss: 0.551
[46]  [900/1724] loss: 0.511, ave_loss: 0.550
[47]  [920/1724] loss: 0.396, ave_loss: 0.547
[48]  [940/1724] loss: 0.528, ave_loss: 0.547
[49]  [960/1724] loss: 0.626, ave_loss: 0.548
[50]  [980/1724] loss: 0.517, ave_loss: 0.548
[51]  [1000/1724] loss: 0.447, ave_loss: 0.546
[52]  [1020/1724] loss: 0.667, ave_loss: 0.548
[53]  [1040/1724] loss: 0.671, ave_loss: 0.550
[54]  [1060/1724] loss: 0.669, ave_loss: 0.553
[55]  [1080/1724] loss: 0.560, ave_loss: 0.553
[56]  [1100/1724] loss: 0.411, ave_loss: 0.550
[57]  [1120/1724] loss: 0.551, ave_loss: 0.550
[58]  [1140/1724] loss: 0.743, ave_loss: 0.554
[59]  [1160/1724] loss: 0.648, ave_loss: 0.555
[60]  [1180/1724] loss: 0.599, ave_loss: 0.556
[61]  [1200/1724] loss: 0.440, ave_loss: 0.554
[62]  [1220/1724] loss: 0.528, ave_loss: 0.554
[63]  [1240/1724] loss: 0.519, ave_loss: 0.553
[64]  [1260/1724] loss: 0.506, ave_loss: 0.552
[65]  [1280/1724] loss: 0.670, ave_loss: 0.554
[66]  [1300/1724] loss: 0.512, ave_loss: 0.553
[67]  [1320/1724] loss: 0.575, ave_loss: 0.554
[68]  [1340/1724] loss: 0.562, ave_loss: 0.554
[69]  [1360/1724] loss: 0.439, ave_loss: 0.552
[70]  [1380/1724] loss: 0.634, ave_loss: 0.553
[71]  [1400/1724] loss: 0.535, ave_loss: 0.553
[72]  [1420/1724] loss: 0.544, ave_loss: 0.553
[73]  [1440/1724] loss: 0.628, ave_loss: 0.554
[74]  [1460/1724] loss: 0.489, ave_loss: 0.553
[75]  [1480/1724] loss: 0.437, ave_loss: 0.552
[76]  [1500/1724] loss: 0.600, ave_loss: 0.552
[77]  [1520/1724] loss: 0.407, ave_loss: 0.550
[78]  [1540/1724] loss: 0.459, ave_loss: 0.549
[79]  [1560/1724] loss: 0.611, ave_loss: 0.550
[80]  [1580/1724] loss: 0.515, ave_loss: 0.550
[81]  [1600/1724] loss: 0.533, ave_loss: 0.549
[82]  [1620/1724] loss: 0.639, ave_loss: 0.550
[83]  [1640/1724] loss: 0.631, ave_loss: 0.551
[84]  [1660/1724] loss: 0.491, ave_loss: 0.551
[85]  [1680/1724] loss: 0.525, ave_loss: 0.550
[86]  [1700/1724] loss: 0.538, ave_loss: 0.550
[87]  [1720/1724] loss: 0.523, ave_loss: 0.550
[88]  [1740/1724] loss: 0.402, ave_loss: 0.548

Finished Training finishing at 2021-08-30 18:21:51.060940
printing_out epoch  11.22969837587007 learning rate: 0.0005153561248318907
0.00036863495918228726
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.482e-01
Validation Loss: 2.248e+05
Validation ROC: 0.6292
Saving model
17.770301624129928 epochs left to go

Training Epoch 11.22969837587007/30 starting at 2021-08-30 18:23:50.582146
[1]  [0/1724] loss: 0.721, ave_loss: 0.721
[2]  [20/1724] loss: 0.644, ave_loss: 0.682
[3]  [40/1724] loss: 0.501, ave_loss: 0.622
[4]  [60/1724] loss: 0.446, ave_loss: 0.578
[5]  [80/1724] loss: 0.427, ave_loss: 0.548
[6]  [100/1724] loss: 0.603, ave_loss: 0.557
[7]  [120/1724] loss: 0.506, ave_loss: 0.550
[8]  [140/1724] loss: 1.121, ave_loss: 0.621
[9]  [160/1724] loss: 0.365, ave_loss: 0.593
[10]  [180/1724] loss: 0.569, ave_loss: 0.590
[11]  [200/1724] loss: 0.728, ave_loss: 0.603
[12]  [220/1724] loss: 0.443, ave_loss: 0.590
[13]  [240/1724] loss: 0.535, ave_loss: 0.585
[14]  [260/1724] loss: 0.649, ave_loss: 0.590
[15]  [280/1724] loss: 0.390, ave_loss: 0.577
[16]  [300/1724] loss: 0.426, ave_loss: 0.567
[17]  [320/1724] loss: 0.679, ave_loss: 0.574
[18]  [340/1724] loss: 0.579, ave_loss: 0.574
[19]  [360/1724] loss: 0.373, ave_loss: 0.563
[20]  [380/1724] loss: 0.448, ave_loss: 0.558
[21]  [400/1724] loss: 0.431, ave_loss: 0.552
[22]  [420/1724] loss: 0.521, ave_loss: 0.550
[23]  [440/1724] loss: 0.664, ave_loss: 0.555
[24]  [460/1724] loss: 0.419, ave_loss: 0.549
[25]  [480/1724] loss: 0.478, ave_loss: 0.547
[26]  [500/1724] loss: 0.611, ave_loss: 0.549
[27]  [520/1724] loss: 0.598, ave_loss: 0.551
[28]  [540/1724] loss: 0.556, ave_loss: 0.551
[29]  [560/1724] loss: 0.491, ave_loss: 0.549
[30]  [580/1724] loss: 0.629, ave_loss: 0.552
[31]  [600/1724] loss: 0.617, ave_loss: 0.554
[32]  [620/1724] loss: 0.399, ave_loss: 0.549
[33]  [640/1724] loss: 0.477, ave_loss: 0.547
[34]  [660/1724] loss: 0.524, ave_loss: 0.546
[35]  [680/1724] loss: 0.586, ave_loss: 0.547
[36]  [700/1724] loss: 0.689, ave_loss: 0.551
[37]  [720/1724] loss: 0.589, ave_loss: 0.552
[38]  [740/1724] loss: 0.606, ave_loss: 0.554
[39]  [760/1724] loss: 0.489, ave_loss: 0.552
[40]  [780/1724] loss: 0.476, ave_loss: 0.550
[41]  [800/1724] loss: 0.382, ave_loss: 0.546
[42]  [820/1724] loss: 0.551, ave_loss: 0.546
[43]  [840/1724] loss: 0.542, ave_loss: 0.546
[44]  [860/1724] loss: 0.599, ave_loss: 0.547
[45]  [880/1724] loss: 0.632, ave_loss: 0.549
[46]  [900/1724] loss: 0.424, ave_loss: 0.546
[47]  [920/1724] loss: 0.571, ave_loss: 0.547
[48]  [940/1724] loss: 0.450, ave_loss: 0.545
[49]  [960/1724] loss: 0.475, ave_loss: 0.543
[50]  [980/1724] loss: 0.559, ave_loss: 0.544
[51]  [1000/1724] loss: 0.530, ave_loss: 0.543
[52]  [1020/1724] loss: 0.555, ave_loss: 0.544
[53]  [1040/1724] loss: 0.703, ave_loss: 0.547
[54]  [1060/1724] loss: 0.450, ave_loss: 0.545
[55]  [1080/1724] loss: 0.577, ave_loss: 0.545
[56]  [1100/1724] loss: 0.437, ave_loss: 0.544
[57]  [1120/1724] loss: 0.483, ave_loss: 0.542
[58]  [1140/1724] loss: 0.564, ave_loss: 0.543
[59]  [1160/1724] loss: 0.489, ave_loss: 0.542
[60]  [1180/1724] loss: 0.458, ave_loss: 0.541
[61]  [1200/1724] loss: 0.619, ave_loss: 0.542
[62]  [1220/1724] loss: 0.383, ave_loss: 0.539
[63]  [1240/1724] loss: 0.565, ave_loss: 0.540
[64]  [1260/1724] loss: 0.646, ave_loss: 0.541
[65]  [1280/1724] loss: 0.806, ave_loss: 0.545
[66]  [1300/1724] loss: 0.594, ave_loss: 0.546
[67]  [1320/1724] loss: 0.444, ave_loss: 0.545
[68]  [1340/1724] loss: 0.381, ave_loss: 0.542
[69]  [1360/1724] loss: 0.505, ave_loss: 0.542
[70]  [1380/1724] loss: 0.485, ave_loss: 0.541
[71]  [1400/1724] loss: 0.485, ave_loss: 0.540
[72]  [1420/1724] loss: 0.470, ave_loss: 0.539
[73]  [1440/1724] loss: 0.546, ave_loss: 0.539
[74]  [1460/1724] loss: 0.445, ave_loss: 0.538
[75]  [1480/1724] loss: 0.387, ave_loss: 0.536
[76]  [1500/1724] loss: 0.639, ave_loss: 0.537
[77]  [1520/1724] loss: 0.512, ave_loss: 0.537
[78]  [1540/1724] loss: 0.452, ave_loss: 0.536
[79]  [1560/1724] loss: 0.552, ave_loss: 0.536
[80]  [1580/1724] loss: 0.490, ave_loss: 0.535
[81]  [1600/1724] loss: 0.607, ave_loss: 0.536
[82]  [1620/1724] loss: 0.551, ave_loss: 0.537
[83]  [1640/1724] loss: 0.436, ave_loss: 0.535
[84]  [1660/1724] loss: 0.505, ave_loss: 0.535
[85]  [1680/1724] loss: 0.538, ave_loss: 0.535
[86]  [1700/1724] loss: 0.669, ave_loss: 0.537
[87]  [1720/1724] loss: 0.702, ave_loss: 0.538
[88]  [1740/1724] loss: 0.617, ave_loss: 0.539

Finished Training finishing at 2021-08-30 18:26:32.320489
printing_out epoch  12.250580046403712 learning rate: 0.0005153561248318907
0.0003575759104068186
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.394e-01
Validation Loss: 2.166e+05
Validation ROC: 0.6197
No improvement, still saving model
16.749419953596288 epochs left to go

Training Epoch 12.250580046403712/30 starting at 2021-08-30 18:27:20.569351
[1]  [0/1724] loss: 0.508, ave_loss: 0.508
[2]  [20/1724] loss: 0.478, ave_loss: 0.493
[3]  [40/1724] loss: 0.595, ave_loss: 0.527
[4]  [60/1724] loss: 0.568, ave_loss: 0.537
[5]  [80/1724] loss: 0.547, ave_loss: 0.539
[6]  [100/1724] loss: 0.459, ave_loss: 0.526
[7]  [120/1724] loss: 0.508, ave_loss: 0.523
[8]  [140/1724] loss: 0.406, ave_loss: 0.509
[9]  [160/1724] loss: 0.610, ave_loss: 0.520
[10]  [180/1724] loss: 0.676, ave_loss: 0.535
[11]  [200/1724] loss: 0.436, ave_loss: 0.526
[12]  [220/1724] loss: 0.637, ave_loss: 0.536
[13]  [240/1724] loss: 0.594, ave_loss: 0.540
[14]  [260/1724] loss: 0.696, ave_loss: 0.551
[15]  [280/1724] loss: 0.550, ave_loss: 0.551
[16]  [300/1724] loss: 0.579, ave_loss: 0.553
[17]  [320/1724] loss: 0.557, ave_loss: 0.553
[18]  [340/1724] loss: 0.464, ave_loss: 0.548
[19]  [360/1724] loss: 0.636, ave_loss: 0.553
[20]  [380/1724] loss: 0.574, ave_loss: 0.554
[21]  [400/1724] loss: 0.513, ave_loss: 0.552
[22]  [420/1724] loss: 0.509, ave_loss: 0.550
[23]  [440/1724] loss: 0.486, ave_loss: 0.547
[24]  [460/1724] loss: 0.409, ave_loss: 0.541
[25]  [480/1724] loss: 0.388, ave_loss: 0.535
[26]  [500/1724] loss: 0.737, ave_loss: 0.543
[27]  [520/1724] loss: 0.479, ave_loss: 0.541
[28]  [540/1724] loss: 0.487, ave_loss: 0.539
[29]  [560/1724] loss: 0.668, ave_loss: 0.543
[30]  [580/1724] loss: 0.660, ave_loss: 0.547
[31]  [600/1724] loss: 0.603, ave_loss: 0.549
[32]  [620/1724] loss: 0.465, ave_loss: 0.546
[33]  [640/1724] loss: 0.598, ave_loss: 0.548
[34]  [660/1724] loss: 0.380, ave_loss: 0.543
[35]  [680/1724] loss: 0.421, ave_loss: 0.539
[36]  [700/1724] loss: 0.602, ave_loss: 0.541
[37]  [720/1724] loss: 0.354, ave_loss: 0.536
[38]  [740/1724] loss: 0.727, ave_loss: 0.541
[39]  [760/1724] loss: 0.555, ave_loss: 0.541
[40]  [780/1724] loss: 0.484, ave_loss: 0.540
[41]  [800/1724] loss: 0.560, ave_loss: 0.540
[42]  [820/1724] loss: 0.508, ave_loss: 0.540
[43]  [840/1724] loss: 0.442, ave_loss: 0.537
[44]  [860/1724] loss: 0.637, ave_loss: 0.540
[45]  [880/1724] loss: 0.472, ave_loss: 0.538
[46]  [900/1724] loss: 0.373, ave_loss: 0.535
[47]  [920/1724] loss: 0.518, ave_loss: 0.534
[48]  [940/1724] loss: 0.583, ave_loss: 0.535
[49]  [960/1724] loss: 0.837, ave_loss: 0.541
[50]  [980/1724] loss: 0.422, ave_loss: 0.539
[51]  [1000/1724] loss: 0.540, ave_loss: 0.539
[52]  [1020/1724] loss: 0.511, ave_loss: 0.539
[53]  [1040/1724] loss: 0.567, ave_loss: 0.539
[54]  [1060/1724] loss: 0.551, ave_loss: 0.539
[55]  [1080/1724] loss: 0.575, ave_loss: 0.540
[56]  [1100/1724] loss: 0.588, ave_loss: 0.541
[57]  [1120/1724] loss: 0.595, ave_loss: 0.542
[58]  [1140/1724] loss: 0.596, ave_loss: 0.543
[59]  [1160/1724] loss: 0.601, ave_loss: 0.544
[60]  [1180/1724] loss: 0.385, ave_loss: 0.541
[61]  [1200/1724] loss: 0.488, ave_loss: 0.540
[62]  [1220/1724] loss: 0.499, ave_loss: 0.539
[63]  [1240/1724] loss: 0.687, ave_loss: 0.542
[64]  [1260/1724] loss: 0.538, ave_loss: 0.542
[65]  [1280/1724] loss: 0.428, ave_loss: 0.540
[66]  [1300/1724] loss: 0.493, ave_loss: 0.539
[67]  [1320/1724] loss: 0.490, ave_loss: 0.539
[68]  [1340/1724] loss: 0.713, ave_loss: 0.541
[69]  [1360/1724] loss: 0.537, ave_loss: 0.541
[70]  [1380/1724] loss: 0.463, ave_loss: 0.540
[71]  [1400/1724] loss: 0.658, ave_loss: 0.542
[72]  [1420/1724] loss: 0.601, ave_loss: 0.542
[73]  [1440/1724] loss: 0.497, ave_loss: 0.542
[74]  [1460/1724] loss: 0.526, ave_loss: 0.542
[75]  [1480/1724] loss: 0.721, ave_loss: 0.544
[76]  [1500/1724] loss: 0.584, ave_loss: 0.545
[77]  [1520/1724] loss: 0.553, ave_loss: 0.545
[78]  [1540/1724] loss: 0.537, ave_loss: 0.545
[79]  [1560/1724] loss: 0.651, ave_loss: 0.546
[80]  [1580/1724] loss: 0.394, ave_loss: 0.544
[81]  [1600/1724] loss: 0.497, ave_loss: 0.543
[82]  [1620/1724] loss: 0.543, ave_loss: 0.543
[83]  [1640/1724] loss: 0.469, ave_loss: 0.543
[84]  [1660/1724] loss: 0.516, ave_loss: 0.542
[85]  [1680/1724] loss: 0.561, ave_loss: 0.542
[86]  [1700/1724] loss: 0.663, ave_loss: 0.544
[87]  [1720/1724] loss: 0.501, ave_loss: 0.543
[88]  [1740/1724] loss: 0.588, ave_loss: 0.544

Finished Training finishing at 2021-08-30 18:29:50.072764
printing_out epoch  13.271461716937354 learning rate: 0.0005153561248318907
0.00034684863309461403
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.438e-01
Validation Loss: 2.044e+05
Validation ROC: 0.6045
No improvement, still saving model
15.728538283062646 epochs left to go

Training Epoch 13.271461716937354/30 starting at 2021-08-30 18:30:38.176180
[1]  [0/1724] loss: 0.632, ave_loss: 0.632
[2]  [20/1724] loss: 0.270, ave_loss: 0.451
[3]  [40/1724] loss: 0.375, ave_loss: 0.426
[4]  [60/1724] loss: 0.514, ave_loss: 0.448
[5]  [80/1724] loss: 0.835, ave_loss: 0.525
[6]  [100/1724] loss: 0.533, ave_loss: 0.527
[7]  [120/1724] loss: 0.624, ave_loss: 0.540
[8]  [140/1724] loss: 0.633, ave_loss: 0.552
[9]  [160/1724] loss: 0.536, ave_loss: 0.550
[10]  [180/1724] loss: 0.702, ave_loss: 0.565
[11]  [200/1724] loss: 0.718, ave_loss: 0.579
[12]  [220/1724] loss: 0.437, ave_loss: 0.567
[13]  [240/1724] loss: 0.494, ave_loss: 0.562
[14]  [260/1724] loss: 0.575, ave_loss: 0.563
[15]  [280/1724] loss: 0.610, ave_loss: 0.566
[16]  [300/1724] loss: 0.516, ave_loss: 0.563
[17]  [320/1724] loss: 0.663, ave_loss: 0.569
[18]  [340/1724] loss: 0.487, ave_loss: 0.564
[19]  [360/1724] loss: 0.633, ave_loss: 0.568
[20]  [380/1724] loss: 0.599, ave_loss: 0.569
[21]  [400/1724] loss: 0.405, ave_loss: 0.562
[22]  [420/1724] loss: 0.325, ave_loss: 0.551
[23]  [440/1724] loss: 0.441, ave_loss: 0.546
[24]  [460/1724] loss: 0.512, ave_loss: 0.545
[25]  [480/1724] loss: 0.734, ave_loss: 0.552
[26]  [500/1724] loss: 0.522, ave_loss: 0.551
[27]  [520/1724] loss: 0.624, ave_loss: 0.554
[28]  [540/1724] loss: 0.537, ave_loss: 0.553
[29]  [560/1724] loss: 0.496, ave_loss: 0.551
[30]  [580/1724] loss: 0.573, ave_loss: 0.552
[31]  [600/1724] loss: 0.682, ave_loss: 0.556
[32]  [620/1724] loss: 0.463, ave_loss: 0.553
[33]  [640/1724] loss: 0.343, ave_loss: 0.547
[34]  [660/1724] loss: 0.616, ave_loss: 0.549
[35]  [680/1724] loss: 0.430, ave_loss: 0.545
[36]  [700/1724] loss: 0.454, ave_loss: 0.543
[37]  [720/1724] loss: 0.703, ave_loss: 0.547
[38]  [740/1724] loss: 0.572, ave_loss: 0.548
[39]  [760/1724] loss: 0.609, ave_loss: 0.549
[40]  [780/1724] loss: 0.466, ave_loss: 0.547
[41]  [800/1724] loss: 0.507, ave_loss: 0.546
[42]  [820/1724] loss: 0.437, ave_loss: 0.544
[43]  [840/1724] loss: 0.480, ave_loss: 0.542
[44]  [860/1724] loss: 0.449, ave_loss: 0.540
[45]  [880/1724] loss: 0.552, ave_loss: 0.540
[46]  [900/1724] loss: 0.480, ave_loss: 0.539
[47]  [920/1724] loss: 0.475, ave_loss: 0.538
[48]  [940/1724] loss: 0.567, ave_loss: 0.538
[49]  [960/1724] loss: 0.622, ave_loss: 0.540
[50]  [980/1724] loss: 0.569, ave_loss: 0.541
[51]  [1000/1724] loss: 0.510, ave_loss: 0.540
[52]  [1020/1724] loss: 0.545, ave_loss: 0.540
[53]  [1040/1724] loss: 0.629, ave_loss: 0.542
[54]  [1060/1724] loss: 0.419, ave_loss: 0.540
[55]  [1080/1724] loss: 0.561, ave_loss: 0.540
[56]  [1100/1724] loss: 0.769, ave_loss: 0.544
[57]  [1120/1724] loss: 0.443, ave_loss: 0.542
[58]  [1140/1724] loss: 0.614, ave_loss: 0.543
[59]  [1160/1724] loss: 0.390, ave_loss: 0.541
[60]  [1180/1724] loss: 0.684, ave_loss: 0.543
[61]  [1200/1724] loss: 0.556, ave_loss: 0.543
[62]  [1220/1724] loss: 0.573, ave_loss: 0.544
[63]  [1240/1724] loss: 0.655, ave_loss: 0.546
[64]  [1260/1724] loss: 0.530, ave_loss: 0.545
[65]  [1280/1724] loss: 0.566, ave_loss: 0.546
[66]  [1300/1724] loss: 0.575, ave_loss: 0.546
[67]  [1320/1724] loss: 0.663, ave_loss: 0.548
[68]  [1340/1724] loss: 0.533, ave_loss: 0.548
[69]  [1360/1724] loss: 0.515, ave_loss: 0.547
[70]  [1380/1724] loss: 0.384, ave_loss: 0.545
[71]  [1400/1724] loss: 0.493, ave_loss: 0.544
[72]  [1420/1724] loss: 0.669, ave_loss: 0.546
[73]  [1440/1724] loss: 0.371, ave_loss: 0.544
[74]  [1460/1724] loss: 0.533, ave_loss: 0.543
[75]  [1480/1724] loss: 0.776, ave_loss: 0.547
[76]  [1500/1724] loss: 0.409, ave_loss: 0.545
[77]  [1520/1724] loss: 0.424, ave_loss: 0.543
[78]  [1540/1724] loss: 0.531, ave_loss: 0.543
[79]  [1560/1724] loss: 0.584, ave_loss: 0.544
[80]  [1580/1724] loss: 0.385, ave_loss: 0.542
[81]  [1600/1724] loss: 0.619, ave_loss: 0.542
[82]  [1620/1724] loss: 0.416, ave_loss: 0.541
[83]  [1640/1724] loss: 0.614, ave_loss: 0.542
[84]  [1660/1724] loss: 0.458, ave_loss: 0.541
[85]  [1680/1724] loss: 0.590, ave_loss: 0.541
[86]  [1700/1724] loss: 0.451, ave_loss: 0.540
[87]  [1720/1724] loss: 0.564, ave_loss: 0.541
[88]  [1740/1724] loss: 0.670, ave_loss: 0.542

Finished Training finishing at 2021-08-30 18:32:56.642241
printing_out epoch  14.292343387470998 learning rate: 0.0005153561248318907
0.0003364431741017756
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.421e-01
Validation Loss: 2.162e+05
Validation ROC: 0.6298
Saving model
14.707656612529002 epochs left to go

Training Epoch 14.292343387470998/30 starting at 2021-08-30 18:33:48.587817
[1]  [0/1724] loss: 0.779, ave_loss: 0.779
[2]  [20/1724] loss: 0.576, ave_loss: 0.678
[3]  [40/1724] loss: 0.573, ave_loss: 0.643
[4]  [60/1724] loss: 0.434, ave_loss: 0.591
[5]  [80/1724] loss: 0.457, ave_loss: 0.564
[6]  [100/1724] loss: 0.510, ave_loss: 0.555
[7]  [120/1724] loss: 0.673, ave_loss: 0.572
[8]  [140/1724] loss: 0.296, ave_loss: 0.537
[9]  [160/1724] loss: 0.503, ave_loss: 0.533
[10]  [180/1724] loss: 0.617, ave_loss: 0.542
[11]  [200/1724] loss: 0.548, ave_loss: 0.542
[12]  [220/1724] loss: 0.557, ave_loss: 0.544
[13]  [240/1724] loss: 0.611, ave_loss: 0.549
[14]  [260/1724] loss: 0.560, ave_loss: 0.550
[15]  [280/1724] loss: 0.457, ave_loss: 0.543
[16]  [300/1724] loss: 0.402, ave_loss: 0.535
[17]  [320/1724] loss: 0.475, ave_loss: 0.531
[18]  [340/1724] loss: 0.577, ave_loss: 0.534
[19]  [360/1724] loss: 0.597, ave_loss: 0.537
[20]  [380/1724] loss: 0.493, ave_loss: 0.535
[21]  [400/1724] loss: 0.553, ave_loss: 0.536
[22]  [420/1724] loss: 0.573, ave_loss: 0.537
[23]  [440/1724] loss: 0.553, ave_loss: 0.538
[24]  [460/1724] loss: 0.564, ave_loss: 0.539
[25]  [480/1724] loss: 0.547, ave_loss: 0.539
[26]  [500/1724] loss: 0.671, ave_loss: 0.544
[27]  [520/1724] loss: 0.463, ave_loss: 0.541
[28]  [540/1724] loss: 0.502, ave_loss: 0.540
[29]  [560/1724] loss: 0.584, ave_loss: 0.542
[30]  [580/1724] loss: 0.769, ave_loss: 0.549
[31]  [600/1724] loss: 0.473, ave_loss: 0.547
[32]  [620/1724] loss: 0.568, ave_loss: 0.547
[33]  [640/1724] loss: 0.487, ave_loss: 0.545
[34]  [660/1724] loss: 0.493, ave_loss: 0.544
[35]  [680/1724] loss: 0.471, ave_loss: 0.542
[36]  [700/1724] loss: 0.712, ave_loss: 0.547
[37]  [720/1724] loss: 0.817, ave_loss: 0.554
[38]  [740/1724] loss: 0.493, ave_loss: 0.552
[39]  [760/1724] loss: 0.762, ave_loss: 0.558
[40]  [780/1724] loss: 0.671, ave_loss: 0.561
[41]  [800/1724] loss: 0.583, ave_loss: 0.561
[42]  [820/1724] loss: 0.479, ave_loss: 0.559
[43]  [840/1724] loss: 0.434, ave_loss: 0.556
[44]  [860/1724] loss: 0.598, ave_loss: 0.557
[45]  [880/1724] loss: 0.683, ave_loss: 0.560
[46]  [900/1724] loss: 0.431, ave_loss: 0.557
[47]  [920/1724] loss: 0.814, ave_loss: 0.563
[48]  [940/1724] loss: 0.509, ave_loss: 0.561
[49]  [960/1724] loss: 0.628, ave_loss: 0.563
[50]  [980/1724] loss: 0.534, ave_loss: 0.562
[51]  [1000/1724] loss: 0.557, ave_loss: 0.562
[52]  [1020/1724] loss: 0.508, ave_loss: 0.561
[53]  [1040/1724] loss: 0.524, ave_loss: 0.560
[54]  [1060/1724] loss: 0.427, ave_loss: 0.558
[55]  [1080/1724] loss: 0.448, ave_loss: 0.556
[56]  [1100/1724] loss: 0.540, ave_loss: 0.556
[57]  [1120/1724] loss: 0.430, ave_loss: 0.553
[58]  [1140/1724] loss: 0.543, ave_loss: 0.553
[59]  [1160/1724] loss: 0.390, ave_loss: 0.551
[60]  [1180/1724] loss: 0.385, ave_loss: 0.548
[61]  [1200/1724] loss: 0.450, ave_loss: 0.546
[62]  [1220/1724] loss: 0.440, ave_loss: 0.544
[63]  [1240/1724] loss: 0.460, ave_loss: 0.543
[64]  [1260/1724] loss: 0.619, ave_loss: 0.544
[65]  [1280/1724] loss: 0.723, ave_loss: 0.547
[66]  [1300/1724] loss: 0.481, ave_loss: 0.546
[67]  [1320/1724] loss: 0.565, ave_loss: 0.546
[68]  [1340/1724] loss: 0.616, ave_loss: 0.547
[69]  [1360/1724] loss: 0.488, ave_loss: 0.546
[70]  [1380/1724] loss: 0.562, ave_loss: 0.547
[71]  [1400/1724] loss: 0.510, ave_loss: 0.546
[72]  [1420/1724] loss: 0.408, ave_loss: 0.544
[73]  [1440/1724] loss: 0.550, ave_loss: 0.544
[74]  [1460/1724] loss: 0.549, ave_loss: 0.544
[75]  [1480/1724] loss: 0.638, ave_loss: 0.546
[76]  [1500/1724] loss: 0.400, ave_loss: 0.544
[77]  [1520/1724] loss: 0.526, ave_loss: 0.544
[78]  [1540/1724] loss: 0.568, ave_loss: 0.544
[79]  [1560/1724] loss: 0.522, ave_loss: 0.544
[80]  [1580/1724] loss: 0.418, ave_loss: 0.542
[81]  [1600/1724] loss: 0.695, ave_loss: 0.544
[82]  [1620/1724] loss: 0.503, ave_loss: 0.543
[83]  [1640/1724] loss: 0.567, ave_loss: 0.544
[84]  [1660/1724] loss: 0.461, ave_loss: 0.543
[85]  [1680/1724] loss: 0.533, ave_loss: 0.543
[86]  [1700/1724] loss: 0.610, ave_loss: 0.543
[87]  [1720/1724] loss: 0.409, ave_loss: 0.542
[88]  [1740/1724] loss: 0.627, ave_loss: 0.543

Finished Training finishing at 2021-08-30 18:36:13.928027
printing_out epoch  15.31322505800464 learning rate: 0.0005153561248318907
0.0003263498788787223
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.428e-01
Validation Loss: 2.192e+05
Validation ROC: 0.6245
No improvement, still saving model
13.68677494199536 epochs left to go

Training Epoch 15.31322505800464/30 starting at 2021-08-30 18:37:00.569839
[1]  [0/1724] loss: 0.700, ave_loss: 0.700
[2]  [20/1724] loss: 0.707, ave_loss: 0.704
[3]  [40/1724] loss: 0.488, ave_loss: 0.632
[4]  [60/1724] loss: 0.613, ave_loss: 0.627
[5]  [80/1724] loss: 0.542, ave_loss: 0.610
[6]  [100/1724] loss: 0.623, ave_loss: 0.612
[7]  [120/1724] loss: 0.685, ave_loss: 0.623
[8]  [140/1724] loss: 0.419, ave_loss: 0.597
[9]  [160/1724] loss: 0.399, ave_loss: 0.575
[10]  [180/1724] loss: 0.509, ave_loss: 0.569
[11]  [200/1724] loss: 0.516, ave_loss: 0.564
[12]  [220/1724] loss: 0.410, ave_loss: 0.551
[13]  [240/1724] loss: 0.623, ave_loss: 0.557
[14]  [260/1724] loss: 0.450, ave_loss: 0.549
[15]  [280/1724] loss: 0.559, ave_loss: 0.550
[16]  [300/1724] loss: 0.343, ave_loss: 0.537
[17]  [320/1724] loss: 0.418, ave_loss: 0.530
[18]  [340/1724] loss: 0.580, ave_loss: 0.532
[19]  [360/1724] loss: 0.431, ave_loss: 0.527
[20]  [380/1724] loss: 0.637, ave_loss: 0.533
[21]  [400/1724] loss: 0.478, ave_loss: 0.530
[22]  [420/1724] loss: 0.488, ave_loss: 0.528
[23]  [440/1724] loss: 0.682, ave_loss: 0.535
[24]  [460/1724] loss: 0.671, ave_loss: 0.541
[25]  [480/1724] loss: 0.573, ave_loss: 0.542
[26]  [500/1724] loss: 0.523, ave_loss: 0.541
[27]  [520/1724] loss: 0.564, ave_loss: 0.542
[28]  [540/1724] loss: 0.589, ave_loss: 0.544
[29]  [560/1724] loss: 0.710, ave_loss: 0.549
[30]  [580/1724] loss: 0.611, ave_loss: 0.551
[31]  [600/1724] loss: 0.427, ave_loss: 0.547
[32]  [620/1724] loss: 0.489, ave_loss: 0.546
[33]  [640/1724] loss: 0.480, ave_loss: 0.544
[34]  [660/1724] loss: 0.379, ave_loss: 0.539
[35]  [680/1724] loss: 0.482, ave_loss: 0.537
[36]  [700/1724] loss: 0.510, ave_loss: 0.536
[37]  [720/1724] loss: 0.424, ave_loss: 0.533
[38]  [740/1724] loss: 0.521, ave_loss: 0.533
[39]  [760/1724] loss: 0.559, ave_loss: 0.534
[40]  [780/1724] loss: 0.513, ave_loss: 0.533
[41]  [800/1724] loss: 0.408, ave_loss: 0.530
[42]  [820/1724] loss: 0.502, ave_loss: 0.529
[43]  [840/1724] loss: 0.460, ave_loss: 0.528
[44]  [860/1724] loss: 0.517, ave_loss: 0.528
[45]  [880/1724] loss: 0.480, ave_loss: 0.527
[46]  [900/1724] loss: 0.544, ave_loss: 0.527
[47]  [920/1724] loss: 0.377, ave_loss: 0.524
[48]  [940/1724] loss: 0.687, ave_loss: 0.527
[49]  [960/1724] loss: 0.557, ave_loss: 0.528
[50]  [980/1724] loss: 0.546, ave_loss: 0.528
[51]  [1000/1724] loss: 0.574, ave_loss: 0.529
[52]  [1020/1724] loss: 0.470, ave_loss: 0.528
[53]  [1040/1724] loss: 0.398, ave_loss: 0.525
[54]  [1060/1724] loss: 0.545, ave_loss: 0.526
[55]  [1080/1724] loss: 0.474, ave_loss: 0.525
[56]  [1100/1724] loss: 0.544, ave_loss: 0.525
[57]  [1120/1724] loss: 0.739, ave_loss: 0.529
[58]  [1140/1724] loss: 0.510, ave_loss: 0.529
[59]  [1160/1724] loss: 0.449, ave_loss: 0.527
[60]  [1180/1724] loss: 0.530, ave_loss: 0.527
[61]  [1200/1724] loss: 0.486, ave_loss: 0.527
[62]  [1220/1724] loss: 0.498, ave_loss: 0.526
[63]  [1240/1724] loss: 0.366, ave_loss: 0.524
[64]  [1260/1724] loss: 0.588, ave_loss: 0.525
[65]  [1280/1724] loss: 0.478, ave_loss: 0.524
[66]  [1300/1724] loss: 0.587, ave_loss: 0.525
[67]  [1320/1724] loss: 0.541, ave_loss: 0.525
[68]  [1340/1724] loss: 0.475, ave_loss: 0.524
[69]  [1360/1724] loss: 0.366, ave_loss: 0.522
[70]  [1380/1724] loss: 0.489, ave_loss: 0.522
[71]  [1400/1724] loss: 0.450, ave_loss: 0.521
[72]  [1420/1724] loss: 0.549, ave_loss: 0.521
[73]  [1440/1724] loss: 0.551, ave_loss: 0.521
[74]  [1460/1724] loss: 0.541, ave_loss: 0.522
[75]  [1480/1724] loss: 0.416, ave_loss: 0.520
[76]  [1500/1724] loss: 0.580, ave_loss: 0.521
[77]  [1520/1724] loss: 0.439, ave_loss: 0.520
[78]  [1540/1724] loss: 0.490, ave_loss: 0.520
[79]  [1560/1724] loss: 0.460, ave_loss: 0.519
[80]  [1580/1724] loss: 0.624, ave_loss: 0.520
[81]  [1600/1724] loss: 0.406, ave_loss: 0.519
[82]  [1620/1724] loss: 0.658, ave_loss: 0.520
[83]  [1640/1724] loss: 0.398, ave_loss: 0.519
[84]  [1660/1724] loss: 0.413, ave_loss: 0.518
[85]  [1680/1724] loss: 0.636, ave_loss: 0.519
[86]  [1700/1724] loss: 0.428, ave_loss: 0.518
[87]  [1720/1724] loss: 0.484, ave_loss: 0.518
[88]  [1740/1724] loss: 0.456, ave_loss: 0.517

Finished Training finishing at 2021-08-30 18:39:26.054713
printing_out epoch  16.334106728538284 learning rate: 0.0005153561248318907
0.00031655938251236066
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.169e-01
Validation Loss: 2.112e+05
Validation ROC: 0.6289
No improvement, still saving model
12.665893271461716 epochs left to go

Training Epoch 16.334106728538284/30 starting at 2021-08-30 18:41:10.621948
[1]  [0/1724] loss: 0.592, ave_loss: 0.592
[2]  [20/1724] loss: 0.302, ave_loss: 0.447
[3]  [40/1724] loss: 0.513, ave_loss: 0.469
[4]  [60/1724] loss: 0.516, ave_loss: 0.481
[5]  [80/1724] loss: 0.443, ave_loss: 0.473
[6]  [100/1724] loss: 0.600, ave_loss: 0.494
[7]  [120/1724] loss: 0.649, ave_loss: 0.516
[8]  [140/1724] loss: 0.503, ave_loss: 0.515
[9]  [160/1724] loss: 0.684, ave_loss: 0.533
[10]  [180/1724] loss: 0.572, ave_loss: 0.537
[11]  [200/1724] loss: 0.636, ave_loss: 0.546
[12]  [220/1724] loss: 0.650, ave_loss: 0.555
[13]  [240/1724] loss: 0.542, ave_loss: 0.554
[14]  [260/1724] loss: 0.417, ave_loss: 0.544
[15]  [280/1724] loss: 0.453, ave_loss: 0.538
[16]  [300/1724] loss: 0.697, ave_loss: 0.548
[17]  [320/1724] loss: 0.712, ave_loss: 0.558
[18]  [340/1724] loss: 0.757, ave_loss: 0.569
[19]  [360/1724] loss: 0.409, ave_loss: 0.560
[20]  [380/1724] loss: 0.660, ave_loss: 0.565
[21]  [400/1724] loss: 0.602, ave_loss: 0.567
[22]  [420/1724] loss: 0.528, ave_loss: 0.565
[23]  [440/1724] loss: 0.496, ave_loss: 0.562
[24]  [460/1724] loss: 0.349, ave_loss: 0.553
[25]  [480/1724] loss: 0.547, ave_loss: 0.553
[26]  [500/1724] loss: 0.916, ave_loss: 0.567
[27]  [520/1724] loss: 0.650, ave_loss: 0.570
[28]  [540/1724] loss: 0.554, ave_loss: 0.570
[29]  [560/1724] loss: 0.476, ave_loss: 0.566
[30]  [580/1724] loss: 0.648, ave_loss: 0.569
[31]  [600/1724] loss: 0.427, ave_loss: 0.564
[32]  [620/1724] loss: 0.570, ave_loss: 0.565
[33]  [640/1724] loss: 0.491, ave_loss: 0.562
[34]  [660/1724] loss: 0.469, ave_loss: 0.560
[35]  [680/1724] loss: 0.671, ave_loss: 0.563
[36]  [700/1724] loss: 0.412, ave_loss: 0.559
[37]  [720/1724] loss: 0.586, ave_loss: 0.559
[38]  [740/1724] loss: 0.418, ave_loss: 0.556
[39]  [760/1724] loss: 0.503, ave_loss: 0.554
[40]  [780/1724] loss: 0.503, ave_loss: 0.553
[41]  [800/1724] loss: 0.504, ave_loss: 0.552
[42]  [820/1724] loss: 0.582, ave_loss: 0.553
[43]  [840/1724] loss: 0.508, ave_loss: 0.552
[44]  [860/1724] loss: 0.521, ave_loss: 0.551
[45]  [880/1724] loss: 0.490, ave_loss: 0.549
[46]  [900/1724] loss: 0.321, ave_loss: 0.544
[47]  [920/1724] loss: 0.541, ave_loss: 0.544
[48]  [940/1724] loss: 0.478, ave_loss: 0.543
[49]  [960/1724] loss: 0.558, ave_loss: 0.543
[50]  [980/1724] loss: 0.456, ave_loss: 0.542
[51]  [1000/1724] loss: 0.530, ave_loss: 0.541
[52]  [1020/1724] loss: 0.545, ave_loss: 0.541
[53]  [1040/1724] loss: 0.522, ave_loss: 0.541
[54]  [1060/1724] loss: 0.445, ave_loss: 0.539
[55]  [1080/1724] loss: 0.529, ave_loss: 0.539
[56]  [1100/1724] loss: 0.315, ave_loss: 0.535
[57]  [1120/1724] loss: 0.512, ave_loss: 0.535
[58]  [1140/1724] loss: 0.399, ave_loss: 0.532
[59]  [1160/1724] loss: 0.473, ave_loss: 0.531
[60]  [1180/1724] loss: 0.484, ave_loss: 0.531
[61]  [1200/1724] loss: 0.428, ave_loss: 0.529
[62]  [1220/1724] loss: 0.427, ave_loss: 0.527
[63]  [1240/1724] loss: 0.428, ave_loss: 0.526
[64]  [1260/1724] loss: 0.475, ave_loss: 0.525
[65]  [1280/1724] loss: 0.681, ave_loss: 0.527
[66]  [1300/1724] loss: 0.448, ave_loss: 0.526
[67]  [1320/1724] loss: 0.731, ave_loss: 0.529
[68]  [1340/1724] loss: 0.722, ave_loss: 0.532
[69]  [1360/1724] loss: 0.518, ave_loss: 0.532
[70]  [1380/1724] loss: 0.508, ave_loss: 0.531
[71]  [1400/1724] loss: 0.600, ave_loss: 0.532
[72]  [1420/1724] loss: 0.458, ave_loss: 0.531
[73]  [1440/1724] loss: 0.424, ave_loss: 0.530
[74]  [1460/1724] loss: 0.602, ave_loss: 0.531
[75]  [1480/1724] loss: 0.603, ave_loss: 0.532
[76]  [1500/1724] loss: 0.480, ave_loss: 0.531
[77]  [1520/1724] loss: 0.474, ave_loss: 0.530
[78]  [1540/1724] loss: 0.440, ave_loss: 0.529
[79]  [1560/1724] loss: 0.528, ave_loss: 0.529
[80]  [1580/1724] loss: 0.549, ave_loss: 0.529
[81]  [1600/1724] loss: 0.477, ave_loss: 0.529
[82]  [1620/1724] loss: 0.384, ave_loss: 0.527
[83]  [1640/1724] loss: 0.502, ave_loss: 0.527
[84]  [1660/1724] loss: 0.559, ave_loss: 0.527
[85]  [1680/1724] loss: 0.578, ave_loss: 0.528
[86]  [1700/1724] loss: 0.625, ave_loss: 0.529
[87]  [1720/1724] loss: 0.424, ave_loss: 0.528
[88]  [1740/1724] loss: 0.588, ave_loss: 0.528

Finished Training finishing at 2021-08-30 18:43:36.595460
printing_out epoch  17.354988399071924 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.283e-01
Validation Loss: 2.203e+05
Validation ROC: 0.6465
Saving model
11.645011600928076 epochs left to go

Training Epoch 17.354988399071924/30 starting at 2021-08-30 18:44:19.107489
[1]  [0/1724] loss: 0.577, ave_loss: 0.577
[2]  [20/1724] loss: 0.454, ave_loss: 0.516
[3]  [40/1724] loss: 0.596, ave_loss: 0.542
[4]  [60/1724] loss: 0.557, ave_loss: 0.546
[5]  [80/1724] loss: 0.492, ave_loss: 0.535
[6]  [100/1724] loss: 0.619, ave_loss: 0.549
[7]  [120/1724] loss: 0.505, ave_loss: 0.543
[8]  [140/1724] loss: 0.465, ave_loss: 0.533
[9]  [160/1724] loss: 0.539, ave_loss: 0.534
[10]  [180/1724] loss: 0.396, ave_loss: 0.520
[11]  [200/1724] loss: 0.465, ave_loss: 0.515
[12]  [220/1724] loss: 0.584, ave_loss: 0.521
[13]  [240/1724] loss: 0.575, ave_loss: 0.525
[14]  [260/1724] loss: 0.474, ave_loss: 0.521
[15]  [280/1724] loss: 0.536, ave_loss: 0.522
[16]  [300/1724] loss: 0.583, ave_loss: 0.526
[17]  [320/1724] loss: 0.567, ave_loss: 0.528
[18]  [340/1724] loss: 0.500, ave_loss: 0.527
[19]  [360/1724] loss: 0.630, ave_loss: 0.532
[20]  [380/1724] loss: 0.515, ave_loss: 0.531
[21]  [400/1724] loss: 0.431, ave_loss: 0.527
[22]  [420/1724] loss: 0.515, ave_loss: 0.526
[23]  [440/1724] loss: 0.508, ave_loss: 0.525
[24]  [460/1724] loss: 0.536, ave_loss: 0.526
[25]  [480/1724] loss: 0.576, ave_loss: 0.528
[26]  [500/1724] loss: 0.462, ave_loss: 0.525
[27]  [520/1724] loss: 0.495, ave_loss: 0.524
[28]  [540/1724] loss: 0.607, ave_loss: 0.527
[29]  [560/1724] loss: 0.491, ave_loss: 0.526
[30]  [580/1724] loss: 0.600, ave_loss: 0.528
[31]  [600/1724] loss: 0.577, ave_loss: 0.530
[32]  [620/1724] loss: 0.450, ave_loss: 0.527
[33]  [640/1724] loss: 0.490, ave_loss: 0.526
[34]  [660/1724] loss: 0.631, ave_loss: 0.529
[35]  [680/1724] loss: 0.481, ave_loss: 0.528
[36]  [700/1724] loss: 0.422, ave_loss: 0.525
[37]  [720/1724] loss: 0.485, ave_loss: 0.524
[38]  [740/1724] loss: 0.410, ave_loss: 0.521
[39]  [760/1724] loss: 0.512, ave_loss: 0.521
[40]  [780/1724] loss: 0.399, ave_loss: 0.518
[41]  [800/1724] loss: 0.448, ave_loss: 0.516
[42]  [820/1724] loss: 0.524, ave_loss: 0.516
[43]  [840/1724] loss: 0.439, ave_loss: 0.514
[44]  [860/1724] loss: 0.524, ave_loss: 0.515
[45]  [880/1724] loss: 0.513, ave_loss: 0.515
[46]  [900/1724] loss: 0.637, ave_loss: 0.517
[47]  [920/1724] loss: 0.622, ave_loss: 0.519
[48]  [940/1724] loss: 0.487, ave_loss: 0.519
[49]  [960/1724] loss: 0.516, ave_loss: 0.519
[50]  [980/1724] loss: 0.562, ave_loss: 0.520
[51]  [1000/1724] loss: 0.570, ave_loss: 0.521
[52]  [1020/1724] loss: 0.448, ave_loss: 0.519
[53]  [1040/1724] loss: 0.479, ave_loss: 0.518
[54]  [1060/1724] loss: 0.619, ave_loss: 0.520
[55]  [1080/1724] loss: 0.601, ave_loss: 0.522
[56]  [1100/1724] loss: 0.669, ave_loss: 0.524
[57]  [1120/1724] loss: 0.560, ave_loss: 0.525
[58]  [1140/1724] loss: 0.795, ave_loss: 0.530
[59]  [1160/1724] loss: 0.703, ave_loss: 0.533
[60]  [1180/1724] loss: 0.604, ave_loss: 0.534
[61]  [1200/1724] loss: 0.493, ave_loss: 0.533
[62]  [1220/1724] loss: 0.464, ave_loss: 0.532
[63]  [1240/1724] loss: 0.434, ave_loss: 0.530
[64]  [1260/1724] loss: 0.659, ave_loss: 0.532
[65]  [1280/1724] loss: 0.611, ave_loss: 0.534
[66]  [1300/1724] loss: 0.604, ave_loss: 0.535
[67]  [1320/1724] loss: 0.324, ave_loss: 0.532
[68]  [1340/1724] loss: 0.341, ave_loss: 0.529
[69]  [1360/1724] loss: 0.412, ave_loss: 0.527
[70]  [1380/1724] loss: 0.536, ave_loss: 0.527
[71]  [1400/1724] loss: 0.337, ave_loss: 0.525
[72]  [1420/1724] loss: 0.501, ave_loss: 0.524
[73]  [1440/1724] loss: 0.389, ave_loss: 0.522
[74]  [1460/1724] loss: 0.378, ave_loss: 0.520
[75]  [1480/1724] loss: 0.501, ave_loss: 0.520
[76]  [1500/1724] loss: 0.413, ave_loss: 0.519
[77]  [1520/1724] loss: 0.393, ave_loss: 0.517
[78]  [1540/1724] loss: 0.481, ave_loss: 0.517
[79]  [1560/1724] loss: 0.497, ave_loss: 0.516
[80]  [1580/1724] loss: 0.556, ave_loss: 0.517
[81]  [1600/1724] loss: 0.265, ave_loss: 0.514
[82]  [1620/1724] loss: 0.651, ave_loss: 0.515
[83]  [1640/1724] loss: 0.571, ave_loss: 0.516
[84]  [1660/1724] loss: 0.610, ave_loss: 0.517
[85]  [1680/1724] loss: 0.497, ave_loss: 0.517
[86]  [1700/1724] loss: 0.379, ave_loss: 0.515
[87]  [1720/1724] loss: 0.443, ave_loss: 0.515
[88]  [1740/1724] loss: 0.415, ave_loss: 0.513

Finished Training finishing at 2021-08-30 18:46:50.031027
printing_out epoch  18.37587006960557 learning rate: 0.00046850556802899155
0.00044081688895847813
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.134e-01
Validation Loss: 2.134e+05
Validation ROC: 0.6526
Saving model
10.624129930394432 epochs left to go

Training Epoch 18.37587006960557/30 starting at 2021-08-30 18:47:36.720543
[1]  [0/1724] loss: 0.613, ave_loss: 0.613
[2]  [20/1724] loss: 0.622, ave_loss: 0.617
[3]  [40/1724] loss: 0.447, ave_loss: 0.561
[4]  [60/1724] loss: 0.531, ave_loss: 0.553
[5]  [80/1724] loss: 0.481, ave_loss: 0.539
[6]  [100/1724] loss: 0.587, ave_loss: 0.547
[7]  [120/1724] loss: 0.613, ave_loss: 0.556
[8]  [140/1724] loss: 0.464, ave_loss: 0.545
[9]  [160/1724] loss: 0.475, ave_loss: 0.537
[10]  [180/1724] loss: 0.583, ave_loss: 0.542
[11]  [200/1724] loss: 0.577, ave_loss: 0.545
[12]  [220/1724] loss: 0.508, ave_loss: 0.542
[13]  [240/1724] loss: 0.512, ave_loss: 0.540
[14]  [260/1724] loss: 0.502, ave_loss: 0.537
[15]  [280/1724] loss: 0.507, ave_loss: 0.535
[16]  [300/1724] loss: 0.406, ave_loss: 0.527
[17]  [320/1724] loss: 0.603, ave_loss: 0.531
[18]  [340/1724] loss: 0.543, ave_loss: 0.532
[19]  [360/1724] loss: 0.360, ave_loss: 0.523
[20]  [380/1724] loss: 0.509, ave_loss: 0.522
[21]  [400/1724] loss: 0.878, ave_loss: 0.539
[22]  [420/1724] loss: 0.514, ave_loss: 0.538
[23]  [440/1724] loss: 0.748, ave_loss: 0.547
[24]  [460/1724] loss: 0.394, ave_loss: 0.541
[25]  [480/1724] loss: 0.505, ave_loss: 0.539
[26]  [500/1724] loss: 0.638, ave_loss: 0.543
[27]  [520/1724] loss: 0.586, ave_loss: 0.545
[28]  [540/1724] loss: 0.439, ave_loss: 0.541
[29]  [560/1724] loss: 0.447, ave_loss: 0.538
[30]  [580/1724] loss: 0.614, ave_loss: 0.540
[31]  [600/1724] loss: 0.446, ave_loss: 0.537
[32]  [620/1724] loss: 0.409, ave_loss: 0.533
[33]  [640/1724] loss: 0.598, ave_loss: 0.535
[34]  [660/1724] loss: 0.462, ave_loss: 0.533
[35]  [680/1724] loss: 0.501, ave_loss: 0.532
[36]  [700/1724] loss: 0.560, ave_loss: 0.533
[37]  [720/1724] loss: 0.576, ave_loss: 0.534
[38]  [740/1724] loss: 0.504, ave_loss: 0.533
[39]  [760/1724] loss: 0.545, ave_loss: 0.534
[40]  [780/1724] loss: 0.516, ave_loss: 0.533
[41]  [800/1724] loss: 0.615, ave_loss: 0.535
[42]  [820/1724] loss: 0.353, ave_loss: 0.531
[43]  [840/1724] loss: 0.353, ave_loss: 0.527
[44]  [860/1724] loss: 0.567, ave_loss: 0.528
[45]  [880/1724] loss: 0.640, ave_loss: 0.530
[46]  [900/1724] loss: 0.521, ave_loss: 0.530
[47]  [920/1724] loss: 0.581, ave_loss: 0.531
[48]  [940/1724] loss: 0.434, ave_loss: 0.529
[49]  [960/1724] loss: 0.502, ave_loss: 0.528
[50]  [980/1724] loss: 0.369, ave_loss: 0.525
[51]  [1000/1724] loss: 0.684, ave_loss: 0.528
[52]  [1020/1724] loss: 0.448, ave_loss: 0.527
[53]  [1040/1724] loss: 0.664, ave_loss: 0.529
[54]  [1060/1724] loss: 0.356, ave_loss: 0.526
[55]  [1080/1724] loss: 0.511, ave_loss: 0.526
[56]  [1100/1724] loss: 0.475, ave_loss: 0.525
[57]  [1120/1724] loss: 0.468, ave_loss: 0.524
[58]  [1140/1724] loss: 0.558, ave_loss: 0.525
[59]  [1160/1724] loss: 0.692, ave_loss: 0.527
[60]  [1180/1724] loss: 0.498, ave_loss: 0.527
[61]  [1200/1724] loss: 0.600, ave_loss: 0.528
[62]  [1220/1724] loss: 0.432, ave_loss: 0.527
[63]  [1240/1724] loss: 0.319, ave_loss: 0.523
[64]  [1260/1724] loss: 0.562, ave_loss: 0.524
[65]  [1280/1724] loss: 0.609, ave_loss: 0.525
[66]  [1300/1724] loss: 0.625, ave_loss: 0.527
[67]  [1320/1724] loss: 0.732, ave_loss: 0.530
[68]  [1340/1724] loss: 0.471, ave_loss: 0.529
[69]  [1360/1724] loss: 0.494, ave_loss: 0.528
[70]  [1380/1724] loss: 0.439, ave_loss: 0.527
[71]  [1400/1724] loss: 0.447, ave_loss: 0.526
[72]  [1420/1724] loss: 0.431, ave_loss: 0.525
[73]  [1440/1724] loss: 0.489, ave_loss: 0.524
[74]  [1460/1724] loss: 0.364, ave_loss: 0.522
[75]  [1480/1724] loss: 0.483, ave_loss: 0.521
[76]  [1500/1724] loss: 0.422, ave_loss: 0.520
[77]  [1520/1724] loss: 0.375, ave_loss: 0.518
[78]  [1540/1724] loss: 0.467, ave_loss: 0.518
[79]  [1560/1724] loss: 0.399, ave_loss: 0.516
[80]  [1580/1724] loss: 0.497, ave_loss: 0.516
[81]  [1600/1724] loss: 0.564, ave_loss: 0.516
[82]  [1620/1724] loss: 0.621, ave_loss: 0.518
[83]  [1640/1724] loss: 0.385, ave_loss: 0.516
[84]  [1660/1724] loss: 0.575, ave_loss: 0.517
[85]  [1680/1724] loss: 0.548, ave_loss: 0.517
[86]  [1700/1724] loss: 0.521, ave_loss: 0.517
[87]  [1720/1724] loss: 0.410, ave_loss: 0.516
[88]  [1740/1724] loss: 0.593, ave_loss: 0.517

Finished Training finishing at 2021-08-30 18:50:02.657563
printing_out epoch  19.396751740139212 learning rate: 0.00046850556802899155
0.00042759238228972377
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.169e-01
Validation Loss: 2.193e+05
Validation ROC: 0.6519
No improvement, still saving model
9.603248259860788 epochs left to go

Training Epoch 19.396751740139212/30 starting at 2021-08-30 18:51:00.254409
[1]  [0/1724] loss: 0.668, ave_loss: 0.668
[2]  [20/1724] loss: 0.559, ave_loss: 0.613
[3]  [40/1724] loss: 0.433, ave_loss: 0.553
[4]  [60/1724] loss: 0.647, ave_loss: 0.577
[5]  [80/1724] loss: 0.312, ave_loss: 0.524
[6]  [100/1724] loss: 0.230, ave_loss: 0.475
[7]  [120/1724] loss: 0.455, ave_loss: 0.472
[8]  [140/1724] loss: 0.572, ave_loss: 0.484
[9]  [160/1724] loss: 0.547, ave_loss: 0.491
[10]  [180/1724] loss: 0.461, ave_loss: 0.488
[11]  [200/1724] loss: 0.527, ave_loss: 0.492
[12]  [220/1724] loss: 0.401, ave_loss: 0.484
[13]  [240/1724] loss: 0.518, ave_loss: 0.487
[14]  [260/1724] loss: 0.507, ave_loss: 0.488
[15]  [280/1724] loss: 0.550, ave_loss: 0.492
[16]  [300/1724] loss: 0.533, ave_loss: 0.495
[17]  [320/1724] loss: 0.299, ave_loss: 0.483
[18]  [340/1724] loss: 0.451, ave_loss: 0.482
[19]  [360/1724] loss: 0.509, ave_loss: 0.483
[20]  [380/1724] loss: 0.509, ave_loss: 0.484
[21]  [400/1724] loss: 0.403, ave_loss: 0.480
[22]  [420/1724] loss: 0.423, ave_loss: 0.478
[23]  [440/1724] loss: 0.538, ave_loss: 0.480
[24]  [460/1724] loss: 0.677, ave_loss: 0.489
[25]  [480/1724] loss: 0.549, ave_loss: 0.491
[26]  [500/1724] loss: 0.529, ave_loss: 0.493
[27]  [520/1724] loss: 0.384, ave_loss: 0.488
[28]  [540/1724] loss: 0.500, ave_loss: 0.489
[29]  [560/1724] loss: 0.593, ave_loss: 0.493
[30]  [580/1724] loss: 0.471, ave_loss: 0.492
[31]  [600/1724] loss: 0.559, ave_loss: 0.494
[32]  [620/1724] loss: 0.694, ave_loss: 0.500
[33]  [640/1724] loss: 0.524, ave_loss: 0.501
[34]  [660/1724] loss: 0.420, ave_loss: 0.499
[35]  [680/1724] loss: 0.599, ave_loss: 0.501
[36]  [700/1724] loss: 0.544, ave_loss: 0.503
[37]  [720/1724] loss: 0.430, ave_loss: 0.501
[38]  [740/1724] loss: 0.443, ave_loss: 0.499
[39]  [760/1724] loss: 0.475, ave_loss: 0.498
[40]  [780/1724] loss: 0.496, ave_loss: 0.498
[41]  [800/1724] loss: 0.482, ave_loss: 0.498
[42]  [820/1724] loss: 0.549, ave_loss: 0.499
[43]  [840/1724] loss: 0.679, ave_loss: 0.503
[44]  [860/1724] loss: 0.724, ave_loss: 0.508
[45]  [880/1724] loss: 0.407, ave_loss: 0.506
[46]  [900/1724] loss: 0.552, ave_loss: 0.507
[47]  [920/1724] loss: 0.511, ave_loss: 0.507
[48]  [940/1724] loss: 0.518, ave_loss: 0.507
[49]  [960/1724] loss: 0.345, ave_loss: 0.504
[50]  [980/1724] loss: 0.454, ave_loss: 0.503
[51]  [1000/1724] loss: 0.606, ave_loss: 0.505
[52]  [1020/1724] loss: 0.500, ave_loss: 0.505
[53]  [1040/1724] loss: 0.437, ave_loss: 0.504
[54]  [1060/1724] loss: 0.450, ave_loss: 0.503
[55]  [1080/1724] loss: 0.442, ave_loss: 0.502
[56]  [1100/1724] loss: 0.493, ave_loss: 0.502
[57]  [1120/1724] loss: 0.581, ave_loss: 0.503
[58]  [1140/1724] loss: 0.515, ave_loss: 0.503
[59]  [1160/1724] loss: 0.353, ave_loss: 0.501
[60]  [1180/1724] loss: 0.538, ave_loss: 0.501
[61]  [1200/1724] loss: 0.393, ave_loss: 0.499
[62]  [1220/1724] loss: 0.363, ave_loss: 0.497
[63]  [1240/1724] loss: 0.409, ave_loss: 0.496
[64]  [1260/1724] loss: 0.522, ave_loss: 0.496
[65]  [1280/1724] loss: 0.557, ave_loss: 0.497
[66]  [1300/1724] loss: 0.411, ave_loss: 0.496
[67]  [1320/1724] loss: 0.477, ave_loss: 0.496
[68]  [1340/1724] loss: 0.812, ave_loss: 0.500
[69]  [1360/1724] loss: 0.592, ave_loss: 0.502
[70]  [1380/1724] loss: 0.486, ave_loss: 0.501
[71]  [1400/1724] loss: 0.396, ave_loss: 0.500
[72]  [1420/1724] loss: 0.531, ave_loss: 0.500
[73]  [1440/1724] loss: 0.326, ave_loss: 0.498
[74]  [1460/1724] loss: 0.390, ave_loss: 0.496
[75]  [1480/1724] loss: 0.403, ave_loss: 0.495
[76]  [1500/1724] loss: 0.353, ave_loss: 0.493
[77]  [1520/1724] loss: 0.415, ave_loss: 0.492
[78]  [1540/1724] loss: 0.606, ave_loss: 0.494
[79]  [1560/1724] loss: 0.706, ave_loss: 0.496
[80]  [1580/1724] loss: 0.583, ave_loss: 0.498
[81]  [1600/1724] loss: 0.475, ave_loss: 0.497
[82]  [1620/1724] loss: 0.336, ave_loss: 0.495
[83]  [1640/1724] loss: 0.678, ave_loss: 0.498
[84]  [1660/1724] loss: 0.395, ave_loss: 0.496
[85]  [1680/1724] loss: 0.425, ave_loss: 0.495
[86]  [1700/1724] loss: 0.429, ave_loss: 0.495
[87]  [1720/1724] loss: 0.633, ave_loss: 0.496
[88]  [1740/1724] loss: 0.453, ave_loss: 0.496

Finished Training finishing at 2021-08-30 18:53:24.795152
printing_out epoch  20.417633410672853 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.958e-01
Validation Loss: 2.138e+05
Validation ROC: 0.6454
No improvement, still saving model
8.582366589327147 epochs left to go

Training Epoch 20.417633410672853/30 starting at 2021-08-30 18:54:17.019950
[1]  [0/1724] loss: 0.535, ave_loss: 0.535
[2]  [20/1724] loss: 0.421, ave_loss: 0.478
[3]  [40/1724] loss: 0.553, ave_loss: 0.503
[4]  [60/1724] loss: 0.555, ave_loss: 0.516
[5]  [80/1724] loss: 0.401, ave_loss: 0.493
[6]  [100/1724] loss: 0.370, ave_loss: 0.472
[7]  [120/1724] loss: 0.536, ave_loss: 0.482
[8]  [140/1724] loss: 0.407, ave_loss: 0.472
[9]  [160/1724] loss: 0.747, ave_loss: 0.503
[10]  [180/1724] loss: 0.504, ave_loss: 0.503
[11]  [200/1724] loss: 0.394, ave_loss: 0.493
[12]  [220/1724] loss: 0.707, ave_loss: 0.511
[13]  [240/1724] loss: 0.680, ave_loss: 0.524
[14]  [260/1724] loss: 0.401, ave_loss: 0.515
[15]  [280/1724] loss: 0.368, ave_loss: 0.505
[16]  [300/1724] loss: 0.575, ave_loss: 0.510
[17]  [320/1724] loss: 0.579, ave_loss: 0.514
[18]  [340/1724] loss: 0.600, ave_loss: 0.518
[19]  [360/1724] loss: 0.567, ave_loss: 0.521
[20]  [380/1724] loss: 0.457, ave_loss: 0.518
[21]  [400/1724] loss: 0.331, ave_loss: 0.509
[22]  [420/1724] loss: 0.553, ave_loss: 0.511
[23]  [440/1724] loss: 0.527, ave_loss: 0.512
[24]  [460/1724] loss: 0.567, ave_loss: 0.514
[25]  [480/1724] loss: 0.466, ave_loss: 0.512
[26]  [500/1724] loss: 0.468, ave_loss: 0.510
[27]  [520/1724] loss: 0.473, ave_loss: 0.509
[28]  [540/1724] loss: 0.586, ave_loss: 0.512
[29]  [560/1724] loss: 0.588, ave_loss: 0.514
[30]  [580/1724] loss: 0.552, ave_loss: 0.515
[31]  [600/1724] loss: 0.625, ave_loss: 0.519
[32]  [620/1724] loss: 0.468, ave_loss: 0.517
[33]  [640/1724] loss: 0.488, ave_loss: 0.517
[34]  [660/1724] loss: 0.418, ave_loss: 0.514
[35]  [680/1724] loss: 0.610, ave_loss: 0.516
[36]  [700/1724] loss: 0.378, ave_loss: 0.513
[37]  [720/1724] loss: 0.531, ave_loss: 0.513
[38]  [740/1724] loss: 0.524, ave_loss: 0.513
[39]  [760/1724] loss: 0.813, ave_loss: 0.521
[40]  [780/1724] loss: 0.447, ave_loss: 0.519
[41]  [800/1724] loss: 0.364, ave_loss: 0.515
[42]  [820/1724] loss: 0.420, ave_loss: 0.513
[43]  [840/1724] loss: 0.545, ave_loss: 0.514
[44]  [860/1724] loss: 0.448, ave_loss: 0.512
[45]  [880/1724] loss: 0.417, ave_loss: 0.510
[46]  [900/1724] loss: 0.624, ave_loss: 0.513
[47]  [920/1724] loss: 0.423, ave_loss: 0.511
[48]  [940/1724] loss: 0.507, ave_loss: 0.511
[49]  [960/1724] loss: 0.500, ave_loss: 0.511
[50]  [980/1724] loss: 0.577, ave_loss: 0.512
[51]  [1000/1724] loss: 0.569, ave_loss: 0.513
[52]  [1020/1724] loss: 0.733, ave_loss: 0.517
[53]  [1040/1724] loss: 0.428, ave_loss: 0.516
[54]  [1060/1724] loss: 0.602, ave_loss: 0.517
[55]  [1080/1724] loss: 0.702, ave_loss: 0.520
[56]  [1100/1724] loss: 0.518, ave_loss: 0.520
[57]  [1120/1724] loss: 0.589, ave_loss: 0.522
[58]  [1140/1724] loss: 0.585, ave_loss: 0.523
[59]  [1160/1724] loss: 0.650, ave_loss: 0.525
[60]  [1180/1724] loss: 0.505, ave_loss: 0.525
[61]  [1200/1724] loss: 0.363, ave_loss: 0.522
[62]  [1220/1724] loss: 0.549, ave_loss: 0.522
[63]  [1240/1724] loss: 0.504, ave_loss: 0.522
[64]  [1260/1724] loss: 0.584, ave_loss: 0.523
[65]  [1280/1724] loss: 0.474, ave_loss: 0.522
[66]  [1300/1724] loss: 0.564, ave_loss: 0.523
[67]  [1320/1724] loss: 0.607, ave_loss: 0.524
[68]  [1340/1724] loss: 0.454, ave_loss: 0.523
[69]  [1360/1724] loss: 0.529, ave_loss: 0.523
[70]  [1380/1724] loss: 0.432, ave_loss: 0.522
[71]  [1400/1724] loss: 0.557, ave_loss: 0.522
[72]  [1420/1724] loss: 0.573, ave_loss: 0.523
[73]  [1440/1724] loss: 0.475, ave_loss: 0.522
[74]  [1460/1724] loss: 0.477, ave_loss: 0.522
[75]  [1480/1724] loss: 0.436, ave_loss: 0.521
[76]  [1500/1724] loss: 0.509, ave_loss: 0.521
[77]  [1520/1724] loss: 0.551, ave_loss: 0.521
[78]  [1540/1724] loss: 0.623, ave_loss: 0.522
[79]  [1560/1724] loss: 0.516, ave_loss: 0.522
[80]  [1580/1724] loss: 0.542, ave_loss: 0.522
[81]  [1600/1724] loss: 0.537, ave_loss: 0.523
[82]  [1620/1724] loss: 0.450, ave_loss: 0.522
[83]  [1640/1724] loss: 0.571, ave_loss: 0.522
[84]  [1660/1724] loss: 0.587, ave_loss: 0.523
[85]  [1680/1724] loss: 0.524, ave_loss: 0.523
[86]  [1700/1724] loss: 0.448, ave_loss: 0.522
[87]  [1720/1724] loss: 0.495, ave_loss: 0.522
[88]  [1740/1724] loss: 0.523, ave_loss: 0.522

Finished Training finishing at 2021-08-30 18:56:33.922172
printing_out epoch  21.438515081206496 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.219e-01
Validation Loss: 2.111e+05
Validation ROC: 0.6468
No improvement, still saving model
7.561484918793504 epochs left to go

Training Epoch 21.438515081206496/30 starting at 2021-08-30 18:58:18.051807
[1]  [0/1724] loss: 0.450, ave_loss: 0.450
[2]  [20/1724] loss: 0.535, ave_loss: 0.493
[3]  [40/1724] loss: 0.409, ave_loss: 0.465
[4]  [60/1724] loss: 0.567, ave_loss: 0.490
[5]  [80/1724] loss: 0.564, ave_loss: 0.505
[6]  [100/1724] loss: 0.622, ave_loss: 0.524
[7]  [120/1724] loss: 0.307, ave_loss: 0.493
[8]  [140/1724] loss: 0.464, ave_loss: 0.490
[9]  [160/1724] loss: 0.571, ave_loss: 0.499
[10]  [180/1724] loss: 0.586, ave_loss: 0.507
[11]  [200/1724] loss: 0.510, ave_loss: 0.508
[12]  [220/1724] loss: 0.485, ave_loss: 0.506
[13]  [240/1724] loss: 0.523, ave_loss: 0.507
[14]  [260/1724] loss: 0.345, ave_loss: 0.496
[15]  [280/1724] loss: 0.306, ave_loss: 0.483
[16]  [300/1724] loss: 0.443, ave_loss: 0.480
[17]  [320/1724] loss: 0.449, ave_loss: 0.478
[18]  [340/1724] loss: 0.468, ave_loss: 0.478
[19]  [360/1724] loss: 0.460, ave_loss: 0.477
[20]  [380/1724] loss: 0.666, ave_loss: 0.486
[21]  [400/1724] loss: 0.516, ave_loss: 0.488
[22]  [420/1724] loss: 0.534, ave_loss: 0.490
[23]  [440/1724] loss: 0.553, ave_loss: 0.493
[24]  [460/1724] loss: 0.540, ave_loss: 0.495
[25]  [480/1724] loss: 0.463, ave_loss: 0.493
[26]  [500/1724] loss: 0.492, ave_loss: 0.493
[27]  [520/1724] loss: 0.472, ave_loss: 0.493
[28]  [540/1724] loss: 0.464, ave_loss: 0.492
[29]  [560/1724] loss: 0.616, ave_loss: 0.496
[30]  [580/1724] loss: 0.494, ave_loss: 0.496
[31]  [600/1724] loss: 0.602, ave_loss: 0.499
[32]  [620/1724] loss: 0.438, ave_loss: 0.497
[33]  [640/1724] loss: 0.568, ave_loss: 0.499
[34]  [660/1724] loss: 0.522, ave_loss: 0.500
[35]  [680/1724] loss: 0.442, ave_loss: 0.498
[36]  [700/1724] loss: 0.543, ave_loss: 0.500
[37]  [720/1724] loss: 0.535, ave_loss: 0.501
[38]  [740/1724] loss: 0.458, ave_loss: 0.500
[39]  [760/1724] loss: 0.485, ave_loss: 0.499
[40]  [780/1724] loss: 0.379, ave_loss: 0.496
[41]  [800/1724] loss: 0.494, ave_loss: 0.496
[42]  [820/1724] loss: 0.436, ave_loss: 0.495
[43]  [840/1724] loss: 0.561, ave_loss: 0.496
[44]  [860/1724] loss: 0.486, ave_loss: 0.496
[45]  [880/1724] loss: 0.598, ave_loss: 0.498
[46]  [900/1724] loss: 0.563, ave_loss: 0.500
[47]  [920/1724] loss: 0.625, ave_loss: 0.502
[48]  [940/1724] loss: 0.345, ave_loss: 0.499
[49]  [960/1724] loss: 0.403, ave_loss: 0.497
[50]  [980/1724] loss: 0.485, ave_loss: 0.497
[51]  [1000/1724] loss: 0.736, ave_loss: 0.502
[52]  [1020/1724] loss: 0.463, ave_loss: 0.501
[53]  [1040/1724] loss: 0.615, ave_loss: 0.503
[54]  [1060/1724] loss: 0.371, ave_loss: 0.501
[55]  [1080/1724] loss: 0.550, ave_loss: 0.501
[56]  [1100/1724] loss: 0.445, ave_loss: 0.500
[57]  [1120/1724] loss: 0.424, ave_loss: 0.499
[58]  [1140/1724] loss: 0.490, ave_loss: 0.499
[59]  [1160/1724] loss: 0.422, ave_loss: 0.498
[60]  [1180/1724] loss: 0.376, ave_loss: 0.496
[61]  [1200/1724] loss: 0.451, ave_loss: 0.495
[62]  [1220/1724] loss: 0.667, ave_loss: 0.498
[63]  [1240/1724] loss: 0.632, ave_loss: 0.500
[64]  [1260/1724] loss: 0.421, ave_loss: 0.499
[65]  [1280/1724] loss: 0.450, ave_loss: 0.498
[66]  [1300/1724] loss: 0.568, ave_loss: 0.499
[67]  [1320/1724] loss: 0.578, ave_loss: 0.500
[68]  [1340/1724] loss: 0.671, ave_loss: 0.503
[69]  [1360/1724] loss: 0.475, ave_loss: 0.502
[70]  [1380/1724] loss: 0.458, ave_loss: 0.502
[71]  [1400/1724] loss: 0.425, ave_loss: 0.500
[72]  [1420/1724] loss: 0.426, ave_loss: 0.499
[73]  [1440/1724] loss: 0.463, ave_loss: 0.499
[74]  [1460/1724] loss: 0.360, ave_loss: 0.497
[75]  [1480/1724] loss: 0.502, ave_loss: 0.497
[76]  [1500/1724] loss: 0.469, ave_loss: 0.497
[77]  [1520/1724] loss: 0.501, ave_loss: 0.497
[78]  [1540/1724] loss: 0.452, ave_loss: 0.496
[79]  [1560/1724] loss: 0.421, ave_loss: 0.495
[80]  [1580/1724] loss: 0.558, ave_loss: 0.496
[81]  [1600/1724] loss: 0.363, ave_loss: 0.494
[82]  [1620/1724] loss: 0.598, ave_loss: 0.496
[83]  [1640/1724] loss: 0.453, ave_loss: 0.495
[84]  [1660/1724] loss: 0.531, ave_loss: 0.496
[85]  [1680/1724] loss: 0.442, ave_loss: 0.495
[86]  [1700/1724] loss: 0.394, ave_loss: 0.494
[87]  [1720/1724] loss: 0.438, ave_loss: 0.493
[88]  [1740/1724] loss: 0.409, ave_loss: 0.492

Finished Training finishing at 2021-08-30 19:00:38.997610
printing_out epoch  22.45939675174014 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.922e-01
Validation Loss: 2.140e+05
Validation ROC: 0.6387
No improvement, still saving model
6.54060324825986 epochs left to go

Training Epoch 22.45939675174014/30 starting at 2021-08-30 19:01:19.992851
[1]  [0/1724] loss: 0.401, ave_loss: 0.401
[2]  [20/1724] loss: 0.551, ave_loss: 0.476
[3]  [40/1724] loss: 0.418, ave_loss: 0.457
[4]  [60/1724] loss: 0.478, ave_loss: 0.462
[5]  [80/1724] loss: 0.638, ave_loss: 0.497
[6]  [100/1724] loss: 0.524, ave_loss: 0.502
[7]  [120/1724] loss: 0.525, ave_loss: 0.505
[8]  [140/1724] loss: 0.552, ave_loss: 0.511
[9]  [160/1724] loss: 0.353, ave_loss: 0.493
[10]  [180/1724] loss: 0.414, ave_loss: 0.485
[11]  [200/1724] loss: 0.579, ave_loss: 0.494
[12]  [220/1724] loss: 0.544, ave_loss: 0.498
[13]  [240/1724] loss: 0.544, ave_loss: 0.502
[14]  [260/1724] loss: 0.475, ave_loss: 0.500
[15]  [280/1724] loss: 0.442, ave_loss: 0.496
[16]  [300/1724] loss: 0.677, ave_loss: 0.507
[17]  [320/1724] loss: 0.478, ave_loss: 0.505
[18]  [340/1724] loss: 0.696, ave_loss: 0.516
[19]  [360/1724] loss: 0.494, ave_loss: 0.515
[20]  [380/1724] loss: 0.548, ave_loss: 0.516
[21]  [400/1724] loss: 0.515, ave_loss: 0.516
[22]  [420/1724] loss: 0.415, ave_loss: 0.512
[23]  [440/1724] loss: 0.512, ave_loss: 0.512
[24]  [460/1724] loss: 0.457, ave_loss: 0.510
[25]  [480/1724] loss: 0.413, ave_loss: 0.506
[26]  [500/1724] loss: 0.599, ave_loss: 0.509
[27]  [520/1724] loss: 0.469, ave_loss: 0.508
[28]  [540/1724] loss: 0.586, ave_loss: 0.511
[29]  [560/1724] loss: 0.562, ave_loss: 0.512
[30]  [580/1724] loss: 0.493, ave_loss: 0.512
[31]  [600/1724] loss: 0.628, ave_loss: 0.515
[32]  [620/1724] loss: 0.549, ave_loss: 0.517
[33]  [640/1724] loss: 0.518, ave_loss: 0.517
[34]  [660/1724] loss: 0.588, ave_loss: 0.519
[35]  [680/1724] loss: 0.503, ave_loss: 0.518
[36]  [700/1724] loss: 0.375, ave_loss: 0.514
[37]  [720/1724] loss: 0.451, ave_loss: 0.513
[38]  [740/1724] loss: 0.536, ave_loss: 0.513
[39]  [760/1724] loss: 0.677, ave_loss: 0.517
[40]  [780/1724] loss: 0.441, ave_loss: 0.515
[41]  [800/1724] loss: 0.651, ave_loss: 0.519
[42]  [820/1724] loss: 0.562, ave_loss: 0.520
[43]  [840/1724] loss: 0.434, ave_loss: 0.518
[44]  [860/1724] loss: 0.443, ave_loss: 0.516
[45]  [880/1724] loss: 0.456, ave_loss: 0.515
[46]  [900/1724] loss: 0.542, ave_loss: 0.515
[47]  [920/1724] loss: 0.325, ave_loss: 0.511
[48]  [940/1724] loss: 0.494, ave_loss: 0.511
[49]  [960/1724] loss: 0.493, ave_loss: 0.511
[50]  [980/1724] loss: 0.609, ave_loss: 0.513
[51]  [1000/1724] loss: 0.390, ave_loss: 0.510
[52]  [1020/1724] loss: 0.508, ave_loss: 0.510
[53]  [1040/1724] loss: 0.482, ave_loss: 0.510
[54]  [1060/1724] loss: 0.553, ave_loss: 0.510
[55]  [1080/1724] loss: 0.499, ave_loss: 0.510
[56]  [1100/1724] loss: 0.405, ave_loss: 0.508
[57]  [1120/1724] loss: 0.601, ave_loss: 0.510
[58]  [1140/1724] loss: 0.564, ave_loss: 0.511
[59]  [1160/1724] loss: 0.403, ave_loss: 0.509
[60]  [1180/1724] loss: 0.600, ave_loss: 0.511
[61]  [1200/1724] loss: 0.445, ave_loss: 0.509
[62]  [1220/1724] loss: 0.332, ave_loss: 0.507
[63]  [1240/1724] loss: 0.493, ave_loss: 0.506
[64]  [1260/1724] loss: 0.410, ave_loss: 0.505
[65]  [1280/1724] loss: 0.574, ave_loss: 0.506
[66]  [1300/1724] loss: 0.467, ave_loss: 0.505
[67]  [1320/1724] loss: 0.607, ave_loss: 0.507
[68]  [1340/1724] loss: 0.494, ave_loss: 0.507
[69]  [1360/1724] loss: 0.523, ave_loss: 0.507
[70]  [1380/1724] loss: 0.436, ave_loss: 0.506
[71]  [1400/1724] loss: 0.583, ave_loss: 0.507
[72]  [1420/1724] loss: 0.498, ave_loss: 0.507
[73]  [1440/1724] loss: 0.512, ave_loss: 0.507
[74]  [1460/1724] loss: 0.547, ave_loss: 0.507
[75]  [1480/1724] loss: 0.534, ave_loss: 0.508
[76]  [1500/1724] loss: 0.424, ave_loss: 0.507
[77]  [1520/1724] loss: 0.525, ave_loss: 0.507
[78]  [1540/1724] loss: 0.405, ave_loss: 0.506
[79]  [1560/1724] loss: 0.473, ave_loss: 0.505
[80]  [1580/1724] loss: 0.437, ave_loss: 0.504
[81]  [1600/1724] loss: 0.309, ave_loss: 0.502
[82]  [1620/1724] loss: 0.356, ave_loss: 0.500
[83]  [1640/1724] loss: 0.465, ave_loss: 0.500
[84]  [1660/1724] loss: 0.476, ave_loss: 0.500
[85]  [1680/1724] loss: 0.487, ave_loss: 0.499
[86]  [1700/1724] loss: 0.494, ave_loss: 0.499
[87]  [1720/1724] loss: 0.581, ave_loss: 0.500
[88]  [1740/1724] loss: 0.515, ave_loss: 0.500

Finished Training finishing at 2021-08-30 19:03:43.786317
printing_out epoch  23.48027842227378 learning rate: 0.0003199956068772566
0.0003103957386709389
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.004e-01
Validation Loss: 2.202e+05
Validation ROC: 0.6408
No improvement, still saving model
5.519721577726219 epochs left to go

Training Epoch 23.48027842227378/30 starting at 2021-08-30 19:04:23.178317
[1]  [0/1724] loss: 0.678, ave_loss: 0.678
[2]  [20/1724] loss: 0.637, ave_loss: 0.658
[3]  [40/1724] loss: 0.489, ave_loss: 0.602
[4]  [60/1724] loss: 0.498, ave_loss: 0.576
[5]  [80/1724] loss: 0.517, ave_loss: 0.564
[6]  [100/1724] loss: 0.421, ave_loss: 0.540
[7]  [120/1724] loss: 0.357, ave_loss: 0.514
[8]  [140/1724] loss: 0.589, ave_loss: 0.523
[9]  [160/1724] loss: 0.435, ave_loss: 0.514
[10]  [180/1724] loss: 0.292, ave_loss: 0.491
[11]  [200/1724] loss: 0.542, ave_loss: 0.496
[12]  [220/1724] loss: 0.387, ave_loss: 0.487
[13]  [240/1724] loss: 0.528, ave_loss: 0.490
[14]  [260/1724] loss: 0.461, ave_loss: 0.488
[15]  [280/1724] loss: 0.586, ave_loss: 0.495
[16]  [300/1724] loss: 0.348, ave_loss: 0.485
[17]  [320/1724] loss: 0.591, ave_loss: 0.492
[18]  [340/1724] loss: 0.553, ave_loss: 0.495
[19]  [360/1724] loss: 0.556, ave_loss: 0.498
[20]  [380/1724] loss: 0.403, ave_loss: 0.493
[21]  [400/1724] loss: 0.428, ave_loss: 0.490
[22]  [420/1724] loss: 0.659, ave_loss: 0.498
[23]  [440/1724] loss: 0.358, ave_loss: 0.492
[24]  [460/1724] loss: 0.703, ave_loss: 0.501
[25]  [480/1724] loss: 0.453, ave_loss: 0.499
[26]  [500/1724] loss: 0.505, ave_loss: 0.499
[27]  [520/1724] loss: 0.400, ave_loss: 0.495
[28]  [540/1724] loss: 0.685, ave_loss: 0.502
[29]  [560/1724] loss: 0.496, ave_loss: 0.502
[30]  [580/1724] loss: 0.495, ave_loss: 0.502
[31]  [600/1724] loss: 0.580, ave_loss: 0.504
[32]  [620/1724] loss: 0.431, ave_loss: 0.502
[33]  [640/1724] loss: 0.643, ave_loss: 0.506
[34]  [660/1724] loss: 0.402, ave_loss: 0.503
[35]  [680/1724] loss: 0.456, ave_loss: 0.502
[36]  [700/1724] loss: 0.330, ave_loss: 0.497
[37]  [720/1724] loss: 0.410, ave_loss: 0.495
[38]  [740/1724] loss: 0.507, ave_loss: 0.495
[39]  [760/1724] loss: 0.653, ave_loss: 0.499
[40]  [780/1724] loss: 0.404, ave_loss: 0.497
[41]  [800/1724] loss: 0.591, ave_loss: 0.499
[42]  [820/1724] loss: 0.454, ave_loss: 0.498
[43]  [840/1724] loss: 0.373, ave_loss: 0.495
[44]  [860/1724] loss: 0.392, ave_loss: 0.493
[45]  [880/1724] loss: 0.622, ave_loss: 0.496
[46]  [900/1724] loss: 0.557, ave_loss: 0.497
[47]  [920/1724] loss: 0.392, ave_loss: 0.495
[48]  [940/1724] loss: 0.468, ave_loss: 0.494
[49]  [960/1724] loss: 0.621, ave_loss: 0.497
[50]  [980/1724] loss: 0.402, ave_loss: 0.495
[51]  [1000/1724] loss: 0.507, ave_loss: 0.495
[52]  [1020/1724] loss: 0.417, ave_loss: 0.494
[53]  [1040/1724] loss: 0.466, ave_loss: 0.493
[54]  [1060/1724] loss: 0.491, ave_loss: 0.493
[55]  [1080/1724] loss: 0.427, ave_loss: 0.492
[56]  [1100/1724] loss: 0.592, ave_loss: 0.494
[57]  [1120/1724] loss: 0.500, ave_loss: 0.494
[58]  [1140/1724] loss: 0.447, ave_loss: 0.493
[59]  [1160/1724] loss: 0.419, ave_loss: 0.492
[60]  [1180/1724] loss: 0.429, ave_loss: 0.491
[61]  [1200/1724] loss: 0.612, ave_loss: 0.493
[62]  [1220/1724] loss: 0.680, ave_loss: 0.496
[63]  [1240/1724] loss: 0.472, ave_loss: 0.495
[64]  [1260/1724] loss: 0.447, ave_loss: 0.494
[65]  [1280/1724] loss: 0.441, ave_loss: 0.494
[66]  [1300/1724] loss: 0.472, ave_loss: 0.493
[67]  [1320/1724] loss: 0.583, ave_loss: 0.495
[68]  [1340/1724] loss: 0.452, ave_loss: 0.494
[69]  [1360/1724] loss: 0.402, ave_loss: 0.493
[70]  [1380/1724] loss: 0.523, ave_loss: 0.493
[71]  [1400/1724] loss: 0.329, ave_loss: 0.491
[72]  [1420/1724] loss: 0.415, ave_loss: 0.490
[73]  [1440/1724] loss: 0.485, ave_loss: 0.490
[74]  [1460/1724] loss: 0.432, ave_loss: 0.489
[75]  [1480/1724] loss: 0.457, ave_loss: 0.489
[76]  [1500/1724] loss: 0.370, ave_loss: 0.487
[77]  [1520/1724] loss: 0.582, ave_loss: 0.488
[78]  [1540/1724] loss: 0.402, ave_loss: 0.487
[79]  [1560/1724] loss: 0.466, ave_loss: 0.487
[80]  [1580/1724] loss: 0.489, ave_loss: 0.487
[81]  [1600/1724] loss: 0.410, ave_loss: 0.486
[82]  [1620/1724] loss: 0.531, ave_loss: 0.486
[83]  [1640/1724] loss: 0.482, ave_loss: 0.486
[84]  [1660/1724] loss: 0.424, ave_loss: 0.486
[85]  [1680/1724] loss: 0.452, ave_loss: 0.485
[86]  [1700/1724] loss: 0.364, ave_loss: 0.484
[87]  [1720/1724] loss: 0.474, ave_loss: 0.484
[88]  [1740/1724] loss: 0.610, ave_loss: 0.485

Finished Training finishing at 2021-08-30 19:06:44.586873
printing_out epoch  24.501160092807424 learning rate: 0.00029090509716114235
0.0002821779442463081
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.852e-01
Validation Loss: 2.162e+05
Validation ROC: 0.6367
No improvement, still saving model
4.4988399071925755 epochs left to go

Training Epoch 24.501160092807424/30 starting at 2021-08-30 19:07:35.669304
[1]  [0/1724] loss: 0.502, ave_loss: 0.502
[2]  [20/1724] loss: 0.533, ave_loss: 0.518
[3]  [40/1724] loss: 0.602, ave_loss: 0.546
[4]  [60/1724] loss: 0.410, ave_loss: 0.512
[5]  [80/1724] loss: 0.519, ave_loss: 0.513
[6]  [100/1724] loss: 0.387, ave_loss: 0.492
[7]  [120/1724] loss: 0.386, ave_loss: 0.477
[8]  [140/1724] loss: 0.791, ave_loss: 0.516
[9]  [160/1724] loss: 0.446, ave_loss: 0.509
[10]  [180/1724] loss: 0.467, ave_loss: 0.504
[11]  [200/1724] loss: 0.650, ave_loss: 0.518
[12]  [220/1724] loss: 0.395, ave_loss: 0.507
[13]  [240/1724] loss: 0.573, ave_loss: 0.513
[14]  [260/1724] loss: 0.470, ave_loss: 0.510
[15]  [280/1724] loss: 0.436, ave_loss: 0.505
[16]  [300/1724] loss: 0.462, ave_loss: 0.502
[17]  [320/1724] loss: 0.379, ave_loss: 0.495
[18]  [340/1724] loss: 0.455, ave_loss: 0.493
[19]  [360/1724] loss: 0.572, ave_loss: 0.497
[20]  [380/1724] loss: 0.615, ave_loss: 0.503
[21]  [400/1724] loss: 0.359, ave_loss: 0.496
[22]  [420/1724] loss: 0.429, ave_loss: 0.493
[23]  [440/1724] loss: 0.578, ave_loss: 0.496
[24]  [460/1724] loss: 0.507, ave_loss: 0.497
[25]  [480/1724] loss: 0.390, ave_loss: 0.493
[26]  [500/1724] loss: 0.354, ave_loss: 0.487
[27]  [520/1724] loss: 0.620, ave_loss: 0.492
[28]  [540/1724] loss: 0.575, ave_loss: 0.495
[29]  [560/1724] loss: 0.472, ave_loss: 0.494
[30]  [580/1724] loss: 0.462, ave_loss: 0.493
[31]  [600/1724] loss: 0.547, ave_loss: 0.495
[32]  [620/1724] loss: 0.632, ave_loss: 0.499
[33]  [640/1724] loss: 0.463, ave_loss: 0.498
[34]  [660/1724] loss: 0.398, ave_loss: 0.495
[35]  [680/1724] loss: 0.439, ave_loss: 0.494
[36]  [700/1724] loss: 0.491, ave_loss: 0.494
[37]  [720/1724] loss: 0.598, ave_loss: 0.496
[38]  [740/1724] loss: 0.483, ave_loss: 0.496
[39]  [760/1724] loss: 0.497, ave_loss: 0.496
[40]  [780/1724] loss: 0.551, ave_loss: 0.497
[41]  [800/1724] loss: 0.342, ave_loss: 0.494
[42]  [820/1724] loss: 0.574, ave_loss: 0.496
[43]  [840/1724] loss: 0.459, ave_loss: 0.495
[44]  [860/1724] loss: 0.427, ave_loss: 0.493
[45]  [880/1724] loss: 0.547, ave_loss: 0.494
[46]  [900/1724] loss: 0.382, ave_loss: 0.492
[47]  [920/1724] loss: 0.378, ave_loss: 0.489
[48]  [940/1724] loss: 0.711, ave_loss: 0.494
[49]  [960/1724] loss: 0.481, ave_loss: 0.494
[50]  [980/1724] loss: 0.492, ave_loss: 0.494
[51]  [1000/1724] loss: 0.453, ave_loss: 0.493
[52]  [1020/1724] loss: 0.557, ave_loss: 0.494
[53]  [1040/1724] loss: 0.628, ave_loss: 0.497
[54]  [1060/1724] loss: 0.498, ave_loss: 0.497
[55]  [1080/1724] loss: 0.424, ave_loss: 0.495
[56]  [1100/1724] loss: 0.387, ave_loss: 0.493
[57]  [1120/1724] loss: 0.460, ave_loss: 0.493
[58]  [1140/1724] loss: 0.577, ave_loss: 0.494
[59]  [1160/1724] loss: 0.611, ave_loss: 0.496
[60]  [1180/1724] loss: 0.371, ave_loss: 0.494
[61]  [1200/1724] loss: 0.411, ave_loss: 0.493
[62]  [1220/1724] loss: 0.360, ave_loss: 0.491
[63]  [1240/1724] loss: 0.607, ave_loss: 0.493
[64]  [1260/1724] loss: 0.443, ave_loss: 0.492
[65]  [1280/1724] loss: 0.490, ave_loss: 0.492
[66]  [1300/1724] loss: 0.535, ave_loss: 0.492
[67]  [1320/1724] loss: 0.642, ave_loss: 0.495
[68]  [1340/1724] loss: 0.526, ave_loss: 0.495
[69]  [1360/1724] loss: 0.556, ave_loss: 0.496
[70]  [1380/1724] loss: 0.497, ave_loss: 0.496
[71]  [1400/1724] loss: 0.448, ave_loss: 0.495
[72]  [1420/1724] loss: 0.522, ave_loss: 0.496
[73]  [1440/1724] loss: 0.348, ave_loss: 0.494
[74]  [1460/1724] loss: 0.534, ave_loss: 0.494
[75]  [1480/1724] loss: 0.426, ave_loss: 0.493
[76]  [1500/1724] loss: 0.558, ave_loss: 0.494
[77]  [1520/1724] loss: 0.483, ave_loss: 0.494
[78]  [1540/1724] loss: 0.493, ave_loss: 0.494
[79]  [1560/1724] loss: 0.482, ave_loss: 0.494
[80]  [1580/1724] loss: 0.614, ave_loss: 0.495
[81]  [1600/1724] loss: 0.454, ave_loss: 0.495
[82]  [1620/1724] loss: 0.579, ave_loss: 0.496
[83]  [1640/1724] loss: 0.589, ave_loss: 0.497
[84]  [1660/1724] loss: 0.501, ave_loss: 0.497
[85]  [1680/1724] loss: 0.512, ave_loss: 0.497
[86]  [1700/1724] loss: 0.418, ave_loss: 0.496
[87]  [1720/1724] loss: 0.419, ave_loss: 0.495
[88]  [1740/1724] loss: 0.520, ave_loss: 0.496

Finished Training finishing at 2021-08-30 19:10:03.220051
printing_out epoch  25.52204176334107 learning rate: 0.00026445917923740214
0.0002565254038602801
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.957e-01
Validation Loss: 2.140e+05
Validation ROC: 0.6315
No improvement, still saving model
3.4779582366589317 epochs left to go

Training Epoch 25.52204176334107/30 starting at 2021-08-30 19:10:56.913292
[1]  [0/1724] loss: 0.528, ave_loss: 0.528
[2]  [20/1724] loss: 0.605, ave_loss: 0.566
[3]  [40/1724] loss: 0.717, ave_loss: 0.617
[4]  [60/1724] loss: 0.501, ave_loss: 0.588
[5]  [80/1724] loss: 0.530, ave_loss: 0.576
[6]  [100/1724] loss: 0.383, ave_loss: 0.544
[7]  [120/1724] loss: 0.610, ave_loss: 0.553
[8]  [140/1724] loss: 0.538, ave_loss: 0.552
[9]  [160/1724] loss: 0.564, ave_loss: 0.553
[10]  [180/1724] loss: 0.637, ave_loss: 0.561
[11]  [200/1724] loss: 0.511, ave_loss: 0.557
[12]  [220/1724] loss: 0.532, ave_loss: 0.555
[13]  [240/1724] loss: 0.338, ave_loss: 0.538
[14]  [260/1724] loss: 0.629, ave_loss: 0.544
[15]  [280/1724] loss: 0.564, ave_loss: 0.546
[16]  [300/1724] loss: 0.655, ave_loss: 0.553
[17]  [320/1724] loss: 0.485, ave_loss: 0.549
[18]  [340/1724] loss: 0.447, ave_loss: 0.543
[19]  [360/1724] loss: 0.449, ave_loss: 0.538
[20]  [380/1724] loss: 0.557, ave_loss: 0.539
[21]  [400/1724] loss: 0.487, ave_loss: 0.536
[22]  [420/1724] loss: 0.581, ave_loss: 0.538
[23]  [440/1724] loss: 0.553, ave_loss: 0.539
[24]  [460/1724] loss: 0.358, ave_loss: 0.532
[25]  [480/1724] loss: 0.539, ave_loss: 0.532
[26]  [500/1724] loss: 0.392, ave_loss: 0.526
[27]  [520/1724] loss: 0.723, ave_loss: 0.534
[28]  [540/1724] loss: 0.504, ave_loss: 0.533
[29]  [560/1724] loss: 0.511, ave_loss: 0.532
[30]  [580/1724] loss: 0.488, ave_loss: 0.531
[31]  [600/1724] loss: 0.480, ave_loss: 0.529
[32]  [620/1724] loss: 0.525, ave_loss: 0.529
[33]  [640/1724] loss: 0.521, ave_loss: 0.529
[34]  [660/1724] loss: 0.446, ave_loss: 0.526
[35]  [680/1724] loss: 0.448, ave_loss: 0.524
[36]  [700/1724] loss: 0.417, ave_loss: 0.521
[37]  [720/1724] loss: 0.580, ave_loss: 0.522
[38]  [740/1724] loss: 0.439, ave_loss: 0.520
[39]  [760/1724] loss: 0.436, ave_loss: 0.518
[40]  [780/1724] loss: 0.388, ave_loss: 0.515
[41]  [800/1724] loss: 0.641, ave_loss: 0.518
[42]  [820/1724] loss: 0.528, ave_loss: 0.518
[43]  [840/1724] loss: 0.504, ave_loss: 0.518
[44]  [860/1724] loss: 0.583, ave_loss: 0.519
[45]  [880/1724] loss: 0.577, ave_loss: 0.521
[46]  [900/1724] loss: 0.357, ave_loss: 0.517
[47]  [920/1724] loss: 0.412, ave_loss: 0.515
[48]  [940/1724] loss: 0.497, ave_loss: 0.514
[49]  [960/1724] loss: 0.484, ave_loss: 0.514
[50]  [980/1724] loss: 0.355, ave_loss: 0.511
[51]  [1000/1724] loss: 0.520, ave_loss: 0.511
[52]  [1020/1724] loss: 0.546, ave_loss: 0.512
[53]  [1040/1724] loss: 0.416, ave_loss: 0.510
[54]  [1060/1724] loss: 0.410, ave_loss: 0.508
[55]  [1080/1724] loss: 0.466, ave_loss: 0.507
[56]  [1100/1724] loss: 0.528, ave_loss: 0.508
[57]  [1120/1724] loss: 0.444, ave_loss: 0.506
[58]  [1140/1724] loss: 0.643, ave_loss: 0.509
[59]  [1160/1724] loss: 0.481, ave_loss: 0.508
[60]  [1180/1724] loss: 0.535, ave_loss: 0.509
[61]  [1200/1724] loss: 0.568, ave_loss: 0.510
[62]  [1220/1724] loss: 0.584, ave_loss: 0.511
[63]  [1240/1724] loss: 0.463, ave_loss: 0.510
[64]  [1260/1724] loss: 0.591, ave_loss: 0.511
[65]  [1280/1724] loss: 0.573, ave_loss: 0.512
[66]  [1300/1724] loss: 0.378, ave_loss: 0.510
[67]  [1320/1724] loss: 0.416, ave_loss: 0.509
[68]  [1340/1724] loss: 0.580, ave_loss: 0.510
[69]  [1360/1724] loss: 0.548, ave_loss: 0.511
[70]  [1380/1724] loss: 0.405, ave_loss: 0.509
[71]  [1400/1724] loss: 0.526, ave_loss: 0.509
[72]  [1420/1724] loss: 0.646, ave_loss: 0.511
[73]  [1440/1724] loss: 0.598, ave_loss: 0.512
[74]  [1460/1724] loss: 0.386, ave_loss: 0.511
[75]  [1480/1724] loss: 0.438, ave_loss: 0.510
[76]  [1500/1724] loss: 0.293, ave_loss: 0.507
[77]  [1520/1724] loss: 0.470, ave_loss: 0.506
[78]  [1540/1724] loss: 0.704, ave_loss: 0.509
[79]  [1560/1724] loss: 0.502, ave_loss: 0.509
[80]  [1580/1724] loss: 0.431, ave_loss: 0.508
[81]  [1600/1724] loss: 0.514, ave_loss: 0.508
[82]  [1620/1724] loss: 0.533, ave_loss: 0.508
[83]  [1640/1724] loss: 0.614, ave_loss: 0.509
[84]  [1660/1724] loss: 0.617, ave_loss: 0.511
[85]  [1680/1724] loss: 0.554, ave_loss: 0.511
[86]  [1700/1724] loss: 0.682, ave_loss: 0.513
[87]  [1720/1724] loss: 0.482, ave_loss: 0.513
[88]  [1740/1724] loss: 0.512, ave_loss: 0.513

Finished Training finishing at 2021-08-30 19:13:15.259800
printing_out epoch  26.54292343387471 learning rate: 0.00024041743567036557
0.0002332049126002546
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.129e-01
Validation Loss: 2.131e+05
Validation ROC: 0.6332
No improvement, still saving model
2.4570765661252914 epochs left to go

Training Epoch 26.54292343387471/30 starting at 2021-08-30 19:14:55.464238
[1]  [0/1724] loss: 0.703, ave_loss: 0.703
[2]  [20/1724] loss: 0.556, ave_loss: 0.629
[3]  [40/1724] loss: 0.578, ave_loss: 0.612
[4]  [60/1724] loss: 0.284, ave_loss: 0.530
[5]  [80/1724] loss: 0.441, ave_loss: 0.512
[6]  [100/1724] loss: 0.555, ave_loss: 0.519
[7]  [120/1724] loss: 0.381, ave_loss: 0.500
[8]  [140/1724] loss: 0.542, ave_loss: 0.505
[9]  [160/1724] loss: 0.581, ave_loss: 0.513
[10]  [180/1724] loss: 0.456, ave_loss: 0.508
[11]  [200/1724] loss: 0.457, ave_loss: 0.503
[12]  [220/1724] loss: 0.393, ave_loss: 0.494
[13]  [240/1724] loss: 0.459, ave_loss: 0.491
[14]  [260/1724] loss: 0.591, ave_loss: 0.498
[15]  [280/1724] loss: 0.434, ave_loss: 0.494
[16]  [300/1724] loss: 0.532, ave_loss: 0.496
[17]  [320/1724] loss: 0.497, ave_loss: 0.496
[18]  [340/1724] loss: 0.445, ave_loss: 0.494
[19]  [360/1724] loss: 0.443, ave_loss: 0.491
[20]  [380/1724] loss: 0.485, ave_loss: 0.491
[21]  [400/1724] loss: 0.547, ave_loss: 0.493
[22]  [420/1724] loss: 0.470, ave_loss: 0.492
[23]  [440/1724] loss: 0.800, ave_loss: 0.506
[24]  [460/1724] loss: 0.372, ave_loss: 0.500
[25]  [480/1724] loss: 0.477, ave_loss: 0.499
[26]  [500/1724] loss: 0.530, ave_loss: 0.500
[27]  [520/1724] loss: 0.304, ave_loss: 0.493
[28]  [540/1724] loss: 0.434, ave_loss: 0.491
[29]  [560/1724] loss: 0.638, ave_loss: 0.496
[30]  [580/1724] loss: 0.375, ave_loss: 0.492
[31]  [600/1724] loss: 0.331, ave_loss: 0.487
[32]  [620/1724] loss: 0.345, ave_loss: 0.482
[33]  [640/1724] loss: 0.569, ave_loss: 0.485
[34]  [660/1724] loss: 0.599, ave_loss: 0.488
[35]  [680/1724] loss: 0.504, ave_loss: 0.489
[36]  [700/1724] loss: 0.517, ave_loss: 0.490
[37]  [720/1724] loss: 0.479, ave_loss: 0.489
[38]  [740/1724] loss: 0.485, ave_loss: 0.489
[39]  [760/1724] loss: 0.557, ave_loss: 0.491
[40]  [780/1724] loss: 0.412, ave_loss: 0.489
[41]  [800/1724] loss: 0.569, ave_loss: 0.491
[42]  [820/1724] loss: 0.421, ave_loss: 0.489
[43]  [840/1724] loss: 0.359, ave_loss: 0.486
[44]  [860/1724] loss: 0.552, ave_loss: 0.488
[45]  [880/1724] loss: 0.427, ave_loss: 0.486
[46]  [900/1724] loss: 0.501, ave_loss: 0.487
[47]  [920/1724] loss: 0.582, ave_loss: 0.489
[48]  [940/1724] loss: 0.391, ave_loss: 0.487
[49]  [960/1724] loss: 0.460, ave_loss: 0.486
[50]  [980/1724] loss: 0.426, ave_loss: 0.485
[51]  [1000/1724] loss: 0.538, ave_loss: 0.486
[52]  [1020/1724] loss: 0.461, ave_loss: 0.485
[53]  [1040/1724] loss: 0.483, ave_loss: 0.485
[54]  [1060/1724] loss: 0.314, ave_loss: 0.482
[55]  [1080/1724] loss: 0.548, ave_loss: 0.483
[56]  [1100/1724] loss: 0.642, ave_loss: 0.486
[57]  [1120/1724] loss: 0.496, ave_loss: 0.486
[58]  [1140/1724] loss: 0.436, ave_loss: 0.486
[59]  [1160/1724] loss: 0.489, ave_loss: 0.486
[60]  [1180/1724] loss: 0.459, ave_loss: 0.485
[61]  [1200/1724] loss: 0.605, ave_loss: 0.487
[62]  [1220/1724] loss: 0.492, ave_loss: 0.487
[63]  [1240/1724] loss: 0.398, ave_loss: 0.486
[64]  [1260/1724] loss: 0.595, ave_loss: 0.487
[65]  [1280/1724] loss: 0.486, ave_loss: 0.487
[66]  [1300/1724] loss: 0.571, ave_loss: 0.489
[67]  [1320/1724] loss: 0.458, ave_loss: 0.488
[68]  [1340/1724] loss: 0.523, ave_loss: 0.489
[69]  [1360/1724] loss: 0.448, ave_loss: 0.488
[70]  [1380/1724] loss: 0.534, ave_loss: 0.489
[71]  [1400/1724] loss: 0.495, ave_loss: 0.489
[72]  [1420/1724] loss: 0.502, ave_loss: 0.489
[73]  [1440/1724] loss: 0.463, ave_loss: 0.489
[74]  [1460/1724] loss: 0.499, ave_loss: 0.489
[75]  [1480/1724] loss: 0.543, ave_loss: 0.490
[76]  [1500/1724] loss: 0.538, ave_loss: 0.490
[77]  [1520/1724] loss: 0.545, ave_loss: 0.491
[78]  [1540/1724] loss: 0.586, ave_loss: 0.492
[79]  [1560/1724] loss: 0.465, ave_loss: 0.492
[80]  [1580/1724] loss: 0.511, ave_loss: 0.492
[81]  [1600/1724] loss: 0.370, ave_loss: 0.491
[82]  [1620/1724] loss: 0.449, ave_loss: 0.490
[83]  [1640/1724] loss: 0.458, ave_loss: 0.490
[84]  [1660/1724] loss: 0.460, ave_loss: 0.489
[85]  [1680/1724] loss: 0.479, ave_loss: 0.489
[86]  [1700/1724] loss: 0.440, ave_loss: 0.489
[87]  [1720/1724] loss: 0.638, ave_loss: 0.490
[88]  [1740/1724] loss: 0.480, ave_loss: 0.490

Finished Training finishing at 2021-08-30 19:17:18.805350
printing_out epoch  27.563805104408353 learning rate: 0.00021856130515487778
0.00021200446600023144
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.902e-01
Validation Loss: 2.207e+05
Validation ROC: 0.6438
No improvement, still saving model
1.4361948955916475 epochs left to go

Training Epoch 27.563805104408353/30 starting at 2021-08-30 19:17:59.655423
[1]  [0/1724] loss: 0.301, ave_loss: 0.301
[2]  [20/1724] loss: 0.680, ave_loss: 0.490
[3]  [40/1724] loss: 0.423, ave_loss: 0.468
[4]  [60/1724] loss: 0.471, ave_loss: 0.468
[5]  [80/1724] loss: 0.247, ave_loss: 0.424
[6]  [100/1724] loss: 0.593, ave_loss: 0.452
[7]  [120/1724] loss: 0.536, ave_loss: 0.464
[8]  [140/1724] loss: 0.384, ave_loss: 0.454
[9]  [160/1724] loss: 0.454, ave_loss: 0.454
[10]  [180/1724] loss: 0.688, ave_loss: 0.478
[11]  [200/1724] loss: 0.612, ave_loss: 0.490
[12]  [220/1724] loss: 0.594, ave_loss: 0.498
[13]  [240/1724] loss: 0.439, ave_loss: 0.494
[14]  [260/1724] loss: 0.626, ave_loss: 0.503
[15]  [280/1724] loss: 0.619, ave_loss: 0.511
[16]  [300/1724] loss: 0.424, ave_loss: 0.506
[17]  [320/1724] loss: 0.517, ave_loss: 0.506
[18]  [340/1724] loss: 0.461, ave_loss: 0.504
[19]  [360/1724] loss: 0.417, ave_loss: 0.499
[20]  [380/1724] loss: 0.431, ave_loss: 0.496
[21]  [400/1724] loss: 0.423, ave_loss: 0.492
[22]  [420/1724] loss: 0.371, ave_loss: 0.487
[23]  [440/1724] loss: 0.533, ave_loss: 0.489
[24]  [460/1724] loss: 0.479, ave_loss: 0.488
[25]  [480/1724] loss: 0.329, ave_loss: 0.482
[26]  [500/1724] loss: 0.355, ave_loss: 0.477
[27]  [520/1724] loss: 0.556, ave_loss: 0.480
[28]  [540/1724] loss: 0.426, ave_loss: 0.478
[29]  [560/1724] loss: 0.473, ave_loss: 0.478
[30]  [580/1724] loss: 0.462, ave_loss: 0.477
[31]  [600/1724] loss: 0.513, ave_loss: 0.479
[32]  [620/1724] loss: 0.501, ave_loss: 0.479
[33]  [640/1724] loss: 0.407, ave_loss: 0.477
[34]  [660/1724] loss: 0.480, ave_loss: 0.477
[35]  [680/1724] loss: 0.644, ave_loss: 0.482
[36]  [700/1724] loss: 0.357, ave_loss: 0.478
[37]  [720/1724] loss: 0.586, ave_loss: 0.481
[38]  [740/1724] loss: 0.642, ave_loss: 0.486
[39]  [760/1724] loss: 0.441, ave_loss: 0.484
[40]  [780/1724] loss: 0.447, ave_loss: 0.484
[41]  [800/1724] loss: 0.494, ave_loss: 0.484
[42]  [820/1724] loss: 0.449, ave_loss: 0.483
[43]  [840/1724] loss: 0.441, ave_loss: 0.482
[44]  [860/1724] loss: 0.552, ave_loss: 0.484
[45]  [880/1724] loss: 0.529, ave_loss: 0.485
[46]  [900/1724] loss: 0.671, ave_loss: 0.489
[47]  [920/1724] loss: 0.506, ave_loss: 0.489
[48]  [940/1724] loss: 0.309, ave_loss: 0.485
[49]  [960/1724] loss: 0.467, ave_loss: 0.485
[50]  [980/1724] loss: 0.466, ave_loss: 0.485
[51]  [1000/1724] loss: 0.531, ave_loss: 0.485
[52]  [1020/1724] loss: 0.573, ave_loss: 0.487
[53]  [1040/1724] loss: 0.437, ave_loss: 0.486
[54]  [1060/1724] loss: 0.252, ave_loss: 0.482
[55]  [1080/1724] loss: 0.490, ave_loss: 0.482
[56]  [1100/1724] loss: 0.569, ave_loss: 0.484
[57]  [1120/1724] loss: 0.387, ave_loss: 0.482
[58]  [1140/1724] loss: 0.444, ave_loss: 0.481
[59]  [1160/1724] loss: 0.403, ave_loss: 0.480
[60]  [1180/1724] loss: 0.370, ave_loss: 0.478
[61]  [1200/1724] loss: 0.404, ave_loss: 0.477
[62]  [1220/1724] loss: 0.596, ave_loss: 0.479
[63]  [1240/1724] loss: 0.302, ave_loss: 0.476
[64]  [1260/1724] loss: 0.505, ave_loss: 0.476
[65]  [1280/1724] loss: 0.539, ave_loss: 0.477
[66]  [1300/1724] loss: 0.485, ave_loss: 0.477
[67]  [1320/1724] loss: 0.510, ave_loss: 0.478
[68]  [1340/1724] loss: 0.399, ave_loss: 0.477
[69]  [1360/1724] loss: 0.369, ave_loss: 0.475
[70]  [1380/1724] loss: 0.513, ave_loss: 0.476
[71]  [1400/1724] loss: 0.576, ave_loss: 0.477
[72]  [1420/1724] loss: 0.367, ave_loss: 0.476
[73]  [1440/1724] loss: 0.355, ave_loss: 0.474
[74]  [1460/1724] loss: 0.650, ave_loss: 0.476
[75]  [1480/1724] loss: 0.510, ave_loss: 0.477
[76]  [1500/1724] loss: 0.630, ave_loss: 0.479
[77]  [1520/1724] loss: 0.613, ave_loss: 0.481
[78]  [1540/1724] loss: 0.654, ave_loss: 0.483
[79]  [1560/1724] loss: 0.460, ave_loss: 0.482
[80]  [1580/1724] loss: 0.403, ave_loss: 0.482
[81]  [1600/1724] loss: 0.447, ave_loss: 0.481
[82]  [1620/1724] loss: 0.572, ave_loss: 0.482
[83]  [1640/1724] loss: 0.529, ave_loss: 0.483
[84]  [1660/1724] loss: 0.431, ave_loss: 0.482
[85]  [1680/1724] loss: 0.482, ave_loss: 0.482
[86]  [1700/1724] loss: 0.547, ave_loss: 0.483
[87]  [1720/1724] loss: 0.535, ave_loss: 0.483
[88]  [1740/1724] loss: 0.596, ave_loss: 0.485

Finished Training finishing at 2021-08-30 19:20:22.790356
printing_out epoch  28.584686774941996 learning rate: 0.00019869209559534342
0.00019273133272748312
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.848e-01
Validation Loss: 2.159e+05
Validation ROC: 0.6416
No improvement, still saving model
0.4153132250580036 epochs left to go

Training Epoch 28.584686774941996/30 starting at 2021-08-30 19:21:07.246956
[1]  [0/1724] loss: 0.546, ave_loss: 0.546
[2]  [20/1724] loss: 0.482, ave_loss: 0.514
[3]  [40/1724] loss: 0.575, ave_loss: 0.534
[4]  [60/1724] loss: 0.509, ave_loss: 0.528
[5]  [80/1724] loss: 0.408, ave_loss: 0.504
[6]  [100/1724] loss: 0.359, ave_loss: 0.480
[7]  [120/1724] loss: 0.468, ave_loss: 0.478
[8]  [140/1724] loss: 0.471, ave_loss: 0.477
[9]  [160/1724] loss: 0.506, ave_loss: 0.480
[10]  [180/1724] loss: 0.504, ave_loss: 0.483
[11]  [200/1724] loss: 0.501, ave_loss: 0.484
[12]  [220/1724] loss: 0.544, ave_loss: 0.489
[13]  [240/1724] loss: 0.485, ave_loss: 0.489
[14]  [260/1724] loss: 0.538, ave_loss: 0.493
[15]  [280/1724] loss: 0.413, ave_loss: 0.487
[16]  [300/1724] loss: 0.467, ave_loss: 0.486
[17]  [320/1724] loss: 0.429, ave_loss: 0.483
[18]  [340/1724] loss: 0.482, ave_loss: 0.483
[19]  [360/1724] loss: 0.502, ave_loss: 0.484
[20]  [380/1724] loss: 0.499, ave_loss: 0.484
[21]  [400/1724] loss: 0.521, ave_loss: 0.486
[22]  [420/1724] loss: 0.376, ave_loss: 0.481
[23]  [440/1724] loss: 0.577, ave_loss: 0.485
[24]  [460/1724] loss: 0.376, ave_loss: 0.481
[25]  [480/1724] loss: 0.284, ave_loss: 0.473
[26]  [500/1724] loss: 0.566, ave_loss: 0.476
[27]  [520/1724] loss: 0.452, ave_loss: 0.476
[28]  [540/1724] loss: 0.399, ave_loss: 0.473
[29]  [560/1724] loss: 0.392, ave_loss: 0.470
[30]  [580/1724] loss: 0.671, ave_loss: 0.477
[31]  [600/1724] loss: 0.535, ave_loss: 0.479
[32]  [620/1724] loss: 0.515, ave_loss: 0.480
[33]  [640/1724] loss: 0.487, ave_loss: 0.480
[34]  [660/1724] loss: 0.365, ave_loss: 0.477
[35]  [680/1724] loss: 0.427, ave_loss: 0.475
[36]  [700/1724] loss: 0.415, ave_loss: 0.473
[37]  [720/1724] loss: 0.437, ave_loss: 0.473
[38]  [740/1724] loss: 0.386, ave_loss: 0.470
[39]  [760/1724] loss: 0.441, ave_loss: 0.469
[40]  [780/1724] loss: 0.479, ave_loss: 0.470
[41]  [800/1724] loss: 0.393, ave_loss: 0.468
[42]  [820/1724] loss: 0.535, ave_loss: 0.469
[43]  [840/1724] loss: 0.507, ave_loss: 0.470
[44]  [860/1724] loss: 0.374, ave_loss: 0.468
[45]  [880/1724] loss: 0.534, ave_loss: 0.470
[46]  [900/1724] loss: 0.295, ave_loss: 0.466
[47]  [920/1724] loss: 0.451, ave_loss: 0.466
[48]  [940/1724] loss: 0.742, ave_loss: 0.471
[49]  [960/1724] loss: 0.481, ave_loss: 0.471
[50]  [980/1724] loss: 0.439, ave_loss: 0.471
[51]  [1000/1724] loss: 0.445, ave_loss: 0.470
[52]  [1020/1724] loss: 0.412, ave_loss: 0.469
[53]  [1040/1724] loss: 0.414, ave_loss: 0.468
[54]  [1060/1724] loss: 0.495, ave_loss: 0.469
[55]  [1080/1724] loss: 0.448, ave_loss: 0.468
[56]  [1100/1724] loss: 0.477, ave_loss: 0.468
[57]  [1120/1724] loss: 0.530, ave_loss: 0.470
[58]  [1140/1724] loss: 0.361, ave_loss: 0.468
[59]  [1160/1724] loss: 0.582, ave_loss: 0.470
[60]  [1180/1724] loss: 0.545, ave_loss: 0.471
[61]  [1200/1724] loss: 0.521, ave_loss: 0.472
[62]  [1220/1724] loss: 0.345, ave_loss: 0.470
[63]  [1240/1724] loss: 0.480, ave_loss: 0.470
[64]  [1260/1724] loss: 0.604, ave_loss: 0.472
[65]  [1280/1724] loss: 0.490, ave_loss: 0.472
[66]  [1300/1724] loss: 0.339, ave_loss: 0.470
[67]  [1320/1724] loss: 0.653, ave_loss: 0.473
[68]  [1340/1724] loss: 0.523, ave_loss: 0.474
[69]  [1360/1724] loss: 0.472, ave_loss: 0.474
[70]  [1380/1724] loss: 0.459, ave_loss: 0.473
[71]  [1400/1724] loss: 0.466, ave_loss: 0.473
[72]  [1420/1724] loss: 0.579, ave_loss: 0.475
[73]  [1440/1724] loss: 0.437, ave_loss: 0.474
[74]  [1460/1724] loss: 0.361, ave_loss: 0.473
[75]  [1480/1724] loss: 0.365, ave_loss: 0.471
[76]  [1500/1724] loss: 0.333, ave_loss: 0.469
[77]  [1520/1724] loss: 0.359, ave_loss: 0.468
[78]  [1540/1724] loss: 0.581, ave_loss: 0.469
[79]  [1560/1724] loss: 0.620, ave_loss: 0.471
[80]  [1580/1724] loss: 0.436, ave_loss: 0.471
[81]  [1600/1724] loss: 0.598, ave_loss: 0.473
[82]  [1620/1724] loss: 0.406, ave_loss: 0.472
[83]  [1640/1724] loss: 0.454, ave_loss: 0.472
[84]  [1660/1724] loss: 0.526, ave_loss: 0.472
[85]  [1680/1724] loss: 0.394, ave_loss: 0.471
[86]  [1700/1724] loss: 0.636, ave_loss: 0.473
[87]  [1720/1724] loss: 0.490, ave_loss: 0.473
[88]  [1740/1724] loss: 0.573, ave_loss: 0.474

Finished Training finishing at 2021-08-30 19:23:24.562056
printing_out epoch  29.605568445475637 learning rate: 0.00018062917781394856
0.0001752103024795301
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.745e-01
Validation Loss: 2.189e+05
Validation ROC: 0.6401
No improvement, still saving model
saving results
