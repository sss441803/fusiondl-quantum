reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
Arguments:  Namespace(channels_spatial=['c16', 'c8', 'c4', 'c2'], channels_temporal=['c2', 'c2', 'c2', 'c2', 'c2', 'c2', 'c2'], input_div=1.0, kernel_spatial=4, kernel_temporal=8, linear_sizes=[20, 5], no_scalars=False, subsampling=10, tcn_hidden=2, tcn_layers=6, tcn_type='c')
...done
Training on 1724 shots, testing on 857 shots
Classical convolution with channels  2 16
Classical convolution with channels  16 8
Classical convolution with channels  8 4
Classical convolution with channels  4 2
InputBlock parameters:  14 2 64 ['c16', 'c8', 'c4', 'c2'] 4 [20, 5] 0.08
TCN parameters:  19 1 ['c2', 'c2', 'c2', 'c2', 'c2', 'c2', 'c2'] 8 0.08
29 epochs left to go

Training Epoch 0/30 starting at 2021-08-30 17:53:37.994852
[1]  [0/1724] loss: 1.367, ave_loss: 1.367
[2]  [20/1724] loss: 1.097, ave_loss: 1.232
[3]  [40/1724] loss: 1.042, ave_loss: 1.169
[4]  [60/1724] loss: 0.977, ave_loss: 1.121
[5]  [80/1724] loss: 1.045, ave_loss: 1.106
[6]  [100/1724] loss: 0.977, ave_loss: 1.084
[7]  [120/1724] loss: 0.835, ave_loss: 1.049
[8]  [140/1724] loss: 0.727, ave_loss: 1.008
[9]  [160/1724] loss: 0.735, ave_loss: 0.978
[10]  [180/1724] loss: 0.746, ave_loss: 0.955
[11]  [200/1724] loss: 0.750, ave_loss: 0.936
[12]  [220/1724] loss: 0.682, ave_loss: 0.915
[13]  [240/1724] loss: 0.704, ave_loss: 0.899
[14]  [260/1724] loss: 0.878, ave_loss: 0.897
[15]  [280/1724] loss: 0.673, ave_loss: 0.882
[16]  [300/1724] loss: 0.671, ave_loss: 0.869
[17]  [320/1724] loss: 0.713, ave_loss: 0.860
[18]  [340/1724] loss: 0.645, ave_loss: 0.848
[19]  [360/1724] loss: 0.760, ave_loss: 0.843
[20]  [380/1724] loss: 0.618, ave_loss: 0.832
[21]  [400/1724] loss: 0.770, ave_loss: 0.829
[22]  [420/1724] loss: 0.721, ave_loss: 0.824
[23]  [440/1724] loss: 0.678, ave_loss: 0.818
[24]  [460/1724] loss: 0.679, ave_loss: 0.812
[25]  [480/1724] loss: 0.684, ave_loss: 0.807
[26]  [500/1724] loss: 0.744, ave_loss: 0.805
[27]  [520/1724] loss: 0.573, ave_loss: 0.796
[28]  [540/1724] loss: 0.816, ave_loss: 0.797
[29]  [560/1724] loss: 0.739, ave_loss: 0.795
[30]  [580/1724] loss: 0.787, ave_loss: 0.794
[31]  [600/1724] loss: 0.769, ave_loss: 0.794
[32]  [620/1724] loss: 0.833, ave_loss: 0.795
[33]  [640/1724] loss: 0.754, ave_loss: 0.794
[34]  [660/1724] loss: 0.595, ave_loss: 0.788
[35]  [680/1724] loss: 0.714, ave_loss: 0.786
[36]  [700/1724] loss: 0.596, ave_loss: 0.780
[37]  [720/1724] loss: 0.904, ave_loss: 0.784
[38]  [740/1724] loss: 0.820, ave_loss: 0.785
[39]  [760/1724] loss: 0.547, ave_loss: 0.779
[40]  [780/1724] loss: 0.701, ave_loss: 0.777
[41]  [800/1724] loss: 0.511, ave_loss: 0.770
[42]  [820/1724] loss: 0.726, ave_loss: 0.769
[43]  [840/1724] loss: 0.725, ave_loss: 0.768
[44]  [860/1724] loss: 0.772, ave_loss: 0.768
[45]  [880/1724] loss: 0.687, ave_loss: 0.766
[46]  [900/1724] loss: 0.637, ave_loss: 0.764
[47]  [920/1724] loss: 0.655, ave_loss: 0.761
[48]  [940/1724] loss: 0.650, ave_loss: 0.759
[49]  [960/1724] loss: 0.809, ave_loss: 0.760
[50]  [980/1724] loss: 0.615, ave_loss: 0.757
[51]  [1000/1724] loss: 0.752, ave_loss: 0.757
[52]  [1020/1724] loss: 0.689, ave_loss: 0.756
[53]  [1040/1724] loss: 0.615, ave_loss: 0.753
[54]  [1060/1724] loss: 0.652, ave_loss: 0.751
[55]  [1080/1724] loss: 0.791, ave_loss: 0.752
[56]  [1100/1724] loss: 0.662, ave_loss: 0.750
[57]  [1120/1724] loss: 0.672, ave_loss: 0.749
[58]  [1140/1724] loss: 0.550, ave_loss: 0.745
[59]  [1160/1724] loss: 0.651, ave_loss: 0.744
[60]  [1180/1724] loss: 0.780, ave_loss: 0.744
[61]  [1200/1724] loss: 0.628, ave_loss: 0.742
[62]  [1220/1724] loss: 0.637, ave_loss: 0.741
[63]  [1240/1724] loss: 0.647, ave_loss: 0.739
[64]  [1260/1724] loss: 0.626, ave_loss: 0.738
[65]  [1280/1724] loss: 0.596, ave_loss: 0.735
[66]  [1300/1724] loss: 0.621, ave_loss: 0.734
[67]  [1320/1724] loss: 0.587, ave_loss: 0.731
[68]  [1340/1724] loss: 0.767, ave_loss: 0.732
[69]  [1360/1724] loss: 0.802, ave_loss: 0.733
[70]  [1380/1724] loss: 0.650, ave_loss: 0.732
[71]  [1400/1724] loss: 0.601, ave_loss: 0.730
[72]  [1420/1724] loss: 0.637, ave_loss: 0.729
[73]  [1440/1724] loss: 0.687, ave_loss: 0.728
[74]  [1460/1724] loss: 0.632, ave_loss: 0.727
[75]  [1480/1724] loss: 0.616, ave_loss: 0.725
[76]  [1500/1724] loss: 0.608, ave_loss: 0.724
[77]  [1520/1724] loss: 0.574, ave_loss: 0.722
[78]  [1540/1724] loss: 0.748, ave_loss: 0.722
[79]  [1560/1724] loss: 0.679, ave_loss: 0.722
[80]  [1580/1724] loss: 0.567, ave_loss: 0.720
[81]  [1600/1724] loss: 0.756, ave_loss: 0.720
[82]  [1620/1724] loss: 0.560, ave_loss: 0.718
[83]  [1640/1724] loss: 0.686, ave_loss: 0.718
[84]  [1660/1724] loss: 0.560, ave_loss: 0.716
[85]  [1680/1724] loss: 0.707, ave_loss: 0.716
[86]  [1700/1724] loss: 0.612, ave_loss: 0.715
[87]  [1720/1724] loss: 0.705, ave_loss: 0.714
[88]  [1740/1724] loss: 0.740, ave_loss: 0.715

Finished Training finishing at 2021-08-30 17:58:05.834452
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 7.148e-01
Validation Loss: 2.065e+05
Validation ROC: 0.1871
Saving model
27.979118329466356 epochs left to go

Training Epoch 1.0208816705336428/30 starting at 2021-08-30 18:00:52.267106
[1]  [0/1724] loss: 0.650, ave_loss: 0.650
[2]  [20/1724] loss: 0.692, ave_loss: 0.671
[3]  [40/1724] loss: 0.742, ave_loss: 0.695
[4]  [60/1724] loss: 0.756, ave_loss: 0.710
[5]  [80/1724] loss: 0.582, ave_loss: 0.684
[6]  [100/1724] loss: 0.719, ave_loss: 0.690
[7]  [120/1724] loss: 0.737, ave_loss: 0.697
[8]  [140/1724] loss: 0.536, ave_loss: 0.677
[9]  [160/1724] loss: 0.732, ave_loss: 0.683
[10]  [180/1724] loss: 0.581, ave_loss: 0.673
[11]  [200/1724] loss: 0.555, ave_loss: 0.662
[12]  [220/1724] loss: 0.750, ave_loss: 0.669
[13]  [240/1724] loss: 0.582, ave_loss: 0.663
[14]  [260/1724] loss: 0.732, ave_loss: 0.668
[15]  [280/1724] loss: 0.734, ave_loss: 0.672
[16]  [300/1724] loss: 0.615, ave_loss: 0.668
[17]  [320/1724] loss: 0.672, ave_loss: 0.669
[18]  [340/1724] loss: 0.648, ave_loss: 0.668
[19]  [360/1724] loss: 0.694, ave_loss: 0.669
[20]  [380/1724] loss: 0.808, ave_loss: 0.676
[21]  [400/1724] loss: 0.851, ave_loss: 0.684
[22]  [420/1724] loss: 0.688, ave_loss: 0.684
[23]  [440/1724] loss: 0.656, ave_loss: 0.683
[24]  [460/1724] loss: 0.695, ave_loss: 0.684
[25]  [480/1724] loss: 0.615, ave_loss: 0.681
[26]  [500/1724] loss: 0.598, ave_loss: 0.678
[27]  [520/1724] loss: 0.732, ave_loss: 0.680
[28]  [540/1724] loss: 0.718, ave_loss: 0.681
[29]  [560/1724] loss: 0.640, ave_loss: 0.680
[30]  [580/1724] loss: 0.651, ave_loss: 0.679
[31]  [600/1724] loss: 0.609, ave_loss: 0.676
[32]  [620/1724] loss: 0.669, ave_loss: 0.676
[33]  [640/1724] loss: 0.764, ave_loss: 0.679
[34]  [660/1724] loss: 0.764, ave_loss: 0.681
[35]  [680/1724] loss: 0.710, ave_loss: 0.682
[36]  [700/1724] loss: 0.623, ave_loss: 0.681
[37]  [720/1724] loss: 0.722, ave_loss: 0.682
[38]  [740/1724] loss: 0.737, ave_loss: 0.683
[39]  [760/1724] loss: 0.717, ave_loss: 0.684
[40]  [780/1724] loss: 0.702, ave_loss: 0.685
[41]  [800/1724] loss: 0.591, ave_loss: 0.682
[42]  [820/1724] loss: 0.696, ave_loss: 0.683
[43]  [840/1724] loss: 0.649, ave_loss: 0.682
[44]  [860/1724] loss: 0.657, ave_loss: 0.681
[45]  [880/1724] loss: 0.650, ave_loss: 0.680
[46]  [900/1724] loss: 0.481, ave_loss: 0.676
[47]  [920/1724] loss: 0.761, ave_loss: 0.678
[48]  [940/1724] loss: 0.618, ave_loss: 0.677
[49]  [960/1724] loss: 0.680, ave_loss: 0.677
[50]  [980/1724] loss: 0.568, ave_loss: 0.675
[51]  [1000/1724] loss: 0.609, ave_loss: 0.673
[52]  [1020/1724] loss: 0.627, ave_loss: 0.672
[53]  [1040/1724] loss: 0.564, ave_loss: 0.670
[54]  [1060/1724] loss: 0.625, ave_loss: 0.670
[55]  [1080/1724] loss: 0.642, ave_loss: 0.669
[56]  [1100/1724] loss: 0.679, ave_loss: 0.669
[57]  [1120/1724] loss: 0.669, ave_loss: 0.669
[58]  [1140/1724] loss: 0.793, ave_loss: 0.671
[59]  [1160/1724] loss: 0.602, ave_loss: 0.670
[60]  [1180/1724] loss: 0.772, ave_loss: 0.672
[61]  [1200/1724] loss: 0.564, ave_loss: 0.670
[62]  [1220/1724] loss: 0.720, ave_loss: 0.671
[63]  [1240/1724] loss: 0.590, ave_loss: 0.670
[64]  [1260/1724] loss: 0.541, ave_loss: 0.668
[65]  [1280/1724] loss: 0.600, ave_loss: 0.667
[66]  [1300/1724] loss: 0.836, ave_loss: 0.669
[67]  [1320/1724] loss: 0.829, ave_loss: 0.672
[68]  [1340/1724] loss: 0.670, ave_loss: 0.671
[69]  [1360/1724] loss: 0.611, ave_loss: 0.671
[70]  [1380/1724] loss: 0.613, ave_loss: 0.670
[71]  [1400/1724] loss: 0.694, ave_loss: 0.670
[72]  [1420/1724] loss: 0.626, ave_loss: 0.669
[73]  [1440/1724] loss: 0.752, ave_loss: 0.671
[74]  [1460/1724] loss: 0.570, ave_loss: 0.669
[75]  [1480/1724] loss: 0.623, ave_loss: 0.669
[76]  [1500/1724] loss: 0.724, ave_loss: 0.669
[77]  [1520/1724] loss: 0.622, ave_loss: 0.669
[78]  [1540/1724] loss: 0.621, ave_loss: 0.668
[79]  [1560/1724] loss: 0.652, ave_loss: 0.668
[80]  [1580/1724] loss: 0.619, ave_loss: 0.667
[81]  [1600/1724] loss: 0.622, ave_loss: 0.667
[82]  [1620/1724] loss: 0.614, ave_loss: 0.666
[83]  [1640/1724] loss: 0.712, ave_loss: 0.667
[84]  [1660/1724] loss: 0.665, ave_loss: 0.667
[85]  [1680/1724] loss: 0.738, ave_loss: 0.667
[86]  [1700/1724] loss: 0.712, ave_loss: 0.668
[87]  [1720/1724] loss: 0.783, ave_loss: 0.669
[88]  [1740/1724] loss: 0.521, ave_loss: 0.668

Finished Training finishing at 2021-08-30 18:03:32.745727
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.676e-01
Validation Loss: 1.997e+05
Validation ROC: 0.2100
Saving model
26.958236658932716 epochs left to go

Training Epoch 2.0417633410672855/30 starting at 2021-08-30 18:04:25.807418
[1]  [0/1724] loss: 0.583, ave_loss: 0.583
[2]  [20/1724] loss: 0.641, ave_loss: 0.612
[3]  [40/1724] loss: 0.511, ave_loss: 0.578
[4]  [60/1724] loss: 0.683, ave_loss: 0.604
[5]  [80/1724] loss: 0.680, ave_loss: 0.619
[6]  [100/1724] loss: 0.540, ave_loss: 0.606
[7]  [120/1724] loss: 0.671, ave_loss: 0.615
[8]  [140/1724] loss: 0.660, ave_loss: 0.621
[9]  [160/1724] loss: 0.594, ave_loss: 0.618
[10]  [180/1724] loss: 0.646, ave_loss: 0.621
[11]  [200/1724] loss: 0.552, ave_loss: 0.615
[12]  [220/1724] loss: 0.561, ave_loss: 0.610
[13]  [240/1724] loss: 0.747, ave_loss: 0.621
[14]  [260/1724] loss: 0.505, ave_loss: 0.612
[15]  [280/1724] loss: 0.645, ave_loss: 0.615
[16]  [300/1724] loss: 0.587, ave_loss: 0.613
[17]  [320/1724] loss: 0.516, ave_loss: 0.607
[18]  [340/1724] loss: 0.760, ave_loss: 0.616
[19]  [360/1724] loss: 0.671, ave_loss: 0.619
[20]  [380/1724] loss: 0.794, ave_loss: 0.627
[21]  [400/1724] loss: 0.611, ave_loss: 0.627
[22]  [420/1724] loss: 0.714, ave_loss: 0.631
[23]  [440/1724] loss: 0.611, ave_loss: 0.630
[24]  [460/1724] loss: 0.551, ave_loss: 0.626
[25]  [480/1724] loss: 0.599, ave_loss: 0.625
[26]  [500/1724] loss: 0.524, ave_loss: 0.621
[27]  [520/1724] loss: 0.577, ave_loss: 0.620
[28]  [540/1724] loss: 0.554, ave_loss: 0.617
[29]  [560/1724] loss: 0.641, ave_loss: 0.618
[30]  [580/1724] loss: 0.609, ave_loss: 0.618
[31]  [600/1724] loss: 0.691, ave_loss: 0.620
[32]  [620/1724] loss: 0.713, ave_loss: 0.623
[33]  [640/1724] loss: 0.587, ave_loss: 0.622
[34]  [660/1724] loss: 0.615, ave_loss: 0.622
[35]  [680/1724] loss: 0.877, ave_loss: 0.629
[36]  [700/1724] loss: 0.843, ave_loss: 0.635
[37]  [720/1724] loss: 0.600, ave_loss: 0.634
[38]  [740/1724] loss: 0.631, ave_loss: 0.634
[39]  [760/1724] loss: 0.591, ave_loss: 0.633
[40]  [780/1724] loss: 0.628, ave_loss: 0.633
[41]  [800/1724] loss: 0.528, ave_loss: 0.630
[42]  [820/1724] loss: 0.592, ave_loss: 0.629
[43]  [840/1724] loss: 0.651, ave_loss: 0.630
[44]  [860/1724] loss: 0.757, ave_loss: 0.633
[45]  [880/1724] loss: 0.572, ave_loss: 0.631
[46]  [900/1724] loss: 0.549, ave_loss: 0.630
[47]  [920/1724] loss: 0.606, ave_loss: 0.629
[48]  [940/1724] loss: 0.656, ave_loss: 0.630
[49]  [960/1724] loss: 0.658, ave_loss: 0.630
[50]  [980/1724] loss: 0.574, ave_loss: 0.629
[51]  [1000/1724] loss: 0.569, ave_loss: 0.628
[52]  [1020/1724] loss: 0.713, ave_loss: 0.630
[53]  [1040/1724] loss: 0.730, ave_loss: 0.632
[54]  [1060/1724] loss: 0.721, ave_loss: 0.633
[55]  [1080/1724] loss: 0.697, ave_loss: 0.634
[56]  [1100/1724] loss: 0.705, ave_loss: 0.636
[57]  [1120/1724] loss: 0.456, ave_loss: 0.632
[58]  [1140/1724] loss: 0.674, ave_loss: 0.633
[59]  [1160/1724] loss: 0.577, ave_loss: 0.632
[60]  [1180/1724] loss: 0.720, ave_loss: 0.634
[61]  [1200/1724] loss: 0.595, ave_loss: 0.633
[62]  [1220/1724] loss: 0.599, ave_loss: 0.632
[63]  [1240/1724] loss: 0.699, ave_loss: 0.634
[64]  [1260/1724] loss: 0.615, ave_loss: 0.633
[65]  [1280/1724] loss: 0.672, ave_loss: 0.634
[66]  [1300/1724] loss: 0.667, ave_loss: 0.634
[67]  [1320/1724] loss: 0.571, ave_loss: 0.633
[68]  [1340/1724] loss: 0.570, ave_loss: 0.632
[69]  [1360/1724] loss: 0.691, ave_loss: 0.633
[70]  [1380/1724] loss: 0.524, ave_loss: 0.632
[71]  [1400/1724] loss: 0.668, ave_loss: 0.632
[72]  [1420/1724] loss: 0.645, ave_loss: 0.632
[73]  [1440/1724] loss: 0.563, ave_loss: 0.631
[74]  [1460/1724] loss: 0.634, ave_loss: 0.632
[75]  [1480/1724] loss: 0.586, ave_loss: 0.631
[76]  [1500/1724] loss: 0.611, ave_loss: 0.631
[77]  [1520/1724] loss: 0.568, ave_loss: 0.630
[78]  [1540/1724] loss: 0.620, ave_loss: 0.630
[79]  [1560/1724] loss: 0.731, ave_loss: 0.631
[80]  [1580/1724] loss: 0.661, ave_loss: 0.631
[81]  [1600/1724] loss: 0.701, ave_loss: 0.632
[82]  [1620/1724] loss: 0.531, ave_loss: 0.631
[83]  [1640/1724] loss: 0.682, ave_loss: 0.632
[84]  [1660/1724] loss: 0.771, ave_loss: 0.633
[85]  [1680/1724] loss: 0.642, ave_loss: 0.633
[86]  [1700/1724] loss: 0.629, ave_loss: 0.633
[87]  [1720/1724] loss: 0.864, ave_loss: 0.636
[88]  [1740/1724] loss: 0.590, ave_loss: 0.635

Finished Training finishing at 2021-08-30 18:06:48.462431
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.355e-01
Validation Loss: 2.041e+05
Validation ROC: 0.2409
Saving model
25.937354988399072 epochs left to go

Training Epoch 3.062645011600928/30 starting at 2021-08-30 18:07:38.375629
[1]  [0/1724] loss: 0.707, ave_loss: 0.707
[2]  [20/1724] loss: 0.596, ave_loss: 0.652
[3]  [40/1724] loss: 0.531, ave_loss: 0.611
[4]  [60/1724] loss: 0.824, ave_loss: 0.664
[5]  [80/1724] loss: 0.526, ave_loss: 0.637
[6]  [100/1724] loss: 0.588, ave_loss: 0.629
[7]  [120/1724] loss: 0.574, ave_loss: 0.621
[8]  [140/1724] loss: 0.559, ave_loss: 0.613
[9]  [160/1724] loss: 0.699, ave_loss: 0.623
[10]  [180/1724] loss: 0.717, ave_loss: 0.632
[11]  [200/1724] loss: 0.584, ave_loss: 0.628
[12]  [220/1724] loss: 0.674, ave_loss: 0.632
[13]  [240/1724] loss: 0.802, ave_loss: 0.645
[14]  [260/1724] loss: 0.669, ave_loss: 0.646
[15]  [280/1724] loss: 0.763, ave_loss: 0.654
[16]  [300/1724] loss: 0.632, ave_loss: 0.653
[17]  [320/1724] loss: 0.613, ave_loss: 0.650
[18]  [340/1724] loss: 0.431, ave_loss: 0.638
[19]  [360/1724] loss: 0.627, ave_loss: 0.638
[20]  [380/1724] loss: 0.707, ave_loss: 0.641
[21]  [400/1724] loss: 0.700, ave_loss: 0.644
[22]  [420/1724] loss: 0.533, ave_loss: 0.639
[23]  [440/1724] loss: 0.644, ave_loss: 0.639
[24]  [460/1724] loss: 0.718, ave_loss: 0.642
[25]  [480/1724] loss: 0.631, ave_loss: 0.642
[26]  [500/1724] loss: 0.737, ave_loss: 0.646
[27]  [520/1724] loss: 0.638, ave_loss: 0.645
[28]  [540/1724] loss: 0.661, ave_loss: 0.646
[29]  [560/1724] loss: 0.768, ave_loss: 0.650
[30]  [580/1724] loss: 0.598, ave_loss: 0.648
[31]  [600/1724] loss: 0.605, ave_loss: 0.647
[32]  [620/1724] loss: 0.611, ave_loss: 0.646
[33]  [640/1724] loss: 0.586, ave_loss: 0.644
[34]  [660/1724] loss: 0.562, ave_loss: 0.642
[35]  [680/1724] loss: 0.502, ave_loss: 0.638
[36]  [700/1724] loss: 0.657, ave_loss: 0.638
[37]  [720/1724] loss: 0.454, ave_loss: 0.633
[38]  [740/1724] loss: 0.650, ave_loss: 0.634
[39]  [760/1724] loss: 0.546, ave_loss: 0.631
[40]  [780/1724] loss: 0.690, ave_loss: 0.633
[41]  [800/1724] loss: 0.579, ave_loss: 0.632
[42]  [820/1724] loss: 0.563, ave_loss: 0.630
[43]  [840/1724] loss: 0.588, ave_loss: 0.629
[44]  [860/1724] loss: 0.713, ave_loss: 0.631
[45]  [880/1724] loss: 0.562, ave_loss: 0.629
[46]  [900/1724] loss: 0.629, ave_loss: 0.629
[47]  [920/1724] loss: 0.636, ave_loss: 0.629
[48]  [940/1724] loss: 0.699, ave_loss: 0.631
[49]  [960/1724] loss: 0.691, ave_loss: 0.632
[50]  [980/1724] loss: 0.441, ave_loss: 0.628
[51]  [1000/1724] loss: 0.670, ave_loss: 0.629
[52]  [1020/1724] loss: 0.513, ave_loss: 0.627
[53]  [1040/1724] loss: 0.634, ave_loss: 0.627
[54]  [1060/1724] loss: 0.508, ave_loss: 0.625
[55]  [1080/1724] loss: 0.501, ave_loss: 0.623
[56]  [1100/1724] loss: 0.590, ave_loss: 0.622
[57]  [1120/1724] loss: 0.732, ave_loss: 0.624
[58]  [1140/1724] loss: 0.442, ave_loss: 0.621
[59]  [1160/1724] loss: 0.564, ave_loss: 0.620
[60]  [1180/1724] loss: 0.679, ave_loss: 0.621
[61]  [1200/1724] loss: 0.574, ave_loss: 0.620
[62]  [1220/1724] loss: 0.689, ave_loss: 0.621
[63]  [1240/1724] loss: 0.605, ave_loss: 0.621
[64]  [1260/1724] loss: 0.714, ave_loss: 0.622
[65]  [1280/1724] loss: 0.615, ave_loss: 0.622
[66]  [1300/1724] loss: 0.697, ave_loss: 0.623
[67]  [1320/1724] loss: 0.504, ave_loss: 0.622
[68]  [1340/1724] loss: 0.519, ave_loss: 0.620
[69]  [1360/1724] loss: 0.593, ave_loss: 0.620
[70]  [1380/1724] loss: 0.590, ave_loss: 0.619
[71]  [1400/1724] loss: 0.630, ave_loss: 0.619
[72]  [1420/1724] loss: 0.539, ave_loss: 0.618
[73]  [1440/1724] loss: 0.563, ave_loss: 0.618
[74]  [1460/1724] loss: 0.680, ave_loss: 0.618
[75]  [1480/1724] loss: 0.675, ave_loss: 0.619
[76]  [1500/1724] loss: 0.742, ave_loss: 0.621
[77]  [1520/1724] loss: 0.756, ave_loss: 0.623
[78]  [1540/1724] loss: 0.621, ave_loss: 0.623
[79]  [1560/1724] loss: 0.603, ave_loss: 0.622
[80]  [1580/1724] loss: 0.580, ave_loss: 0.622
[81]  [1600/1724] loss: 0.604, ave_loss: 0.622
[82]  [1620/1724] loss: 0.758, ave_loss: 0.623
[83]  [1640/1724] loss: 0.531, ave_loss: 0.622
[84]  [1660/1724] loss: 0.586, ave_loss: 0.622
[85]  [1680/1724] loss: 0.618, ave_loss: 0.622
[86]  [1700/1724] loss: 0.538, ave_loss: 0.621
[87]  [1720/1724] loss: 0.578, ave_loss: 0.620
[88]  [1740/1724] loss: 0.439, ave_loss: 0.618

Finished Training finishing at 2021-08-30 18:09:40.705859
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.181e-01
Validation Loss: 2.055e+05
Validation ROC: 0.2740
Saving model
24.916473317865428 epochs left to go

Training Epoch 4.083526682134571/30 starting at 2021-08-30 18:10:30.758931
[1]  [0/1724] loss: 0.583, ave_loss: 0.583
[2]  [20/1724] loss: 0.722, ave_loss: 0.653
[3]  [40/1724] loss: 0.562, ave_loss: 0.622
[4]  [60/1724] loss: 0.517, ave_loss: 0.596
[5]  [80/1724] loss: 0.704, ave_loss: 0.618
[6]  [100/1724] loss: 0.640, ave_loss: 0.621
[7]  [120/1724] loss: 0.618, ave_loss: 0.621
[8]  [140/1724] loss: 0.525, ave_loss: 0.609
[9]  [160/1724] loss: 0.565, ave_loss: 0.604
[10]  [180/1724] loss: 0.603, ave_loss: 0.604
[11]  [200/1724] loss: 0.598, ave_loss: 0.603
[12]  [220/1724] loss: 0.731, ave_loss: 0.614
[13]  [240/1724] loss: 0.606, ave_loss: 0.613
[14]  [260/1724] loss: 0.571, ave_loss: 0.610
[15]  [280/1724] loss: 0.631, ave_loss: 0.612
[16]  [300/1724] loss: 0.668, ave_loss: 0.615
[17]  [320/1724] loss: 0.581, ave_loss: 0.613
[18]  [340/1724] loss: 0.685, ave_loss: 0.617
[19]  [360/1724] loss: 0.636, ave_loss: 0.618
[20]  [380/1724] loss: 0.590, ave_loss: 0.617
[21]  [400/1724] loss: 0.754, ave_loss: 0.623
[22]  [420/1724] loss: 0.588, ave_loss: 0.622
[23]  [440/1724] loss: 0.603, ave_loss: 0.621
[24]  [460/1724] loss: 0.786, ave_loss: 0.628
[25]  [480/1724] loss: 0.690, ave_loss: 0.630
[26]  [500/1724] loss: 0.500, ave_loss: 0.625
[27]  [520/1724] loss: 0.525, ave_loss: 0.622
[28]  [540/1724] loss: 0.605, ave_loss: 0.621
[29]  [560/1724] loss: 0.704, ave_loss: 0.624
[30]  [580/1724] loss: 0.538, ave_loss: 0.621
[31]  [600/1724] loss: 0.720, ave_loss: 0.624
[32]  [620/1724] loss: 0.492, ave_loss: 0.620
[33]  [640/1724] loss: 0.465, ave_loss: 0.615
[34]  [660/1724] loss: 0.594, ave_loss: 0.615
[35]  [680/1724] loss: 0.632, ave_loss: 0.615
[36]  [700/1724] loss: 0.665, ave_loss: 0.617
[37]  [720/1724] loss: 0.652, ave_loss: 0.618
[38]  [740/1724] loss: 0.562, ave_loss: 0.616
[39]  [760/1724] loss: 0.746, ave_loss: 0.619
[40]  [780/1724] loss: 0.695, ave_loss: 0.621
[41]  [800/1724] loss: 0.535, ave_loss: 0.619
[42]  [820/1724] loss: 0.630, ave_loss: 0.619
[43]  [840/1724] loss: 0.588, ave_loss: 0.619
[44]  [860/1724] loss: 0.613, ave_loss: 0.619
[45]  [880/1724] loss: 0.529, ave_loss: 0.617
[46]  [900/1724] loss: 0.534, ave_loss: 0.615
[47]  [920/1724] loss: 0.474, ave_loss: 0.612
[48]  [940/1724] loss: 0.649, ave_loss: 0.613
[49]  [960/1724] loss: 0.574, ave_loss: 0.612
[50]  [980/1724] loss: 0.552, ave_loss: 0.611
[51]  [1000/1724] loss: 0.545, ave_loss: 0.609
[52]  [1020/1724] loss: 0.670, ave_loss: 0.610
[53]  [1040/1724] loss: 0.612, ave_loss: 0.610
[54]  [1060/1724] loss: 0.603, ave_loss: 0.610
[55]  [1080/1724] loss: 0.529, ave_loss: 0.609
[56]  [1100/1724] loss: 0.584, ave_loss: 0.608
[57]  [1120/1724] loss: 0.523, ave_loss: 0.607
[58]  [1140/1724] loss: 0.607, ave_loss: 0.607
[59]  [1160/1724] loss: 0.645, ave_loss: 0.608
[60]  [1180/1724] loss: 0.767, ave_loss: 0.610
[61]  [1200/1724] loss: 0.585, ave_loss: 0.610
[62]  [1220/1724] loss: 0.618, ave_loss: 0.610
[63]  [1240/1724] loss: 0.699, ave_loss: 0.611
[64]  [1260/1724] loss: 0.538, ave_loss: 0.610
[65]  [1280/1724] loss: 0.573, ave_loss: 0.610
[66]  [1300/1724] loss: 0.646, ave_loss: 0.610
[67]  [1320/1724] loss: 0.651, ave_loss: 0.611
[68]  [1340/1724] loss: 0.637, ave_loss: 0.611
[69]  [1360/1724] loss: 0.503, ave_loss: 0.610
[70]  [1380/1724] loss: 0.549, ave_loss: 0.609
[71]  [1400/1724] loss: 0.572, ave_loss: 0.608
[72]  [1420/1724] loss: 0.554, ave_loss: 0.607
[73]  [1440/1724] loss: 0.500, ave_loss: 0.606
[74]  [1460/1724] loss: 0.674, ave_loss: 0.607
[75]  [1480/1724] loss: 0.451, ave_loss: 0.605
[76]  [1500/1724] loss: 0.605, ave_loss: 0.605
[77]  [1520/1724] loss: 0.555, ave_loss: 0.604
[78]  [1540/1724] loss: 0.531, ave_loss: 0.603
[79]  [1560/1724] loss: 0.590, ave_loss: 0.603
[80]  [1580/1724] loss: 0.557, ave_loss: 0.602
[81]  [1600/1724] loss: 0.584, ave_loss: 0.602
[82]  [1620/1724] loss: 0.553, ave_loss: 0.602
[83]  [1640/1724] loss: 0.641, ave_loss: 0.602
[84]  [1660/1724] loss: 0.610, ave_loss: 0.602
[85]  [1680/1724] loss: 0.559, ave_loss: 0.602
[86]  [1700/1724] loss: 0.631, ave_loss: 0.602
[87]  [1720/1724] loss: 0.475, ave_loss: 0.601
[88]  [1740/1724] loss: 0.661, ave_loss: 0.601

Finished Training finishing at 2021-08-30 18:12:31.349452
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.013e-01
Validation Loss: 2.116e+05
Validation ROC: 0.2989
Saving model
23.895591647331788 epochs left to go

Training Epoch 5.104408352668213/30 starting at 2021-08-30 18:13:21.474536
[1]  [0/1724] loss: 0.679, ave_loss: 0.679
[2]  [20/1724] loss: 0.490, ave_loss: 0.584
[3]  [40/1724] loss: 0.655, ave_loss: 0.608
[4]  [60/1724] loss: 0.662, ave_loss: 0.621
[5]  [80/1724] loss: 0.645, ave_loss: 0.626
[6]  [100/1724] loss: 0.710, ave_loss: 0.640
[7]  [120/1724] loss: 0.552, ave_loss: 0.627
[8]  [140/1724] loss: 0.666, ave_loss: 0.632
[9]  [160/1724] loss: 0.535, ave_loss: 0.621
[10]  [180/1724] loss: 0.661, ave_loss: 0.625
[11]  [200/1724] loss: 0.583, ave_loss: 0.621
[12]  [220/1724] loss: 0.517, ave_loss: 0.613
[13]  [240/1724] loss: 0.547, ave_loss: 0.608
[14]  [260/1724] loss: 0.546, ave_loss: 0.603
[15]  [280/1724] loss: 0.503, ave_loss: 0.597
[16]  [300/1724] loss: 0.504, ave_loss: 0.591
[17]  [320/1724] loss: 0.588, ave_loss: 0.591
[18]  [340/1724] loss: 0.781, ave_loss: 0.601
[19]  [360/1724] loss: 0.565, ave_loss: 0.599
[20]  [380/1724] loss: 0.574, ave_loss: 0.598
[21]  [400/1724] loss: 0.643, ave_loss: 0.600
[22]  [420/1724] loss: 0.718, ave_loss: 0.605
[23]  [440/1724] loss: 0.664, ave_loss: 0.608
[24]  [460/1724] loss: 0.540, ave_loss: 0.605
[25]  [480/1724] loss: 0.691, ave_loss: 0.609
[26]  [500/1724] loss: 0.808, ave_loss: 0.616
[27]  [520/1724] loss: 0.759, ave_loss: 0.622
[28]  [540/1724] loss: 0.548, ave_loss: 0.619
[29]  [560/1724] loss: 0.783, ave_loss: 0.625
[30]  [580/1724] loss: 0.652, ave_loss: 0.626
[31]  [600/1724] loss: 0.545, ave_loss: 0.623
[32]  [620/1724] loss: 0.492, ave_loss: 0.619
[33]  [640/1724] loss: 0.581, ave_loss: 0.618
[34]  [660/1724] loss: 0.667, ave_loss: 0.619
[35]  [680/1724] loss: 0.452, ave_loss: 0.614
[36]  [700/1724] loss: 0.614, ave_loss: 0.614
[37]  [720/1724] loss: 0.728, ave_loss: 0.617
[38]  [740/1724] loss: 0.573, ave_loss: 0.616
[39]  [760/1724] loss: 0.697, ave_loss: 0.618
[40]  [780/1724] loss: 0.674, ave_loss: 0.620
[41]  [800/1724] loss: 0.543, ave_loss: 0.618
[42]  [820/1724] loss: 0.646, ave_loss: 0.619
[43]  [840/1724] loss: 0.596, ave_loss: 0.618
[44]  [860/1724] loss: 0.629, ave_loss: 0.618
[45]  [880/1724] loss: 0.570, ave_loss: 0.617
[46]  [900/1724] loss: 0.633, ave_loss: 0.618
[47]  [920/1724] loss: 0.687, ave_loss: 0.619
[48]  [940/1724] loss: 0.565, ave_loss: 0.618
[49]  [960/1724] loss: 0.632, ave_loss: 0.618
[50]  [980/1724] loss: 0.579, ave_loss: 0.617
[51]  [1000/1724] loss: 0.607, ave_loss: 0.617
[52]  [1020/1724] loss: 0.566, ave_loss: 0.616
[53]  [1040/1724] loss: 0.462, ave_loss: 0.613
[54]  [1060/1724] loss: 0.628, ave_loss: 0.614
[55]  [1080/1724] loss: 0.560, ave_loss: 0.613
[56]  [1100/1724] loss: 0.643, ave_loss: 0.613
[57]  [1120/1724] loss: 0.569, ave_loss: 0.612
[58]  [1140/1724] loss: 0.658, ave_loss: 0.613
[59]  [1160/1724] loss: 0.733, ave_loss: 0.615
[60]  [1180/1724] loss: 0.527, ave_loss: 0.614
[61]  [1200/1724] loss: 0.553, ave_loss: 0.613
[62]  [1220/1724] loss: 0.639, ave_loss: 0.613
[63]  [1240/1724] loss: 0.557, ave_loss: 0.612
[64]  [1260/1724] loss: 0.646, ave_loss: 0.613
[65]  [1280/1724] loss: 0.583, ave_loss: 0.612
[66]  [1300/1724] loss: 0.513, ave_loss: 0.611
[67]  [1320/1724] loss: 0.546, ave_loss: 0.610
[68]  [1340/1724] loss: 0.614, ave_loss: 0.610
[69]  [1360/1724] loss: 0.504, ave_loss: 0.608
[70]  [1380/1724] loss: 0.562, ave_loss: 0.608
[71]  [1400/1724] loss: 0.593, ave_loss: 0.607
[72]  [1420/1724] loss: 0.509, ave_loss: 0.606
[73]  [1440/1724] loss: 0.495, ave_loss: 0.605
[74]  [1460/1724] loss: 0.521, ave_loss: 0.603
[75]  [1480/1724] loss: 0.652, ave_loss: 0.604
[76]  [1500/1724] loss: 0.647, ave_loss: 0.605
[77]  [1520/1724] loss: 0.544, ave_loss: 0.604
[78]  [1540/1724] loss: 0.527, ave_loss: 0.603
[79]  [1560/1724] loss: 0.596, ave_loss: 0.603
[80]  [1580/1724] loss: 0.557, ave_loss: 0.602
[81]  [1600/1724] loss: 0.607, ave_loss: 0.602
[82]  [1620/1724] loss: 0.633, ave_loss: 0.603
[83]  [1640/1724] loss: 0.546, ave_loss: 0.602
[84]  [1660/1724] loss: 0.589, ave_loss: 0.602
[85]  [1680/1724] loss: 0.510, ave_loss: 0.601
[86]  [1700/1724] loss: 0.623, ave_loss: 0.601
[87]  [1720/1724] loss: 0.589, ave_loss: 0.601
[88]  [1740/1724] loss: 0.753, ave_loss: 0.603

Finished Training finishing at 2021-08-30 18:15:24.587965
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.026e-01
Validation Loss: 2.110e+05
Validation ROC: 0.3120
Saving model
22.874709976798144 epochs left to go

Training Epoch 6.125290023201856/30 starting at 2021-08-30 18:16:10.677822
[1]  [0/1724] loss: 0.593, ave_loss: 0.593
[2]  [20/1724] loss: 0.441, ave_loss: 0.517
[3]  [40/1724] loss: 0.627, ave_loss: 0.554
[4]  [60/1724] loss: 0.663, ave_loss: 0.581
[5]  [80/1724] loss: 0.816, ave_loss: 0.628
[6]  [100/1724] loss: 0.535, ave_loss: 0.613
[7]  [120/1724] loss: 0.582, ave_loss: 0.608
[8]  [140/1724] loss: 0.466, ave_loss: 0.590
[9]  [160/1724] loss: 0.691, ave_loss: 0.602
[10]  [180/1724] loss: 0.529, ave_loss: 0.594
[11]  [200/1724] loss: 0.563, ave_loss: 0.591
[12]  [220/1724] loss: 0.435, ave_loss: 0.578
[13]  [240/1724] loss: 0.536, ave_loss: 0.575
[14]  [260/1724] loss: 0.567, ave_loss: 0.575
[15]  [280/1724] loss: 0.665, ave_loss: 0.581
[16]  [300/1724] loss: 0.455, ave_loss: 0.573
[17]  [320/1724] loss: 0.430, ave_loss: 0.564
[18]  [340/1724] loss: 0.551, ave_loss: 0.564
[19]  [360/1724] loss: 0.628, ave_loss: 0.567
[20]  [380/1724] loss: 0.556, ave_loss: 0.567
[21]  [400/1724] loss: 0.546, ave_loss: 0.566
[22]  [420/1724] loss: 0.496, ave_loss: 0.562
[23]  [440/1724] loss: 0.485, ave_loss: 0.559
[24]  [460/1724] loss: 0.532, ave_loss: 0.558
[25]  [480/1724] loss: 0.556, ave_loss: 0.558
[26]  [500/1724] loss: 0.574, ave_loss: 0.558
[27]  [520/1724] loss: 0.547, ave_loss: 0.558
[28]  [540/1724] loss: 0.786, ave_loss: 0.566
[29]  [560/1724] loss: 0.641, ave_loss: 0.569
[30]  [580/1724] loss: 0.610, ave_loss: 0.570
[31]  [600/1724] loss: 0.564, ave_loss: 0.570
[32]  [620/1724] loss: 0.492, ave_loss: 0.568
[33]  [640/1724] loss: 0.559, ave_loss: 0.567
[34]  [660/1724] loss: 0.536, ave_loss: 0.566
[35]  [680/1724] loss: 0.480, ave_loss: 0.564
[36]  [700/1724] loss: 0.777, ave_loss: 0.570
[37]  [720/1724] loss: 0.630, ave_loss: 0.571
[38]  [740/1724] loss: 0.592, ave_loss: 0.572
[39]  [760/1724] loss: 0.464, ave_loss: 0.569
[40]  [780/1724] loss: 0.618, ave_loss: 0.570
[41]  [800/1724] loss: 0.496, ave_loss: 0.569
[42]  [820/1724] loss: 0.626, ave_loss: 0.570
[43]  [840/1724] loss: 0.657, ave_loss: 0.572
[44]  [860/1724] loss: 0.619, ave_loss: 0.573
[45]  [880/1724] loss: 0.763, ave_loss: 0.577
[46]  [900/1724] loss: 0.622, ave_loss: 0.578
[47]  [920/1724] loss: 0.623, ave_loss: 0.579
[48]  [940/1724] loss: 0.656, ave_loss: 0.581
[49]  [960/1724] loss: 0.616, ave_loss: 0.581
[50]  [980/1724] loss: 0.576, ave_loss: 0.581
[51]  [1000/1724] loss: 0.450, ave_loss: 0.579
[52]  [1020/1724] loss: 0.631, ave_loss: 0.580
[53]  [1040/1724] loss: 0.540, ave_loss: 0.579
[54]  [1060/1724] loss: 0.578, ave_loss: 0.579
[55]  [1080/1724] loss: 0.484, ave_loss: 0.577
[56]  [1100/1724] loss: 0.536, ave_loss: 0.577
[57]  [1120/1724] loss: 0.494, ave_loss: 0.575
[58]  [1140/1724] loss: 0.730, ave_loss: 0.578
[59]  [1160/1724] loss: 0.610, ave_loss: 0.578
[60]  [1180/1724] loss: 0.536, ave_loss: 0.578
[61]  [1200/1724] loss: 0.564, ave_loss: 0.577
[62]  [1220/1724] loss: 0.553, ave_loss: 0.577
[63]  [1240/1724] loss: 0.487, ave_loss: 0.576
[64]  [1260/1724] loss: 0.565, ave_loss: 0.575
[65]  [1280/1724] loss: 0.577, ave_loss: 0.575
[66]  [1300/1724] loss: 0.644, ave_loss: 0.576
[67]  [1320/1724] loss: 0.537, ave_loss: 0.576
[68]  [1340/1724] loss: 0.692, ave_loss: 0.578
[69]  [1360/1724] loss: 0.580, ave_loss: 0.578
[70]  [1380/1724] loss: 0.532, ave_loss: 0.577
[71]  [1400/1724] loss: 0.608, ave_loss: 0.577
[72]  [1420/1724] loss: 0.597, ave_loss: 0.578
[73]  [1440/1724] loss: 0.547, ave_loss: 0.577
[74]  [1460/1724] loss: 0.743, ave_loss: 0.580
[75]  [1480/1724] loss: 0.756, ave_loss: 0.582
[76]  [1500/1724] loss: 0.650, ave_loss: 0.583
[77]  [1520/1724] loss: 0.624, ave_loss: 0.583
[78]  [1540/1724] loss: 0.531, ave_loss: 0.583
[79]  [1560/1724] loss: 0.500, ave_loss: 0.582
[80]  [1580/1724] loss: 0.551, ave_loss: 0.581
[81]  [1600/1724] loss: 0.613, ave_loss: 0.582
[82]  [1620/1724] loss: 0.610, ave_loss: 0.582
[83]  [1640/1724] loss: 0.654, ave_loss: 0.583
[84]  [1660/1724] loss: 0.656, ave_loss: 0.584
[85]  [1680/1724] loss: 0.648, ave_loss: 0.584
[86]  [1700/1724] loss: 0.661, ave_loss: 0.585
[87]  [1720/1724] loss: 0.505, ave_loss: 0.584
[88]  [1740/1724] loss: 0.706, ave_loss: 0.586

Finished Training finishing at 2021-08-30 18:18:19.364092
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.858e-01
Validation Loss: 2.126e+05
Validation ROC: 0.3198
Saving model
21.8538283062645 epochs left to go

Training Epoch 7.146171693735499/30 starting at 2021-08-30 18:19:05.747020
[1]  [0/1724] loss: 0.513, ave_loss: 0.513
[2]  [20/1724] loss: 0.484, ave_loss: 0.499
[3]  [40/1724] loss: 0.723, ave_loss: 0.573
[4]  [60/1724] loss: 0.533, ave_loss: 0.563
[5]  [80/1724] loss: 0.527, ave_loss: 0.556
[6]  [100/1724] loss: 0.638, ave_loss: 0.570
[7]  [120/1724] loss: 0.498, ave_loss: 0.560
[8]  [140/1724] loss: 0.588, ave_loss: 0.563
[9]  [160/1724] loss: 0.601, ave_loss: 0.567
[10]  [180/1724] loss: 0.658, ave_loss: 0.576
[11]  [200/1724] loss: 0.553, ave_loss: 0.574
[12]  [220/1724] loss: 0.722, ave_loss: 0.587
[13]  [240/1724] loss: 0.610, ave_loss: 0.588
[14]  [260/1724] loss: 0.728, ave_loss: 0.598
[15]  [280/1724] loss: 0.678, ave_loss: 0.604
[16]  [300/1724] loss: 0.532, ave_loss: 0.599
[17]  [320/1724] loss: 0.726, ave_loss: 0.607
[18]  [340/1724] loss: 0.727, ave_loss: 0.613
[19]  [360/1724] loss: 0.567, ave_loss: 0.611
[20]  [380/1724] loss: 0.619, ave_loss: 0.611
[21]  [400/1724] loss: 0.560, ave_loss: 0.609
[22]  [420/1724] loss: 0.526, ave_loss: 0.605
[23]  [440/1724] loss: 0.600, ave_loss: 0.605
[24]  [460/1724] loss: 0.522, ave_loss: 0.601
[25]  [480/1724] loss: 0.645, ave_loss: 0.603
[26]  [500/1724] loss: 0.581, ave_loss: 0.602
[27]  [520/1724] loss: 0.672, ave_loss: 0.605
[28]  [540/1724] loss: 0.420, ave_loss: 0.598
[29]  [560/1724] loss: 0.531, ave_loss: 0.596
[30]  [580/1724] loss: 0.608, ave_loss: 0.596
[31]  [600/1724] loss: 0.676, ave_loss: 0.599
[32]  [620/1724] loss: 0.660, ave_loss: 0.601
[33]  [640/1724] loss: 0.638, ave_loss: 0.602
[34]  [660/1724] loss: 0.564, ave_loss: 0.601
[35]  [680/1724] loss: 0.536, ave_loss: 0.599
[36]  [700/1724] loss: 0.509, ave_loss: 0.597
[37]  [720/1724] loss: 0.567, ave_loss: 0.596
[38]  [740/1724] loss: 0.592, ave_loss: 0.596
[39]  [760/1724] loss: 0.506, ave_loss: 0.593
[40]  [780/1724] loss: 0.640, ave_loss: 0.594
[41]  [800/1724] loss: 0.589, ave_loss: 0.594
[42]  [820/1724] loss: 0.509, ave_loss: 0.592
[43]  [840/1724] loss: 0.493, ave_loss: 0.590
[44]  [860/1724] loss: 0.551, ave_loss: 0.589
[45]  [880/1724] loss: 0.560, ave_loss: 0.588
[46]  [900/1724] loss: 0.472, ave_loss: 0.586
[47]  [920/1724] loss: 0.563, ave_loss: 0.585
[48]  [940/1724] loss: 0.588, ave_loss: 0.585
[49]  [960/1724] loss: 0.805, ave_loss: 0.590
[50]  [980/1724] loss: 0.530, ave_loss: 0.589
[51]  [1000/1724] loss: 0.554, ave_loss: 0.588
[52]  [1020/1724] loss: 0.580, ave_loss: 0.588
[53]  [1040/1724] loss: 0.561, ave_loss: 0.587
[54]  [1060/1724] loss: 0.606, ave_loss: 0.588
[55]  [1080/1724] loss: 0.703, ave_loss: 0.590
[56]  [1100/1724] loss: 0.476, ave_loss: 0.588
[57]  [1120/1724] loss: 0.716, ave_loss: 0.590
[58]  [1140/1724] loss: 0.624, ave_loss: 0.591
[59]  [1160/1724] loss: 0.608, ave_loss: 0.591
[60]  [1180/1724] loss: 0.593, ave_loss: 0.591
[61]  [1200/1724] loss: 0.603, ave_loss: 0.591
[62]  [1220/1724] loss: 0.536, ave_loss: 0.590
[63]  [1240/1724] loss: 0.477, ave_loss: 0.589
[64]  [1260/1724] loss: 0.499, ave_loss: 0.587
[65]  [1280/1724] loss: 0.604, ave_loss: 0.587
[66]  [1300/1724] loss: 0.568, ave_loss: 0.587
[67]  [1320/1724] loss: 0.682, ave_loss: 0.588
[68]  [1340/1724] loss: 0.422, ave_loss: 0.586
[69]  [1360/1724] loss: 0.507, ave_loss: 0.585
[70]  [1380/1724] loss: 0.555, ave_loss: 0.584
[71]  [1400/1724] loss: 0.556, ave_loss: 0.584
[72]  [1420/1724] loss: 0.578, ave_loss: 0.584
[73]  [1440/1724] loss: 0.558, ave_loss: 0.584
[74]  [1460/1724] loss: 0.596, ave_loss: 0.584
[75]  [1480/1724] loss: 0.606, ave_loss: 0.584
[76]  [1500/1724] loss: 0.491, ave_loss: 0.583
[77]  [1520/1724] loss: 0.641, ave_loss: 0.584
[78]  [1540/1724] loss: 0.577, ave_loss: 0.584
[79]  [1560/1724] loss: 0.544, ave_loss: 0.583
[80]  [1580/1724] loss: 0.480, ave_loss: 0.582
[81]  [1600/1724] loss: 0.574, ave_loss: 0.582
[82]  [1620/1724] loss: 0.485, ave_loss: 0.580
[83]  [1640/1724] loss: 0.546, ave_loss: 0.580
[84]  [1660/1724] loss: 0.606, ave_loss: 0.580
[85]  [1680/1724] loss: 0.538, ave_loss: 0.580
[86]  [1700/1724] loss: 0.481, ave_loss: 0.579
[87]  [1720/1724] loss: 0.452, ave_loss: 0.577
[88]  [1740/1724] loss: 0.513, ave_loss: 0.577

Finished Training finishing at 2021-08-30 18:21:13.095709
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.765e-01
Validation Loss: 2.228e+05
Validation ROC: 0.3344
Saving model
20.832946635730856 epochs left to go

Training Epoch 8.167053364269142/30 starting at 2021-08-30 18:22:02.885126
[1]  [0/1724] loss: 0.484, ave_loss: 0.484
[2]  [20/1724] loss: 0.587, ave_loss: 0.536
[3]  [40/1724] loss: 0.556, ave_loss: 0.542
[4]  [60/1724] loss: 0.556, ave_loss: 0.546
[5]  [80/1724] loss: 0.608, ave_loss: 0.558
[6]  [100/1724] loss: 0.615, ave_loss: 0.568
[7]  [120/1724] loss: 0.475, ave_loss: 0.554
[8]  [140/1724] loss: 0.633, ave_loss: 0.564
[9]  [160/1724] loss: 0.680, ave_loss: 0.577
[10]  [180/1724] loss: 0.521, ave_loss: 0.572
[11]  [200/1724] loss: 0.463, ave_loss: 0.562
[12]  [220/1724] loss: 0.576, ave_loss: 0.563
[13]  [240/1724] loss: 0.602, ave_loss: 0.566
[14]  [260/1724] loss: 0.561, ave_loss: 0.566
[15]  [280/1724] loss: 0.525, ave_loss: 0.563
[16]  [300/1724] loss: 0.548, ave_loss: 0.562
[17]  [320/1724] loss: 0.563, ave_loss: 0.562
[18]  [340/1724] loss: 0.729, ave_loss: 0.571
[19]  [360/1724] loss: 0.474, ave_loss: 0.566
[20]  [380/1724] loss: 0.674, ave_loss: 0.572
[21]  [400/1724] loss: 0.504, ave_loss: 0.568
[22]  [420/1724] loss: 0.510, ave_loss: 0.566
[23]  [440/1724] loss: 0.628, ave_loss: 0.568
[24]  [460/1724] loss: 0.591, ave_loss: 0.569
[25]  [480/1724] loss: 0.520, ave_loss: 0.567
[26]  [500/1724] loss: 0.509, ave_loss: 0.565
[27]  [520/1724] loss: 0.551, ave_loss: 0.565
[28]  [540/1724] loss: 0.548, ave_loss: 0.564
[29]  [560/1724] loss: 0.610, ave_loss: 0.566
[30]  [580/1724] loss: 0.476, ave_loss: 0.563
[31]  [600/1724] loss: 0.714, ave_loss: 0.567
[32]  [620/1724] loss: 0.554, ave_loss: 0.567
[33]  [640/1724] loss: 0.505, ave_loss: 0.565
[34]  [660/1724] loss: 0.484, ave_loss: 0.563
[35]  [680/1724] loss: 0.594, ave_loss: 0.564
[36]  [700/1724] loss: 0.638, ave_loss: 0.566
[37]  [720/1724] loss: 0.555, ave_loss: 0.565
[38]  [740/1724] loss: 0.722, ave_loss: 0.570
[39]  [760/1724] loss: 0.500, ave_loss: 0.568
[40]  [780/1724] loss: 0.517, ave_loss: 0.567
[41]  [800/1724] loss: 0.526, ave_loss: 0.566
[42]  [820/1724] loss: 0.543, ave_loss: 0.565
[43]  [840/1724] loss: 0.475, ave_loss: 0.563
[44]  [860/1724] loss: 0.580, ave_loss: 0.563
[45]  [880/1724] loss: 0.512, ave_loss: 0.562
[46]  [900/1724] loss: 0.531, ave_loss: 0.561
[47]  [920/1724] loss: 0.514, ave_loss: 0.560
[48]  [940/1724] loss: 0.532, ave_loss: 0.560
[49]  [960/1724] loss: 0.504, ave_loss: 0.559
[50]  [980/1724] loss: 0.441, ave_loss: 0.556
[51]  [1000/1724] loss: 0.662, ave_loss: 0.558
[52]  [1020/1724] loss: 0.473, ave_loss: 0.557
[53]  [1040/1724] loss: 0.582, ave_loss: 0.557
[54]  [1060/1724] loss: 0.458, ave_loss: 0.555
[55]  [1080/1724] loss: 0.445, ave_loss: 0.553
[56]  [1100/1724] loss: 0.567, ave_loss: 0.554
[57]  [1120/1724] loss: 0.549, ave_loss: 0.554
[58]  [1140/1724] loss: 0.535, ave_loss: 0.553
[59]  [1160/1724] loss: 0.804, ave_loss: 0.558
[60]  [1180/1724] loss: 0.644, ave_loss: 0.559
[61]  [1200/1724] loss: 0.684, ave_loss: 0.561
[62]  [1220/1724] loss: 0.750, ave_loss: 0.564
[63]  [1240/1724] loss: 0.553, ave_loss: 0.564
[64]  [1260/1724] loss: 0.415, ave_loss: 0.562
[65]  [1280/1724] loss: 0.558, ave_loss: 0.562
[66]  [1300/1724] loss: 0.572, ave_loss: 0.562
[67]  [1320/1724] loss: 0.523, ave_loss: 0.561
[68]  [1340/1724] loss: 0.512, ave_loss: 0.560
[69]  [1360/1724] loss: 0.434, ave_loss: 0.559
[70]  [1380/1724] loss: 0.541, ave_loss: 0.558
[71]  [1400/1724] loss: 0.618, ave_loss: 0.559
[72]  [1420/1724] loss: 0.502, ave_loss: 0.558
[73]  [1440/1724] loss: 0.439, ave_loss: 0.557
[74]  [1460/1724] loss: 0.551, ave_loss: 0.557
[75]  [1480/1724] loss: 0.487, ave_loss: 0.556
[76]  [1500/1724] loss: 0.596, ave_loss: 0.556
[77]  [1520/1724] loss: 0.480, ave_loss: 0.555
[78]  [1540/1724] loss: 0.661, ave_loss: 0.557
[79]  [1560/1724] loss: 0.676, ave_loss: 0.558
[80]  [1580/1724] loss: 0.479, ave_loss: 0.557
[81]  [1600/1724] loss: 0.466, ave_loss: 0.556
[82]  [1620/1724] loss: 0.440, ave_loss: 0.555
[83]  [1640/1724] loss: 0.671, ave_loss: 0.556
[84]  [1660/1724] loss: 0.510, ave_loss: 0.555
[85]  [1680/1724] loss: 0.458, ave_loss: 0.554
[86]  [1700/1724] loss: 0.535, ave_loss: 0.554
[87]  [1720/1724] loss: 0.523, ave_loss: 0.554
[88]  [1740/1724] loss: 0.516, ave_loss: 0.553

Finished Training finishing at 2021-08-30 18:24:19.268597
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.533e-01
Validation Loss: 2.255e+05
Validation ROC: 0.3762
Saving model
19.812064965197216 epochs left to go

Training Epoch 9.187935034802784/30 starting at 2021-08-30 18:25:17.119079
[1]  [0/1724] loss: 0.636, ave_loss: 0.636
[2]  [20/1724] loss: 0.570, ave_loss: 0.603
[3]  [40/1724] loss: 0.470, ave_loss: 0.558
[4]  [60/1724] loss: 0.573, ave_loss: 0.562
[5]  [80/1724] loss: 0.575, ave_loss: 0.565
[6]  [100/1724] loss: 0.586, ave_loss: 0.568
[7]  [120/1724] loss: 0.677, ave_loss: 0.584
[8]  [140/1724] loss: 0.519, ave_loss: 0.576
[9]  [160/1724] loss: 0.521, ave_loss: 0.570
[10]  [180/1724] loss: 0.485, ave_loss: 0.561
[11]  [200/1724] loss: 0.471, ave_loss: 0.553
[12]  [220/1724] loss: 0.508, ave_loss: 0.549
[13]  [240/1724] loss: 0.536, ave_loss: 0.548
[14]  [260/1724] loss: 0.753, ave_loss: 0.563
[15]  [280/1724] loss: 0.748, ave_loss: 0.575
[16]  [300/1724] loss: 0.458, ave_loss: 0.568
[17]  [320/1724] loss: 0.586, ave_loss: 0.569
[18]  [340/1724] loss: 0.601, ave_loss: 0.571
[19]  [360/1724] loss: 0.584, ave_loss: 0.571
[20]  [380/1724] loss: 0.560, ave_loss: 0.571
[21]  [400/1724] loss: 0.510, ave_loss: 0.568
[22]  [420/1724] loss: 0.443, ave_loss: 0.562
[23]  [440/1724] loss: 0.577, ave_loss: 0.563
[24]  [460/1724] loss: 0.567, ave_loss: 0.563
[25]  [480/1724] loss: 0.533, ave_loss: 0.562
[26]  [500/1724] loss: 0.496, ave_loss: 0.559
[27]  [520/1724] loss: 0.598, ave_loss: 0.561
[28]  [540/1724] loss: 0.603, ave_loss: 0.562
[29]  [560/1724] loss: 0.508, ave_loss: 0.560
[30]  [580/1724] loss: 0.587, ave_loss: 0.561
[31]  [600/1724] loss: 0.531, ave_loss: 0.560
[32]  [620/1724] loss: 0.686, ave_loss: 0.564
[33]  [640/1724] loss: 0.624, ave_loss: 0.566
[34]  [660/1724] loss: 0.627, ave_loss: 0.568
[35]  [680/1724] loss: 0.504, ave_loss: 0.566
[36]  [700/1724] loss: 0.446, ave_loss: 0.563
[37]  [720/1724] loss: 0.525, ave_loss: 0.562
[38]  [740/1724] loss: 0.667, ave_loss: 0.564
[39]  [760/1724] loss: 0.553, ave_loss: 0.564
[40]  [780/1724] loss: 0.602, ave_loss: 0.565
[41]  [800/1724] loss: 0.449, ave_loss: 0.562
[42]  [820/1724] loss: 0.547, ave_loss: 0.562
[43]  [840/1724] loss: 0.510, ave_loss: 0.561
[44]  [860/1724] loss: 0.488, ave_loss: 0.559
[45]  [880/1724] loss: 0.555, ave_loss: 0.559
[46]  [900/1724] loss: 0.602, ave_loss: 0.560
[47]  [920/1724] loss: 0.556, ave_loss: 0.560
[48]  [940/1724] loss: 0.679, ave_loss: 0.562
[49]  [960/1724] loss: 0.559, ave_loss: 0.562
[50]  [980/1724] loss: 0.483, ave_loss: 0.561
[51]  [1000/1724] loss: 0.629, ave_loss: 0.562
[52]  [1020/1724] loss: 0.573, ave_loss: 0.562
[53]  [1040/1724] loss: 0.551, ave_loss: 0.562
[54]  [1060/1724] loss: 0.642, ave_loss: 0.563
[55]  [1080/1724] loss: 0.439, ave_loss: 0.561
[56]  [1100/1724] loss: 0.467, ave_loss: 0.559
[57]  [1120/1724] loss: 0.477, ave_loss: 0.558
[58]  [1140/1724] loss: 0.542, ave_loss: 0.558
[59]  [1160/1724] loss: 0.576, ave_loss: 0.558
[60]  [1180/1724] loss: 0.651, ave_loss: 0.560
[61]  [1200/1724] loss: 0.569, ave_loss: 0.560
[62]  [1220/1724] loss: 0.470, ave_loss: 0.558
[63]  [1240/1724] loss: 0.615, ave_loss: 0.559
[64]  [1260/1724] loss: 0.468, ave_loss: 0.558
[65]  [1280/1724] loss: 0.729, ave_loss: 0.560
[66]  [1300/1724] loss: 0.565, ave_loss: 0.561
[67]  [1320/1724] loss: 0.628, ave_loss: 0.562
[68]  [1340/1724] loss: 0.682, ave_loss: 0.563
[69]  [1360/1724] loss: 0.491, ave_loss: 0.562
[70]  [1380/1724] loss: 0.552, ave_loss: 0.562
[71]  [1400/1724] loss: 0.550, ave_loss: 0.562
[72]  [1420/1724] loss: 0.547, ave_loss: 0.562
[73]  [1440/1724] loss: 0.544, ave_loss: 0.561
[74]  [1460/1724] loss: 0.536, ave_loss: 0.561
[75]  [1480/1724] loss: 0.523, ave_loss: 0.561
[76]  [1500/1724] loss: 0.475, ave_loss: 0.560
[77]  [1520/1724] loss: 0.562, ave_loss: 0.560
[78]  [1540/1724] loss: 0.571, ave_loss: 0.560
[79]  [1560/1724] loss: 0.478, ave_loss: 0.559
[80]  [1580/1724] loss: 0.651, ave_loss: 0.560
[81]  [1600/1724] loss: 0.629, ave_loss: 0.561
[82]  [1620/1724] loss: 0.550, ave_loss: 0.561
[83]  [1640/1724] loss: 0.494, ave_loss: 0.560
[84]  [1660/1724] loss: 0.539, ave_loss: 0.559
[85]  [1680/1724] loss: 0.559, ave_loss: 0.559
[86]  [1700/1724] loss: 0.514, ave_loss: 0.559
[87]  [1720/1724] loss: 0.685, ave_loss: 0.560
[88]  [1740/1724] loss: 0.574, ave_loss: 0.561

Finished Training finishing at 2021-08-30 18:27:30.526134
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.606e-01
Validation Loss: 2.229e+05
Validation ROC: 0.4174
Saving model
18.791183294663576 epochs left to go

Training Epoch 10.208816705336426/30 starting at 2021-08-30 18:28:27.688032
[1]  [0/1724] loss: 0.615, ave_loss: 0.615
[2]  [20/1724] loss: 0.585, ave_loss: 0.600
[3]  [40/1724] loss: 0.527, ave_loss: 0.576
[4]  [60/1724] loss: 0.588, ave_loss: 0.579
[5]  [80/1724] loss: 0.474, ave_loss: 0.558
[6]  [100/1724] loss: 0.485, ave_loss: 0.546
[7]  [120/1724] loss: 0.507, ave_loss: 0.540
[8]  [140/1724] loss: 0.443, ave_loss: 0.528
[9]  [160/1724] loss: 0.504, ave_loss: 0.525
[10]  [180/1724] loss: 0.645, ave_loss: 0.537
[11]  [200/1724] loss: 0.571, ave_loss: 0.540
[12]  [220/1724] loss: 0.570, ave_loss: 0.543
[13]  [240/1724] loss: 0.566, ave_loss: 0.545
[14]  [260/1724] loss: 0.455, ave_loss: 0.538
[15]  [280/1724] loss: 0.435, ave_loss: 0.531
[16]  [300/1724] loss: 0.511, ave_loss: 0.530
[17]  [320/1724] loss: 0.577, ave_loss: 0.533
[18]  [340/1724] loss: 0.670, ave_loss: 0.540
[19]  [360/1724] loss: 0.460, ave_loss: 0.536
[20]  [380/1724] loss: 0.473, ave_loss: 0.533
[21]  [400/1724] loss: 0.608, ave_loss: 0.537
[22]  [420/1724] loss: 0.447, ave_loss: 0.533
[23]  [440/1724] loss: 0.511, ave_loss: 0.532
[24]  [460/1724] loss: 0.537, ave_loss: 0.532
[25]  [480/1724] loss: 0.504, ave_loss: 0.531
[26]  [500/1724] loss: 0.490, ave_loss: 0.529
[27]  [520/1724] loss: 0.706, ave_loss: 0.536
[28]  [540/1724] loss: 0.625, ave_loss: 0.539
[29]  [560/1724] loss: 0.467, ave_loss: 0.536
[30]  [580/1724] loss: 0.525, ave_loss: 0.536
[31]  [600/1724] loss: 0.509, ave_loss: 0.535
[32]  [620/1724] loss: 0.424, ave_loss: 0.532
[33]  [640/1724] loss: 0.548, ave_loss: 0.532
[34]  [660/1724] loss: 0.525, ave_loss: 0.532
[35]  [680/1724] loss: 0.508, ave_loss: 0.531
[36]  [700/1724] loss: 0.483, ave_loss: 0.530
[37]  [720/1724] loss: 0.642, ave_loss: 0.533
[38]  [740/1724] loss: 0.519, ave_loss: 0.533
[39]  [760/1724] loss: 0.565, ave_loss: 0.533
[40]  [780/1724] loss: 0.543, ave_loss: 0.534
[41]  [800/1724] loss: 0.628, ave_loss: 0.536
[42]  [820/1724] loss: 0.506, ave_loss: 0.535
[43]  [840/1724] loss: 0.421, ave_loss: 0.533
[44]  [860/1724] loss: 0.377, ave_loss: 0.529
[45]  [880/1724] loss: 0.546, ave_loss: 0.529
[46]  [900/1724] loss: 0.475, ave_loss: 0.528
[47]  [920/1724] loss: 0.507, ave_loss: 0.528
[48]  [940/1724] loss: 0.513, ave_loss: 0.527
[49]  [960/1724] loss: 0.509, ave_loss: 0.527
[50]  [980/1724] loss: 0.576, ave_loss: 0.528
[51]  [1000/1724] loss: 0.391, ave_loss: 0.525
[52]  [1020/1724] loss: 0.648, ave_loss: 0.528
[53]  [1040/1724] loss: 0.604, ave_loss: 0.529
[54]  [1060/1724] loss: 0.686, ave_loss: 0.532
[55]  [1080/1724] loss: 0.574, ave_loss: 0.533
[56]  [1100/1724] loss: 0.395, ave_loss: 0.530
[57]  [1120/1724] loss: 0.533, ave_loss: 0.530
[58]  [1140/1724] loss: 0.691, ave_loss: 0.533
[59]  [1160/1724] loss: 0.672, ave_loss: 0.536
[60]  [1180/1724] loss: 0.557, ave_loss: 0.536
[61]  [1200/1724] loss: 0.471, ave_loss: 0.535
[62]  [1220/1724] loss: 0.582, ave_loss: 0.536
[63]  [1240/1724] loss: 0.548, ave_loss: 0.536
[64]  [1260/1724] loss: 0.713, ave_loss: 0.539
[65]  [1280/1724] loss: 0.603, ave_loss: 0.540
[66]  [1300/1724] loss: 0.498, ave_loss: 0.539
[67]  [1320/1724] loss: 0.563, ave_loss: 0.539
[68]  [1340/1724] loss: 0.509, ave_loss: 0.539
[69]  [1360/1724] loss: 0.440, ave_loss: 0.537
[70]  [1380/1724] loss: 0.498, ave_loss: 0.537
[71]  [1400/1724] loss: 0.479, ave_loss: 0.536
[72]  [1420/1724] loss: 0.553, ave_loss: 0.536
[73]  [1440/1724] loss: 0.657, ave_loss: 0.538
[74]  [1460/1724] loss: 0.441, ave_loss: 0.537
[75]  [1480/1724] loss: 0.489, ave_loss: 0.536
[76]  [1500/1724] loss: 0.533, ave_loss: 0.536
[77]  [1520/1724] loss: 0.524, ave_loss: 0.536
[78]  [1540/1724] loss: 0.510, ave_loss: 0.535
[79]  [1560/1724] loss: 0.529, ave_loss: 0.535
[80]  [1580/1724] loss: 0.518, ave_loss: 0.535
[81]  [1600/1724] loss: 0.493, ave_loss: 0.535
[82]  [1620/1724] loss: 0.514, ave_loss: 0.534
[83]  [1640/1724] loss: 0.634, ave_loss: 0.536
[84]  [1660/1724] loss: 0.584, ave_loss: 0.536
[85]  [1680/1724] loss: 0.495, ave_loss: 0.536
[86]  [1700/1724] loss: 0.539, ave_loss: 0.536
[87]  [1720/1724] loss: 0.559, ave_loss: 0.536
[88]  [1740/1724] loss: 0.491, ave_loss: 0.535

Finished Training finishing at 2021-08-30 18:30:35.873437
printing_out epoch  11.22969837587007 learning rate: 0.0005153561248318907
0.00036863495918228726
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.355e-01
Validation Loss: 2.302e+05
Validation ROC: 0.4436
Saving model
17.770301624129928 epochs left to go

Training Epoch 11.22969837587007/30 starting at 2021-08-30 18:31:22.457581
[1]  [0/1724] loss: 0.676, ave_loss: 0.676
[2]  [20/1724] loss: 0.640, ave_loss: 0.658
[3]  [40/1724] loss: 0.428, ave_loss: 0.581
[4]  [60/1724] loss: 0.519, ave_loss: 0.566
[5]  [80/1724] loss: 0.486, ave_loss: 0.550
[6]  [100/1724] loss: 0.631, ave_loss: 0.563
[7]  [120/1724] loss: 0.556, ave_loss: 0.562
[8]  [140/1724] loss: 0.606, ave_loss: 0.568
[9]  [160/1724] loss: 0.427, ave_loss: 0.552
[10]  [180/1724] loss: 0.684, ave_loss: 0.565
[11]  [200/1724] loss: 0.513, ave_loss: 0.560
[12]  [220/1724] loss: 0.549, ave_loss: 0.560
[13]  [240/1724] loss: 0.404, ave_loss: 0.548
[14]  [260/1724] loss: 0.610, ave_loss: 0.552
[15]  [280/1724] loss: 0.497, ave_loss: 0.548
[16]  [300/1724] loss: 0.482, ave_loss: 0.544
[17]  [320/1724] loss: 0.791, ave_loss: 0.559
[18]  [340/1724] loss: 0.572, ave_loss: 0.560
[19]  [360/1724] loss: 0.503, ave_loss: 0.557
[20]  [380/1724] loss: 0.504, ave_loss: 0.554
[21]  [400/1724] loss: 0.527, ave_loss: 0.553
[22]  [420/1724] loss: 0.452, ave_loss: 0.548
[23]  [440/1724] loss: 0.570, ave_loss: 0.549
[24]  [460/1724] loss: 0.461, ave_loss: 0.545
[25]  [480/1724] loss: 0.435, ave_loss: 0.541
[26]  [500/1724] loss: 0.644, ave_loss: 0.545
[27]  [520/1724] loss: 0.409, ave_loss: 0.540
[28]  [540/1724] loss: 0.626, ave_loss: 0.543
[29]  [560/1724] loss: 0.579, ave_loss: 0.544
[30]  [580/1724] loss: 0.585, ave_loss: 0.546
[31]  [600/1724] loss: 0.662, ave_loss: 0.549
[32]  [620/1724] loss: 0.435, ave_loss: 0.546
[33]  [640/1724] loss: 0.543, ave_loss: 0.546
[34]  [660/1724] loss: 0.613, ave_loss: 0.548
[35]  [680/1724] loss: 0.502, ave_loss: 0.546
[36]  [700/1724] loss: 0.526, ave_loss: 0.546
[37]  [720/1724] loss: 0.472, ave_loss: 0.544
[38]  [740/1724] loss: 0.542, ave_loss: 0.544
[39]  [760/1724] loss: 0.544, ave_loss: 0.544
[40]  [780/1724] loss: 0.429, ave_loss: 0.541
[41]  [800/1724] loss: 0.451, ave_loss: 0.539
[42]  [820/1724] loss: 0.578, ave_loss: 0.540
[43]  [840/1724] loss: 0.570, ave_loss: 0.540
[44]  [860/1724] loss: 0.697, ave_loss: 0.544
[45]  [880/1724] loss: 0.539, ave_loss: 0.544
[46]  [900/1724] loss: 0.563, ave_loss: 0.544
[47]  [920/1724] loss: 0.673, ave_loss: 0.547
[48]  [940/1724] loss: 0.480, ave_loss: 0.545
[49]  [960/1724] loss: 0.500, ave_loss: 0.545
[50]  [980/1724] loss: 0.505, ave_loss: 0.544
[51]  [1000/1724] loss: 0.534, ave_loss: 0.544
[52]  [1020/1724] loss: 0.453, ave_loss: 0.542
[53]  [1040/1724] loss: 0.515, ave_loss: 0.541
[54]  [1060/1724] loss: 0.539, ave_loss: 0.541
[55]  [1080/1724] loss: 0.597, ave_loss: 0.542
[56]  [1100/1724] loss: 0.430, ave_loss: 0.540
[57]  [1120/1724] loss: 0.498, ave_loss: 0.540
[58]  [1140/1724] loss: 0.586, ave_loss: 0.540
[59]  [1160/1724] loss: 0.421, ave_loss: 0.538
[60]  [1180/1724] loss: 0.464, ave_loss: 0.537
[61]  [1200/1724] loss: 0.608, ave_loss: 0.538
[62]  [1220/1724] loss: 0.433, ave_loss: 0.536
[63]  [1240/1724] loss: 0.631, ave_loss: 0.538
[64]  [1260/1724] loss: 0.639, ave_loss: 0.540
[65]  [1280/1724] loss: 0.637, ave_loss: 0.541
[66]  [1300/1724] loss: 0.432, ave_loss: 0.539
[67]  [1320/1724] loss: 0.585, ave_loss: 0.540
[68]  [1340/1724] loss: 0.479, ave_loss: 0.539
[69]  [1360/1724] loss: 0.403, ave_loss: 0.537
[70]  [1380/1724] loss: 0.558, ave_loss: 0.538
[71]  [1400/1724] loss: 0.507, ave_loss: 0.537
[72]  [1420/1724] loss: 0.478, ave_loss: 0.536
[73]  [1440/1724] loss: 0.537, ave_loss: 0.536
[74]  [1460/1724] loss: 0.386, ave_loss: 0.534
[75]  [1480/1724] loss: 0.451, ave_loss: 0.533
[76]  [1500/1724] loss: 0.579, ave_loss: 0.534
[77]  [1520/1724] loss: 0.497, ave_loss: 0.533
[78]  [1540/1724] loss: 0.435, ave_loss: 0.532
[79]  [1560/1724] loss: 0.451, ave_loss: 0.531
[80]  [1580/1724] loss: 0.598, ave_loss: 0.532
[81]  [1600/1724] loss: 0.599, ave_loss: 0.533
[82]  [1620/1724] loss: 0.510, ave_loss: 0.532
[83]  [1640/1724] loss: 0.552, ave_loss: 0.533
[84]  [1660/1724] loss: 0.541, ave_loss: 0.533
[85]  [1680/1724] loss: 0.531, ave_loss: 0.533
[86]  [1700/1724] loss: 0.728, ave_loss: 0.535
[87]  [1720/1724] loss: 0.616, ave_loss: 0.536
[88]  [1740/1724] loss: 0.593, ave_loss: 0.537

Finished Training finishing at 2021-08-30 18:33:21.353362
printing_out epoch  12.250580046403712 learning rate: 0.0005153561248318907
0.0003575759104068186
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.365e-01
Validation Loss: 2.324e+05
Validation ROC: 0.5135
Saving model
16.749419953596288 epochs left to go

Training Epoch 12.250580046403712/30 starting at 2021-08-30 18:34:04.932586
[1]  [0/1724] loss: 0.580, ave_loss: 0.580
[2]  [20/1724] loss: 0.404, ave_loss: 0.492
[3]  [40/1724] loss: 0.591, ave_loss: 0.525
[4]  [60/1724] loss: 0.485, ave_loss: 0.515
[5]  [80/1724] loss: 0.452, ave_loss: 0.502
[6]  [100/1724] loss: 0.512, ave_loss: 0.504
[7]  [120/1724] loss: 0.526, ave_loss: 0.507
[8]  [140/1724] loss: 0.458, ave_loss: 0.501
[9]  [160/1724] loss: 0.682, ave_loss: 0.521
[10]  [180/1724] loss: 0.590, ave_loss: 0.528
[11]  [200/1724] loss: 0.479, ave_loss: 0.523
[12]  [220/1724] loss: 0.555, ave_loss: 0.526
[13]  [240/1724] loss: 0.576, ave_loss: 0.530
[14]  [260/1724] loss: 0.461, ave_loss: 0.525
[15]  [280/1724] loss: 0.528, ave_loss: 0.525
[16]  [300/1724] loss: 0.530, ave_loss: 0.525
[17]  [320/1724] loss: 0.530, ave_loss: 0.526
[18]  [340/1724] loss: 0.513, ave_loss: 0.525
[19]  [360/1724] loss: 0.725, ave_loss: 0.536
[20]  [380/1724] loss: 0.477, ave_loss: 0.533
[21]  [400/1724] loss: 0.594, ave_loss: 0.536
[22]  [420/1724] loss: 0.467, ave_loss: 0.532
[23]  [440/1724] loss: 0.481, ave_loss: 0.530
[24]  [460/1724] loss: 0.433, ave_loss: 0.526
[25]  [480/1724] loss: 0.461, ave_loss: 0.524
[26]  [500/1724] loss: 0.579, ave_loss: 0.526
[27]  [520/1724] loss: 0.540, ave_loss: 0.526
[28]  [540/1724] loss: 0.527, ave_loss: 0.526
[29]  [560/1724] loss: 0.525, ave_loss: 0.526
[30]  [580/1724] loss: 0.604, ave_loss: 0.529
[31]  [600/1724] loss: 0.547, ave_loss: 0.529
[32]  [620/1724] loss: 0.460, ave_loss: 0.527
[33]  [640/1724] loss: 0.483, ave_loss: 0.526
[34]  [660/1724] loss: 0.366, ave_loss: 0.521
[35]  [680/1724] loss: 0.404, ave_loss: 0.518
[36]  [700/1724] loss: 0.605, ave_loss: 0.520
[37]  [720/1724] loss: 0.389, ave_loss: 0.517
[38]  [740/1724] loss: 0.595, ave_loss: 0.519
[39]  [760/1724] loss: 0.587, ave_loss: 0.520
[40]  [780/1724] loss: 0.541, ave_loss: 0.521
[41]  [800/1724] loss: 0.503, ave_loss: 0.521
[42]  [820/1724] loss: 0.470, ave_loss: 0.519
[43]  [840/1724] loss: 0.395, ave_loss: 0.516
[44]  [860/1724] loss: 0.690, ave_loss: 0.520
[45]  [880/1724] loss: 0.536, ave_loss: 0.521
[46]  [900/1724] loss: 0.504, ave_loss: 0.520
[47]  [920/1724] loss: 0.466, ave_loss: 0.519
[48]  [940/1724] loss: 0.514, ave_loss: 0.519
[49]  [960/1724] loss: 0.556, ave_loss: 0.520
[50]  [980/1724] loss: 0.479, ave_loss: 0.519
[51]  [1000/1724] loss: 0.437, ave_loss: 0.517
[52]  [1020/1724] loss: 0.536, ave_loss: 0.518
[53]  [1040/1724] loss: 0.553, ave_loss: 0.518
[54]  [1060/1724] loss: 0.584, ave_loss: 0.520
[55]  [1080/1724] loss: 0.571, ave_loss: 0.521
[56]  [1100/1724] loss: 0.573, ave_loss: 0.522
[57]  [1120/1724] loss: 0.497, ave_loss: 0.521
[58]  [1140/1724] loss: 0.552, ave_loss: 0.522
[59]  [1160/1724] loss: 0.585, ave_loss: 0.523
[60]  [1180/1724] loss: 0.368, ave_loss: 0.520
[61]  [1200/1724] loss: 0.512, ave_loss: 0.520
[62]  [1220/1724] loss: 0.503, ave_loss: 0.520
[63]  [1240/1724] loss: 0.611, ave_loss: 0.521
[64]  [1260/1724] loss: 0.452, ave_loss: 0.520
[65]  [1280/1724] loss: 0.437, ave_loss: 0.519
[66]  [1300/1724] loss: 0.500, ave_loss: 0.519
[67]  [1320/1724] loss: 0.583, ave_loss: 0.520
[68]  [1340/1724] loss: 0.482, ave_loss: 0.519
[69]  [1360/1724] loss: 0.558, ave_loss: 0.520
[70]  [1380/1724] loss: 0.481, ave_loss: 0.519
[71]  [1400/1724] loss: 0.655, ave_loss: 0.521
[72]  [1420/1724] loss: 0.595, ave_loss: 0.522
[73]  [1440/1724] loss: 0.532, ave_loss: 0.522
[74]  [1460/1724] loss: 0.476, ave_loss: 0.521
[75]  [1480/1724] loss: 0.788, ave_loss: 0.525
[76]  [1500/1724] loss: 0.632, ave_loss: 0.526
[77]  [1520/1724] loss: 0.498, ave_loss: 0.526
[78]  [1540/1724] loss: 0.458, ave_loss: 0.525
[79]  [1560/1724] loss: 0.421, ave_loss: 0.524
[80]  [1580/1724] loss: 0.428, ave_loss: 0.523
[81]  [1600/1724] loss: 0.488, ave_loss: 0.522
[82]  [1620/1724] loss: 0.506, ave_loss: 0.522
[83]  [1640/1724] loss: 0.488, ave_loss: 0.522
[84]  [1660/1724] loss: 0.543, ave_loss: 0.522
[85]  [1680/1724] loss: 0.553, ave_loss: 0.522
[86]  [1700/1724] loss: 0.574, ave_loss: 0.523
[87]  [1720/1724] loss: 0.537, ave_loss: 0.523
[88]  [1740/1724] loss: 0.607, ave_loss: 0.524

Finished Training finishing at 2021-08-30 18:36:06.371751
printing_out epoch  13.271461716937354 learning rate: 0.0005153561248318907
0.00034684863309461403
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.239e-01
Validation Loss: 2.315e+05
Validation ROC: 0.6039
Saving model
15.728538283062646 epochs left to go

Training Epoch 13.271461716937354/30 starting at 2021-08-30 18:36:48.837410
[1]  [0/1724] loss: 0.665, ave_loss: 0.665
[2]  [20/1724] loss: 0.408, ave_loss: 0.536
[3]  [40/1724] loss: 0.315, ave_loss: 0.463
[4]  [60/1724] loss: 0.627, ave_loss: 0.504
[5]  [80/1724] loss: 0.579, ave_loss: 0.519
[6]  [100/1724] loss: 0.519, ave_loss: 0.519
[7]  [120/1724] loss: 0.568, ave_loss: 0.526
[8]  [140/1724] loss: 0.608, ave_loss: 0.536
[9]  [160/1724] loss: 0.550, ave_loss: 0.538
[10]  [180/1724] loss: 0.581, ave_loss: 0.542
[11]  [200/1724] loss: 0.666, ave_loss: 0.553
[12]  [220/1724] loss: 0.504, ave_loss: 0.549
[13]  [240/1724] loss: 0.462, ave_loss: 0.542
[14]  [260/1724] loss: 0.654, ave_loss: 0.550
[15]  [280/1724] loss: 0.524, ave_loss: 0.549
[16]  [300/1724] loss: 0.460, ave_loss: 0.543
[17]  [320/1724] loss: 0.636, ave_loss: 0.549
[18]  [340/1724] loss: 0.406, ave_loss: 0.541
[19]  [360/1724] loss: 0.528, ave_loss: 0.540
[20]  [380/1724] loss: 0.496, ave_loss: 0.538
[21]  [400/1724] loss: 0.504, ave_loss: 0.536
[22]  [420/1724] loss: 0.375, ave_loss: 0.529
[23]  [440/1724] loss: 0.534, ave_loss: 0.529
[24]  [460/1724] loss: 0.611, ave_loss: 0.533
[25]  [480/1724] loss: 0.489, ave_loss: 0.531
[26]  [500/1724] loss: 0.593, ave_loss: 0.533
[27]  [520/1724] loss: 0.585, ave_loss: 0.535
[28]  [540/1724] loss: 0.433, ave_loss: 0.531
[29]  [560/1724] loss: 0.471, ave_loss: 0.529
[30]  [580/1724] loss: 0.570, ave_loss: 0.531
[31]  [600/1724] loss: 0.669, ave_loss: 0.535
[32]  [620/1724] loss: 0.438, ave_loss: 0.532
[33]  [640/1724] loss: 0.419, ave_loss: 0.529
[34]  [660/1724] loss: 0.609, ave_loss: 0.531
[35]  [680/1724] loss: 0.382, ave_loss: 0.527
[36]  [700/1724] loss: 0.431, ave_loss: 0.524
[37]  [720/1724] loss: 0.583, ave_loss: 0.526
[38]  [740/1724] loss: 0.509, ave_loss: 0.525
[39]  [760/1724] loss: 0.662, ave_loss: 0.529
[40]  [780/1724] loss: 0.514, ave_loss: 0.528
[41]  [800/1724] loss: 0.558, ave_loss: 0.529
[42]  [820/1724] loss: 0.458, ave_loss: 0.528
[43]  [840/1724] loss: 0.579, ave_loss: 0.529
[44]  [860/1724] loss: 0.419, ave_loss: 0.526
[45]  [880/1724] loss: 0.476, ave_loss: 0.525
[46]  [900/1724] loss: 0.496, ave_loss: 0.524
[47]  [920/1724] loss: 0.470, ave_loss: 0.523
[48]  [940/1724] loss: 0.546, ave_loss: 0.524
[49]  [960/1724] loss: 0.651, ave_loss: 0.526
[50]  [980/1724] loss: 0.541, ave_loss: 0.527
[51]  [1000/1724] loss: 0.570, ave_loss: 0.528
[52]  [1020/1724] loss: 0.546, ave_loss: 0.528
[53]  [1040/1724] loss: 0.598, ave_loss: 0.529
[54]  [1060/1724] loss: 0.427, ave_loss: 0.527
[55]  [1080/1724] loss: 0.459, ave_loss: 0.526
[56]  [1100/1724] loss: 0.720, ave_loss: 0.530
[57]  [1120/1724] loss: 0.395, ave_loss: 0.527
[58]  [1140/1724] loss: 0.611, ave_loss: 0.529
[59]  [1160/1724] loss: 0.347, ave_loss: 0.526
[60]  [1180/1724] loss: 0.588, ave_loss: 0.527
[61]  [1200/1724] loss: 0.552, ave_loss: 0.527
[62]  [1220/1724] loss: 0.502, ave_loss: 0.527
[63]  [1240/1724] loss: 0.642, ave_loss: 0.528
[64]  [1260/1724] loss: 0.437, ave_loss: 0.527
[65]  [1280/1724] loss: 0.646, ave_loss: 0.529
[66]  [1300/1724] loss: 0.500, ave_loss: 0.528
[67]  [1320/1724] loss: 0.604, ave_loss: 0.529
[68]  [1340/1724] loss: 0.518, ave_loss: 0.529
[69]  [1360/1724] loss: 0.465, ave_loss: 0.528
[70]  [1380/1724] loss: 0.464, ave_loss: 0.527
[71]  [1400/1724] loss: 0.470, ave_loss: 0.527
[72]  [1420/1724] loss: 0.523, ave_loss: 0.527
[73]  [1440/1724] loss: 0.463, ave_loss: 0.526
[74]  [1460/1724] loss: 0.581, ave_loss: 0.526
[75]  [1480/1724] loss: 0.658, ave_loss: 0.528
[76]  [1500/1724] loss: 0.434, ave_loss: 0.527
[77]  [1520/1724] loss: 0.358, ave_loss: 0.525
[78]  [1540/1724] loss: 0.556, ave_loss: 0.525
[79]  [1560/1724] loss: 0.667, ave_loss: 0.527
[80]  [1580/1724] loss: 0.472, ave_loss: 0.526
[81]  [1600/1724] loss: 0.526, ave_loss: 0.526
[82]  [1620/1724] loss: 0.317, ave_loss: 0.524
[83]  [1640/1724] loss: 0.531, ave_loss: 0.524
[84]  [1660/1724] loss: 0.509, ave_loss: 0.524
[85]  [1680/1724] loss: 0.584, ave_loss: 0.524
[86]  [1700/1724] loss: 0.453, ave_loss: 0.524
[87]  [1720/1724] loss: 0.533, ave_loss: 0.524
[88]  [1740/1724] loss: 0.570, ave_loss: 0.524

Finished Training finishing at 2021-08-30 18:38:46.207571
printing_out epoch  14.292343387470998 learning rate: 0.0005153561248318907
0.0003364431741017756
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.242e-01
Validation Loss: 2.406e+05
Validation ROC: 0.6176
Saving model
14.707656612529002 epochs left to go

Training Epoch 14.292343387470998/30 starting at 2021-08-30 18:39:32.520781
[1]  [0/1724] loss: 0.694, ave_loss: 0.694
[2]  [20/1724] loss: 0.450, ave_loss: 0.572
[3]  [40/1724] loss: 0.526, ave_loss: 0.557
[4]  [60/1724] loss: 0.592, ave_loss: 0.566
[5]  [80/1724] loss: 0.534, ave_loss: 0.559
[6]  [100/1724] loss: 0.554, ave_loss: 0.558
[7]  [120/1724] loss: 0.585, ave_loss: 0.562
[8]  [140/1724] loss: 0.353, ave_loss: 0.536
[9]  [160/1724] loss: 0.603, ave_loss: 0.543
[10]  [180/1724] loss: 0.487, ave_loss: 0.538
[11]  [200/1724] loss: 0.481, ave_loss: 0.533
[12]  [220/1724] loss: 0.540, ave_loss: 0.533
[13]  [240/1724] loss: 0.513, ave_loss: 0.532
[14]  [260/1724] loss: 0.498, ave_loss: 0.529
[15]  [280/1724] loss: 0.521, ave_loss: 0.529
[16]  [300/1724] loss: 0.427, ave_loss: 0.522
[17]  [320/1724] loss: 0.480, ave_loss: 0.520
[18]  [340/1724] loss: 0.546, ave_loss: 0.521
[19]  [360/1724] loss: 0.635, ave_loss: 0.527
[20]  [380/1724] loss: 0.461, ave_loss: 0.524
[21]  [400/1724] loss: 0.579, ave_loss: 0.527
[22]  [420/1724] loss: 0.485, ave_loss: 0.525
[23]  [440/1724] loss: 0.462, ave_loss: 0.522
[24]  [460/1724] loss: 0.517, ave_loss: 0.522
[25]  [480/1724] loss: 0.511, ave_loss: 0.521
[26]  [500/1724] loss: 0.619, ave_loss: 0.525
[27]  [520/1724] loss: 0.489, ave_loss: 0.524
[28]  [540/1724] loss: 0.344, ave_loss: 0.517
[29]  [560/1724] loss: 0.562, ave_loss: 0.519
[30]  [580/1724] loss: 0.532, ave_loss: 0.519
[31]  [600/1724] loss: 0.392, ave_loss: 0.515
[32]  [620/1724] loss: 0.441, ave_loss: 0.513
[33]  [640/1724] loss: 0.482, ave_loss: 0.512
[34]  [660/1724] loss: 0.458, ave_loss: 0.510
[35]  [680/1724] loss: 0.477, ave_loss: 0.509
[36]  [700/1724] loss: 0.566, ave_loss: 0.511
[37]  [720/1724] loss: 0.613, ave_loss: 0.514
[38]  [740/1724] loss: 0.504, ave_loss: 0.513
[39]  [760/1724] loss: 0.514, ave_loss: 0.513
[40]  [780/1724] loss: 0.477, ave_loss: 0.513
[41]  [800/1724] loss: 0.529, ave_loss: 0.513
[42]  [820/1724] loss: 0.392, ave_loss: 0.510
[43]  [840/1724] loss: 0.396, ave_loss: 0.507
[44]  [860/1724] loss: 0.559, ave_loss: 0.509
[45]  [880/1724] loss: 0.600, ave_loss: 0.511
[46]  [900/1724] loss: 0.382, ave_loss: 0.508
[47]  [920/1724] loss: 0.606, ave_loss: 0.510
[48]  [940/1724] loss: 0.571, ave_loss: 0.511
[49]  [960/1724] loss: 0.516, ave_loss: 0.511
[50]  [980/1724] loss: 0.494, ave_loss: 0.511
[51]  [1000/1724] loss: 0.447, ave_loss: 0.510
[52]  [1020/1724] loss: 0.512, ave_loss: 0.510
[53]  [1040/1724] loss: 0.582, ave_loss: 0.511
[54]  [1060/1724] loss: 0.441, ave_loss: 0.510
[55]  [1080/1724] loss: 0.475, ave_loss: 0.509
[56]  [1100/1724] loss: 0.434, ave_loss: 0.508
[57]  [1120/1724] loss: 0.463, ave_loss: 0.507
[58]  [1140/1724] loss: 0.595, ave_loss: 0.509
[59]  [1160/1724] loss: 0.398, ave_loss: 0.507
[60]  [1180/1724] loss: 0.386, ave_loss: 0.505
[61]  [1200/1724] loss: 0.459, ave_loss: 0.504
[62]  [1220/1724] loss: 0.457, ave_loss: 0.503
[63]  [1240/1724] loss: 0.465, ave_loss: 0.503
[64]  [1260/1724] loss: 0.587, ave_loss: 0.504
[65]  [1280/1724] loss: 0.624, ave_loss: 0.506
[66]  [1300/1724] loss: 0.522, ave_loss: 0.506
[67]  [1320/1724] loss: 0.514, ave_loss: 0.506
[68]  [1340/1724] loss: 0.528, ave_loss: 0.506
[69]  [1360/1724] loss: 0.562, ave_loss: 0.507
[70]  [1380/1724] loss: 0.594, ave_loss: 0.508
[71]  [1400/1724] loss: 0.574, ave_loss: 0.509
[72]  [1420/1724] loss: 0.453, ave_loss: 0.509
[73]  [1440/1724] loss: 0.547, ave_loss: 0.509
[74]  [1460/1724] loss: 0.427, ave_loss: 0.508
[75]  [1480/1724] loss: 0.498, ave_loss: 0.508
[76]  [1500/1724] loss: 0.453, ave_loss: 0.507
[77]  [1520/1724] loss: 0.451, ave_loss: 0.506
[78]  [1540/1724] loss: 0.415, ave_loss: 0.505
[79]  [1560/1724] loss: 0.590, ave_loss: 0.506
[80]  [1580/1724] loss: 0.410, ave_loss: 0.505
[81]  [1600/1724] loss: 0.495, ave_loss: 0.505
[82]  [1620/1724] loss: 0.537, ave_loss: 0.505
[83]  [1640/1724] loss: 0.483, ave_loss: 0.505
[84]  [1660/1724] loss: 0.481, ave_loss: 0.505
[85]  [1680/1724] loss: 0.543, ave_loss: 0.505
[86]  [1700/1724] loss: 0.558, ave_loss: 0.506
[87]  [1720/1724] loss: 0.530, ave_loss: 0.506
[88]  [1740/1724] loss: 0.684, ave_loss: 0.508

Finished Training finishing at 2021-08-30 18:41:35.167632
printing_out epoch  15.31322505800464 learning rate: 0.0005153561248318907
0.0003263498788787223
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.082e-01
Validation Loss: 2.472e+05
Validation ROC: 0.6326
Saving model
13.68677494199536 epochs left to go

Training Epoch 15.31322505800464/30 starting at 2021-08-30 18:42:21.882969
[1]  [0/1724] loss: 0.762, ave_loss: 0.762
[2]  [20/1724] loss: 0.636, ave_loss: 0.699
[3]  [40/1724] loss: 0.530, ave_loss: 0.643
[4]  [60/1724] loss: 0.535, ave_loss: 0.616
[5]  [80/1724] loss: 0.579, ave_loss: 0.608
[6]  [100/1724] loss: 0.587, ave_loss: 0.605
[7]  [120/1724] loss: 0.655, ave_loss: 0.612
[8]  [140/1724] loss: 0.508, ave_loss: 0.599
[9]  [160/1724] loss: 0.480, ave_loss: 0.586
[10]  [180/1724] loss: 0.469, ave_loss: 0.574
[11]  [200/1724] loss: 0.423, ave_loss: 0.560
[12]  [220/1724] loss: 0.418, ave_loss: 0.549
[13]  [240/1724] loss: 0.614, ave_loss: 0.554
[14]  [260/1724] loss: 0.404, ave_loss: 0.543
[15]  [280/1724] loss: 0.488, ave_loss: 0.539
[16]  [300/1724] loss: 0.356, ave_loss: 0.528
[17]  [320/1724] loss: 0.405, ave_loss: 0.521
[18]  [340/1724] loss: 0.584, ave_loss: 0.524
[19]  [360/1724] loss: 0.451, ave_loss: 0.520
[20]  [380/1724] loss: 0.603, ave_loss: 0.524
[21]  [400/1724] loss: 0.512, ave_loss: 0.524
[22]  [420/1724] loss: 0.590, ave_loss: 0.527
[23]  [440/1724] loss: 0.550, ave_loss: 0.528
[24]  [460/1724] loss: 0.549, ave_loss: 0.529
[25]  [480/1724] loss: 0.522, ave_loss: 0.528
[26]  [500/1724] loss: 0.538, ave_loss: 0.529
[27]  [520/1724] loss: 0.651, ave_loss: 0.533
[28]  [540/1724] loss: 0.520, ave_loss: 0.533
[29]  [560/1724] loss: 0.640, ave_loss: 0.537
[30]  [580/1724] loss: 0.593, ave_loss: 0.538
[31]  [600/1724] loss: 0.411, ave_loss: 0.534
[32]  [620/1724] loss: 0.446, ave_loss: 0.532
[33]  [640/1724] loss: 0.394, ave_loss: 0.527
[34]  [660/1724] loss: 0.473, ave_loss: 0.526
[35]  [680/1724] loss: 0.483, ave_loss: 0.525
[36]  [700/1724] loss: 0.488, ave_loss: 0.524
[37]  [720/1724] loss: 0.498, ave_loss: 0.523
[38]  [740/1724] loss: 0.443, ave_loss: 0.521
[39]  [760/1724] loss: 0.556, ave_loss: 0.522
[40]  [780/1724] loss: 0.578, ave_loss: 0.523
[41]  [800/1724] loss: 0.495, ave_loss: 0.522
[42]  [820/1724] loss: 0.439, ave_loss: 0.520
[43]  [840/1724] loss: 0.502, ave_loss: 0.520
[44]  [860/1724] loss: 0.510, ave_loss: 0.520
[45]  [880/1724] loss: 0.480, ave_loss: 0.519
[46]  [900/1724] loss: 0.515, ave_loss: 0.519
[47]  [920/1724] loss: 0.406, ave_loss: 0.516
[48]  [940/1724] loss: 0.680, ave_loss: 0.520
[49]  [960/1724] loss: 0.475, ave_loss: 0.519
[50]  [980/1724] loss: 0.593, ave_loss: 0.520
[51]  [1000/1724] loss: 0.543, ave_loss: 0.521
[52]  [1020/1724] loss: 0.452, ave_loss: 0.519
[53]  [1040/1724] loss: 0.449, ave_loss: 0.518
[54]  [1060/1724] loss: 0.422, ave_loss: 0.516
[55]  [1080/1724] loss: 0.443, ave_loss: 0.515
[56]  [1100/1724] loss: 0.488, ave_loss: 0.515
[57]  [1120/1724] loss: 0.641, ave_loss: 0.517
[58]  [1140/1724] loss: 0.467, ave_loss: 0.516
[59]  [1160/1724] loss: 0.408, ave_loss: 0.514
[60]  [1180/1724] loss: 0.463, ave_loss: 0.513
[61]  [1200/1724] loss: 0.442, ave_loss: 0.512
[62]  [1220/1724] loss: 0.390, ave_loss: 0.510
[63]  [1240/1724] loss: 0.402, ave_loss: 0.508
[64]  [1260/1724] loss: 0.542, ave_loss: 0.509
[65]  [1280/1724] loss: 0.439, ave_loss: 0.508
[66]  [1300/1724] loss: 0.485, ave_loss: 0.507
[67]  [1320/1724] loss: 0.434, ave_loss: 0.506
[68]  [1340/1724] loss: 0.365, ave_loss: 0.504
[69]  [1360/1724] loss: 0.477, ave_loss: 0.504
[70]  [1380/1724] loss: 0.455, ave_loss: 0.503
[71]  [1400/1724] loss: 0.444, ave_loss: 0.502
[72]  [1420/1724] loss: 0.431, ave_loss: 0.501
[73]  [1440/1724] loss: 0.617, ave_loss: 0.503
[74]  [1460/1724] loss: 0.558, ave_loss: 0.504
[75]  [1480/1724] loss: 0.488, ave_loss: 0.503
[76]  [1500/1724] loss: 0.502, ave_loss: 0.503
[77]  [1520/1724] loss: 0.384, ave_loss: 0.502
[78]  [1540/1724] loss: 0.461, ave_loss: 0.501
[79]  [1560/1724] loss: 0.539, ave_loss: 0.502
[80]  [1580/1724] loss: 0.467, ave_loss: 0.501
[81]  [1600/1724] loss: 0.443, ave_loss: 0.501
[82]  [1620/1724] loss: 0.589, ave_loss: 0.502
[83]  [1640/1724] loss: 0.391, ave_loss: 0.500
[84]  [1660/1724] loss: 0.456, ave_loss: 0.500
[85]  [1680/1724] loss: 0.566, ave_loss: 0.501
[86]  [1700/1724] loss: 0.385, ave_loss: 0.499
[87]  [1720/1724] loss: 0.495, ave_loss: 0.499
[88]  [1740/1724] loss: 0.485, ave_loss: 0.499

Finished Training finishing at 2021-08-30 18:44:20.475056
printing_out epoch  16.334106728538284 learning rate: 0.0005153561248318907
0.00031655938251236066
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.991e-01
Validation Loss: 2.453e+05
Validation ROC: 0.6461
Saving model
12.665893271461716 epochs left to go

Training Epoch 16.334106728538284/30 starting at 2021-08-30 18:45:14.702071
[1]  [0/1724] loss: 0.608, ave_loss: 0.608
[2]  [20/1724] loss: 0.342, ave_loss: 0.475
[3]  [40/1724] loss: 0.539, ave_loss: 0.496
[4]  [60/1724] loss: 0.469, ave_loss: 0.489
[5]  [80/1724] loss: 0.435, ave_loss: 0.479
[6]  [100/1724] loss: 0.625, ave_loss: 0.503
[7]  [120/1724] loss: 0.450, ave_loss: 0.495
[8]  [140/1724] loss: 0.413, ave_loss: 0.485
[9]  [160/1724] loss: 0.541, ave_loss: 0.491
[10]  [180/1724] loss: 0.562, ave_loss: 0.498
[11]  [200/1724] loss: 0.502, ave_loss: 0.499
[12]  [220/1724] loss: 0.527, ave_loss: 0.501
[13]  [240/1724] loss: 0.450, ave_loss: 0.497
[14]  [260/1724] loss: 0.392, ave_loss: 0.490
[15]  [280/1724] loss: 0.523, ave_loss: 0.492
[16]  [300/1724] loss: 0.688, ave_loss: 0.504
[17]  [320/1724] loss: 0.511, ave_loss: 0.505
[18]  [340/1724] loss: 0.626, ave_loss: 0.511
[19]  [360/1724] loss: 0.471, ave_loss: 0.509
[20]  [380/1724] loss: 0.679, ave_loss: 0.518
[21]  [400/1724] loss: 0.501, ave_loss: 0.517
[22]  [420/1724] loss: 0.369, ave_loss: 0.510
[23]  [440/1724] loss: 0.537, ave_loss: 0.511
[24]  [460/1724] loss: 0.393, ave_loss: 0.506
[25]  [480/1724] loss: 0.574, ave_loss: 0.509
[26]  [500/1724] loss: 0.669, ave_loss: 0.515
[27]  [520/1724] loss: 0.594, ave_loss: 0.518
[28]  [540/1724] loss: 0.482, ave_loss: 0.517
[29]  [560/1724] loss: 0.547, ave_loss: 0.518
[30]  [580/1724] loss: 0.607, ave_loss: 0.521
[31]  [600/1724] loss: 0.425, ave_loss: 0.518
[32]  [620/1724] loss: 0.429, ave_loss: 0.515
[33]  [640/1724] loss: 0.494, ave_loss: 0.514
[34]  [660/1724] loss: 0.503, ave_loss: 0.514
[35]  [680/1724] loss: 0.571, ave_loss: 0.516
[36]  [700/1724] loss: 0.349, ave_loss: 0.511
[37]  [720/1724] loss: 0.459, ave_loss: 0.510
[38]  [740/1724] loss: 0.404, ave_loss: 0.507
[39]  [760/1724] loss: 0.441, ave_loss: 0.505
[40]  [780/1724] loss: 0.513, ave_loss: 0.505
[41]  [800/1724] loss: 0.504, ave_loss: 0.505
[42]  [820/1724] loss: 0.489, ave_loss: 0.505
[43]  [840/1724] loss: 0.503, ave_loss: 0.505
[44]  [860/1724] loss: 0.525, ave_loss: 0.505
[45]  [880/1724] loss: 0.526, ave_loss: 0.506
[46]  [900/1724] loss: 0.373, ave_loss: 0.503
[47]  [920/1724] loss: 0.547, ave_loss: 0.504
[48]  [940/1724] loss: 0.478, ave_loss: 0.503
[49]  [960/1724] loss: 0.532, ave_loss: 0.504
[50]  [980/1724] loss: 0.403, ave_loss: 0.502
[51]  [1000/1724] loss: 0.374, ave_loss: 0.499
[52]  [1020/1724] loss: 0.501, ave_loss: 0.499
[53]  [1040/1724] loss: 0.406, ave_loss: 0.498
[54]  [1060/1724] loss: 0.484, ave_loss: 0.497
[55]  [1080/1724] loss: 0.458, ave_loss: 0.497
[56]  [1100/1724] loss: 0.365, ave_loss: 0.494
[57]  [1120/1724] loss: 0.527, ave_loss: 0.495
[58]  [1140/1724] loss: 0.485, ave_loss: 0.495
[59]  [1160/1724] loss: 0.500, ave_loss: 0.495
[60]  [1180/1724] loss: 0.464, ave_loss: 0.494
[61]  [1200/1724] loss: 0.368, ave_loss: 0.492
[62]  [1220/1724] loss: 0.430, ave_loss: 0.491
[63]  [1240/1724] loss: 0.403, ave_loss: 0.490
[64]  [1260/1724] loss: 0.410, ave_loss: 0.489
[65]  [1280/1724] loss: 0.643, ave_loss: 0.491
[66]  [1300/1724] loss: 0.530, ave_loss: 0.492
[67]  [1320/1724] loss: 0.699, ave_loss: 0.495
[68]  [1340/1724] loss: 0.606, ave_loss: 0.496
[69]  [1360/1724] loss: 0.347, ave_loss: 0.494
[70]  [1380/1724] loss: 0.557, ave_loss: 0.495
[71]  [1400/1724] loss: 0.597, ave_loss: 0.496
[72]  [1420/1724] loss: 0.378, ave_loss: 0.495
[73]  [1440/1724] loss: 0.317, ave_loss: 0.492
[74]  [1460/1724] loss: 0.553, ave_loss: 0.493
[75]  [1480/1724] loss: 0.460, ave_loss: 0.493
[76]  [1500/1724] loss: 0.512, ave_loss: 0.493
[77]  [1520/1724] loss: 0.530, ave_loss: 0.494
[78]  [1540/1724] loss: 0.488, ave_loss: 0.493
[79]  [1560/1724] loss: 0.468, ave_loss: 0.493
[80]  [1580/1724] loss: 0.572, ave_loss: 0.494
[81]  [1600/1724] loss: 0.380, ave_loss: 0.493
[82]  [1620/1724] loss: 0.464, ave_loss: 0.492
[83]  [1640/1724] loss: 0.549, ave_loss: 0.493
[84]  [1660/1724] loss: 0.422, ave_loss: 0.492
[85]  [1680/1724] loss: 0.536, ave_loss: 0.493
[86]  [1700/1724] loss: 0.521, ave_loss: 0.493
[87]  [1720/1724] loss: 0.406, ave_loss: 0.492
[88]  [1740/1724] loss: 0.476, ave_loss: 0.492

Finished Training finishing at 2021-08-30 18:47:21.863580
printing_out epoch  17.354988399071924 learning rate: 0.0005153561248318907
0.00030706260103698985
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.918e-01
Validation Loss: 2.525e+05
Validation ROC: 0.6451
No improvement, still saving model
11.645011600928076 epochs left to go

Training Epoch 17.354988399071924/30 starting at 2021-08-30 18:48:11.946030
[1]  [0/1724] loss: 0.626, ave_loss: 0.626
[2]  [20/1724] loss: 0.509, ave_loss: 0.568
[3]  [40/1724] loss: 0.632, ave_loss: 0.589
[4]  [60/1724] loss: 0.436, ave_loss: 0.551
[5]  [80/1724] loss: 0.509, ave_loss: 0.542
[6]  [100/1724] loss: 0.447, ave_loss: 0.526
[7]  [120/1724] loss: 0.544, ave_loss: 0.529
[8]  [140/1724] loss: 0.442, ave_loss: 0.518
[9]  [160/1724] loss: 0.547, ave_loss: 0.521
[10]  [180/1724] loss: 0.368, ave_loss: 0.506
[11]  [200/1724] loss: 0.422, ave_loss: 0.498
[12]  [220/1724] loss: 0.491, ave_loss: 0.498
[13]  [240/1724] loss: 0.623, ave_loss: 0.507
[14]  [260/1724] loss: 0.453, ave_loss: 0.503
[15]  [280/1724] loss: 0.457, ave_loss: 0.500
[16]  [300/1724] loss: 0.402, ave_loss: 0.494
[17]  [320/1724] loss: 0.441, ave_loss: 0.491
[18]  [340/1724] loss: 0.403, ave_loss: 0.486
[19]  [360/1724] loss: 0.501, ave_loss: 0.487
[20]  [380/1724] loss: 0.445, ave_loss: 0.485
[21]  [400/1724] loss: 0.426, ave_loss: 0.482
[22]  [420/1724] loss: 0.408, ave_loss: 0.479
[23]  [440/1724] loss: 0.577, ave_loss: 0.483
[24]  [460/1724] loss: 0.547, ave_loss: 0.486
[25]  [480/1724] loss: 0.452, ave_loss: 0.484
[26]  [500/1724] loss: 0.466, ave_loss: 0.484
[27]  [520/1724] loss: 0.514, ave_loss: 0.485
[28]  [540/1724] loss: 0.613, ave_loss: 0.489
[29]  [560/1724] loss: 0.386, ave_loss: 0.486
[30]  [580/1724] loss: 0.581, ave_loss: 0.489
[31]  [600/1724] loss: 0.531, ave_loss: 0.490
[32]  [620/1724] loss: 0.510, ave_loss: 0.491
[33]  [640/1724] loss: 0.453, ave_loss: 0.490
[34]  [660/1724] loss: 0.576, ave_loss: 0.492
[35]  [680/1724] loss: 0.447, ave_loss: 0.491
[36]  [700/1724] loss: 0.392, ave_loss: 0.488
[37]  [720/1724] loss: 0.474, ave_loss: 0.488
[38]  [740/1724] loss: 0.480, ave_loss: 0.488
[39]  [760/1724] loss: 0.444, ave_loss: 0.487
[40]  [780/1724] loss: 0.395, ave_loss: 0.484
[41]  [800/1724] loss: 0.384, ave_loss: 0.482
[42]  [820/1724] loss: 0.436, ave_loss: 0.481
[43]  [840/1724] loss: 0.375, ave_loss: 0.478
[44]  [860/1724] loss: 0.457, ave_loss: 0.478
[45]  [880/1724] loss: 0.497, ave_loss: 0.478
[46]  [900/1724] loss: 0.687, ave_loss: 0.483
[47]  [920/1724] loss: 0.554, ave_loss: 0.484
[48]  [940/1724] loss: 0.376, ave_loss: 0.482
[49]  [960/1724] loss: 0.565, ave_loss: 0.484
[50]  [980/1724] loss: 0.394, ave_loss: 0.482
[51]  [1000/1724] loss: 0.573, ave_loss: 0.484
[52]  [1020/1724] loss: 0.471, ave_loss: 0.483
[53]  [1040/1724] loss: 0.519, ave_loss: 0.484
[54]  [1060/1724] loss: 0.540, ave_loss: 0.485
[55]  [1080/1724] loss: 0.590, ave_loss: 0.487
[56]  [1100/1724] loss: 0.591, ave_loss: 0.489
[57]  [1120/1724] loss: 0.438, ave_loss: 0.488
[58]  [1140/1724] loss: 0.630, ave_loss: 0.490
[59]  [1160/1724] loss: 0.539, ave_loss: 0.491
[60]  [1180/1724] loss: 0.491, ave_loss: 0.491
[61]  [1200/1724] loss: 0.480, ave_loss: 0.491
[62]  [1220/1724] loss: 0.389, ave_loss: 0.489
[63]  [1240/1724] loss: 0.355, ave_loss: 0.487
[64]  [1260/1724] loss: 0.545, ave_loss: 0.488
[65]  [1280/1724] loss: 0.609, ave_loss: 0.490
[66]  [1300/1724] loss: 0.485, ave_loss: 0.490
[67]  [1320/1724] loss: 0.431, ave_loss: 0.489
[68]  [1340/1724] loss: 0.413, ave_loss: 0.488
[69]  [1360/1724] loss: 0.404, ave_loss: 0.487
[70]  [1380/1724] loss: 0.522, ave_loss: 0.487
[71]  [1400/1724] loss: 0.408, ave_loss: 0.486
[72]  [1420/1724] loss: 0.384, ave_loss: 0.485
[73]  [1440/1724] loss: 0.388, ave_loss: 0.483
[74]  [1460/1724] loss: 0.354, ave_loss: 0.482
[75]  [1480/1724] loss: 0.508, ave_loss: 0.482
[76]  [1500/1724] loss: 0.452, ave_loss: 0.482
[77]  [1520/1724] loss: 0.496, ave_loss: 0.482
[78]  [1540/1724] loss: 0.478, ave_loss: 0.482
[79]  [1560/1724] loss: 0.453, ave_loss: 0.481
[80]  [1580/1724] loss: 0.557, ave_loss: 0.482
[81]  [1600/1724] loss: 0.314, ave_loss: 0.480
[82]  [1620/1724] loss: 0.742, ave_loss: 0.483
[83]  [1640/1724] loss: 0.539, ave_loss: 0.484
[84]  [1660/1724] loss: 0.548, ave_loss: 0.485
[85]  [1680/1724] loss: 0.443, ave_loss: 0.484
[86]  [1700/1724] loss: 0.515, ave_loss: 0.485
[87]  [1720/1724] loss: 0.463, ave_loss: 0.485
[88]  [1740/1724] loss: 0.381, ave_loss: 0.483

Finished Training finishing at 2021-08-30 18:50:26.656019
printing_out epoch  18.37587006960557 learning rate: 0.0005153561248318907
0.00029785072300588016
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.833e-01
Validation Loss: 2.572e+05
Validation ROC: 0.6406
No improvement, still saving model
10.624129930394432 epochs left to go

Training Epoch 18.37587006960557/30 starting at 2021-08-30 18:51:16.769096
[1]  [0/1724] loss: 0.578, ave_loss: 0.578
[2]  [20/1724] loss: 0.607, ave_loss: 0.593
[3]  [40/1724] loss: 0.449, ave_loss: 0.545
[4]  [60/1724] loss: 0.529, ave_loss: 0.541
[5]  [80/1724] loss: 0.420, ave_loss: 0.517
[6]  [100/1724] loss: 0.520, ave_loss: 0.517
[7]  [120/1724] loss: 0.440, ave_loss: 0.506
[8]  [140/1724] loss: 0.516, ave_loss: 0.507
[9]  [160/1724] loss: 0.414, ave_loss: 0.497
[10]  [180/1724] loss: 0.443, ave_loss: 0.492
[11]  [200/1724] loss: 0.481, ave_loss: 0.491
[12]  [220/1724] loss: 0.489, ave_loss: 0.491
[13]  [240/1724] loss: 0.538, ave_loss: 0.494
[14]  [260/1724] loss: 0.337, ave_loss: 0.483
[15]  [280/1724] loss: 0.539, ave_loss: 0.487
[16]  [300/1724] loss: 0.427, ave_loss: 0.483
[17]  [320/1724] loss: 0.535, ave_loss: 0.486
[18]  [340/1724] loss: 0.622, ave_loss: 0.494
[19]  [360/1724] loss: 0.492, ave_loss: 0.494
[20]  [380/1724] loss: 0.492, ave_loss: 0.493
[21]  [400/1724] loss: 0.791, ave_loss: 0.508
[22]  [420/1724] loss: 0.483, ave_loss: 0.507
[23]  [440/1724] loss: 0.629, ave_loss: 0.512
[24]  [460/1724] loss: 0.357, ave_loss: 0.505
[25]  [480/1724] loss: 0.444, ave_loss: 0.503
[26]  [500/1724] loss: 0.459, ave_loss: 0.501
[27]  [520/1724] loss: 0.586, ave_loss: 0.504
[28]  [540/1724] loss: 0.508, ave_loss: 0.504
[29]  [560/1724] loss: 0.418, ave_loss: 0.502
[30]  [580/1724] loss: 0.629, ave_loss: 0.506
[31]  [600/1724] loss: 0.400, ave_loss: 0.502
[32]  [620/1724] loss: 0.482, ave_loss: 0.502
[33]  [640/1724] loss: 0.574, ave_loss: 0.504
[34]  [660/1724] loss: 0.482, ave_loss: 0.503
[35]  [680/1724] loss: 0.575, ave_loss: 0.505
[36]  [700/1724] loss: 0.422, ave_loss: 0.503
[37]  [720/1724] loss: 0.566, ave_loss: 0.505
[38]  [740/1724] loss: 0.430, ave_loss: 0.503
[39]  [760/1724] loss: 0.426, ave_loss: 0.501
[40]  [780/1724] loss: 0.485, ave_loss: 0.500
[41]  [800/1724] loss: 0.544, ave_loss: 0.501
[42]  [820/1724] loss: 0.389, ave_loss: 0.499
[43]  [840/1724] loss: 0.348, ave_loss: 0.495
[44]  [860/1724] loss: 0.476, ave_loss: 0.495
[45]  [880/1724] loss: 0.544, ave_loss: 0.496
[46]  [900/1724] loss: 0.415, ave_loss: 0.494
[47]  [920/1724] loss: 0.552, ave_loss: 0.495
[48]  [940/1724] loss: 0.501, ave_loss: 0.496
[49]  [960/1724] loss: 0.533, ave_loss: 0.496
[50]  [980/1724] loss: 0.368, ave_loss: 0.494
[51]  [1000/1724] loss: 0.528, ave_loss: 0.494
[52]  [1020/1724] loss: 0.432, ave_loss: 0.493
[53]  [1040/1724] loss: 0.589, ave_loss: 0.495
[54]  [1060/1724] loss: 0.371, ave_loss: 0.493
[55]  [1080/1724] loss: 0.472, ave_loss: 0.492
[56]  [1100/1724] loss: 0.394, ave_loss: 0.491
[57]  [1120/1724] loss: 0.356, ave_loss: 0.488
[58]  [1140/1724] loss: 0.447, ave_loss: 0.487
[59]  [1160/1724] loss: 0.686, ave_loss: 0.491
[60]  [1180/1724] loss: 0.439, ave_loss: 0.490
[61]  [1200/1724] loss: 0.609, ave_loss: 0.492
[62]  [1220/1724] loss: 0.502, ave_loss: 0.492
[63]  [1240/1724] loss: 0.334, ave_loss: 0.490
[64]  [1260/1724] loss: 0.488, ave_loss: 0.490
[65]  [1280/1724] loss: 0.460, ave_loss: 0.489
[66]  [1300/1724] loss: 0.556, ave_loss: 0.490
[67]  [1320/1724] loss: 0.580, ave_loss: 0.491
[68]  [1340/1724] loss: 0.446, ave_loss: 0.491
[69]  [1360/1724] loss: 0.398, ave_loss: 0.489
[70]  [1380/1724] loss: 0.393, ave_loss: 0.488
[71]  [1400/1724] loss: 0.495, ave_loss: 0.488
[72]  [1420/1724] loss: 0.384, ave_loss: 0.487
[73]  [1440/1724] loss: 0.481, ave_loss: 0.487
[74]  [1460/1724] loss: 0.332, ave_loss: 0.485
[75]  [1480/1724] loss: 0.463, ave_loss: 0.484
[76]  [1500/1724] loss: 0.515, ave_loss: 0.485
[77]  [1520/1724] loss: 0.352, ave_loss: 0.483
[78]  [1540/1724] loss: 0.566, ave_loss: 0.484
[79]  [1560/1724] loss: 0.342, ave_loss: 0.482
[80]  [1580/1724] loss: 0.547, ave_loss: 0.483
[81]  [1600/1724] loss: 0.597, ave_loss: 0.484
[82]  [1620/1724] loss: 0.526, ave_loss: 0.485
[83]  [1640/1724] loss: 0.335, ave_loss: 0.483
[84]  [1660/1724] loss: 0.624, ave_loss: 0.485
[85]  [1680/1724] loss: 0.381, ave_loss: 0.484
[86]  [1700/1724] loss: 0.441, ave_loss: 0.483
[87]  [1720/1724] loss: 0.430, ave_loss: 0.482
[88]  [1740/1724] loss: 0.403, ave_loss: 0.482

Finished Training finishing at 2021-08-30 18:53:26.720115
printing_out epoch  19.396751740139212 learning rate: 0.0005153561248318907
0.00028891520131570374
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.816e-01
Validation Loss: 2.593e+05
Validation ROC: 0.6318
No improvement, still saving model
9.603248259860788 epochs left to go

Training Epoch 19.396751740139212/30 starting at 2021-08-30 18:54:14.351584
[1]  [0/1724] loss: 0.583, ave_loss: 0.583
[2]  [20/1724] loss: 0.480, ave_loss: 0.531
[3]  [40/1724] loss: 0.452, ave_loss: 0.505
[4]  [60/1724] loss: 0.599, ave_loss: 0.528
[5]  [80/1724] loss: 0.305, ave_loss: 0.484
[6]  [100/1724] loss: 0.323, ave_loss: 0.457
[7]  [120/1724] loss: 0.553, ave_loss: 0.471
[8]  [140/1724] loss: 0.472, ave_loss: 0.471
[9]  [160/1724] loss: 0.560, ave_loss: 0.481
[10]  [180/1724] loss: 0.460, ave_loss: 0.479
[11]  [200/1724] loss: 0.545, ave_loss: 0.485
[12]  [220/1724] loss: 0.353, ave_loss: 0.474
[13]  [240/1724] loss: 0.537, ave_loss: 0.479
[14]  [260/1724] loss: 0.465, ave_loss: 0.478
[15]  [280/1724] loss: 0.371, ave_loss: 0.471
[16]  [300/1724] loss: 0.485, ave_loss: 0.471
[17]  [320/1724] loss: 0.299, ave_loss: 0.461
[18]  [340/1724] loss: 0.428, ave_loss: 0.459
[19]  [360/1724] loss: 0.543, ave_loss: 0.464
[20]  [380/1724] loss: 0.403, ave_loss: 0.461
[21]  [400/1724] loss: 0.422, ave_loss: 0.459
[22]  [420/1724] loss: 0.494, ave_loss: 0.461
[23]  [440/1724] loss: 0.595, ave_loss: 0.466
[24]  [460/1724] loss: 0.725, ave_loss: 0.477
[25]  [480/1724] loss: 0.527, ave_loss: 0.479
[26]  [500/1724] loss: 0.451, ave_loss: 0.478
[27]  [520/1724] loss: 0.357, ave_loss: 0.474
[28]  [540/1724] loss: 0.422, ave_loss: 0.472
[29]  [560/1724] loss: 0.536, ave_loss: 0.474
[30]  [580/1724] loss: 0.459, ave_loss: 0.473
[31]  [600/1724] loss: 0.697, ave_loss: 0.481
[32]  [620/1724] loss: 0.523, ave_loss: 0.482
[33]  [640/1724] loss: 0.453, ave_loss: 0.481
[34]  [660/1724] loss: 0.423, ave_loss: 0.479
[35]  [680/1724] loss: 0.643, ave_loss: 0.484
[36]  [700/1724] loss: 0.520, ave_loss: 0.485
[37]  [720/1724] loss: 0.440, ave_loss: 0.484
[38]  [740/1724] loss: 0.477, ave_loss: 0.484
[39]  [760/1724] loss: 0.456, ave_loss: 0.483
[40]  [780/1724] loss: 0.528, ave_loss: 0.484
[41]  [800/1724] loss: 0.460, ave_loss: 0.483
[42]  [820/1724] loss: 0.535, ave_loss: 0.485
[43]  [840/1724] loss: 0.438, ave_loss: 0.484
[44]  [860/1724] loss: 0.719, ave_loss: 0.489
[45]  [880/1724] loss: 0.493, ave_loss: 0.489
[46]  [900/1724] loss: 0.649, ave_loss: 0.492
[47]  [920/1724] loss: 0.451, ave_loss: 0.492
[48]  [940/1724] loss: 0.554, ave_loss: 0.493
[49]  [960/1724] loss: 0.402, ave_loss: 0.491
[50]  [980/1724] loss: 0.474, ave_loss: 0.491
[51]  [1000/1724] loss: 0.506, ave_loss: 0.491
[52]  [1020/1724] loss: 0.566, ave_loss: 0.492
[53]  [1040/1724] loss: 0.525, ave_loss: 0.493
[54]  [1060/1724] loss: 0.414, ave_loss: 0.492
[55]  [1080/1724] loss: 0.444, ave_loss: 0.491
[56]  [1100/1724] loss: 0.550, ave_loss: 0.492
[57]  [1120/1724] loss: 0.513, ave_loss: 0.492
[58]  [1140/1724] loss: 0.530, ave_loss: 0.493
[59]  [1160/1724] loss: 0.477, ave_loss: 0.493
[60]  [1180/1724] loss: 0.490, ave_loss: 0.493
[61]  [1200/1724] loss: 0.486, ave_loss: 0.492
[62]  [1220/1724] loss: 0.365, ave_loss: 0.490
[63]  [1240/1724] loss: 0.438, ave_loss: 0.490
[64]  [1260/1724] loss: 0.540, ave_loss: 0.490
[65]  [1280/1724] loss: 0.510, ave_loss: 0.491
[66]  [1300/1724] loss: 0.467, ave_loss: 0.490
[67]  [1320/1724] loss: 0.490, ave_loss: 0.490
[68]  [1340/1724] loss: 0.648, ave_loss: 0.493
[69]  [1360/1724] loss: 0.535, ave_loss: 0.493
[70]  [1380/1724] loss: 0.473, ave_loss: 0.493
[71]  [1400/1724] loss: 0.411, ave_loss: 0.492
[72]  [1420/1724] loss: 0.418, ave_loss: 0.491
[73]  [1440/1724] loss: 0.369, ave_loss: 0.489
[74]  [1460/1724] loss: 0.341, ave_loss: 0.487
[75]  [1480/1724] loss: 0.471, ave_loss: 0.487
[76]  [1500/1724] loss: 0.400, ave_loss: 0.486
[77]  [1520/1724] loss: 0.439, ave_loss: 0.485
[78]  [1540/1724] loss: 0.613, ave_loss: 0.487
[79]  [1560/1724] loss: 0.725, ave_loss: 0.490
[80]  [1580/1724] loss: 0.511, ave_loss: 0.490
[81]  [1600/1724] loss: 0.307, ave_loss: 0.488
[82]  [1620/1724] loss: 0.423, ave_loss: 0.487
[83]  [1640/1724] loss: 0.585, ave_loss: 0.488
[84]  [1660/1724] loss: 0.439, ave_loss: 0.488
[85]  [1680/1724] loss: 0.457, ave_loss: 0.487
[86]  [1700/1724] loss: 0.542, ave_loss: 0.488
[87]  [1720/1724] loss: 0.634, ave_loss: 0.490
[88]  [1740/1724] loss: 0.394, ave_loss: 0.488

Finished Training finishing at 2021-08-30 18:56:23.585357
printing_out epoch  20.417633410672853 learning rate: 0.0005153561248318907
0.0002802477452762326
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.885e-01
Validation Loss: 2.621e+05
Validation ROC: 0.6256
No improvement, still saving model
8.582366589327147 epochs left to go

Training Epoch 20.417633410672853/30 starting at 2021-08-30 18:57:10.568070
[1]  [0/1724] loss: 0.454, ave_loss: 0.454
[2]  [20/1724] loss: 0.464, ave_loss: 0.459
[3]  [40/1724] loss: 0.516, ave_loss: 0.478
[4]  [60/1724] loss: 0.591, ave_loss: 0.506
[5]  [80/1724] loss: 0.367, ave_loss: 0.478
[6]  [100/1724] loss: 0.498, ave_loss: 0.482
[7]  [120/1724] loss: 0.585, ave_loss: 0.496
[8]  [140/1724] loss: 0.428, ave_loss: 0.488
[9]  [160/1724] loss: 0.581, ave_loss: 0.498
[10]  [180/1724] loss: 0.499, ave_loss: 0.498
[11]  [200/1724] loss: 0.422, ave_loss: 0.491
[12]  [220/1724] loss: 0.669, ave_loss: 0.506
[13]  [240/1724] loss: 0.662, ave_loss: 0.518
[14]  [260/1724] loss: 0.452, ave_loss: 0.513
[15]  [280/1724] loss: 0.441, ave_loss: 0.509
[16]  [300/1724] loss: 0.448, ave_loss: 0.505
[17]  [320/1724] loss: 0.622, ave_loss: 0.512
[18]  [340/1724] loss: 0.506, ave_loss: 0.511
[19]  [360/1724] loss: 0.495, ave_loss: 0.511
[20]  [380/1724] loss: 0.469, ave_loss: 0.508
[21]  [400/1724] loss: 0.348, ave_loss: 0.501
[22]  [420/1724] loss: 0.440, ave_loss: 0.498
[23]  [440/1724] loss: 0.538, ave_loss: 0.500
[24]  [460/1724] loss: 0.494, ave_loss: 0.500
[25]  [480/1724] loss: 0.391, ave_loss: 0.495
[26]  [500/1724] loss: 0.620, ave_loss: 0.500
[27]  [520/1724] loss: 0.413, ave_loss: 0.497
[28]  [540/1724] loss: 0.513, ave_loss: 0.497
[29]  [560/1724] loss: 0.566, ave_loss: 0.500
[30]  [580/1724] loss: 0.452, ave_loss: 0.498
[31]  [600/1724] loss: 0.636, ave_loss: 0.503
[32]  [620/1724] loss: 0.524, ave_loss: 0.503
[33]  [640/1724] loss: 0.542, ave_loss: 0.504
[34]  [660/1724] loss: 0.428, ave_loss: 0.502
[35]  [680/1724] loss: 0.552, ave_loss: 0.504
[36]  [700/1724] loss: 0.516, ave_loss: 0.504
[37]  [720/1724] loss: 0.513, ave_loss: 0.504
[38]  [740/1724] loss: 0.474, ave_loss: 0.503
[39]  [760/1724] loss: 0.619, ave_loss: 0.506
[40]  [780/1724] loss: 0.451, ave_loss: 0.505
[41]  [800/1724] loss: 0.479, ave_loss: 0.504
[42]  [820/1724] loss: 0.520, ave_loss: 0.505
[43]  [840/1724] loss: 0.499, ave_loss: 0.505
[44]  [860/1724] loss: 0.415, ave_loss: 0.503
[45]  [880/1724] loss: 0.529, ave_loss: 0.503
[46]  [900/1724] loss: 0.563, ave_loss: 0.504
[47]  [920/1724] loss: 0.368, ave_loss: 0.502
[48]  [940/1724] loss: 0.398, ave_loss: 0.499
[49]  [960/1724] loss: 0.528, ave_loss: 0.500
[50]  [980/1724] loss: 0.590, ave_loss: 0.502
[51]  [1000/1724] loss: 0.464, ave_loss: 0.501
[52]  [1020/1724] loss: 0.710, ave_loss: 0.505
[53]  [1040/1724] loss: 0.357, ave_loss: 0.502
[54]  [1060/1724] loss: 0.600, ave_loss: 0.504
[55]  [1080/1724] loss: 0.524, ave_loss: 0.504
[56]  [1100/1724] loss: 0.441, ave_loss: 0.503
[57]  [1120/1724] loss: 0.477, ave_loss: 0.503
[58]  [1140/1724] loss: 0.647, ave_loss: 0.505
[59]  [1160/1724] loss: 0.660, ave_loss: 0.508
[60]  [1180/1724] loss: 0.509, ave_loss: 0.508
[61]  [1200/1724] loss: 0.421, ave_loss: 0.507
[62]  [1220/1724] loss: 0.448, ave_loss: 0.506
[63]  [1240/1724] loss: 0.506, ave_loss: 0.506
[64]  [1260/1724] loss: 0.570, ave_loss: 0.507
[65]  [1280/1724] loss: 0.427, ave_loss: 0.505
[66]  [1300/1724] loss: 0.512, ave_loss: 0.505
[67]  [1320/1724] loss: 0.524, ave_loss: 0.506
[68]  [1340/1724] loss: 0.474, ave_loss: 0.505
[69]  [1360/1724] loss: 0.426, ave_loss: 0.504
[70]  [1380/1724] loss: 0.385, ave_loss: 0.502
[71]  [1400/1724] loss: 0.589, ave_loss: 0.504
[72]  [1420/1724] loss: 0.550, ave_loss: 0.504
[73]  [1440/1724] loss: 0.514, ave_loss: 0.504
[74]  [1460/1724] loss: 0.459, ave_loss: 0.504
[75]  [1480/1724] loss: 0.516, ave_loss: 0.504
[76]  [1500/1724] loss: 0.443, ave_loss: 0.503
[77]  [1520/1724] loss: 0.440, ave_loss: 0.502
[78]  [1540/1724] loss: 0.508, ave_loss: 0.502
[79]  [1560/1724] loss: 0.583, ave_loss: 0.503
[80]  [1580/1724] loss: 0.478, ave_loss: 0.503
[81]  [1600/1724] loss: 0.623, ave_loss: 0.505
[82]  [1620/1724] loss: 0.431, ave_loss: 0.504
[83]  [1640/1724] loss: 0.614, ave_loss: 0.505
[84]  [1660/1724] loss: 0.545, ave_loss: 0.505
[85]  [1680/1724] loss: 0.496, ave_loss: 0.505
[86]  [1700/1724] loss: 0.478, ave_loss: 0.505
[87]  [1720/1724] loss: 0.482, ave_loss: 0.505
[88]  [1740/1724] loss: 0.557, ave_loss: 0.505

Finished Training finishing at 2021-08-30 18:59:17.395556
printing_out epoch  21.438515081206496 learning rate: 0.0005153561248318907
0.00027184031291794565
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 5.054e-01
Validation Loss: 2.687e+05
Validation ROC: 0.6190
No improvement, still saving model
7.561484918793504 epochs left to go

Training Epoch 21.438515081206496/30 starting at 2021-08-30 19:00:03.346976
[1]  [0/1724] loss: 0.382, ave_loss: 0.382
[2]  [20/1724] loss: 0.524, ave_loss: 0.453
[3]  [40/1724] loss: 0.385, ave_loss: 0.430
[4]  [60/1724] loss: 0.473, ave_loss: 0.441
[5]  [80/1724] loss: 0.635, ave_loss: 0.479
[6]  [100/1724] loss: 0.532, ave_loss: 0.488
[7]  [120/1724] loss: 0.415, ave_loss: 0.478
[8]  [140/1724] loss: 0.450, ave_loss: 0.474
[9]  [160/1724] loss: 0.526, ave_loss: 0.480
[10]  [180/1724] loss: 0.609, ave_loss: 0.493
[11]  [200/1724] loss: 0.528, ave_loss: 0.496
[12]  [220/1724] loss: 0.453, ave_loss: 0.493
[13]  [240/1724] loss: 0.434, ave_loss: 0.488
[14]  [260/1724] loss: 0.395, ave_loss: 0.481
[15]  [280/1724] loss: 0.438, ave_loss: 0.479
[16]  [300/1724] loss: 0.447, ave_loss: 0.477
[17]  [320/1724] loss: 0.460, ave_loss: 0.476
[18]  [340/1724] loss: 0.449, ave_loss: 0.474
[19]  [360/1724] loss: 0.471, ave_loss: 0.474
[20]  [380/1724] loss: 0.582, ave_loss: 0.479
[21]  [400/1724] loss: 0.541, ave_loss: 0.482
[22]  [420/1724] loss: 0.443, ave_loss: 0.480
[23]  [440/1724] loss: 0.459, ave_loss: 0.480
[24]  [460/1724] loss: 0.488, ave_loss: 0.480
[25]  [480/1724] loss: 0.420, ave_loss: 0.478
[26]  [500/1724] loss: 0.405, ave_loss: 0.475
[27]  [520/1724] loss: 0.428, ave_loss: 0.473
[28]  [540/1724] loss: 0.358, ave_loss: 0.469
[29]  [560/1724] loss: 0.508, ave_loss: 0.470
[30]  [580/1724] loss: 0.494, ave_loss: 0.471
[31]  [600/1724] loss: 0.673, ave_loss: 0.478
[32]  [620/1724] loss: 0.424, ave_loss: 0.476
[33]  [640/1724] loss: 0.541, ave_loss: 0.478
[34]  [660/1724] loss: 0.384, ave_loss: 0.475
[35]  [680/1724] loss: 0.408, ave_loss: 0.473
[36]  [700/1724] loss: 0.557, ave_loss: 0.476
[37]  [720/1724] loss: 0.463, ave_loss: 0.475
[38]  [740/1724] loss: 0.457, ave_loss: 0.475
[39]  [760/1724] loss: 0.442, ave_loss: 0.474
[40]  [780/1724] loss: 0.361, ave_loss: 0.471
[41]  [800/1724] loss: 0.521, ave_loss: 0.472
[42]  [820/1724] loss: 0.450, ave_loss: 0.472
[43]  [840/1724] loss: 0.532, ave_loss: 0.473
[44]  [860/1724] loss: 0.543, ave_loss: 0.475
[45]  [880/1724] loss: 0.591, ave_loss: 0.477
[46]  [900/1724] loss: 0.505, ave_loss: 0.478
[47]  [920/1724] loss: 0.563, ave_loss: 0.480
[48]  [940/1724] loss: 0.341, ave_loss: 0.477
[49]  [960/1724] loss: 0.377, ave_loss: 0.475
[50]  [980/1724] loss: 0.426, ave_loss: 0.474
[51]  [1000/1724] loss: 0.606, ave_loss: 0.476
[52]  [1020/1724] loss: 0.443, ave_loss: 0.476
[53]  [1040/1724] loss: 0.647, ave_loss: 0.479
[54]  [1060/1724] loss: 0.404, ave_loss: 0.478
[55]  [1080/1724] loss: 0.542, ave_loss: 0.479
[56]  [1100/1724] loss: 0.423, ave_loss: 0.478
[57]  [1120/1724] loss: 0.370, ave_loss: 0.476
[58]  [1140/1724] loss: 0.506, ave_loss: 0.476
[59]  [1160/1724] loss: 0.497, ave_loss: 0.477
[60]  [1180/1724] loss: 0.387, ave_loss: 0.475
[61]  [1200/1724] loss: 0.443, ave_loss: 0.475
[62]  [1220/1724] loss: 0.550, ave_loss: 0.476
[63]  [1240/1724] loss: 0.480, ave_loss: 0.476
[64]  [1260/1724] loss: 0.386, ave_loss: 0.475
[65]  [1280/1724] loss: 0.470, ave_loss: 0.475
[66]  [1300/1724] loss: 0.482, ave_loss: 0.475
[67]  [1320/1724] loss: 0.623, ave_loss: 0.477
[68]  [1340/1724] loss: 0.503, ave_loss: 0.477
[69]  [1360/1724] loss: 0.490, ave_loss: 0.477
[70]  [1380/1724] loss: 0.489, ave_loss: 0.478
[71]  [1400/1724] loss: 0.483, ave_loss: 0.478
[72]  [1420/1724] loss: 0.528, ave_loss: 0.478
[73]  [1440/1724] loss: 0.458, ave_loss: 0.478
[74]  [1460/1724] loss: 0.513, ave_loss: 0.479
[75]  [1480/1724] loss: 0.489, ave_loss: 0.479
[76]  [1500/1724] loss: 0.465, ave_loss: 0.479
[77]  [1520/1724] loss: 0.500, ave_loss: 0.479
[78]  [1540/1724] loss: 0.465, ave_loss: 0.479
[79]  [1560/1724] loss: 0.367, ave_loss: 0.477
[80]  [1580/1724] loss: 0.573, ave_loss: 0.478
[81]  [1600/1724] loss: 0.332, ave_loss: 0.477
[82]  [1620/1724] loss: 0.646, ave_loss: 0.479
[83]  [1640/1724] loss: 0.420, ave_loss: 0.478
[84]  [1660/1724] loss: 0.569, ave_loss: 0.479
[85]  [1680/1724] loss: 0.426, ave_loss: 0.478
[86]  [1700/1724] loss: 0.361, ave_loss: 0.477
[87]  [1720/1724] loss: 0.367, ave_loss: 0.476
[88]  [1740/1724] loss: 0.485, ave_loss: 0.476

Finished Training finishing at 2021-08-30 19:02:03.266217
printing_out epoch  22.45939675174014 learning rate: 0.0005153561248318907
0.0002636851035304073
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.759e-01
Validation Loss: 2.626e+05
Validation ROC: 0.6144
No improvement, still saving model
6.54060324825986 epochs left to go

Training Epoch 22.45939675174014/30 starting at 2021-08-30 19:02:53.032497
[1]  [0/1724] loss: 0.420, ave_loss: 0.420
[2]  [20/1724] loss: 0.555, ave_loss: 0.487
[3]  [40/1724] loss: 0.389, ave_loss: 0.455
[4]  [60/1724] loss: 0.388, ave_loss: 0.438
[5]  [80/1724] loss: 0.594, ave_loss: 0.469
[6]  [100/1724] loss: 0.378, ave_loss: 0.454
[7]  [120/1724] loss: 0.483, ave_loss: 0.458
[8]  [140/1724] loss: 0.469, ave_loss: 0.459
[9]  [160/1724] loss: 0.392, ave_loss: 0.452
[10]  [180/1724] loss: 0.375, ave_loss: 0.444
[11]  [200/1724] loss: 0.517, ave_loss: 0.451
[12]  [220/1724] loss: 0.479, ave_loss: 0.453
[13]  [240/1724] loss: 0.402, ave_loss: 0.449
[14]  [260/1724] loss: 0.514, ave_loss: 0.454
[15]  [280/1724] loss: 0.475, ave_loss: 0.455
[16]  [300/1724] loss: 0.694, ave_loss: 0.470
[17]  [320/1724] loss: 0.469, ave_loss: 0.470
[18]  [340/1724] loss: 0.624, ave_loss: 0.479
[19]  [360/1724] loss: 0.524, ave_loss: 0.481
[20]  [380/1724] loss: 0.602, ave_loss: 0.487
[21]  [400/1724] loss: 0.477, ave_loss: 0.487
[22]  [420/1724] loss: 0.481, ave_loss: 0.486
[23]  [440/1724] loss: 0.553, ave_loss: 0.489
[24]  [460/1724] loss: 0.484, ave_loss: 0.489
[25]  [480/1724] loss: 0.431, ave_loss: 0.487
[26]  [500/1724] loss: 0.657, ave_loss: 0.493
[27]  [520/1724] loss: 0.523, ave_loss: 0.494
[28]  [540/1724] loss: 0.486, ave_loss: 0.494
[29]  [560/1724] loss: 0.558, ave_loss: 0.496
[30]  [580/1724] loss: 0.510, ave_loss: 0.497
[31]  [600/1724] loss: 0.493, ave_loss: 0.497
[32]  [620/1724] loss: 0.487, ave_loss: 0.496
[33]  [640/1724] loss: 0.449, ave_loss: 0.495
[34]  [660/1724] loss: 0.356, ave_loss: 0.491
[35]  [680/1724] loss: 0.441, ave_loss: 0.489
[36]  [700/1724] loss: 0.371, ave_loss: 0.486
[37]  [720/1724] loss: 0.399, ave_loss: 0.484
[38]  [740/1724] loss: 0.463, ave_loss: 0.483
[39]  [760/1724] loss: 0.523, ave_loss: 0.484
[40]  [780/1724] loss: 0.474, ave_loss: 0.484
[41]  [800/1724] loss: 0.570, ave_loss: 0.486
[42]  [820/1724] loss: 0.501, ave_loss: 0.486
[43]  [840/1724] loss: 0.455, ave_loss: 0.486
[44]  [860/1724] loss: 0.449, ave_loss: 0.485
[45]  [880/1724] loss: 0.428, ave_loss: 0.484
[46]  [900/1724] loss: 0.513, ave_loss: 0.484
[47]  [920/1724] loss: 0.375, ave_loss: 0.482
[48]  [940/1724] loss: 0.476, ave_loss: 0.482
[49]  [960/1724] loss: 0.429, ave_loss: 0.481
[50]  [980/1724] loss: 0.547, ave_loss: 0.482
[51]  [1000/1724] loss: 0.386, ave_loss: 0.480
[52]  [1020/1724] loss: 0.573, ave_loss: 0.482
[53]  [1040/1724] loss: 0.511, ave_loss: 0.482
[54]  [1060/1724] loss: 0.489, ave_loss: 0.483
[55]  [1080/1724] loss: 0.453, ave_loss: 0.482
[56]  [1100/1724] loss: 0.546, ave_loss: 0.483
[57]  [1120/1724] loss: 0.481, ave_loss: 0.483
[58]  [1140/1724] loss: 0.554, ave_loss: 0.484
[59]  [1160/1724] loss: 0.346, ave_loss: 0.482
[60]  [1180/1724] loss: 0.505, ave_loss: 0.482
[61]  [1200/1724] loss: 0.563, ave_loss: 0.484
[62]  [1220/1724] loss: 0.358, ave_loss: 0.482
[63]  [1240/1724] loss: 0.484, ave_loss: 0.482
[64]  [1260/1724] loss: 0.445, ave_loss: 0.481
[65]  [1280/1724] loss: 0.546, ave_loss: 0.482
[66]  [1300/1724] loss: 0.448, ave_loss: 0.482
[67]  [1320/1724] loss: 0.506, ave_loss: 0.482
[68]  [1340/1724] loss: 0.550, ave_loss: 0.483
[69]  [1360/1724] loss: 0.452, ave_loss: 0.483
[70]  [1380/1724] loss: 0.473, ave_loss: 0.482
[71]  [1400/1724] loss: 0.624, ave_loss: 0.484
[72]  [1420/1724] loss: 0.433, ave_loss: 0.484
[73]  [1440/1724] loss: 0.536, ave_loss: 0.484
[74]  [1460/1724] loss: 0.592, ave_loss: 0.486
[75]  [1480/1724] loss: 0.546, ave_loss: 0.487
[76]  [1500/1724] loss: 0.475, ave_loss: 0.487
[77]  [1520/1724] loss: 0.566, ave_loss: 0.488
[78]  [1540/1724] loss: 0.487, ave_loss: 0.488
[79]  [1560/1724] loss: 0.372, ave_loss: 0.486
[80]  [1580/1724] loss: 0.442, ave_loss: 0.486
[81]  [1600/1724] loss: 0.314, ave_loss: 0.483
[82]  [1620/1724] loss: 0.559, ave_loss: 0.484
[83]  [1640/1724] loss: 0.525, ave_loss: 0.485
[84]  [1660/1724] loss: 0.494, ave_loss: 0.485
[85]  [1680/1724] loss: 0.502, ave_loss: 0.485
[86]  [1700/1724] loss: 0.513, ave_loss: 0.485
[87]  [1720/1724] loss: 0.527, ave_loss: 0.486
[88]  [1740/1724] loss: 0.461, ave_loss: 0.486

Finished Training finishing at 2021-08-30 19:04:56.276676
printing_out epoch  23.48027842227378 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.856e-01
Validation Loss: 2.650e+05
Validation ROC: 0.5813
No improvement, still saving model
5.519721577726219 epochs left to go

Training Epoch 23.48027842227378/30 starting at 2021-08-30 19:05:42.424489
[1]  [0/1724] loss: 0.623, ave_loss: 0.623
[2]  [20/1724] loss: 0.613, ave_loss: 0.618
[3]  [40/1724] loss: 0.489, ave_loss: 0.575
[4]  [60/1724] loss: 0.417, ave_loss: 0.535
[5]  [80/1724] loss: 0.487, ave_loss: 0.526
[6]  [100/1724] loss: 0.420, ave_loss: 0.508
[7]  [120/1724] loss: 0.300, ave_loss: 0.478
[8]  [140/1724] loss: 0.592, ave_loss: 0.492
[9]  [160/1724] loss: 0.381, ave_loss: 0.480
[10]  [180/1724] loss: 0.327, ave_loss: 0.465
[11]  [200/1724] loss: 0.471, ave_loss: 0.465
[12]  [220/1724] loss: 0.533, ave_loss: 0.471
[13]  [240/1724] loss: 0.502, ave_loss: 0.473
[14]  [260/1724] loss: 0.460, ave_loss: 0.472
[15]  [280/1724] loss: 0.629, ave_loss: 0.483
[16]  [300/1724] loss: 0.410, ave_loss: 0.478
[17]  [320/1724] loss: 0.580, ave_loss: 0.484
[18]  [340/1724] loss: 0.446, ave_loss: 0.482
[19]  [360/1724] loss: 0.485, ave_loss: 0.482
[20]  [380/1724] loss: 0.372, ave_loss: 0.477
[21]  [400/1724] loss: 0.394, ave_loss: 0.473
[22]  [420/1724] loss: 0.595, ave_loss: 0.478
[23]  [440/1724] loss: 0.457, ave_loss: 0.477
[24]  [460/1724] loss: 0.606, ave_loss: 0.483
[25]  [480/1724] loss: 0.446, ave_loss: 0.481
[26]  [500/1724] loss: 0.520, ave_loss: 0.483
[27]  [520/1724] loss: 0.408, ave_loss: 0.480
[28]  [540/1724] loss: 0.576, ave_loss: 0.483
[29]  [560/1724] loss: 0.448, ave_loss: 0.482
[30]  [580/1724] loss: 0.451, ave_loss: 0.481
[31]  [600/1724] loss: 0.496, ave_loss: 0.482
[32]  [620/1724] loss: 0.341, ave_loss: 0.477
[33]  [640/1724] loss: 0.550, ave_loss: 0.479
[34]  [660/1724] loss: 0.322, ave_loss: 0.475
[35]  [680/1724] loss: 0.481, ave_loss: 0.475
[36]  [700/1724] loss: 0.406, ave_loss: 0.473
[37]  [720/1724] loss: 0.350, ave_loss: 0.470
[38]  [740/1724] loss: 0.482, ave_loss: 0.470
[39]  [760/1724] loss: 0.505, ave_loss: 0.471
[40]  [780/1724] loss: 0.366, ave_loss: 0.468
[41]  [800/1724] loss: 0.537, ave_loss: 0.470
[42]  [820/1724] loss: 0.626, ave_loss: 0.474
[43]  [840/1724] loss: 0.378, ave_loss: 0.471
[44]  [860/1724] loss: 0.480, ave_loss: 0.472
[45]  [880/1724] loss: 0.638, ave_loss: 0.475
[46]  [900/1724] loss: 0.517, ave_loss: 0.476
[47]  [920/1724] loss: 0.453, ave_loss: 0.476
[48]  [940/1724] loss: 0.435, ave_loss: 0.475
[49]  [960/1724] loss: 0.603, ave_loss: 0.478
[50]  [980/1724] loss: 0.436, ave_loss: 0.477
[51]  [1000/1724] loss: 0.439, ave_loss: 0.476
[52]  [1020/1724] loss: 0.398, ave_loss: 0.475
[53]  [1040/1724] loss: 0.482, ave_loss: 0.475
[54]  [1060/1724] loss: 0.452, ave_loss: 0.474
[55]  [1080/1724] loss: 0.474, ave_loss: 0.474
[56]  [1100/1724] loss: 0.536, ave_loss: 0.475
[57]  [1120/1724] loss: 0.403, ave_loss: 0.474
[58]  [1140/1724] loss: 0.332, ave_loss: 0.472
[59]  [1160/1724] loss: 0.477, ave_loss: 0.472
[60]  [1180/1724] loss: 0.410, ave_loss: 0.471
[61]  [1200/1724] loss: 0.486, ave_loss: 0.471
[62]  [1220/1724] loss: 0.617, ave_loss: 0.473
[63]  [1240/1724] loss: 0.513, ave_loss: 0.474
[64]  [1260/1724] loss: 0.377, ave_loss: 0.472
[65]  [1280/1724] loss: 0.377, ave_loss: 0.471
[66]  [1300/1724] loss: 0.444, ave_loss: 0.471
[67]  [1320/1724] loss: 0.512, ave_loss: 0.471
[68]  [1340/1724] loss: 0.390, ave_loss: 0.470
[69]  [1360/1724] loss: 0.411, ave_loss: 0.469
[70]  [1380/1724] loss: 0.513, ave_loss: 0.470
[71]  [1400/1724] loss: 0.349, ave_loss: 0.468
[72]  [1420/1724] loss: 0.453, ave_loss: 0.468
[73]  [1440/1724] loss: 0.399, ave_loss: 0.467
[74]  [1460/1724] loss: 0.518, ave_loss: 0.468
[75]  [1480/1724] loss: 0.401, ave_loss: 0.467
[76]  [1500/1724] loss: 0.329, ave_loss: 0.465
[77]  [1520/1724] loss: 0.486, ave_loss: 0.465
[78]  [1540/1724] loss: 0.530, ave_loss: 0.466
[79]  [1560/1724] loss: 0.352, ave_loss: 0.465
[80]  [1580/1724] loss: 0.481, ave_loss: 0.465
[81]  [1600/1724] loss: 0.369, ave_loss: 0.464
[82]  [1620/1724] loss: 0.448, ave_loss: 0.463
[83]  [1640/1724] loss: 0.437, ave_loss: 0.463
[84]  [1660/1724] loss: 0.466, ave_loss: 0.463
[85]  [1680/1724] loss: 0.401, ave_loss: 0.462
[86]  [1700/1724] loss: 0.379, ave_loss: 0.461
[87]  [1720/1724] loss: 0.484, ave_loss: 0.462
[88]  [1740/1724] loss: 0.493, ave_loss: 0.462

Finished Training finishing at 2021-08-30 19:07:48.453887
printing_out epoch  24.501160092807424 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.620e-01
Validation Loss: 2.692e+05
Validation ROC: 0.6190
No improvement, still saving model
4.4988399071925755 epochs left to go

Training Epoch 24.501160092807424/30 starting at 2021-08-30 19:08:36.519043
[1]  [0/1724] loss: 0.545, ave_loss: 0.545
[2]  [20/1724] loss: 0.437, ave_loss: 0.491
[3]  [40/1724] loss: 0.496, ave_loss: 0.493
[4]  [60/1724] loss: 0.438, ave_loss: 0.479
[5]  [80/1724] loss: 0.526, ave_loss: 0.488
[6]  [100/1724] loss: 0.406, ave_loss: 0.475
[7]  [120/1724] loss: 0.464, ave_loss: 0.473
[8]  [140/1724] loss: 0.723, ave_loss: 0.505
[9]  [160/1724] loss: 0.444, ave_loss: 0.498
[10]  [180/1724] loss: 0.416, ave_loss: 0.490
[11]  [200/1724] loss: 0.492, ave_loss: 0.490
[12]  [220/1724] loss: 0.406, ave_loss: 0.483
[13]  [240/1724] loss: 0.495, ave_loss: 0.484
[14]  [260/1724] loss: 0.424, ave_loss: 0.479
[15]  [280/1724] loss: 0.543, ave_loss: 0.484
[16]  [300/1724] loss: 0.506, ave_loss: 0.485
[17]  [320/1724] loss: 0.373, ave_loss: 0.478
[18]  [340/1724] loss: 0.437, ave_loss: 0.476
[19]  [360/1724] loss: 0.645, ave_loss: 0.485
[20]  [380/1724] loss: 0.589, ave_loss: 0.490
[21]  [400/1724] loss: 0.384, ave_loss: 0.485
[22]  [420/1724] loss: 0.486, ave_loss: 0.485
[23]  [440/1724] loss: 0.428, ave_loss: 0.483
[24]  [460/1724] loss: 0.585, ave_loss: 0.487
[25]  [480/1724] loss: 0.327, ave_loss: 0.481
[26]  [500/1724] loss: 0.426, ave_loss: 0.478
[27]  [520/1724] loss: 0.578, ave_loss: 0.482
[28]  [540/1724] loss: 0.493, ave_loss: 0.483
[29]  [560/1724] loss: 0.469, ave_loss: 0.482
[30]  [580/1724] loss: 0.405, ave_loss: 0.480
[31]  [600/1724] loss: 0.492, ave_loss: 0.480
[32]  [620/1724] loss: 0.499, ave_loss: 0.481
[33]  [640/1724] loss: 0.444, ave_loss: 0.479
[34]  [660/1724] loss: 0.400, ave_loss: 0.477
[35]  [680/1724] loss: 0.375, ave_loss: 0.474
[36]  [700/1724] loss: 0.657, ave_loss: 0.479
[37]  [720/1724] loss: 0.513, ave_loss: 0.480
[38]  [740/1724] loss: 0.443, ave_loss: 0.479
[39]  [760/1724] loss: 0.408, ave_loss: 0.477
[40]  [780/1724] loss: 0.551, ave_loss: 0.479
[41]  [800/1724] loss: 0.339, ave_loss: 0.476
[42]  [820/1724] loss: 0.518, ave_loss: 0.477
[43]  [840/1724] loss: 0.418, ave_loss: 0.475
[44]  [860/1724] loss: 0.402, ave_loss: 0.474
[45]  [880/1724] loss: 0.449, ave_loss: 0.473
[46]  [900/1724] loss: 0.458, ave_loss: 0.473
[47]  [920/1724] loss: 0.354, ave_loss: 0.470
[48]  [940/1724] loss: 0.754, ave_loss: 0.476
[49]  [960/1724] loss: 0.335, ave_loss: 0.473
[50]  [980/1724] loss: 0.598, ave_loss: 0.476
[51]  [1000/1724] loss: 0.532, ave_loss: 0.477
[52]  [1020/1724] loss: 0.461, ave_loss: 0.477
[53]  [1040/1724] loss: 0.660, ave_loss: 0.480
[54]  [1060/1724] loss: 0.442, ave_loss: 0.479
[55]  [1080/1724] loss: 0.556, ave_loss: 0.481
[56]  [1100/1724] loss: 0.459, ave_loss: 0.480
[57]  [1120/1724] loss: 0.483, ave_loss: 0.480
[58]  [1140/1724] loss: 0.506, ave_loss: 0.481
[59]  [1160/1724] loss: 0.618, ave_loss: 0.483
[60]  [1180/1724] loss: 0.392, ave_loss: 0.482
[61]  [1200/1724] loss: 0.486, ave_loss: 0.482
[62]  [1220/1724] loss: 0.417, ave_loss: 0.481
[63]  [1240/1724] loss: 0.467, ave_loss: 0.480
[64]  [1260/1724] loss: 0.513, ave_loss: 0.481
[65]  [1280/1724] loss: 0.587, ave_loss: 0.483
[66]  [1300/1724] loss: 0.438, ave_loss: 0.482
[67]  [1320/1724] loss: 0.488, ave_loss: 0.482
[68]  [1340/1724] loss: 0.444, ave_loss: 0.481
[69]  [1360/1724] loss: 0.540, ave_loss: 0.482
[70]  [1380/1724] loss: 0.424, ave_loss: 0.481
[71]  [1400/1724] loss: 0.481, ave_loss: 0.481
[72]  [1420/1724] loss: 0.468, ave_loss: 0.481
[73]  [1440/1724] loss: 0.329, ave_loss: 0.479
[74]  [1460/1724] loss: 0.529, ave_loss: 0.480
[75]  [1480/1724] loss: 0.389, ave_loss: 0.479
[76]  [1500/1724] loss: 0.525, ave_loss: 0.479
[77]  [1520/1724] loss: 0.431, ave_loss: 0.479
[78]  [1540/1724] loss: 0.487, ave_loss: 0.479
[79]  [1560/1724] loss: 0.482, ave_loss: 0.479
[80]  [1580/1724] loss: 0.640, ave_loss: 0.481
[81]  [1600/1724] loss: 0.422, ave_loss: 0.480
[82]  [1620/1724] loss: 0.465, ave_loss: 0.480
[83]  [1640/1724] loss: 0.476, ave_loss: 0.480
[84]  [1660/1724] loss: 0.507, ave_loss: 0.480
[85]  [1680/1724] loss: 0.473, ave_loss: 0.480
[86]  [1700/1724] loss: 0.353, ave_loss: 0.479
[87]  [1720/1724] loss: 0.384, ave_loss: 0.478
[88]  [1740/1724] loss: 0.416, ave_loss: 0.477

Finished Training finishing at 2021-08-30 19:10:52.486482
printing_out epoch  25.52204176334107 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.768e-01
Validation Loss: 2.642e+05
Validation ROC: 0.6374
No improvement, still saving model
3.4779582366589317 epochs left to go

Training Epoch 25.52204176334107/30 starting at 2021-08-30 19:11:41.149277
[1]  [0/1724] loss: 0.569, ave_loss: 0.569
[2]  [20/1724] loss: 0.454, ave_loss: 0.512
[3]  [40/1724] loss: 0.550, ave_loss: 0.524
[4]  [60/1724] loss: 0.393, ave_loss: 0.491
[5]  [80/1724] loss: 0.571, ave_loss: 0.507
[6]  [100/1724] loss: 0.462, ave_loss: 0.500
[7]  [120/1724] loss: 0.658, ave_loss: 0.522
[8]  [140/1724] loss: 0.394, ave_loss: 0.506
[9]  [160/1724] loss: 0.591, ave_loss: 0.516
[10]  [180/1724] loss: 0.574, ave_loss: 0.522
[11]  [200/1724] loss: 0.376, ave_loss: 0.508
[12]  [220/1724] loss: 0.529, ave_loss: 0.510
[13]  [240/1724] loss: 0.288, ave_loss: 0.493
[14]  [260/1724] loss: 0.517, ave_loss: 0.495
[15]  [280/1724] loss: 0.581, ave_loss: 0.500
[16]  [300/1724] loss: 0.643, ave_loss: 0.509
[17]  [320/1724] loss: 0.391, ave_loss: 0.502
[18]  [340/1724] loss: 0.494, ave_loss: 0.502
[19]  [360/1724] loss: 0.478, ave_loss: 0.501
[20]  [380/1724] loss: 0.490, ave_loss: 0.500
[21]  [400/1724] loss: 0.521, ave_loss: 0.501
[22]  [420/1724] loss: 0.631, ave_loss: 0.507
[23]  [440/1724] loss: 0.482, ave_loss: 0.506
[24]  [460/1724] loss: 0.409, ave_loss: 0.502
[25]  [480/1724] loss: 0.601, ave_loss: 0.506
[26]  [500/1724] loss: 0.375, ave_loss: 0.501
[27]  [520/1724] loss: 0.678, ave_loss: 0.507
[28]  [540/1724] loss: 0.471, ave_loss: 0.506
[29]  [560/1724] loss: 0.588, ave_loss: 0.509
[30]  [580/1724] loss: 0.503, ave_loss: 0.509
[31]  [600/1724] loss: 0.556, ave_loss: 0.510
[32]  [620/1724] loss: 0.572, ave_loss: 0.512
[33]  [640/1724] loss: 0.460, ave_loss: 0.511
[34]  [660/1724] loss: 0.511, ave_loss: 0.511
[35]  [680/1724] loss: 0.399, ave_loss: 0.507
[36]  [700/1724] loss: 0.452, ave_loss: 0.506
[37]  [720/1724] loss: 0.403, ave_loss: 0.503
[38]  [740/1724] loss: 0.448, ave_loss: 0.502
[39]  [760/1724] loss: 0.346, ave_loss: 0.498
[40]  [780/1724] loss: 0.414, ave_loss: 0.496
[41]  [800/1724] loss: 0.617, ave_loss: 0.499
[42]  [820/1724] loss: 0.630, ave_loss: 0.502
[43]  [840/1724] loss: 0.401, ave_loss: 0.499
[44]  [860/1724] loss: 0.508, ave_loss: 0.500
[45]  [880/1724] loss: 0.544, ave_loss: 0.501
[46]  [900/1724] loss: 0.376, ave_loss: 0.498
[47]  [920/1724] loss: 0.414, ave_loss: 0.496
[48]  [940/1724] loss: 0.476, ave_loss: 0.496
[49]  [960/1724] loss: 0.454, ave_loss: 0.495
[50]  [980/1724] loss: 0.348, ave_loss: 0.492
[51]  [1000/1724] loss: 0.515, ave_loss: 0.492
[52]  [1020/1724] loss: 0.462, ave_loss: 0.492
[53]  [1040/1724] loss: 0.461, ave_loss: 0.491
[54]  [1060/1724] loss: 0.390, ave_loss: 0.489
[55]  [1080/1724] loss: 0.310, ave_loss: 0.486
[56]  [1100/1724] loss: 0.456, ave_loss: 0.485
[57]  [1120/1724] loss: 0.454, ave_loss: 0.485
[58]  [1140/1724] loss: 0.505, ave_loss: 0.485
[59]  [1160/1724] loss: 0.531, ave_loss: 0.486
[60]  [1180/1724] loss: 0.638, ave_loss: 0.489
[61]  [1200/1724] loss: 0.482, ave_loss: 0.488
[62]  [1220/1724] loss: 0.571, ave_loss: 0.490
[63]  [1240/1724] loss: 0.415, ave_loss: 0.489
[64]  [1260/1724] loss: 0.499, ave_loss: 0.489
[65]  [1280/1724] loss: 0.495, ave_loss: 0.489
[66]  [1300/1724] loss: 0.399, ave_loss: 0.487
[67]  [1320/1724] loss: 0.438, ave_loss: 0.487
[68]  [1340/1724] loss: 0.467, ave_loss: 0.486
[69]  [1360/1724] loss: 0.450, ave_loss: 0.486
[70]  [1380/1724] loss: 0.402, ave_loss: 0.485
[71]  [1400/1724] loss: 0.459, ave_loss: 0.484
[72]  [1420/1724] loss: 0.545, ave_loss: 0.485
[73]  [1440/1724] loss: 0.477, ave_loss: 0.485
[74]  [1460/1724] loss: 0.432, ave_loss: 0.484
[75]  [1480/1724] loss: 0.409, ave_loss: 0.483
[76]  [1500/1724] loss: 0.311, ave_loss: 0.481
[77]  [1520/1724] loss: 0.469, ave_loss: 0.481
[78]  [1540/1724] loss: 0.650, ave_loss: 0.483
[79]  [1560/1724] loss: 0.450, ave_loss: 0.483
[80]  [1580/1724] loss: 0.433, ave_loss: 0.482
[81]  [1600/1724] loss: 0.552, ave_loss: 0.483
[82]  [1620/1724] loss: 0.449, ave_loss: 0.483
[83]  [1640/1724] loss: 0.482, ave_loss: 0.483
[84]  [1660/1724] loss: 0.714, ave_loss: 0.485
[85]  [1680/1724] loss: 0.451, ave_loss: 0.485
[86]  [1700/1724] loss: 0.542, ave_loss: 0.486
[87]  [1720/1724] loss: 0.507, ave_loss: 0.486
[88]  [1740/1724] loss: 0.671, ave_loss: 0.488

Finished Training finishing at 2021-08-30 19:14:26.604846
printing_out epoch  26.54292343387471 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.879e-01
Validation Loss: 2.594e+05
Validation ROC: 0.6237
No improvement, still saving model
2.4570765661252914 epochs left to go

Training Epoch 26.54292343387471/30 starting at 2021-08-30 19:16:26.870781
[1]  [0/1724] loss: 0.770, ave_loss: 0.770
[2]  [20/1724] loss: 0.531, ave_loss: 0.651
[3]  [40/1724] loss: 0.607, ave_loss: 0.636
[4]  [60/1724] loss: 0.335, ave_loss: 0.561
[5]  [80/1724] loss: 0.402, ave_loss: 0.529
[6]  [100/1724] loss: 0.469, ave_loss: 0.519
[7]  [120/1724] loss: 0.358, ave_loss: 0.496
[8]  [140/1724] loss: 0.539, ave_loss: 0.501
[9]  [160/1724] loss: 0.473, ave_loss: 0.498
[10]  [180/1724] loss: 0.525, ave_loss: 0.501
[11]  [200/1724] loss: 0.424, ave_loss: 0.494
[12]  [220/1724] loss: 0.429, ave_loss: 0.488
[13]  [240/1724] loss: 0.425, ave_loss: 0.484
[14]  [260/1724] loss: 0.518, ave_loss: 0.486
[15]  [280/1724] loss: 0.429, ave_loss: 0.482
[16]  [300/1724] loss: 0.400, ave_loss: 0.477
[17]  [320/1724] loss: 0.442, ave_loss: 0.475
[18]  [340/1724] loss: 0.511, ave_loss: 0.477
[19]  [360/1724] loss: 0.355, ave_loss: 0.471
[20]  [380/1724] loss: 0.506, ave_loss: 0.472
[21]  [400/1724] loss: 0.524, ave_loss: 0.475
[22]  [420/1724] loss: 0.444, ave_loss: 0.473
[23]  [440/1724] loss: 0.669, ave_loss: 0.482
[24]  [460/1724] loss: 0.418, ave_loss: 0.479
[25]  [480/1724] loss: 0.396, ave_loss: 0.476
[26]  [500/1724] loss: 0.410, ave_loss: 0.473
[27]  [520/1724] loss: 0.310, ave_loss: 0.467
[28]  [540/1724] loss: 0.452, ave_loss: 0.467
[29]  [560/1724] loss: 0.651, ave_loss: 0.473
[30]  [580/1724] loss: 0.420, ave_loss: 0.471
[31]  [600/1724] loss: 0.388, ave_loss: 0.469
[32]  [620/1724] loss: 0.366, ave_loss: 0.466
[33]  [640/1724] loss: 0.514, ave_loss: 0.467
[34]  [660/1724] loss: 0.564, ave_loss: 0.470
[35]  [680/1724] loss: 0.450, ave_loss: 0.469
[36]  [700/1724] loss: 0.569, ave_loss: 0.472
[37]  [720/1724] loss: 0.538, ave_loss: 0.474
[38]  [740/1724] loss: 0.455, ave_loss: 0.473
[39]  [760/1724] loss: 0.524, ave_loss: 0.475
[40]  [780/1724] loss: 0.402, ave_loss: 0.473
[41]  [800/1724] loss: 0.583, ave_loss: 0.476
[42]  [820/1724] loss: 0.479, ave_loss: 0.476
[43]  [840/1724] loss: 0.364, ave_loss: 0.473
[44]  [860/1724] loss: 0.472, ave_loss: 0.473
[45]  [880/1724] loss: 0.430, ave_loss: 0.472
[46]  [900/1724] loss: 0.528, ave_loss: 0.473
[47]  [920/1724] loss: 0.538, ave_loss: 0.475
[48]  [940/1724] loss: 0.339, ave_loss: 0.472
[49]  [960/1724] loss: 0.414, ave_loss: 0.471
[50]  [980/1724] loss: 0.331, ave_loss: 0.468
[51]  [1000/1724] loss: 0.532, ave_loss: 0.469
[52]  [1020/1724] loss: 0.391, ave_loss: 0.468
[53]  [1040/1724] loss: 0.437, ave_loss: 0.467
[54]  [1060/1724] loss: 0.316, ave_loss: 0.464
[55]  [1080/1724] loss: 0.429, ave_loss: 0.464
[56]  [1100/1724] loss: 0.562, ave_loss: 0.465
[57]  [1120/1724] loss: 0.434, ave_loss: 0.465
[58]  [1140/1724] loss: 0.378, ave_loss: 0.463
[59]  [1160/1724] loss: 0.435, ave_loss: 0.463
[60]  [1180/1724] loss: 0.496, ave_loss: 0.463
[61]  [1200/1724] loss: 0.700, ave_loss: 0.467
[62]  [1220/1724] loss: 0.460, ave_loss: 0.467
[63]  [1240/1724] loss: 0.346, ave_loss: 0.465
[64]  [1260/1724] loss: 0.470, ave_loss: 0.465
[65]  [1280/1724] loss: 0.476, ave_loss: 0.465
[66]  [1300/1724] loss: 0.536, ave_loss: 0.466
[67]  [1320/1724] loss: 0.370, ave_loss: 0.465
[68]  [1340/1724] loss: 0.452, ave_loss: 0.465
[69]  [1360/1724] loss: 0.488, ave_loss: 0.465
[70]  [1380/1724] loss: 0.464, ave_loss: 0.465
[71]  [1400/1724] loss: 0.464, ave_loss: 0.465
[72]  [1420/1724] loss: 0.428, ave_loss: 0.465
[73]  [1440/1724] loss: 0.393, ave_loss: 0.464
[74]  [1460/1724] loss: 0.443, ave_loss: 0.463
[75]  [1480/1724] loss: 0.531, ave_loss: 0.464
[76]  [1500/1724] loss: 0.413, ave_loss: 0.464
[77]  [1520/1724] loss: 0.484, ave_loss: 0.464
[78]  [1540/1724] loss: 0.514, ave_loss: 0.465
[79]  [1560/1724] loss: 0.411, ave_loss: 0.464
[80]  [1580/1724] loss: 0.409, ave_loss: 0.463
[81]  [1600/1724] loss: 0.352, ave_loss: 0.462
[82]  [1620/1724] loss: 0.437, ave_loss: 0.461
[83]  [1640/1724] loss: 0.414, ave_loss: 0.461
[84]  [1660/1724] loss: 0.468, ave_loss: 0.461
[85]  [1680/1724] loss: 0.462, ave_loss: 0.461
[86]  [1700/1724] loss: 0.448, ave_loss: 0.461
[87]  [1720/1724] loss: 0.492, ave_loss: 0.461
[88]  [1740/1724] loss: 0.462, ave_loss: 0.461

Finished Training finishing at 2021-08-30 19:19:41.181124
printing_out epoch  27.563805104408353 learning rate: 0.0003199956068772566
0.0003103957386709389
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.612e-01
Validation Loss: 2.575e+05
Validation ROC: 0.6220
No improvement, still saving model
1.4361948955916475 epochs left to go

Training Epoch 27.563805104408353/30 starting at 2021-08-30 19:20:28.253399
[1]  [0/1724] loss: 0.322, ave_loss: 0.322
[2]  [20/1724] loss: 0.702, ave_loss: 0.512
[3]  [40/1724] loss: 0.353, ave_loss: 0.459
[4]  [60/1724] loss: 0.430, ave_loss: 0.452
[5]  [80/1724] loss: 0.330, ave_loss: 0.427
[6]  [100/1724] loss: 0.572, ave_loss: 0.451
[7]  [120/1724] loss: 0.460, ave_loss: 0.453
[8]  [140/1724] loss: 0.447, ave_loss: 0.452
[9]  [160/1724] loss: 0.455, ave_loss: 0.452
[10]  [180/1724] loss: 0.536, ave_loss: 0.461
[11]  [200/1724] loss: 0.639, ave_loss: 0.477
[12]  [220/1724] loss: 0.575, ave_loss: 0.485
[13]  [240/1724] loss: 0.392, ave_loss: 0.478
[14]  [260/1724] loss: 0.621, ave_loss: 0.488
[15]  [280/1724] loss: 0.467, ave_loss: 0.487
[16]  [300/1724] loss: 0.384, ave_loss: 0.480
[17]  [320/1724] loss: 0.524, ave_loss: 0.483
[18]  [340/1724] loss: 0.520, ave_loss: 0.485
[19]  [360/1724] loss: 0.403, ave_loss: 0.481
[20]  [380/1724] loss: 0.471, ave_loss: 0.480
[21]  [400/1724] loss: 0.385, ave_loss: 0.476
[22]  [420/1724] loss: 0.336, ave_loss: 0.469
[23]  [440/1724] loss: 0.654, ave_loss: 0.477
[24]  [460/1724] loss: 0.367, ave_loss: 0.473
[25]  [480/1724] loss: 0.346, ave_loss: 0.468
[26]  [500/1724] loss: 0.425, ave_loss: 0.466
[27]  [520/1724] loss: 0.517, ave_loss: 0.468
[28]  [540/1724] loss: 0.406, ave_loss: 0.466
[29]  [560/1724] loss: 0.415, ave_loss: 0.464
[30]  [580/1724] loss: 0.462, ave_loss: 0.464
[31]  [600/1724] loss: 0.544, ave_loss: 0.466
[32]  [620/1724] loss: 0.486, ave_loss: 0.467
[33]  [640/1724] loss: 0.418, ave_loss: 0.466
[34]  [660/1724] loss: 0.570, ave_loss: 0.469
[35]  [680/1724] loss: 0.647, ave_loss: 0.474
[36]  [700/1724] loss: 0.320, ave_loss: 0.469
[37]  [720/1724] loss: 0.619, ave_loss: 0.474
[38]  [740/1724] loss: 0.648, ave_loss: 0.478
[39]  [760/1724] loss: 0.509, ave_loss: 0.479
[40]  [780/1724] loss: 0.380, ave_loss: 0.476
[41]  [800/1724] loss: 0.524, ave_loss: 0.478
[42]  [820/1724] loss: 0.432, ave_loss: 0.477
[43]  [840/1724] loss: 0.499, ave_loss: 0.477
[44]  [860/1724] loss: 0.511, ave_loss: 0.478
[45]  [880/1724] loss: 0.527, ave_loss: 0.479
[46]  [900/1724] loss: 0.461, ave_loss: 0.478
[47]  [920/1724] loss: 0.476, ave_loss: 0.478
[48]  [940/1724] loss: 0.316, ave_loss: 0.475
[49]  [960/1724] loss: 0.524, ave_loss: 0.476
[50]  [980/1724] loss: 0.479, ave_loss: 0.476
[51]  [1000/1724] loss: 0.463, ave_loss: 0.476
[52]  [1020/1724] loss: 0.527, ave_loss: 0.477
[53]  [1040/1724] loss: 0.448, ave_loss: 0.476
[54]  [1060/1724] loss: 0.329, ave_loss: 0.474
[55]  [1080/1724] loss: 0.515, ave_loss: 0.474
[56]  [1100/1724] loss: 0.607, ave_loss: 0.477
[57]  [1120/1724] loss: 0.388, ave_loss: 0.475
[58]  [1140/1724] loss: 0.474, ave_loss: 0.475
[59]  [1160/1724] loss: 0.410, ave_loss: 0.474
[60]  [1180/1724] loss: 0.459, ave_loss: 0.474
[61]  [1200/1724] loss: 0.427, ave_loss: 0.473
[62]  [1220/1724] loss: 0.609, ave_loss: 0.475
[63]  [1240/1724] loss: 0.278, ave_loss: 0.472
[64]  [1260/1724] loss: 0.529, ave_loss: 0.473
[65]  [1280/1724] loss: 0.521, ave_loss: 0.474
[66]  [1300/1724] loss: 0.469, ave_loss: 0.474
[67]  [1320/1724] loss: 0.408, ave_loss: 0.473
[68]  [1340/1724] loss: 0.383, ave_loss: 0.471
[69]  [1360/1724] loss: 0.397, ave_loss: 0.470
[70]  [1380/1724] loss: 0.418, ave_loss: 0.469
[71]  [1400/1724] loss: 0.533, ave_loss: 0.470
[72]  [1420/1724] loss: 0.395, ave_loss: 0.469
[73]  [1440/1724] loss: 0.467, ave_loss: 0.469
[74]  [1460/1724] loss: 0.510, ave_loss: 0.470
[75]  [1480/1724] loss: 0.545, ave_loss: 0.471
[76]  [1500/1724] loss: 0.505, ave_loss: 0.471
[77]  [1520/1724] loss: 0.597, ave_loss: 0.473
[78]  [1540/1724] loss: 0.592, ave_loss: 0.474
[79]  [1560/1724] loss: 0.381, ave_loss: 0.473
[80]  [1580/1724] loss: 0.388, ave_loss: 0.472
[81]  [1600/1724] loss: 0.380, ave_loss: 0.471
[82]  [1620/1724] loss: 0.606, ave_loss: 0.473
[83]  [1640/1724] loss: 0.468, ave_loss: 0.473
[84]  [1660/1724] loss: 0.369, ave_loss: 0.471
[85]  [1680/1724] loss: 0.536, ave_loss: 0.472
[86]  [1700/1724] loss: 0.432, ave_loss: 0.472
[87]  [1720/1724] loss: 0.499, ave_loss: 0.472
[88]  [1740/1724] loss: 0.531, ave_loss: 0.473

Finished Training finishing at 2021-08-30 19:23:04.876206
printing_out epoch  28.584686774941996 learning rate: 0.00029090509716114235
0.0002821779442463081
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.727e-01
Validation Loss: 2.565e+05
Validation ROC: 0.6259
No improvement, still saving model
0.4153132250580036 epochs left to go

Training Epoch 28.584686774941996/30 starting at 2021-08-30 19:23:51.639379
[1]  [0/1724] loss: 0.640, ave_loss: 0.640
[2]  [20/1724] loss: 0.482, ave_loss: 0.561
[3]  [40/1724] loss: 0.456, ave_loss: 0.526
[4]  [60/1724] loss: 0.394, ave_loss: 0.493
[5]  [80/1724] loss: 0.309, ave_loss: 0.456
[6]  [100/1724] loss: 0.354, ave_loss: 0.439
[7]  [120/1724] loss: 0.560, ave_loss: 0.456
[8]  [140/1724] loss: 0.385, ave_loss: 0.448
[9]  [160/1724] loss: 0.570, ave_loss: 0.461
[10]  [180/1724] loss: 0.471, ave_loss: 0.462
[11]  [200/1724] loss: 0.524, ave_loss: 0.468
[12]  [220/1724] loss: 0.500, ave_loss: 0.470
[13]  [240/1724] loss: 0.456, ave_loss: 0.469
[14]  [260/1724] loss: 0.639, ave_loss: 0.481
[15]  [280/1724] loss: 0.372, ave_loss: 0.474
[16]  [300/1724] loss: 0.398, ave_loss: 0.469
[17]  [320/1724] loss: 0.361, ave_loss: 0.463
[18]  [340/1724] loss: 0.459, ave_loss: 0.463
[19]  [360/1724] loss: 0.489, ave_loss: 0.464
[20]  [380/1724] loss: 0.419, ave_loss: 0.462
[21]  [400/1724] loss: 0.504, ave_loss: 0.464
[22]  [420/1724] loss: 0.375, ave_loss: 0.460
[23]  [440/1724] loss: 0.471, ave_loss: 0.460
[24]  [460/1724] loss: 0.389, ave_loss: 0.457
[25]  [480/1724] loss: 0.319, ave_loss: 0.452
[26]  [500/1724] loss: 0.491, ave_loss: 0.453
[27]  [520/1724] loss: 0.484, ave_loss: 0.454
[28]  [540/1724] loss: 0.411, ave_loss: 0.453
[29]  [560/1724] loss: 0.505, ave_loss: 0.455
[30]  [580/1724] loss: 0.534, ave_loss: 0.457
[31]  [600/1724] loss: 0.424, ave_loss: 0.456
[32]  [620/1724] loss: 0.605, ave_loss: 0.461
[33]  [640/1724] loss: 0.453, ave_loss: 0.461
[34]  [660/1724] loss: 0.395, ave_loss: 0.459
[35]  [680/1724] loss: 0.457, ave_loss: 0.459
[36]  [700/1724] loss: 0.546, ave_loss: 0.461
[37]  [720/1724] loss: 0.422, ave_loss: 0.460
[38]  [740/1724] loss: 0.422, ave_loss: 0.459
[39]  [760/1724] loss: 0.380, ave_loss: 0.457
[40]  [780/1724] loss: 0.406, ave_loss: 0.456
[41]  [800/1724] loss: 0.346, ave_loss: 0.453
[42]  [820/1724] loss: 0.490, ave_loss: 0.454
[43]  [840/1724] loss: 0.580, ave_loss: 0.457
[44]  [860/1724] loss: 0.425, ave_loss: 0.456
[45]  [880/1724] loss: 0.484, ave_loss: 0.457
[46]  [900/1724] loss: 0.301, ave_loss: 0.453
[47]  [920/1724] loss: 0.491, ave_loss: 0.454
[48]  [940/1724] loss: 0.676, ave_loss: 0.459
[49]  [960/1724] loss: 0.437, ave_loss: 0.458
[50]  [980/1724] loss: 0.364, ave_loss: 0.456
[51]  [1000/1724] loss: 0.415, ave_loss: 0.456
[52]  [1020/1724] loss: 0.384, ave_loss: 0.454
[53]  [1040/1724] loss: 0.328, ave_loss: 0.452
[54]  [1060/1724] loss: 0.477, ave_loss: 0.452
[55]  [1080/1724] loss: 0.471, ave_loss: 0.453
[56]  [1100/1724] loss: 0.400, ave_loss: 0.452
[57]  [1120/1724] loss: 0.354, ave_loss: 0.450
[58]  [1140/1724] loss: 0.319, ave_loss: 0.448
[59]  [1160/1724] loss: 0.502, ave_loss: 0.449
[60]  [1180/1724] loss: 0.504, ave_loss: 0.450
[61]  [1200/1724] loss: 0.584, ave_loss: 0.452
[62]  [1220/1724] loss: 0.373, ave_loss: 0.451
[63]  [1240/1724] loss: 0.443, ave_loss: 0.450
[64]  [1260/1724] loss: 0.524, ave_loss: 0.452
[65]  [1280/1724] loss: 0.568, ave_loss: 0.453
[66]  [1300/1724] loss: 0.353, ave_loss: 0.452
[67]  [1320/1724] loss: 0.509, ave_loss: 0.453
[68]  [1340/1724] loss: 0.446, ave_loss: 0.453
[69]  [1360/1724] loss: 0.334, ave_loss: 0.451
[70]  [1380/1724] loss: 0.319, ave_loss: 0.449
[71]  [1400/1724] loss: 0.476, ave_loss: 0.449
[72]  [1420/1724] loss: 0.631, ave_loss: 0.452
[73]  [1440/1724] loss: 0.481, ave_loss: 0.452
[74]  [1460/1724] loss: 0.383, ave_loss: 0.451
[75]  [1480/1724] loss: 0.307, ave_loss: 0.449
[76]  [1500/1724] loss: 0.356, ave_loss: 0.448
[77]  [1520/1724] loss: 0.310, ave_loss: 0.446
[78]  [1540/1724] loss: 0.437, ave_loss: 0.446
[79]  [1560/1724] loss: 0.559, ave_loss: 0.448
[80]  [1580/1724] loss: 0.368, ave_loss: 0.447
[81]  [1600/1724] loss: 0.448, ave_loss: 0.447
[82]  [1620/1724] loss: 0.393, ave_loss: 0.446
[83]  [1640/1724] loss: 0.434, ave_loss: 0.446
[84]  [1660/1724] loss: 0.495, ave_loss: 0.447
[85]  [1680/1724] loss: 0.429, ave_loss: 0.446
[86]  [1700/1724] loss: 0.600, ave_loss: 0.448
[87]  [1720/1724] loss: 0.466, ave_loss: 0.448
[88]  [1740/1724] loss: 0.501, ave_loss: 0.449

Finished Training finishing at 2021-08-30 19:26:18.303769
printing_out epoch  29.605568445475637 learning rate: 0.00026445917923740214
0.0002565254038602801
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 4.489e-01
Validation Loss: 2.552e+05
Validation ROC: 0.6481
Saving model
saving results
