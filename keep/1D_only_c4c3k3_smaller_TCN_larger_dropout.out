reading from multiple data folder!**********************************************
Selected signals (determines which signals are used for training):
[q95 safety factor, internal inductance, plasma current, Locked mode amplitude, Normalized Beta, stored energy, Plasma density, Radiated Power Core, Radiated Power Edge, Input Power (beam for d3d), Input Beam Torque, plasma current direction, plasma current target, plasma current error, Electron temperature profile, Electron density profile]
...done
Training on 1724 shots, testing on 857 shots
NO SCALARS ARE USED, ONLY 1D SIGNALS
n_scalars,n_profiles,profile_size= 0 2 64
Classical convolution with channels  2 4
Classical convolution with channels  4 3
InputBlock parameters:  0 2 64 ['c4', 'c3'] 3 10 0.15 2
TCN parameters:  3 1 [5, 5, 5, 5, 5, 5, 5, 5] 5 0.15
Using single GPU..........................................
29 epochs left to go

Training Epoch 0/30 starting at 2021-08-25 15:08:45.112817
[1]  [0/1724] loss: 1.298, ave_loss: 1.298
[2]  [20/1724] loss: 1.890, ave_loss: 1.594
[3]  [40/1724] loss: 1.940, ave_loss: 1.709
[4]  [60/1724] loss: 1.852, ave_loss: 1.745
[5]  [80/1724] loss: 1.984, ave_loss: 1.793
[6]  [100/1724] loss: 1.674, ave_loss: 1.773
[7]  [120/1724] loss: 1.409, ave_loss: 1.721
[8]  [140/1724] loss: 1.877, ave_loss: 1.741
[9]  [160/1724] loss: 1.452, ave_loss: 1.708
[10]  [180/1724] loss: 1.882, ave_loss: 1.726
[11]  [200/1724] loss: 1.849, ave_loss: 1.737
[12]  [220/1724] loss: 1.520, ave_loss: 1.719
[13]  [240/1724] loss: 2.011, ave_loss: 1.741
[14]  [260/1724] loss: 2.007, ave_loss: 1.760
[15]  [280/1724] loss: 1.776, ave_loss: 1.761
[16]  [300/1724] loss: 2.017, ave_loss: 1.777
[17]  [320/1724] loss: 1.035, ave_loss: 1.734
[18]  [340/1724] loss: 2.035, ave_loss: 1.750
[19]  [360/1724] loss: 1.327, ave_loss: 1.728
[20]  [380/1724] loss: 1.473, ave_loss: 1.715
[21]  [400/1724] loss: 1.535, ave_loss: 1.707
[22]  [420/1724] loss: 1.406, ave_loss: 1.693
[23]  [440/1724] loss: 1.498, ave_loss: 1.685
[24]  [460/1724] loss: 1.617, ave_loss: 1.682
[25]  [480/1724] loss: 1.498, ave_loss: 1.674
[26]  [500/1724] loss: 1.271, ave_loss: 1.659
[27]  [520/1724] loss: 1.692, ave_loss: 1.660
[28]  [540/1724] loss: 1.090, ave_loss: 1.640
[29]  [560/1724] loss: 1.639, ave_loss: 1.640
[30]  [580/1724] loss: 1.214, ave_loss: 1.626
[31]  [600/1724] loss: 1.404, ave_loss: 1.618
[32]  [620/1724] loss: 1.209, ave_loss: 1.606
[33]  [640/1724] loss: 1.090, ave_loss: 1.590
[34]  [660/1724] loss: 1.615, ave_loss: 1.591
[35]  [680/1724] loss: 1.329, ave_loss: 1.583
[36]  [700/1724] loss: 1.686, ave_loss: 1.586
[37]  [720/1724] loss: 1.203, ave_loss: 1.576
[38]  [740/1724] loss: 0.959, ave_loss: 1.560
[39]  [760/1724] loss: 1.653, ave_loss: 1.562
[40]  [780/1724] loss: 1.326, ave_loss: 1.556
[41]  [800/1724] loss: 1.706, ave_loss: 1.560
[42]  [820/1724] loss: 1.042, ave_loss: 1.547
[43]  [840/1724] loss: 1.354, ave_loss: 1.543
[44]  [860/1724] loss: 1.053, ave_loss: 1.532
[45]  [880/1724] loss: 1.225, ave_loss: 1.525
[46]  [900/1724] loss: 1.365, ave_loss: 1.521
[47]  [920/1724] loss: 1.178, ave_loss: 1.514
[48]  [940/1724] loss: 1.061, ave_loss: 1.505
[49]  [960/1724] loss: 0.897, ave_loss: 1.492
[50]  [980/1724] loss: 1.585, ave_loss: 1.494
[51]  [1000/1724] loss: 1.283, ave_loss: 1.490
[52]  [1020/1724] loss: 1.285, ave_loss: 1.486
[53]  [1040/1724] loss: 1.363, ave_loss: 1.484
[54]  [1060/1724] loss: 1.118, ave_loss: 1.477
[55]  [1080/1724] loss: 0.962, ave_loss: 1.468
[56]  [1100/1724] loss: 0.992, ave_loss: 1.459
[57]  [1120/1724] loss: 1.089, ave_loss: 1.453
[58]  [1140/1724] loss: 1.459, ave_loss: 1.453
[59]  [1160/1724] loss: 1.109, ave_loss: 1.447
[60]  [1180/1724] loss: 0.965, ave_loss: 1.439
[61]  [1200/1724] loss: 1.042, ave_loss: 1.432
[62]  [1220/1724] loss: 1.165, ave_loss: 1.428
[63]  [1240/1724] loss: 0.936, ave_loss: 1.420
[64]  [1260/1724] loss: 1.199, ave_loss: 1.417
[65]  [1280/1724] loss: 1.181, ave_loss: 1.413
[66]  [1300/1724] loss: 1.140, ave_loss: 1.409
[67]  [1320/1724] loss: 1.167, ave_loss: 1.405
[68]  [1340/1724] loss: 1.159, ave_loss: 1.402
[69]  [1360/1724] loss: 0.932, ave_loss: 1.395
[70]  [1380/1724] loss: 1.052, ave_loss: 1.390
[71]  [1400/1724] loss: 1.051, ave_loss: 1.385
[72]  [1420/1724] loss: 0.941, ave_loss: 1.379
[73]  [1440/1724] loss: 1.021, ave_loss: 1.374
[74]  [1460/1724] loss: 0.983, ave_loss: 1.369
[75]  [1480/1724] loss: 1.056, ave_loss: 1.365
[76]  [1500/1724] loss: 0.994, ave_loss: 1.360
[77]  [1520/1724] loss: 0.875, ave_loss: 1.354
[78]  [1540/1724] loss: 0.867, ave_loss: 1.347
[79]  [1560/1724] loss: 0.986, ave_loss: 1.343
[80]  [1580/1724] loss: 1.024, ave_loss: 1.339
[81]  [1600/1724] loss: 0.849, ave_loss: 1.333
[82]  [1620/1724] loss: 0.923, ave_loss: 1.328
[83]  [1640/1724] loss: 0.916, ave_loss: 1.323
[84]  [1660/1724] loss: 0.949, ave_loss: 1.318
[85]  [1680/1724] loss: 0.877, ave_loss: 1.313
[86]  [1700/1724] loss: 0.872, ave_loss: 1.308
[87]  [1720/1724] loss: 0.768, ave_loss: 1.302
[88]  [1740/1724] loss: 0.853, ave_loss: 1.297

Finished Training finishing at 2021-08-25 15:12:15.349651
printing_out epoch  1.0208816705336428 learning rate: 0.0005153561248318907
0.000499895441086934
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 1.297e+00
Validation Loss: 9.209e-01
Validation ROC: 0.4561
Saving model
27.979118329466356 epochs left to go

Training Epoch 1.0208816705336428/30 starting at 2021-08-25 15:15:06.052348
[1]  [0/1724] loss: 0.881, ave_loss: 0.881
[2]  [20/1724] loss: 1.000, ave_loss: 0.940
[3]  [40/1724] loss: 0.745, ave_loss: 0.875
[4]  [60/1724] loss: 0.771, ave_loss: 0.849
[5]  [80/1724] loss: 0.824, ave_loss: 0.844
[6]  [100/1724] loss: 0.824, ave_loss: 0.841
[7]  [120/1724] loss: 0.783, ave_loss: 0.832
[8]  [140/1724] loss: 0.896, ave_loss: 0.840
[9]  [160/1724] loss: 0.676, ave_loss: 0.822
[10]  [180/1724] loss: 0.808, ave_loss: 0.821
[11]  [200/1724] loss: 0.730, ave_loss: 0.812
[12]  [220/1724] loss: 0.858, ave_loss: 0.816
[13]  [240/1724] loss: 0.783, ave_loss: 0.814
[14]  [260/1724] loss: 0.750, ave_loss: 0.809
[15]  [280/1724] loss: 0.655, ave_loss: 0.799
[16]  [300/1724] loss: 0.778, ave_loss: 0.798
[17]  [320/1724] loss: 0.815, ave_loss: 0.799
[18]  [340/1724] loss: 0.851, ave_loss: 0.801
[19]  [360/1724] loss: 0.580, ave_loss: 0.790
[20]  [380/1724] loss: 0.686, ave_loss: 0.785
[21]  [400/1724] loss: 0.681, ave_loss: 0.780
[22]  [420/1724] loss: 0.733, ave_loss: 0.778
[23]  [440/1724] loss: 0.603, ave_loss: 0.770
[24]  [460/1724] loss: 0.751, ave_loss: 0.769
[25]  [480/1724] loss: 0.677, ave_loss: 0.766
[26]  [500/1724] loss: 0.738, ave_loss: 0.764
[27]  [520/1724] loss: 0.790, ave_loss: 0.765
[28]  [540/1724] loss: 0.766, ave_loss: 0.765
[29]  [560/1724] loss: 0.615, ave_loss: 0.760
[30]  [580/1724] loss: 0.764, ave_loss: 0.760
[31]  [600/1724] loss: 0.837, ave_loss: 0.763
[32]  [620/1724] loss: 0.622, ave_loss: 0.758
[33]  [640/1724] loss: 0.706, ave_loss: 0.757
[34]  [660/1724] loss: 0.798, ave_loss: 0.758
[35]  [680/1724] loss: 0.665, ave_loss: 0.755
[36]  [700/1724] loss: 0.671, ave_loss: 0.753
[37]  [720/1724] loss: 0.643, ave_loss: 0.750
[38]  [740/1724] loss: 0.685, ave_loss: 0.748
[39]  [760/1724] loss: 0.609, ave_loss: 0.745
[40]  [780/1724] loss: 0.831, ave_loss: 0.747
[41]  [800/1724] loss: 0.662, ave_loss: 0.745
[42]  [820/1724] loss: 0.806, ave_loss: 0.746
[43]  [840/1724] loss: 0.708, ave_loss: 0.745
[44]  [860/1724] loss: 0.721, ave_loss: 0.745
[45]  [880/1724] loss: 0.761, ave_loss: 0.745
[46]  [900/1724] loss: 0.721, ave_loss: 0.745
[47]  [920/1724] loss: 0.717, ave_loss: 0.744
[48]  [940/1724] loss: 0.739, ave_loss: 0.744
[49]  [960/1724] loss: 0.978, ave_loss: 0.749
[50]  [980/1724] loss: 0.685, ave_loss: 0.747
[51]  [1000/1724] loss: 0.785, ave_loss: 0.748
[52]  [1020/1724] loss: 0.724, ave_loss: 0.748
[53]  [1040/1724] loss: 0.694, ave_loss: 0.747
[54]  [1060/1724] loss: 0.816, ave_loss: 0.748
[55]  [1080/1724] loss: 0.885, ave_loss: 0.751
[56]  [1100/1724] loss: 0.804, ave_loss: 0.751
[57]  [1120/1724] loss: 0.680, ave_loss: 0.750
[58]  [1140/1724] loss: 0.689, ave_loss: 0.749
[59]  [1160/1724] loss: 0.586, ave_loss: 0.746
[60]  [1180/1724] loss: 0.695, ave_loss: 0.746
[61]  [1200/1724] loss: 0.689, ave_loss: 0.745
[62]  [1220/1724] loss: 0.722, ave_loss: 0.744
[63]  [1240/1724] loss: 0.781, ave_loss: 0.745
[64]  [1260/1724] loss: 0.702, ave_loss: 0.744
[65]  [1280/1724] loss: 0.638, ave_loss: 0.743
[66]  [1300/1724] loss: 0.744, ave_loss: 0.743
[67]  [1320/1724] loss: 0.960, ave_loss: 0.746
[68]  [1340/1724] loss: 0.696, ave_loss: 0.745
[69]  [1360/1724] loss: 0.709, ave_loss: 0.745
[70]  [1380/1724] loss: 0.563, ave_loss: 0.742
[71]  [1400/1724] loss: 0.814, ave_loss: 0.743
[72]  [1420/1724] loss: 0.671, ave_loss: 0.742
[73]  [1440/1724] loss: 0.768, ave_loss: 0.742
[74]  [1460/1724] loss: 0.749, ave_loss: 0.742
[75]  [1480/1724] loss: 0.686, ave_loss: 0.742
[76]  [1500/1724] loss: 0.644, ave_loss: 0.740
[77]  [1520/1724] loss: 0.698, ave_loss: 0.740
[78]  [1540/1724] loss: 0.714, ave_loss: 0.739
[79]  [1560/1724] loss: 0.804, ave_loss: 0.740
[80]  [1580/1724] loss: 0.708, ave_loss: 0.740
[81]  [1600/1724] loss: 0.797, ave_loss: 0.741
[82]  [1620/1724] loss: 0.459, ave_loss: 0.737
[83]  [1640/1724] loss: 0.697, ave_loss: 0.737
[84]  [1660/1724] loss: 0.652, ave_loss: 0.736
[85]  [1680/1724] loss: 0.800, ave_loss: 0.736
[86]  [1700/1724] loss: 0.766, ave_loss: 0.737
[87]  [1720/1724] loss: 0.770, ave_loss: 0.737
[88]  [1740/1724] loss: 0.689, ave_loss: 0.737

Finished Training finishing at 2021-08-25 15:17:15.010751
printing_out epoch  2.0417633410672855 learning rate: 0.0005153561248318907
0.00048489857785432596
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 7.366e-01
Validation Loss: 6.539e-01
Validation ROC: 0.4656
Saving model
26.958236658932716 epochs left to go

Training Epoch 2.0417633410672855/30 starting at 2021-08-25 15:17:37.418518
[1]  [0/1724] loss: 0.779, ave_loss: 0.779
[2]  [20/1724] loss: 0.849, ave_loss: 0.814
[3]  [40/1724] loss: 0.635, ave_loss: 0.754
[4]  [60/1724] loss: 0.656, ave_loss: 0.730
[5]  [80/1724] loss: 0.758, ave_loss: 0.735
[6]  [100/1724] loss: 0.661, ave_loss: 0.723
[7]  [120/1724] loss: 0.728, ave_loss: 0.724
[8]  [140/1724] loss: 0.655, ave_loss: 0.715
[9]  [160/1724] loss: 0.743, ave_loss: 0.718
[10]  [180/1724] loss: 0.610, ave_loss: 0.707
[11]  [200/1724] loss: 0.670, ave_loss: 0.704
[12]  [220/1724] loss: 0.648, ave_loss: 0.699
[13]  [240/1724] loss: 0.771, ave_loss: 0.705
[14]  [260/1724] loss: 0.539, ave_loss: 0.693
[15]  [280/1724] loss: 0.697, ave_loss: 0.693
[16]  [300/1724] loss: 0.818, ave_loss: 0.701
[17]  [320/1724] loss: 0.831, ave_loss: 0.709
[18]  [340/1724] loss: 0.747, ave_loss: 0.711
[19]  [360/1724] loss: 0.698, ave_loss: 0.710
[20]  [380/1724] loss: 0.785, ave_loss: 0.714
[21]  [400/1724] loss: 0.738, ave_loss: 0.715
[22]  [420/1724] loss: 0.605, ave_loss: 0.710
[23]  [440/1724] loss: 0.612, ave_loss: 0.706
[24]  [460/1724] loss: 0.693, ave_loss: 0.705
[25]  [480/1724] loss: 0.692, ave_loss: 0.705
[26]  [500/1724] loss: 0.602, ave_loss: 0.701
[27]  [520/1724] loss: 0.618, ave_loss: 0.698
[28]  [540/1724] loss: 0.615, ave_loss: 0.695
[29]  [560/1724] loss: 0.721, ave_loss: 0.696
[30]  [580/1724] loss: 0.646, ave_loss: 0.694
[31]  [600/1724] loss: 0.742, ave_loss: 0.696
[32]  [620/1724] loss: 0.787, ave_loss: 0.698
[33]  [640/1724] loss: 0.617, ave_loss: 0.696
[34]  [660/1724] loss: 0.649, ave_loss: 0.695
[35]  [680/1724] loss: 0.775, ave_loss: 0.697
[36]  [700/1724] loss: 0.775, ave_loss: 0.699
[37]  [720/1724] loss: 0.765, ave_loss: 0.701
[38]  [740/1724] loss: 0.607, ave_loss: 0.698
[39]  [760/1724] loss: 0.707, ave_loss: 0.699
[40]  [780/1724] loss: 0.557, ave_loss: 0.695
[41]  [800/1724] loss: 0.546, ave_loss: 0.691
[42]  [820/1724] loss: 0.663, ave_loss: 0.691
[43]  [840/1724] loss: 0.570, ave_loss: 0.688
[44]  [860/1724] loss: 0.677, ave_loss: 0.688
[45]  [880/1724] loss: 0.659, ave_loss: 0.687
[46]  [900/1724] loss: 0.714, ave_loss: 0.688
[47]  [920/1724] loss: 0.834, ave_loss: 0.691
[48]  [940/1724] loss: 0.692, ave_loss: 0.691
[49]  [960/1724] loss: 0.632, ave_loss: 0.690
[50]  [980/1724] loss: 0.623, ave_loss: 0.688
[51]  [1000/1724] loss: 0.660, ave_loss: 0.688
[52]  [1020/1724] loss: 0.685, ave_loss: 0.688
[53]  [1040/1724] loss: 0.629, ave_loss: 0.686
[54]  [1060/1724] loss: 0.690, ave_loss: 0.687
[55]  [1080/1724] loss: 0.686, ave_loss: 0.687
[56]  [1100/1724] loss: 0.798, ave_loss: 0.689
[57]  [1120/1724] loss: 0.737, ave_loss: 0.689
[58]  [1140/1724] loss: 0.755, ave_loss: 0.690
[59]  [1160/1724] loss: 0.600, ave_loss: 0.689
[60]  [1180/1724] loss: 0.720, ave_loss: 0.689
[61]  [1200/1724] loss: 0.708, ave_loss: 0.690
[62]  [1220/1724] loss: 0.792, ave_loss: 0.691
[63]  [1240/1724] loss: 0.772, ave_loss: 0.693
[64]  [1260/1724] loss: 0.683, ave_loss: 0.693
[65]  [1280/1724] loss: 0.629, ave_loss: 0.692
[66]  [1300/1724] loss: 0.891, ave_loss: 0.695
[67]  [1320/1724] loss: 0.651, ave_loss: 0.694
[68]  [1340/1724] loss: 0.632, ave_loss: 0.693
[69]  [1360/1724] loss: 0.759, ave_loss: 0.694
[70]  [1380/1724] loss: 0.601, ave_loss: 0.693
[71]  [1400/1724] loss: 0.663, ave_loss: 0.692
[72]  [1420/1724] loss: 0.701, ave_loss: 0.692
[73]  [1440/1724] loss: 0.736, ave_loss: 0.693
[74]  [1460/1724] loss: 0.541, ave_loss: 0.691
[75]  [1480/1724] loss: 0.549, ave_loss: 0.689
[76]  [1500/1724] loss: 0.635, ave_loss: 0.688
[77]  [1520/1724] loss: 0.652, ave_loss: 0.688
[78]  [1540/1724] loss: 0.726, ave_loss: 0.688
[79]  [1560/1724] loss: 0.718, ave_loss: 0.689
[80]  [1580/1724] loss: 0.747, ave_loss: 0.689
[81]  [1600/1724] loss: 0.679, ave_loss: 0.689
[82]  [1620/1724] loss: 0.727, ave_loss: 0.690
[83]  [1640/1724] loss: 0.678, ave_loss: 0.690
[84]  [1660/1724] loss: 0.623, ave_loss: 0.689
[85]  [1680/1724] loss: 0.719, ave_loss: 0.689
[86]  [1700/1724] loss: 0.694, ave_loss: 0.689
[87]  [1720/1724] loss: 1.014, ave_loss: 0.693
[88]  [1740/1724] loss: 0.552, ave_loss: 0.691

Finished Training finishing at 2021-08-25 15:19:14.860172
printing_out epoch  3.062645011600928 learning rate: 0.0005153561248318907
0.00047035162051869614
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.914e-01
Validation Loss: 6.042e-01
Validation ROC: 0.4564
No improvement, still saving model
25.937354988399072 epochs left to go

Training Epoch 3.062645011600928/30 starting at 2021-08-25 15:19:37.613255
[1]  [0/1724] loss: 0.606, ave_loss: 0.606
[2]  [20/1724] loss: 0.728, ave_loss: 0.667
[3]  [40/1724] loss: 0.578, ave_loss: 0.637
[4]  [60/1724] loss: 0.847, ave_loss: 0.690
[5]  [80/1724] loss: 0.571, ave_loss: 0.666
[6]  [100/1724] loss: 0.675, ave_loss: 0.668
[7]  [120/1724] loss: 0.623, ave_loss: 0.661
[8]  [140/1724] loss: 0.538, ave_loss: 0.646
[9]  [160/1724] loss: 0.750, ave_loss: 0.657
[10]  [180/1724] loss: 0.715, ave_loss: 0.663
[11]  [200/1724] loss: 0.697, ave_loss: 0.666
[12]  [220/1724] loss: 0.782, ave_loss: 0.676
[13]  [240/1724] loss: 0.695, ave_loss: 0.677
[14]  [260/1724] loss: 0.813, ave_loss: 0.687
[15]  [280/1724] loss: 0.712, ave_loss: 0.689
[16]  [300/1724] loss: 0.616, ave_loss: 0.684
[17]  [320/1724] loss: 0.711, ave_loss: 0.686
[18]  [340/1724] loss: 0.696, ave_loss: 0.686
[19]  [360/1724] loss: 0.751, ave_loss: 0.690
[20]  [380/1724] loss: 0.791, ave_loss: 0.695
[21]  [400/1724] loss: 0.680, ave_loss: 0.694
[22]  [420/1724] loss: 0.687, ave_loss: 0.694
[23]  [440/1724] loss: 0.725, ave_loss: 0.695
[24]  [460/1724] loss: 0.670, ave_loss: 0.694
[25]  [480/1724] loss: 0.757, ave_loss: 0.696
[26]  [500/1724] loss: 0.689, ave_loss: 0.696
[27]  [520/1724] loss: 0.595, ave_loss: 0.692
[28]  [540/1724] loss: 0.604, ave_loss: 0.689
[29]  [560/1724] loss: 0.825, ave_loss: 0.694
[30]  [580/1724] loss: 0.748, ave_loss: 0.696
[31]  [600/1724] loss: 0.666, ave_loss: 0.695
[32]  [620/1724] loss: 0.600, ave_loss: 0.692
[33]  [640/1724] loss: 0.599, ave_loss: 0.689
[34]  [660/1724] loss: 0.703, ave_loss: 0.689
[35]  [680/1724] loss: 0.661, ave_loss: 0.689
[36]  [700/1724] loss: 0.635, ave_loss: 0.687
[37]  [720/1724] loss: 0.500, ave_loss: 0.682
[38]  [740/1724] loss: 0.678, ave_loss: 0.682
[39]  [760/1724] loss: 0.552, ave_loss: 0.679
[40]  [780/1724] loss: 0.676, ave_loss: 0.679
[41]  [800/1724] loss: 0.634, ave_loss: 0.678
[42]  [820/1724] loss: 0.638, ave_loss: 0.677
[43]  [840/1724] loss: 0.715, ave_loss: 0.677
[44]  [860/1724] loss: 0.827, ave_loss: 0.681
[45]  [880/1724] loss: 0.695, ave_loss: 0.681
[46]  [900/1724] loss: 0.725, ave_loss: 0.682
[47]  [920/1724] loss: 0.749, ave_loss: 0.684
[48]  [940/1724] loss: 0.660, ave_loss: 0.683
[49]  [960/1724] loss: 0.692, ave_loss: 0.683
[50]  [980/1724] loss: 0.686, ave_loss: 0.683
[51]  [1000/1724] loss: 0.719, ave_loss: 0.684
[52]  [1020/1724] loss: 0.702, ave_loss: 0.684
[53]  [1040/1724] loss: 0.698, ave_loss: 0.685
[54]  [1060/1724] loss: 0.622, ave_loss: 0.683
[55]  [1080/1724] loss: 0.573, ave_loss: 0.681
[56]  [1100/1724] loss: 0.748, ave_loss: 0.683
[57]  [1120/1724] loss: 0.789, ave_loss: 0.684
[58]  [1140/1724] loss: 0.649, ave_loss: 0.684
[59]  [1160/1724] loss: 0.738, ave_loss: 0.685
[60]  [1180/1724] loss: 0.716, ave_loss: 0.685
[61]  [1200/1724] loss: 0.705, ave_loss: 0.686
[62]  [1220/1724] loss: 0.685, ave_loss: 0.686
[63]  [1240/1724] loss: 0.607, ave_loss: 0.684
[64]  [1260/1724] loss: 0.683, ave_loss: 0.684
[65]  [1280/1724] loss: 0.556, ave_loss: 0.682
[66]  [1300/1724] loss: 0.756, ave_loss: 0.683
[67]  [1320/1724] loss: 0.603, ave_loss: 0.682
[68]  [1340/1724] loss: 0.653, ave_loss: 0.682
[69]  [1360/1724] loss: 0.699, ave_loss: 0.682
[70]  [1380/1724] loss: 0.621, ave_loss: 0.681
[71]  [1400/1724] loss: 0.617, ave_loss: 0.680
[72]  [1420/1724] loss: 0.588, ave_loss: 0.679
[73]  [1440/1724] loss: 0.546, ave_loss: 0.677
[74]  [1460/1724] loss: 0.768, ave_loss: 0.678
[75]  [1480/1724] loss: 0.673, ave_loss: 0.678
[76]  [1500/1724] loss: 0.823, ave_loss: 0.680
[77]  [1520/1724] loss: 0.699, ave_loss: 0.681
[78]  [1540/1724] loss: 0.605, ave_loss: 0.680
[79]  [1560/1724] loss: 0.707, ave_loss: 0.680
[80]  [1580/1724] loss: 0.668, ave_loss: 0.680
[81]  [1600/1724] loss: 0.617, ave_loss: 0.679
[82]  [1620/1724] loss: 0.801, ave_loss: 0.680
[83]  [1640/1724] loss: 0.618, ave_loss: 0.680
[84]  [1660/1724] loss: 0.710, ave_loss: 0.680
[85]  [1680/1724] loss: 0.744, ave_loss: 0.681
[86]  [1700/1724] loss: 0.651, ave_loss: 0.680
[87]  [1720/1724] loss: 0.748, ave_loss: 0.681
[88]  [1740/1724] loss: 0.531, ave_loss: 0.680

Finished Training finishing at 2021-08-25 15:21:00.725110
printing_out epoch  4.083526682134571 learning rate: 0.0005153561248318907
0.00045624107190313527
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.795e-01
Validation Loss: 5.982e-01
Validation ROC: 0.4639
No improvement, still saving model
24.916473317865428 epochs left to go

Training Epoch 4.083526682134571/30 starting at 2021-08-25 15:21:22.970547
[1]  [0/1724] loss: 0.689, ave_loss: 0.689
[2]  [20/1724] loss: 0.745, ave_loss: 0.717
[3]  [40/1724] loss: 0.623, ave_loss: 0.686
[4]  [60/1724] loss: 0.548, ave_loss: 0.651
[5]  [80/1724] loss: 0.614, ave_loss: 0.644
[6]  [100/1724] loss: 0.651, ave_loss: 0.645
[7]  [120/1724] loss: 0.732, ave_loss: 0.657
[8]  [140/1724] loss: 0.596, ave_loss: 0.650
[9]  [160/1724] loss: 0.672, ave_loss: 0.652
[10]  [180/1724] loss: 0.639, ave_loss: 0.651
[11]  [200/1724] loss: 0.649, ave_loss: 0.651
[12]  [220/1724] loss: 0.740, ave_loss: 0.658
[13]  [240/1724] loss: 0.661, ave_loss: 0.658
[14]  [260/1724] loss: 0.702, ave_loss: 0.661
[15]  [280/1724] loss: 0.730, ave_loss: 0.666
[16]  [300/1724] loss: 0.721, ave_loss: 0.669
[17]  [320/1724] loss: 0.699, ave_loss: 0.671
[18]  [340/1724] loss: 0.774, ave_loss: 0.677
[19]  [360/1724] loss: 0.787, ave_loss: 0.683
[20]  [380/1724] loss: 0.580, ave_loss: 0.678
[21]  [400/1724] loss: 0.776, ave_loss: 0.682
[22]  [420/1724] loss: 0.754, ave_loss: 0.685
[23]  [440/1724] loss: 0.675, ave_loss: 0.685
[24]  [460/1724] loss: 0.916, ave_loss: 0.695
[25]  [480/1724] loss: 0.792, ave_loss: 0.698
[26]  [500/1724] loss: 0.539, ave_loss: 0.692
[27]  [520/1724] loss: 0.632, ave_loss: 0.690
[28]  [540/1724] loss: 0.629, ave_loss: 0.688
[29]  [560/1724] loss: 0.776, ave_loss: 0.691
[30]  [580/1724] loss: 0.652, ave_loss: 0.690
[31]  [600/1724] loss: 0.676, ave_loss: 0.689
[32]  [620/1724] loss: 0.497, ave_loss: 0.683
[33]  [640/1724] loss: 0.680, ave_loss: 0.683
[34]  [660/1724] loss: 0.811, ave_loss: 0.687
[35]  [680/1724] loss: 0.716, ave_loss: 0.688
[36]  [700/1724] loss: 0.808, ave_loss: 0.691
[37]  [720/1724] loss: 0.693, ave_loss: 0.691
[38]  [740/1724] loss: 0.717, ave_loss: 0.692
[39]  [760/1724] loss: 0.681, ave_loss: 0.692
[40]  [780/1724] loss: 0.748, ave_loss: 0.693
[41]  [800/1724] loss: 0.693, ave_loss: 0.693
[42]  [820/1724] loss: 0.719, ave_loss: 0.694
[43]  [840/1724] loss: 0.597, ave_loss: 0.691
[44]  [860/1724] loss: 0.667, ave_loss: 0.691
[45]  [880/1724] loss: 0.595, ave_loss: 0.689
[46]  [900/1724] loss: 0.641, ave_loss: 0.688
[47]  [920/1724] loss: 0.590, ave_loss: 0.686
[48]  [940/1724] loss: 0.775, ave_loss: 0.687
[49]  [960/1724] loss: 0.602, ave_loss: 0.686
[50]  [980/1724] loss: 0.622, ave_loss: 0.684
[51]  [1000/1724] loss: 0.561, ave_loss: 0.682
[52]  [1020/1724] loss: 0.805, ave_loss: 0.684
[53]  [1040/1724] loss: 0.698, ave_loss: 0.685
[54]  [1060/1724] loss: 0.637, ave_loss: 0.684
[55]  [1080/1724] loss: 0.599, ave_loss: 0.682
[56]  [1100/1724] loss: 0.724, ave_loss: 0.683
[57]  [1120/1724] loss: 0.732, ave_loss: 0.684
[58]  [1140/1724] loss: 0.571, ave_loss: 0.682
[59]  [1160/1724] loss: 0.692, ave_loss: 0.682
[60]  [1180/1724] loss: 0.734, ave_loss: 0.683
[61]  [1200/1724] loss: 0.695, ave_loss: 0.683
[62]  [1220/1724] loss: 0.602, ave_loss: 0.682
[63]  [1240/1724] loss: 0.629, ave_loss: 0.681
[64]  [1260/1724] loss: 0.630, ave_loss: 0.680
[65]  [1280/1724] loss: 0.659, ave_loss: 0.680
[66]  [1300/1724] loss: 0.622, ave_loss: 0.679
[67]  [1320/1724] loss: 0.808, ave_loss: 0.681
[68]  [1340/1724] loss: 0.669, ave_loss: 0.681
[69]  [1360/1724] loss: 0.586, ave_loss: 0.679
[70]  [1380/1724] loss: 0.509, ave_loss: 0.677
[71]  [1400/1724] loss: 0.587, ave_loss: 0.676
[72]  [1420/1724] loss: 0.659, ave_loss: 0.675
[73]  [1440/1724] loss: 0.617, ave_loss: 0.675
[74]  [1460/1724] loss: 0.862, ave_loss: 0.677
[75]  [1480/1724] loss: 0.628, ave_loss: 0.676
[76]  [1500/1724] loss: 0.606, ave_loss: 0.676
[77]  [1520/1724] loss: 0.678, ave_loss: 0.676
[78]  [1540/1724] loss: 0.637, ave_loss: 0.675
[79]  [1560/1724] loss: 0.784, ave_loss: 0.676
[80]  [1580/1724] loss: 0.676, ave_loss: 0.676
[81]  [1600/1724] loss: 0.564, ave_loss: 0.675
[82]  [1620/1724] loss: 0.637, ave_loss: 0.675
[83]  [1640/1724] loss: 0.829, ave_loss: 0.676
[84]  [1660/1724] loss: 0.604, ave_loss: 0.676
[85]  [1680/1724] loss: 0.705, ave_loss: 0.676
[86]  [1700/1724] loss: 0.640, ave_loss: 0.676
[87]  [1720/1724] loss: 0.562, ave_loss: 0.674
[88]  [1740/1724] loss: 0.729, ave_loss: 0.675

Finished Training finishing at 2021-08-25 15:22:40.269013
printing_out epoch  5.104408352668213 learning rate: 0.0005153561248318907
0.0004425538397460412
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.748e-01
Validation Loss: 5.820e-01
Validation ROC: 0.4639
No improvement, still saving model
23.895591647331788 epochs left to go

Training Epoch 5.104408352668213/30 starting at 2021-08-25 15:23:05.093328
[1]  [0/1724] loss: 0.782, ave_loss: 0.782
[2]  [20/1724] loss: 0.623, ave_loss: 0.703
[3]  [40/1724] loss: 0.728, ave_loss: 0.711
[4]  [60/1724] loss: 0.669, ave_loss: 0.701
[5]  [80/1724] loss: 0.664, ave_loss: 0.693
[6]  [100/1724] loss: 0.748, ave_loss: 0.702
[7]  [120/1724] loss: 0.559, ave_loss: 0.682
[8]  [140/1724] loss: 0.721, ave_loss: 0.687
[9]  [160/1724] loss: 0.750, ave_loss: 0.694
[10]  [180/1724] loss: 0.755, ave_loss: 0.700
[11]  [200/1724] loss: 0.660, ave_loss: 0.696
[12]  [220/1724] loss: 0.725, ave_loss: 0.699
[13]  [240/1724] loss: 0.697, ave_loss: 0.699
[14]  [260/1724] loss: 0.717, ave_loss: 0.700
[15]  [280/1724] loss: 0.560, ave_loss: 0.691
[16]  [300/1724] loss: 0.630, ave_loss: 0.687
[17]  [320/1724] loss: 0.680, ave_loss: 0.686
[18]  [340/1724] loss: 0.868, ave_loss: 0.697
[19]  [360/1724] loss: 0.730, ave_loss: 0.698
[20]  [380/1724] loss: 0.617, ave_loss: 0.694
[21]  [400/1724] loss: 0.686, ave_loss: 0.694
[22]  [420/1724] loss: 0.739, ave_loss: 0.696
[23]  [440/1724] loss: 0.604, ave_loss: 0.692
[24]  [460/1724] loss: 0.725, ave_loss: 0.693
[25]  [480/1724] loss: 0.776, ave_loss: 0.697
[26]  [500/1724] loss: 0.875, ave_loss: 0.703
[27]  [520/1724] loss: 0.721, ave_loss: 0.704
[28]  [540/1724] loss: 0.530, ave_loss: 0.698
[29]  [560/1724] loss: 0.720, ave_loss: 0.699
[30]  [580/1724] loss: 0.669, ave_loss: 0.698
[31]  [600/1724] loss: 0.703, ave_loss: 0.698
[32]  [620/1724] loss: 0.629, ave_loss: 0.696
[33]  [640/1724] loss: 0.676, ave_loss: 0.695
[34]  [660/1724] loss: 0.665, ave_loss: 0.694
[35]  [680/1724] loss: 0.669, ave_loss: 0.693
[36]  [700/1724] loss: 0.831, ave_loss: 0.697
[37]  [720/1724] loss: 0.681, ave_loss: 0.697
[38]  [740/1724] loss: 0.600, ave_loss: 0.694
[39]  [760/1724] loss: 0.703, ave_loss: 0.695
[40]  [780/1724] loss: 0.675, ave_loss: 0.694
[41]  [800/1724] loss: 0.640, ave_loss: 0.693
[42]  [820/1724] loss: 0.706, ave_loss: 0.693
[43]  [840/1724] loss: 0.703, ave_loss: 0.693
[44]  [860/1724] loss: 0.752, ave_loss: 0.695
[45]  [880/1724] loss: 0.636, ave_loss: 0.693
[46]  [900/1724] loss: 0.840, ave_loss: 0.696
[47]  [920/1724] loss: 0.741, ave_loss: 0.697
[48]  [940/1724] loss: 0.676, ave_loss: 0.697
[49]  [960/1724] loss: 0.597, ave_loss: 0.695
[50]  [980/1724] loss: 0.688, ave_loss: 0.695
[51]  [1000/1724] loss: 0.647, ave_loss: 0.694
[52]  [1020/1724] loss: 0.650, ave_loss: 0.693
[53]  [1040/1724] loss: 0.524, ave_loss: 0.690
[54]  [1060/1724] loss: 0.606, ave_loss: 0.688
[55]  [1080/1724] loss: 0.624, ave_loss: 0.687
[56]  [1100/1724] loss: 0.732, ave_loss: 0.688
[57]  [1120/1724] loss: 0.700, ave_loss: 0.688
[58]  [1140/1724] loss: 0.652, ave_loss: 0.687
[59]  [1160/1724] loss: 0.689, ave_loss: 0.688
[60]  [1180/1724] loss: 0.545, ave_loss: 0.685
[61]  [1200/1724] loss: 0.663, ave_loss: 0.685
[62]  [1220/1724] loss: 0.701, ave_loss: 0.685
[63]  [1240/1724] loss: 0.632, ave_loss: 0.684
[64]  [1260/1724] loss: 0.675, ave_loss: 0.684
[65]  [1280/1724] loss: 0.744, ave_loss: 0.685
[66]  [1300/1724] loss: 0.633, ave_loss: 0.684
[67]  [1320/1724] loss: 0.611, ave_loss: 0.683
[68]  [1340/1724] loss: 0.752, ave_loss: 0.684
[69]  [1360/1724] loss: 0.708, ave_loss: 0.684
[70]  [1380/1724] loss: 0.702, ave_loss: 0.685
[71]  [1400/1724] loss: 0.679, ave_loss: 0.685
[72]  [1420/1724] loss: 0.579, ave_loss: 0.683
[73]  [1440/1724] loss: 0.597, ave_loss: 0.682
[74]  [1460/1724] loss: 0.564, ave_loss: 0.680
[75]  [1480/1724] loss: 0.788, ave_loss: 0.682
[76]  [1500/1724] loss: 0.705, ave_loss: 0.682
[77]  [1520/1724] loss: 0.670, ave_loss: 0.682
[78]  [1540/1724] loss: 0.659, ave_loss: 0.682
[79]  [1560/1724] loss: 0.684, ave_loss: 0.682
[80]  [1580/1724] loss: 0.684, ave_loss: 0.682
[81]  [1600/1724] loss: 0.686, ave_loss: 0.682
[82]  [1620/1724] loss: 0.737, ave_loss: 0.682
[83]  [1640/1724] loss: 0.622, ave_loss: 0.682
[84]  [1660/1724] loss: 0.652, ave_loss: 0.681
[85]  [1680/1724] loss: 0.588, ave_loss: 0.680
[86]  [1700/1724] loss: 0.718, ave_loss: 0.681
[87]  [1720/1724] loss: 0.552, ave_loss: 0.679
[88]  [1740/1724] loss: 0.694, ave_loss: 0.679

Finished Training finishing at 2021-08-25 15:24:25.724146
printing_out epoch  6.125290023201856 learning rate: 0.0005153561248318907
0.00042927722455365994
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.794e-01
Validation Loss: 6.025e-01
Validation ROC: 0.4639
No improvement, still saving model
22.874709976798144 epochs left to go

Training Epoch 6.125290023201856/30 starting at 2021-08-25 15:24:50.098964
[1]  [0/1724] loss: 0.570, ave_loss: 0.570
[2]  [20/1724] loss: 0.520, ave_loss: 0.545
[3]  [40/1724] loss: 0.698, ave_loss: 0.596
[4]  [60/1724] loss: 0.713, ave_loss: 0.625
[5]  [80/1724] loss: 0.875, ave_loss: 0.675
[6]  [100/1724] loss: 0.588, ave_loss: 0.661
[7]  [120/1724] loss: 0.574, ave_loss: 0.648
[8]  [140/1724] loss: 0.596, ave_loss: 0.642
[9]  [160/1724] loss: 0.669, ave_loss: 0.645
[10]  [180/1724] loss: 0.637, ave_loss: 0.644
[11]  [200/1724] loss: 0.645, ave_loss: 0.644
[12]  [220/1724] loss: 0.572, ave_loss: 0.638
[13]  [240/1724] loss: 0.654, ave_loss: 0.639
[14]  [260/1724] loss: 0.788, ave_loss: 0.650
[15]  [280/1724] loss: 0.731, ave_loss: 0.655
[16]  [300/1724] loss: 0.675, ave_loss: 0.657
[17]  [320/1724] loss: 0.568, ave_loss: 0.651
[18]  [340/1724] loss: 0.620, ave_loss: 0.650
[19]  [360/1724] loss: 0.725, ave_loss: 0.654
[20]  [380/1724] loss: 0.601, ave_loss: 0.651
[21]  [400/1724] loss: 0.634, ave_loss: 0.650
[22]  [420/1724] loss: 0.615, ave_loss: 0.649
[23]  [440/1724] loss: 0.642, ave_loss: 0.648
[24]  [460/1724] loss: 0.675, ave_loss: 0.649
[25]  [480/1724] loss: 0.658, ave_loss: 0.650
[26]  [500/1724] loss: 0.648, ave_loss: 0.650
[27]  [520/1724] loss: 0.585, ave_loss: 0.647
[28]  [540/1724] loss: 0.677, ave_loss: 0.648
[29]  [560/1724] loss: 0.655, ave_loss: 0.649
[30]  [580/1724] loss: 0.648, ave_loss: 0.649
[31]  [600/1724] loss: 0.588, ave_loss: 0.647
[32]  [620/1724] loss: 0.628, ave_loss: 0.646
[33]  [640/1724] loss: 0.627, ave_loss: 0.646
[34]  [660/1724] loss: 0.689, ave_loss: 0.647
[35]  [680/1724] loss: 0.650, ave_loss: 0.647
[36]  [700/1724] loss: 0.921, ave_loss: 0.655
[37]  [720/1724] loss: 0.748, ave_loss: 0.657
[38]  [740/1724] loss: 0.711, ave_loss: 0.658
[39]  [760/1724] loss: 0.638, ave_loss: 0.658
[40]  [780/1724] loss: 0.554, ave_loss: 0.655
[41]  [800/1724] loss: 0.620, ave_loss: 0.654
[42]  [820/1724] loss: 0.668, ave_loss: 0.655
[43]  [840/1724] loss: 0.654, ave_loss: 0.655
[44]  [860/1724] loss: 0.819, ave_loss: 0.658
[45]  [880/1724] loss: 0.923, ave_loss: 0.664
[46]  [900/1724] loss: 0.765, ave_loss: 0.667
[47]  [920/1724] loss: 0.666, ave_loss: 0.667
[48]  [940/1724] loss: 0.738, ave_loss: 0.668
[49]  [960/1724] loss: 0.649, ave_loss: 0.668
[50]  [980/1724] loss: 0.625, ave_loss: 0.667
[51]  [1000/1724] loss: 0.587, ave_loss: 0.665
[52]  [1020/1724] loss: 0.598, ave_loss: 0.664
[53]  [1040/1724] loss: 0.646, ave_loss: 0.664
[54]  [1060/1724] loss: 0.706, ave_loss: 0.664
[55]  [1080/1724] loss: 0.669, ave_loss: 0.664
[56]  [1100/1724] loss: 0.760, ave_loss: 0.666
[57]  [1120/1724] loss: 0.619, ave_loss: 0.665
[58]  [1140/1724] loss: 0.842, ave_loss: 0.668
[59]  [1160/1724] loss: 0.660, ave_loss: 0.668
[60]  [1180/1724] loss: 0.592, ave_loss: 0.667
[61]  [1200/1724] loss: 0.677, ave_loss: 0.667
[62]  [1220/1724] loss: 0.716, ave_loss: 0.668
[63]  [1240/1724] loss: 0.633, ave_loss: 0.667
[64]  [1260/1724] loss: 0.694, ave_loss: 0.668
[65]  [1280/1724] loss: 0.684, ave_loss: 0.668
[66]  [1300/1724] loss: 0.616, ave_loss: 0.667
[67]  [1320/1724] loss: 0.686, ave_loss: 0.668
[68]  [1340/1724] loss: 0.700, ave_loss: 0.668
[69]  [1360/1724] loss: 0.676, ave_loss: 0.668
[70]  [1380/1724] loss: 0.582, ave_loss: 0.667
[71]  [1400/1724] loss: 0.627, ave_loss: 0.666
[72]  [1420/1724] loss: 0.699, ave_loss: 0.667
[73]  [1440/1724] loss: 0.576, ave_loss: 0.666
[74]  [1460/1724] loss: 0.865, ave_loss: 0.668
[75]  [1480/1724] loss: 0.685, ave_loss: 0.668
[76]  [1500/1724] loss: 0.845, ave_loss: 0.671
[77]  [1520/1724] loss: 0.776, ave_loss: 0.672
[78]  [1540/1724] loss: 0.634, ave_loss: 0.672
[79]  [1560/1724] loss: 0.837, ave_loss: 0.674
[80]  [1580/1724] loss: 0.680, ave_loss: 0.674
[81]  [1600/1724] loss: 0.719, ave_loss: 0.674
[82]  [1620/1724] loss: 0.591, ave_loss: 0.673
[83]  [1640/1724] loss: 0.810, ave_loss: 0.675
[84]  [1660/1724] loss: 0.755, ave_loss: 0.676
[85]  [1680/1724] loss: 0.671, ave_loss: 0.676
[86]  [1700/1724] loss: 0.605, ave_loss: 0.675
[87]  [1720/1724] loss: 0.482, ave_loss: 0.673
[88]  [1740/1724] loss: 0.722, ave_loss: 0.673

Finished Training finishing at 2021-08-25 15:26:08.369156
printing_out epoch  7.146171693735499 learning rate: 0.0005153561248318907
0.0004163989078170501
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.734e-01
Validation Loss: 6.056e-01
Validation ROC: 0.4485
No improvement, still saving model
21.8538283062645 epochs left to go

Training Epoch 7.146171693735499/30 starting at 2021-08-25 15:26:32.759007
[1]  [0/1724] loss: 0.623, ave_loss: 0.623
[2]  [20/1724] loss: 0.533, ave_loss: 0.578
[3]  [40/1724] loss: 0.788, ave_loss: 0.648
[4]  [60/1724] loss: 0.592, ave_loss: 0.634
[5]  [80/1724] loss: 0.523, ave_loss: 0.612
[6]  [100/1724] loss: 0.679, ave_loss: 0.623
[7]  [120/1724] loss: 0.650, ave_loss: 0.627
[8]  [140/1724] loss: 0.615, ave_loss: 0.625
[9]  [160/1724] loss: 0.612, ave_loss: 0.624
[10]  [180/1724] loss: 0.717, ave_loss: 0.633
[11]  [200/1724] loss: 0.593, ave_loss: 0.630
[12]  [220/1724] loss: 0.697, ave_loss: 0.635
[13]  [240/1724] loss: 0.648, ave_loss: 0.636
[14]  [260/1724] loss: 0.766, ave_loss: 0.645
[15]  [280/1724] loss: 0.750, ave_loss: 0.652
[16]  [300/1724] loss: 0.611, ave_loss: 0.650
[17]  [320/1724] loss: 0.806, ave_loss: 0.659
[18]  [340/1724] loss: 0.753, ave_loss: 0.664
[19]  [360/1724] loss: 0.596, ave_loss: 0.661
[20]  [380/1724] loss: 0.564, ave_loss: 0.656
[21]  [400/1724] loss: 0.617, ave_loss: 0.654
[22]  [420/1724] loss: 0.622, ave_loss: 0.652
[23]  [440/1724] loss: 0.699, ave_loss: 0.654
[24]  [460/1724] loss: 0.652, ave_loss: 0.654
[25]  [480/1724] loss: 0.747, ave_loss: 0.658
[26]  [500/1724] loss: 0.732, ave_loss: 0.661
[27]  [520/1724] loss: 0.749, ave_loss: 0.664
[28]  [540/1724] loss: 0.559, ave_loss: 0.660
[29]  [560/1724] loss: 0.635, ave_loss: 0.660
[30]  [580/1724] loss: 0.688, ave_loss: 0.660
[31]  [600/1724] loss: 0.667, ave_loss: 0.661
[32]  [620/1724] loss: 0.771, ave_loss: 0.664
[33]  [640/1724] loss: 0.670, ave_loss: 0.664
[34]  [660/1724] loss: 0.604, ave_loss: 0.663
[35]  [680/1724] loss: 0.568, ave_loss: 0.660
[36]  [700/1724] loss: 0.579, ave_loss: 0.658
[37]  [720/1724] loss: 0.728, ave_loss: 0.659
[38]  [740/1724] loss: 0.602, ave_loss: 0.658
[39]  [760/1724] loss: 0.662, ave_loss: 0.658
[40]  [780/1724] loss: 0.623, ave_loss: 0.657
[41]  [800/1724] loss: 0.669, ave_loss: 0.657
[42]  [820/1724] loss: 0.651, ave_loss: 0.657
[43]  [840/1724] loss: 0.700, ave_loss: 0.658
[44]  [860/1724] loss: 0.671, ave_loss: 0.659
[45]  [880/1724] loss: 0.753, ave_loss: 0.661
[46]  [900/1724] loss: 0.621, ave_loss: 0.660
[47]  [920/1724] loss: 0.566, ave_loss: 0.658
[48]  [940/1724] loss: 0.689, ave_loss: 0.658
[49]  [960/1724] loss: 0.898, ave_loss: 0.663
[50]  [980/1724] loss: 0.540, ave_loss: 0.661
[51]  [1000/1724] loss: 0.695, ave_loss: 0.662
[52]  [1020/1724] loss: 0.680, ave_loss: 0.662
[53]  [1040/1724] loss: 0.596, ave_loss: 0.661
[54]  [1060/1724] loss: 0.700, ave_loss: 0.661
[55]  [1080/1724] loss: 0.681, ave_loss: 0.662
[56]  [1100/1724] loss: 0.621, ave_loss: 0.661
[57]  [1120/1724] loss: 0.776, ave_loss: 0.663
[58]  [1140/1724] loss: 0.733, ave_loss: 0.664
[59]  [1160/1724] loss: 0.697, ave_loss: 0.665
[60]  [1180/1724] loss: 0.721, ave_loss: 0.666
[61]  [1200/1724] loss: 0.663, ave_loss: 0.666
[62]  [1220/1724] loss: 0.622, ave_loss: 0.665
[63]  [1240/1724] loss: 0.557, ave_loss: 0.663
[64]  [1260/1724] loss: 0.556, ave_loss: 0.662
[65]  [1280/1724] loss: 0.652, ave_loss: 0.661
[66]  [1300/1724] loss: 0.647, ave_loss: 0.661
[67]  [1320/1724] loss: 0.687, ave_loss: 0.662
[68]  [1340/1724] loss: 0.538, ave_loss: 0.660
[69]  [1360/1724] loss: 0.785, ave_loss: 0.662
[70]  [1380/1724] loss: 0.619, ave_loss: 0.661
[71]  [1400/1724] loss: 0.640, ave_loss: 0.661
[72]  [1420/1724] loss: 0.692, ave_loss: 0.661
[73]  [1440/1724] loss: 0.686, ave_loss: 0.662
[74]  [1460/1724] loss: 0.660, ave_loss: 0.662
[75]  [1480/1724] loss: 0.739, ave_loss: 0.663
[76]  [1500/1724] loss: 0.606, ave_loss: 0.662
[77]  [1520/1724] loss: 0.663, ave_loss: 0.662
[78]  [1540/1724] loss: 0.608, ave_loss: 0.661
[79]  [1560/1724] loss: 0.574, ave_loss: 0.660
[80]  [1580/1724] loss: 0.583, ave_loss: 0.659
[81]  [1600/1724] loss: 0.577, ave_loss: 0.658
[82]  [1620/1724] loss: 0.715, ave_loss: 0.659
[83]  [1640/1724] loss: 0.576, ave_loss: 0.658
[84]  [1660/1724] loss: 0.704, ave_loss: 0.658
[85]  [1680/1724] loss: 0.672, ave_loss: 0.658
[86]  [1700/1724] loss: 0.607, ave_loss: 0.658
[87]  [1720/1724] loss: 0.699, ave_loss: 0.658
[88]  [1740/1724] loss: 0.618, ave_loss: 0.658

Finished Training finishing at 2021-08-25 15:27:53.373454
printing_out epoch  8.167053364269142 learning rate: 0.0005153561248318907
0.0004039069405825386
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.579e-01
Validation Loss: 5.831e-01
Validation ROC: 0.4485
No improvement, still saving model
20.832946635730856 epochs left to go

Training Epoch 8.167053364269142/30 starting at 2021-08-25 15:28:15.624585
[1]  [0/1724] loss: 0.505, ave_loss: 0.505
[2]  [20/1724] loss: 0.603, ave_loss: 0.554
[3]  [40/1724] loss: 0.797, ave_loss: 0.635
[4]  [60/1724] loss: 0.739, ave_loss: 0.661
[5]  [80/1724] loss: 0.709, ave_loss: 0.671
[6]  [100/1724] loss: 0.628, ave_loss: 0.664
[7]  [120/1724] loss: 0.556, ave_loss: 0.648
[8]  [140/1724] loss: 0.667, ave_loss: 0.651
[9]  [160/1724] loss: 0.810, ave_loss: 0.668
[10]  [180/1724] loss: 0.711, ave_loss: 0.673
[11]  [200/1724] loss: 0.751, ave_loss: 0.680
[12]  [220/1724] loss: 0.723, ave_loss: 0.683
[13]  [240/1724] loss: 0.747, ave_loss: 0.688
[14]  [260/1724] loss: 0.731, ave_loss: 0.691
[15]  [280/1724] loss: 0.663, ave_loss: 0.689
[16]  [300/1724] loss: 0.728, ave_loss: 0.692
[17]  [320/1724] loss: 0.700, ave_loss: 0.692
[18]  [340/1724] loss: 0.815, ave_loss: 0.699
[19]  [360/1724] loss: 0.506, ave_loss: 0.689
[20]  [380/1724] loss: 0.712, ave_loss: 0.690
[21]  [400/1724] loss: 0.602, ave_loss: 0.686
[22]  [420/1724] loss: 0.613, ave_loss: 0.683
[23]  [440/1724] loss: 0.682, ave_loss: 0.683
[24]  [460/1724] loss: 0.800, ave_loss: 0.687
[25]  [480/1724] loss: 0.625, ave_loss: 0.685
[26]  [500/1724] loss: 0.569, ave_loss: 0.680
[27]  [520/1724] loss: 0.704, ave_loss: 0.681
[28]  [540/1724] loss: 0.526, ave_loss: 0.676
[29]  [560/1724] loss: 0.593, ave_loss: 0.673
[30]  [580/1724] loss: 0.663, ave_loss: 0.673
[31]  [600/1724] loss: 0.759, ave_loss: 0.675
[32]  [620/1724] loss: 0.607, ave_loss: 0.673
[33]  [640/1724] loss: 0.705, ave_loss: 0.674
[34]  [660/1724] loss: 0.777, ave_loss: 0.677
[35]  [680/1724] loss: 0.789, ave_loss: 0.680
[36]  [700/1724] loss: 0.658, ave_loss: 0.680
[37]  [720/1724] loss: 0.680, ave_loss: 0.680
[38]  [740/1724] loss: 0.756, ave_loss: 0.682
[39]  [760/1724] loss: 0.636, ave_loss: 0.681
[40]  [780/1724] loss: 0.703, ave_loss: 0.681
[41]  [800/1724] loss: 0.569, ave_loss: 0.678
[42]  [820/1724] loss: 0.709, ave_loss: 0.679
[43]  [840/1724] loss: 0.699, ave_loss: 0.680
[44]  [860/1724] loss: 0.748, ave_loss: 0.681
[45]  [880/1724] loss: 0.616, ave_loss: 0.680
[46]  [900/1724] loss: 0.621, ave_loss: 0.679
[47]  [920/1724] loss: 0.676, ave_loss: 0.678
[48]  [940/1724] loss: 0.692, ave_loss: 0.679
[49]  [960/1724] loss: 0.671, ave_loss: 0.679
[50]  [980/1724] loss: 0.509, ave_loss: 0.675
[51]  [1000/1724] loss: 0.703, ave_loss: 0.676
[52]  [1020/1724] loss: 0.602, ave_loss: 0.674
[53]  [1040/1724] loss: 0.725, ave_loss: 0.675
[54]  [1060/1724] loss: 0.664, ave_loss: 0.675
[55]  [1080/1724] loss: 0.672, ave_loss: 0.675
[56]  [1100/1724] loss: 0.666, ave_loss: 0.675
[57]  [1120/1724] loss: 0.635, ave_loss: 0.674
[58]  [1140/1724] loss: 0.591, ave_loss: 0.673
[59]  [1160/1724] loss: 0.726, ave_loss: 0.674
[60]  [1180/1724] loss: 0.805, ave_loss: 0.676
[61]  [1200/1724] loss: 0.849, ave_loss: 0.679
[62]  [1220/1724] loss: 0.799, ave_loss: 0.681
[63]  [1240/1724] loss: 0.773, ave_loss: 0.682
[64]  [1260/1724] loss: 0.574, ave_loss: 0.680
[65]  [1280/1724] loss: 0.628, ave_loss: 0.680
[66]  [1300/1724] loss: 0.639, ave_loss: 0.679
[67]  [1320/1724] loss: 0.721, ave_loss: 0.680
[68]  [1340/1724] loss: 0.576, ave_loss: 0.678
[69]  [1360/1724] loss: 0.592, ave_loss: 0.677
[70]  [1380/1724] loss: 0.669, ave_loss: 0.677
[71]  [1400/1724] loss: 0.638, ave_loss: 0.676
[72]  [1420/1724] loss: 0.627, ave_loss: 0.675
[73]  [1440/1724] loss: 0.720, ave_loss: 0.676
[74]  [1460/1724] loss: 0.668, ave_loss: 0.676
[75]  [1480/1724] loss: 0.640, ave_loss: 0.676
[76]  [1500/1724] loss: 0.630, ave_loss: 0.675
[77]  [1520/1724] loss: 0.553, ave_loss: 0.673
[78]  [1540/1724] loss: 0.785, ave_loss: 0.675
[79]  [1560/1724] loss: 0.802, ave_loss: 0.676
[80]  [1580/1724] loss: 0.645, ave_loss: 0.676
[81]  [1600/1724] loss: 0.652, ave_loss: 0.676
[82]  [1620/1724] loss: 0.601, ave_loss: 0.675
[83]  [1640/1724] loss: 0.709, ave_loss: 0.675
[84]  [1660/1724] loss: 0.725, ave_loss: 0.676
[85]  [1680/1724] loss: 0.646, ave_loss: 0.675
[86]  [1700/1724] loss: 0.574, ave_loss: 0.674
[87]  [1720/1724] loss: 0.729, ave_loss: 0.675
[88]  [1740/1724] loss: 0.675, ave_loss: 0.675

Finished Training finishing at 2021-08-25 15:29:34.159363
printing_out epoch  9.187935034802784 learning rate: 0.0005153561248318907
0.00039178973236506245
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.749e-01
Validation Loss: 5.834e-01
Validation ROC: 0.4459
No improvement, still saving model
19.812064965197216 epochs left to go

Training Epoch 9.187935034802784/30 starting at 2021-08-25 15:29:59.820505
[1]  [0/1724] loss: 0.652, ave_loss: 0.652
[2]  [20/1724] loss: 0.729, ave_loss: 0.690
[3]  [40/1724] loss: 0.625, ave_loss: 0.669
[4]  [60/1724] loss: 0.617, ave_loss: 0.656
[5]  [80/1724] loss: 0.684, ave_loss: 0.661
[6]  [100/1724] loss: 0.617, ave_loss: 0.654
[7]  [120/1724] loss: 0.680, ave_loss: 0.658
[8]  [140/1724] loss: 0.736, ave_loss: 0.667
[9]  [160/1724] loss: 0.698, ave_loss: 0.671
[10]  [180/1724] loss: 0.663, ave_loss: 0.670
[11]  [200/1724] loss: 0.533, ave_loss: 0.658
[12]  [220/1724] loss: 0.707, ave_loss: 0.662
[13]  [240/1724] loss: 0.710, ave_loss: 0.665
[14]  [260/1724] loss: 0.712, ave_loss: 0.669
[15]  [280/1724] loss: 0.811, ave_loss: 0.678
[16]  [300/1724] loss: 0.592, ave_loss: 0.673
[17]  [320/1724] loss: 0.638, ave_loss: 0.671
[18]  [340/1724] loss: 0.642, ave_loss: 0.669
[19]  [360/1724] loss: 0.635, ave_loss: 0.667
[20]  [380/1724] loss: 0.686, ave_loss: 0.668
[21]  [400/1724] loss: 0.630, ave_loss: 0.667
[22]  [420/1724] loss: 0.568, ave_loss: 0.662
[23]  [440/1724] loss: 0.684, ave_loss: 0.663
[24]  [460/1724] loss: 0.541, ave_loss: 0.658
[25]  [480/1724] loss: 0.602, ave_loss: 0.656
[26]  [500/1724] loss: 0.634, ave_loss: 0.655
[27]  [520/1724] loss: 0.680, ave_loss: 0.656
[28]  [540/1724] loss: 0.575, ave_loss: 0.653
[29]  [560/1724] loss: 0.565, ave_loss: 0.650
[30]  [580/1724] loss: 0.589, ave_loss: 0.648
[31]  [600/1724] loss: 0.622, ave_loss: 0.647
[32]  [620/1724] loss: 0.881, ave_loss: 0.654
[33]  [640/1724] loss: 0.765, ave_loss: 0.658
[34]  [660/1724] loss: 0.788, ave_loss: 0.661
[35]  [680/1724] loss: 0.592, ave_loss: 0.660
[36]  [700/1724] loss: 0.495, ave_loss: 0.655
[37]  [720/1724] loss: 0.620, ave_loss: 0.654
[38]  [740/1724] loss: 0.692, ave_loss: 0.655
[39]  [760/1724] loss: 0.556, ave_loss: 0.652
[40]  [780/1724] loss: 0.691, ave_loss: 0.653
[41]  [800/1724] loss: 0.641, ave_loss: 0.653
[42]  [820/1724] loss: 0.630, ave_loss: 0.653
[43]  [840/1724] loss: 0.576, ave_loss: 0.651
[44]  [860/1724] loss: 0.639, ave_loss: 0.651
[45]  [880/1724] loss: 0.568, ave_loss: 0.649
[46]  [900/1724] loss: 0.748, ave_loss: 0.651
[47]  [920/1724] loss: 0.688, ave_loss: 0.652
[48]  [940/1724] loss: 0.812, ave_loss: 0.655
[49]  [960/1724] loss: 0.559, ave_loss: 0.653
[50]  [980/1724] loss: 0.642, ave_loss: 0.653
[51]  [1000/1724] loss: 0.871, ave_loss: 0.657
[52]  [1020/1724] loss: 0.625, ave_loss: 0.656
[53]  [1040/1724] loss: 0.608, ave_loss: 0.656
[54]  [1060/1724] loss: 0.749, ave_loss: 0.657
[55]  [1080/1724] loss: 0.628, ave_loss: 0.657
[56]  [1100/1724] loss: 0.575, ave_loss: 0.655
[57]  [1120/1724] loss: 0.583, ave_loss: 0.654
[58]  [1140/1724] loss: 0.611, ave_loss: 0.653
[59]  [1160/1724] loss: 0.721, ave_loss: 0.654
[60]  [1180/1724] loss: 0.709, ave_loss: 0.655
[61]  [1200/1724] loss: 0.685, ave_loss: 0.656
[62]  [1220/1724] loss: 0.664, ave_loss: 0.656
[63]  [1240/1724] loss: 0.647, ave_loss: 0.656
[64]  [1260/1724] loss: 0.676, ave_loss: 0.656
[65]  [1280/1724] loss: 0.795, ave_loss: 0.658
[66]  [1300/1724] loss: 0.703, ave_loss: 0.659
[67]  [1320/1724] loss: 0.682, ave_loss: 0.659
[68]  [1340/1724] loss: 0.772, ave_loss: 0.661
[69]  [1360/1724] loss: 0.627, ave_loss: 0.660
[70]  [1380/1724] loss: 0.617, ave_loss: 0.660
[71]  [1400/1724] loss: 0.644, ave_loss: 0.660
[72]  [1420/1724] loss: 0.650, ave_loss: 0.659
[73]  [1440/1724] loss: 0.739, ave_loss: 0.661
[74]  [1460/1724] loss: 0.702, ave_loss: 0.661
[75]  [1480/1724] loss: 0.739, ave_loss: 0.662
[76]  [1500/1724] loss: 0.656, ave_loss: 0.662
[77]  [1520/1724] loss: 0.758, ave_loss: 0.663
[78]  [1540/1724] loss: 0.803, ave_loss: 0.665
[79]  [1560/1724] loss: 0.615, ave_loss: 0.664
[80]  [1580/1724] loss: 0.743, ave_loss: 0.665
[81]  [1600/1724] loss: 0.638, ave_loss: 0.665
[82]  [1620/1724] loss: 0.588, ave_loss: 0.664
[83]  [1640/1724] loss: 0.630, ave_loss: 0.664
[84]  [1660/1724] loss: 0.605, ave_loss: 0.663
[85]  [1680/1724] loss: 0.667, ave_loss: 0.663
[86]  [1700/1724] loss: 0.607, ave_loss: 0.662
[87]  [1720/1724] loss: 0.749, ave_loss: 0.663
[88]  [1740/1724] loss: 0.645, ave_loss: 0.663

Finished Training finishing at 2021-08-25 15:31:18.097562
printing_out epoch  10.208816705336426 learning rate: 0.0005153561248318907
0.0003800360403941106
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.633e-01
Validation Loss: 5.889e-01
Validation ROC: 0.4459
No improvement, still saving model
18.791183294663576 epochs left to go

Training Epoch 10.208816705336426/30 starting at 2021-08-25 15:31:43.094903
[1]  [0/1724] loss: 0.699, ave_loss: 0.699
[2]  [20/1724] loss: 0.709, ave_loss: 0.704
[3]  [40/1724] loss: 0.638, ave_loss: 0.682
[4]  [60/1724] loss: 0.696, ave_loss: 0.685
[5]  [80/1724] loss: 0.674, ave_loss: 0.683
[6]  [100/1724] loss: 0.655, ave_loss: 0.678
[7]  [120/1724] loss: 0.542, ave_loss: 0.659
[8]  [140/1724] loss: 0.662, ave_loss: 0.659
[9]  [160/1724] loss: 0.628, ave_loss: 0.656
[10]  [180/1724] loss: 0.640, ave_loss: 0.654
[11]  [200/1724] loss: 0.721, ave_loss: 0.660
[12]  [220/1724] loss: 0.649, ave_loss: 0.659
[13]  [240/1724] loss: 0.732, ave_loss: 0.665
[14]  [260/1724] loss: 0.598, ave_loss: 0.660
[15]  [280/1724] loss: 0.552, ave_loss: 0.653
[16]  [300/1724] loss: 0.550, ave_loss: 0.647
[17]  [320/1724] loss: 0.716, ave_loss: 0.651
[18]  [340/1724] loss: 0.786, ave_loss: 0.658
[19]  [360/1724] loss: 0.608, ave_loss: 0.656
[20]  [380/1724] loss: 0.601, ave_loss: 0.653
[21]  [400/1724] loss: 0.717, ave_loss: 0.656
[22]  [420/1724] loss: 0.554, ave_loss: 0.651
[23]  [440/1724] loss: 0.599, ave_loss: 0.649
[24]  [460/1724] loss: 0.696, ave_loss: 0.651
[25]  [480/1724] loss: 0.643, ave_loss: 0.651
[26]  [500/1724] loss: 0.721, ave_loss: 0.653
[27]  [520/1724] loss: 0.685, ave_loss: 0.654
[28]  [540/1724] loss: 0.672, ave_loss: 0.655
[29]  [560/1724] loss: 0.553, ave_loss: 0.652
[30]  [580/1724] loss: 0.632, ave_loss: 0.651
[31]  [600/1724] loss: 0.648, ave_loss: 0.651
[32]  [620/1724] loss: 0.547, ave_loss: 0.648
[33]  [640/1724] loss: 0.620, ave_loss: 0.647
[34]  [660/1724] loss: 0.679, ave_loss: 0.648
[35]  [680/1724] loss: 0.568, ave_loss: 0.645
[36]  [700/1724] loss: 0.608, ave_loss: 0.644
[37]  [720/1724] loss: 0.663, ave_loss: 0.645
[38]  [740/1724] loss: 0.555, ave_loss: 0.643
[39]  [760/1724] loss: 0.545, ave_loss: 0.640
[40]  [780/1724] loss: 0.668, ave_loss: 0.641
[41]  [800/1724] loss: 0.636, ave_loss: 0.641
[42]  [820/1724] loss: 0.540, ave_loss: 0.638
[43]  [840/1724] loss: 0.438, ave_loss: 0.634
[44]  [860/1724] loss: 0.594, ave_loss: 0.633
[45]  [880/1724] loss: 0.705, ave_loss: 0.634
[46]  [900/1724] loss: 0.580, ave_loss: 0.633
[47]  [920/1724] loss: 0.584, ave_loss: 0.632
[48]  [940/1724] loss: 0.713, ave_loss: 0.634
[49]  [960/1724] loss: 0.709, ave_loss: 0.635
[50]  [980/1724] loss: 0.629, ave_loss: 0.635
[51]  [1000/1724] loss: 0.540, ave_loss: 0.633
[52]  [1020/1724] loss: 0.631, ave_loss: 0.633
[53]  [1040/1724] loss: 0.818, ave_loss: 0.637
[54]  [1060/1724] loss: 0.616, ave_loss: 0.636
[55]  [1080/1724] loss: 0.578, ave_loss: 0.635
[56]  [1100/1724] loss: 0.553, ave_loss: 0.634
[57]  [1120/1724] loss: 0.633, ave_loss: 0.634
[58]  [1140/1724] loss: 0.733, ave_loss: 0.636
[59]  [1160/1724] loss: 0.785, ave_loss: 0.638
[60]  [1180/1724] loss: 0.617, ave_loss: 0.638
[61]  [1200/1724] loss: 0.608, ave_loss: 0.637
[62]  [1220/1724] loss: 0.615, ave_loss: 0.637
[63]  [1240/1724] loss: 0.634, ave_loss: 0.637
[64]  [1260/1724] loss: 0.725, ave_loss: 0.638
[65]  [1280/1724] loss: 0.669, ave_loss: 0.639
[66]  [1300/1724] loss: 0.551, ave_loss: 0.637
[67]  [1320/1724] loss: 0.718, ave_loss: 0.639
[68]  [1340/1724] loss: 0.611, ave_loss: 0.638
[69]  [1360/1724] loss: 0.613, ave_loss: 0.638
[70]  [1380/1724] loss: 0.599, ave_loss: 0.637
[71]  [1400/1724] loss: 0.660, ave_loss: 0.638
[72]  [1420/1724] loss: 0.721, ave_loss: 0.639
[73]  [1440/1724] loss: 0.751, ave_loss: 0.640
[74]  [1460/1724] loss: 0.509, ave_loss: 0.638
[75]  [1480/1724] loss: 0.617, ave_loss: 0.638
[76]  [1500/1724] loss: 0.658, ave_loss: 0.638
[77]  [1520/1724] loss: 0.638, ave_loss: 0.638
[78]  [1540/1724] loss: 0.606, ave_loss: 0.638
[79]  [1560/1724] loss: 0.626, ave_loss: 0.638
[80]  [1580/1724] loss: 0.544, ave_loss: 0.637
[81]  [1600/1724] loss: 0.512, ave_loss: 0.635
[82]  [1620/1724] loss: 0.696, ave_loss: 0.636
[83]  [1640/1724] loss: 0.572, ave_loss: 0.635
[84]  [1660/1724] loss: 0.534, ave_loss: 0.634
[85]  [1680/1724] loss: 0.591, ave_loss: 0.633
[86]  [1700/1724] loss: 0.549, ave_loss: 0.632
[87]  [1720/1724] loss: 0.719, ave_loss: 0.633
[88]  [1740/1724] loss: 0.524, ave_loss: 0.632

Finished Training finishing at 2021-08-25 15:33:01.689437
printing_out epoch  11.22969837587007 learning rate: 0.00046850556802899155
0.0004544504009881218
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.322e-01
Validation Loss: 5.672e-01
Validation ROC: 0.4459
No improvement, still saving model
17.770301624129928 epochs left to go

Training Epoch 11.22969837587007/30 starting at 2021-08-25 15:33:24.725028
[1]  [0/1724] loss: 0.728, ave_loss: 0.728
[2]  [20/1724] loss: 0.646, ave_loss: 0.687
[3]  [40/1724] loss: 0.608, ave_loss: 0.661
[4]  [60/1724] loss: 0.675, ave_loss: 0.664
[5]  [80/1724] loss: 0.701, ave_loss: 0.672
[6]  [100/1724] loss: 0.719, ave_loss: 0.680
[7]  [120/1724] loss: 0.695, ave_loss: 0.682
[8]  [140/1724] loss: 0.832, ave_loss: 0.701
[9]  [160/1724] loss: 0.599, ave_loss: 0.689
[10]  [180/1724] loss: 0.622, ave_loss: 0.683
[11]  [200/1724] loss: 0.628, ave_loss: 0.678
[12]  [220/1724] loss: 0.683, ave_loss: 0.678
[13]  [240/1724] loss: 0.585, ave_loss: 0.671
[14]  [260/1724] loss: 0.747, ave_loss: 0.676
[15]  [280/1724] loss: 0.640, ave_loss: 0.674
[16]  [300/1724] loss: 0.602, ave_loss: 0.669
[17]  [320/1724] loss: 0.802, ave_loss: 0.677
[18]  [340/1724] loss: 0.628, ave_loss: 0.675
[19]  [360/1724] loss: 0.588, ave_loss: 0.670
[20]  [380/1724] loss: 0.580, ave_loss: 0.665
[21]  [400/1724] loss: 0.655, ave_loss: 0.665
[22]  [420/1724] loss: 0.587, ave_loss: 0.661
[23]  [440/1724] loss: 0.609, ave_loss: 0.659
[24]  [460/1724] loss: 0.671, ave_loss: 0.660
[25]  [480/1724] loss: 0.564, ave_loss: 0.656
[26]  [500/1724] loss: 0.805, ave_loss: 0.662
[27]  [520/1724] loss: 0.541, ave_loss: 0.657
[28]  [540/1724] loss: 0.961, ave_loss: 0.668
[29]  [560/1724] loss: 0.637, ave_loss: 0.667
[30]  [580/1724] loss: 0.750, ave_loss: 0.670
[31]  [600/1724] loss: 0.643, ave_loss: 0.669
[32]  [620/1724] loss: 0.596, ave_loss: 0.667
[33]  [640/1724] loss: 0.668, ave_loss: 0.667
[34]  [660/1724] loss: 0.636, ave_loss: 0.666
[35]  [680/1724] loss: 0.627, ave_loss: 0.665
[36]  [700/1724] loss: 0.731, ave_loss: 0.666
[37]  [720/1724] loss: 0.588, ave_loss: 0.664
[38]  [740/1724] loss: 0.738, ave_loss: 0.666
[39]  [760/1724] loss: 0.722, ave_loss: 0.668
[40]  [780/1724] loss: 0.531, ave_loss: 0.664
[41]  [800/1724] loss: 0.600, ave_loss: 0.663
[42]  [820/1724] loss: 0.715, ave_loss: 0.664
[43]  [840/1724] loss: 0.714, ave_loss: 0.665
[44]  [860/1724] loss: 0.661, ave_loss: 0.665
[45]  [880/1724] loss: 0.588, ave_loss: 0.663
[46]  [900/1724] loss: 0.657, ave_loss: 0.663
[47]  [920/1724] loss: 0.689, ave_loss: 0.664
[48]  [940/1724] loss: 0.559, ave_loss: 0.662
[49]  [960/1724] loss: 0.610, ave_loss: 0.660
[50]  [980/1724] loss: 0.682, ave_loss: 0.661
[51]  [1000/1724] loss: 0.604, ave_loss: 0.660
[52]  [1020/1724] loss: 0.623, ave_loss: 0.659
[53]  [1040/1724] loss: 0.653, ave_loss: 0.659
[54]  [1060/1724] loss: 0.700, ave_loss: 0.660
[55]  [1080/1724] loss: 0.713, ave_loss: 0.661
[56]  [1100/1724] loss: 0.557, ave_loss: 0.659
[57]  [1120/1724] loss: 0.622, ave_loss: 0.658
[58]  [1140/1724] loss: 0.647, ave_loss: 0.658
[59]  [1160/1724] loss: 0.530, ave_loss: 0.656
[60]  [1180/1724] loss: 0.522, ave_loss: 0.654
[61]  [1200/1724] loss: 0.645, ave_loss: 0.653
[62]  [1220/1724] loss: 0.647, ave_loss: 0.653
[63]  [1240/1724] loss: 0.779, ave_loss: 0.655
[64]  [1260/1724] loss: 0.876, ave_loss: 0.659
[65]  [1280/1724] loss: 0.696, ave_loss: 0.659
[66]  [1300/1724] loss: 0.555, ave_loss: 0.658
[67]  [1320/1724] loss: 0.701, ave_loss: 0.658
[68]  [1340/1724] loss: 0.628, ave_loss: 0.658
[69]  [1360/1724] loss: 0.587, ave_loss: 0.657
[70]  [1380/1724] loss: 0.636, ave_loss: 0.657
[71]  [1400/1724] loss: 0.683, ave_loss: 0.657
[72]  [1420/1724] loss: 0.590, ave_loss: 0.656
[73]  [1440/1724] loss: 0.643, ave_loss: 0.656
[74]  [1460/1724] loss: 0.538, ave_loss: 0.654
[75]  [1480/1724] loss: 0.675, ave_loss: 0.655
[76]  [1500/1724] loss: 0.569, ave_loss: 0.654
[77]  [1520/1724] loss: 0.517, ave_loss: 0.652
[78]  [1540/1724] loss: 0.595, ave_loss: 0.651
[79]  [1560/1724] loss: 0.601, ave_loss: 0.650
[80]  [1580/1724] loss: 0.685, ave_loss: 0.651
[81]  [1600/1724] loss: 0.642, ave_loss: 0.651
[82]  [1620/1724] loss: 0.713, ave_loss: 0.651
[83]  [1640/1724] loss: 0.626, ave_loss: 0.651
[84]  [1660/1724] loss: 0.739, ave_loss: 0.652
[85]  [1680/1724] loss: 0.531, ave_loss: 0.651
[86]  [1700/1724] loss: 0.777, ave_loss: 0.652
[87]  [1720/1724] loss: 0.659, ave_loss: 0.652
[88]  [1740/1724] loss: 0.888, ave_loss: 0.655

Finished Training finishing at 2021-08-25 15:34:42.355176
printing_out epoch  12.250580046403712 learning rate: 0.00042591415275362865
0.0004131367281710198
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.550e-01
Validation Loss: 5.753e-01
Validation ROC: 0.4356
No improvement, still saving model
16.749419953596288 epochs left to go

Training Epoch 12.250580046403712/30 starting at 2021-08-25 15:35:08.331316
[1]  [0/1724] loss: 0.560, ave_loss: 0.560
[2]  [20/1724] loss: 0.747, ave_loss: 0.653
[3]  [40/1724] loss: 0.819, ave_loss: 0.709
[4]  [60/1724] loss: 0.743, ave_loss: 0.717
[5]  [80/1724] loss: 0.543, ave_loss: 0.682
[6]  [100/1724] loss: 0.591, ave_loss: 0.667
[7]  [120/1724] loss: 0.710, ave_loss: 0.673
[8]  [140/1724] loss: 0.572, ave_loss: 0.661
[9]  [160/1724] loss: 0.598, ave_loss: 0.654
[10]  [180/1724] loss: 0.781, ave_loss: 0.666
[11]  [200/1724] loss: 0.578, ave_loss: 0.658
[12]  [220/1724] loss: 0.759, ave_loss: 0.667
[13]  [240/1724] loss: 0.625, ave_loss: 0.664
[14]  [260/1724] loss: 0.641, ave_loss: 0.662
[15]  [280/1724] loss: 0.714, ave_loss: 0.665
[16]  [300/1724] loss: 0.681, ave_loss: 0.666
[17]  [320/1724] loss: 0.686, ave_loss: 0.668
[18]  [340/1724] loss: 0.673, ave_loss: 0.668
[19]  [360/1724] loss: 0.800, ave_loss: 0.675
[20]  [380/1724] loss: 0.630, ave_loss: 0.673
[21]  [400/1724] loss: 0.757, ave_loss: 0.677
[22]  [420/1724] loss: 0.542, ave_loss: 0.670
[23]  [440/1724] loss: 0.608, ave_loss: 0.668
[24]  [460/1724] loss: 0.707, ave_loss: 0.669
[25]  [480/1724] loss: 0.548, ave_loss: 0.665
[26]  [500/1724] loss: 0.607, ave_loss: 0.662
[27]  [520/1724] loss: 0.738, ave_loss: 0.665
[28]  [540/1724] loss: 0.584, ave_loss: 0.662
[29]  [560/1724] loss: 0.717, ave_loss: 0.664
[30]  [580/1724] loss: 0.675, ave_loss: 0.664
[31]  [600/1724] loss: 0.617, ave_loss: 0.663
[32]  [620/1724] loss: 0.560, ave_loss: 0.660
[33]  [640/1724] loss: 0.599, ave_loss: 0.658
[34]  [660/1724] loss: 0.608, ave_loss: 0.656
[35]  [680/1724] loss: 0.653, ave_loss: 0.656
[36]  [700/1724] loss: 0.703, ave_loss: 0.658
[37]  [720/1724] loss: 0.607, ave_loss: 0.656
[38]  [740/1724] loss: 0.758, ave_loss: 0.659
[39]  [760/1724] loss: 0.641, ave_loss: 0.658
[40]  [780/1724] loss: 0.694, ave_loss: 0.659
[41]  [800/1724] loss: 0.523, ave_loss: 0.656
[42]  [820/1724] loss: 0.767, ave_loss: 0.659
[43]  [840/1724] loss: 0.515, ave_loss: 0.655
[44]  [860/1724] loss: 0.780, ave_loss: 0.658
[45]  [880/1724] loss: 0.559, ave_loss: 0.656
[46]  [900/1724] loss: 0.521, ave_loss: 0.653
[47]  [920/1724] loss: 0.640, ave_loss: 0.653
[48]  [940/1724] loss: 0.593, ave_loss: 0.652
[49]  [960/1724] loss: 0.595, ave_loss: 0.650
[50]  [980/1724] loss: 0.698, ave_loss: 0.651
[51]  [1000/1724] loss: 0.546, ave_loss: 0.649
[52]  [1020/1724] loss: 0.714, ave_loss: 0.651
[53]  [1040/1724] loss: 0.686, ave_loss: 0.651
[54]  [1060/1724] loss: 0.651, ave_loss: 0.651
[55]  [1080/1724] loss: 0.628, ave_loss: 0.651
[56]  [1100/1724] loss: 0.557, ave_loss: 0.649
[57]  [1120/1724] loss: 0.605, ave_loss: 0.648
[58]  [1140/1724] loss: 0.780, ave_loss: 0.651
[59]  [1160/1724] loss: 0.638, ave_loss: 0.650
[60]  [1180/1724] loss: 0.507, ave_loss: 0.648
[61]  [1200/1724] loss: 0.693, ave_loss: 0.649
[62]  [1220/1724] loss: 0.660, ave_loss: 0.649
[63]  [1240/1724] loss: 0.723, ave_loss: 0.650
[64]  [1260/1724] loss: 0.529, ave_loss: 0.648
[65]  [1280/1724] loss: 0.516, ave_loss: 0.646
[66]  [1300/1724] loss: 0.691, ave_loss: 0.647
[67]  [1320/1724] loss: 0.660, ave_loss: 0.647
[68]  [1340/1724] loss: 0.541, ave_loss: 0.645
[69]  [1360/1724] loss: 0.721, ave_loss: 0.647
[70]  [1380/1724] loss: 0.671, ave_loss: 0.647
[71]  [1400/1724] loss: 0.807, ave_loss: 0.649
[72]  [1420/1724] loss: 0.782, ave_loss: 0.651
[73]  [1440/1724] loss: 0.615, ave_loss: 0.651
[74]  [1460/1724] loss: 0.643, ave_loss: 0.650
[75]  [1480/1724] loss: 0.835, ave_loss: 0.653
[76]  [1500/1724] loss: 0.673, ave_loss: 0.653
[77]  [1520/1724] loss: 0.649, ave_loss: 0.653
[78]  [1540/1724] loss: 0.670, ave_loss: 0.653
[79]  [1560/1724] loss: 0.620, ave_loss: 0.653
[80]  [1580/1724] loss: 0.522, ave_loss: 0.651
[81]  [1600/1724] loss: 0.728, ave_loss: 0.652
[82]  [1620/1724] loss: 0.701, ave_loss: 0.653
[83]  [1640/1724] loss: 0.650, ave_loss: 0.653
[84]  [1660/1724] loss: 0.663, ave_loss: 0.653
[85]  [1680/1724] loss: 0.509, ave_loss: 0.651
[86]  [1700/1724] loss: 0.739, ave_loss: 0.652
[87]  [1720/1724] loss: 0.586, ave_loss: 0.651
[88]  [1740/1724] loss: 0.726, ave_loss: 0.652

Finished Training finishing at 2021-08-25 15:36:30.173866
printing_out epoch  13.271461716937354 learning rate: 0.00038719468432148055
0.0003755788437918361
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.523e-01
Validation Loss: 5.817e-01
Validation ROC: 0.4356
No improvement, still saving model
15.728538283062646 epochs left to go

Training Epoch 13.271461716937354/30 starting at 2021-08-25 15:36:52.586923
[1]  [0/1724] loss: 0.641, ave_loss: 0.641
[2]  [20/1724] loss: 0.645, ave_loss: 0.643
[3]  [40/1724] loss: 0.568, ave_loss: 0.618
[4]  [60/1724] loss: 0.758, ave_loss: 0.653
[5]  [80/1724] loss: 0.643, ave_loss: 0.651
[6]  [100/1724] loss: 0.690, ave_loss: 0.657
[7]  [120/1724] loss: 0.631, ave_loss: 0.654
[8]  [140/1724] loss: 0.572, ave_loss: 0.644
[9]  [160/1724] loss: 0.631, ave_loss: 0.642
[10]  [180/1724] loss: 0.722, ave_loss: 0.650
[11]  [200/1724] loss: 0.720, ave_loss: 0.656
[12]  [220/1724] loss: 0.656, ave_loss: 0.656
[13]  [240/1724] loss: 0.515, ave_loss: 0.646
[14]  [260/1724] loss: 0.691, ave_loss: 0.649
[15]  [280/1724] loss: 0.612, ave_loss: 0.646
[16]  [300/1724] loss: 0.575, ave_loss: 0.642
[17]  [320/1724] loss: 0.696, ave_loss: 0.645
[18]  [340/1724] loss: 0.627, ave_loss: 0.644
[19]  [360/1724] loss: 0.617, ave_loss: 0.643
[20]  [380/1724] loss: 0.702, ave_loss: 0.646
[21]  [400/1724] loss: 0.624, ave_loss: 0.645
[22]  [420/1724] loss: 0.559, ave_loss: 0.641
[23]  [440/1724] loss: 0.754, ave_loss: 0.646
[24]  [460/1724] loss: 0.718, ave_loss: 0.649
[25]  [480/1724] loss: 0.754, ave_loss: 0.653
[26]  [500/1724] loss: 0.595, ave_loss: 0.651
[27]  [520/1724] loss: 0.648, ave_loss: 0.650
[28]  [540/1724] loss: 0.545, ave_loss: 0.647
[29]  [560/1724] loss: 0.607, ave_loss: 0.645
[30]  [580/1724] loss: 0.614, ave_loss: 0.644
[31]  [600/1724] loss: 0.676, ave_loss: 0.645
[32]  [620/1724] loss: 0.676, ave_loss: 0.646
[33]  [640/1724] loss: 0.535, ave_loss: 0.643
[34]  [660/1724] loss: 0.872, ave_loss: 0.650
[35]  [680/1724] loss: 0.607, ave_loss: 0.648
[36]  [700/1724] loss: 0.633, ave_loss: 0.648
[37]  [720/1724] loss: 0.593, ave_loss: 0.646
[38]  [740/1724] loss: 0.573, ave_loss: 0.645
[39]  [760/1724] loss: 0.730, ave_loss: 0.647
[40]  [780/1724] loss: 0.663, ave_loss: 0.647
[41]  [800/1724] loss: 0.671, ave_loss: 0.648
[42]  [820/1724] loss: 0.522, ave_loss: 0.645
[43]  [840/1724] loss: 0.660, ave_loss: 0.645
[44]  [860/1724] loss: 0.622, ave_loss: 0.645
[45]  [880/1724] loss: 0.650, ave_loss: 0.645
[46]  [900/1724] loss: 0.634, ave_loss: 0.644
[47]  [920/1724] loss: 0.623, ave_loss: 0.644
[48]  [940/1724] loss: 0.652, ave_loss: 0.644
[49]  [960/1724] loss: 0.708, ave_loss: 0.645
[50]  [980/1724] loss: 0.744, ave_loss: 0.647
[51]  [1000/1724] loss: 0.755, ave_loss: 0.650
[52]  [1020/1724] loss: 0.614, ave_loss: 0.649
[53]  [1040/1724] loss: 0.666, ave_loss: 0.649
[54]  [1060/1724] loss: 0.664, ave_loss: 0.649
[55]  [1080/1724] loss: 0.661, ave_loss: 0.650
[56]  [1100/1724] loss: 0.768, ave_loss: 0.652
[57]  [1120/1724] loss: 0.488, ave_loss: 0.649
[58]  [1140/1724] loss: 0.760, ave_loss: 0.651
[59]  [1160/1724] loss: 0.537, ave_loss: 0.649
[60]  [1180/1724] loss: 0.638, ave_loss: 0.649
[61]  [1200/1724] loss: 0.759, ave_loss: 0.651
[62]  [1220/1724] loss: 0.742, ave_loss: 0.652
[63]  [1240/1724] loss: 0.732, ave_loss: 0.653
[64]  [1260/1724] loss: 0.608, ave_loss: 0.653
[65]  [1280/1724] loss: 0.866, ave_loss: 0.656
[66]  [1300/1724] loss: 0.539, ave_loss: 0.654
[67]  [1320/1724] loss: 0.690, ave_loss: 0.655
[68]  [1340/1724] loss: 0.619, ave_loss: 0.654
[69]  [1360/1724] loss: 0.692, ave_loss: 0.655
[70]  [1380/1724] loss: 0.594, ave_loss: 0.654
[71]  [1400/1724] loss: 0.564, ave_loss: 0.652
[72]  [1420/1724] loss: 0.631, ave_loss: 0.652
[73]  [1440/1724] loss: 0.613, ave_loss: 0.652
[74]  [1460/1724] loss: 0.663, ave_loss: 0.652
[75]  [1480/1724] loss: 0.721, ave_loss: 0.653
[76]  [1500/1724] loss: 0.500, ave_loss: 0.651
[77]  [1520/1724] loss: 0.577, ave_loss: 0.650
[78]  [1540/1724] loss: 0.687, ave_loss: 0.650
[79]  [1560/1724] loss: 0.545, ave_loss: 0.649
[80]  [1580/1724] loss: 0.638, ave_loss: 0.649
[81]  [1600/1724] loss: 0.672, ave_loss: 0.649
[82]  [1620/1724] loss: 0.563, ave_loss: 0.648
[83]  [1640/1724] loss: 0.690, ave_loss: 0.649
[84]  [1660/1724] loss: 0.621, ave_loss: 0.648
[85]  [1680/1724] loss: 0.749, ave_loss: 0.649
[86]  [1700/1724] loss: 0.561, ave_loss: 0.648
[87]  [1720/1724] loss: 0.749, ave_loss: 0.650
[88]  [1740/1724] loss: 0.690, ave_loss: 0.650

Finished Training finishing at 2021-08-25 15:38:10.508357
printing_out epoch  14.292343387470998 learning rate: 0.0003519951675649823
0.00034143531253803285
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.500e-01
Validation Loss: 5.811e-01
Validation ROC: 0.4356
No improvement, still saving model
14.707656612529002 epochs left to go

Training Epoch 14.292343387470998/30 starting at 2021-08-25 15:38:32.110685
[1]  [0/1724] loss: 0.843, ave_loss: 0.843
[2]  [20/1724] loss: 0.556, ave_loss: 0.699
[3]  [40/1724] loss: 0.640, ave_loss: 0.680
[4]  [60/1724] loss: 0.670, ave_loss: 0.677
[5]  [80/1724] loss: 0.625, ave_loss: 0.667
[6]  [100/1724] loss: 0.733, ave_loss: 0.678
[7]  [120/1724] loss: 0.641, ave_loss: 0.673
[8]  [140/1724] loss: 0.598, ave_loss: 0.663
[9]  [160/1724] loss: 0.689, ave_loss: 0.666
[10]  [180/1724] loss: 0.593, ave_loss: 0.659
[11]  [200/1724] loss: 0.601, ave_loss: 0.654
[12]  [220/1724] loss: 0.652, ave_loss: 0.653
[13]  [240/1724] loss: 0.687, ave_loss: 0.656
[14]  [260/1724] loss: 0.571, ave_loss: 0.650
[15]  [280/1724] loss: 0.555, ave_loss: 0.644
[16]  [300/1724] loss: 0.695, ave_loss: 0.647
[17]  [320/1724] loss: 0.586, ave_loss: 0.643
[18]  [340/1724] loss: 0.670, ave_loss: 0.645
[19]  [360/1724] loss: 0.665, ave_loss: 0.646
[20]  [380/1724] loss: 0.648, ave_loss: 0.646
[21]  [400/1724] loss: 0.620, ave_loss: 0.645
[22]  [420/1724] loss: 0.609, ave_loss: 0.643
[23]  [440/1724] loss: 0.655, ave_loss: 0.644
[24]  [460/1724] loss: 0.676, ave_loss: 0.645
[25]  [480/1724] loss: 0.639, ave_loss: 0.645
[26]  [500/1724] loss: 0.767, ave_loss: 0.649
[27]  [520/1724] loss: 0.645, ave_loss: 0.649
[28]  [540/1724] loss: 0.641, ave_loss: 0.649
[29]  [560/1724] loss: 0.657, ave_loss: 0.649
[30]  [580/1724] loss: 0.831, ave_loss: 0.655
[31]  [600/1724] loss: 0.479, ave_loss: 0.650
[32]  [620/1724] loss: 0.638, ave_loss: 0.649
[33]  [640/1724] loss: 0.575, ave_loss: 0.647
[34]  [660/1724] loss: 0.657, ave_loss: 0.647
[35]  [680/1724] loss: 0.743, ave_loss: 0.650
[36]  [700/1724] loss: 0.654, ave_loss: 0.650
[37]  [720/1724] loss: 0.711, ave_loss: 0.652
[38]  [740/1724] loss: 0.632, ave_loss: 0.651
[39]  [760/1724] loss: 0.736, ave_loss: 0.653
[40]  [780/1724] loss: 0.618, ave_loss: 0.652
[41]  [800/1724] loss: 0.659, ave_loss: 0.653
[42]  [820/1724] loss: 0.629, ave_loss: 0.652
[43]  [840/1724] loss: 0.549, ave_loss: 0.650
[44]  [860/1724] loss: 0.788, ave_loss: 0.653
[45]  [880/1724] loss: 0.816, ave_loss: 0.656
[46]  [900/1724] loss: 0.674, ave_loss: 0.657
[47]  [920/1724] loss: 0.743, ave_loss: 0.659
[48]  [940/1724] loss: 0.745, ave_loss: 0.660
[49]  [960/1724] loss: 0.502, ave_loss: 0.657
[50]  [980/1724] loss: 0.701, ave_loss: 0.658
[51]  [1000/1724] loss: 0.681, ave_loss: 0.659
[52]  [1020/1724] loss: 0.704, ave_loss: 0.659
[53]  [1040/1724] loss: 0.543, ave_loss: 0.657
[54]  [1060/1724] loss: 0.565, ave_loss: 0.656
[55]  [1080/1724] loss: 0.789, ave_loss: 0.658
[56]  [1100/1724] loss: 0.726, ave_loss: 0.659
[57]  [1120/1724] loss: 0.572, ave_loss: 0.658
[58]  [1140/1724] loss: 0.576, ave_loss: 0.656
[59]  [1160/1724] loss: 0.610, ave_loss: 0.655
[60]  [1180/1724] loss: 0.545, ave_loss: 0.654
[61]  [1200/1724] loss: 0.730, ave_loss: 0.655
[62]  [1220/1724] loss: 0.582, ave_loss: 0.654
[63]  [1240/1724] loss: 0.643, ave_loss: 0.654
[64]  [1260/1724] loss: 0.777, ave_loss: 0.655
[65]  [1280/1724] loss: 0.727, ave_loss: 0.657
[66]  [1300/1724] loss: 0.634, ave_loss: 0.656
[67]  [1320/1724] loss: 0.742, ave_loss: 0.657
[68]  [1340/1724] loss: 0.529, ave_loss: 0.656
[69]  [1360/1724] loss: 0.612, ave_loss: 0.655
[70]  [1380/1724] loss: 0.682, ave_loss: 0.655
[71]  [1400/1724] loss: 0.664, ave_loss: 0.655
[72]  [1420/1724] loss: 0.706, ave_loss: 0.656
[73]  [1440/1724] loss: 0.555, ave_loss: 0.655
[74]  [1460/1724] loss: 0.564, ave_loss: 0.654
[75]  [1480/1724] loss: 0.491, ave_loss: 0.651
[76]  [1500/1724] loss: 0.587, ave_loss: 0.651
[77]  [1520/1724] loss: 0.552, ave_loss: 0.649
[78]  [1540/1724] loss: 0.556, ave_loss: 0.648
[79]  [1560/1724] loss: 0.689, ave_loss: 0.649
[80]  [1580/1724] loss: 0.605, ave_loss: 0.648
[81]  [1600/1724] loss: 0.648, ave_loss: 0.648
[82]  [1620/1724] loss: 0.610, ave_loss: 0.648
[83]  [1640/1724] loss: 0.624, ave_loss: 0.647
[84]  [1660/1724] loss: 0.647, ave_loss: 0.647
[85]  [1680/1724] loss: 0.626, ave_loss: 0.647
[86]  [1700/1724] loss: 0.714, ave_loss: 0.648
[87]  [1720/1724] loss: 0.619, ave_loss: 0.647
[88]  [1740/1724] loss: 0.775, ave_loss: 0.649

Finished Training finishing at 2021-08-25 15:39:48.547662
printing_out epoch  15.31322505800464 learning rate: 0.0003199956068772566
0.0003103957386709389
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.489e-01
Validation Loss: 5.702e-01
Validation ROC: 0.4433
No improvement, still saving model
13.68677494199536 epochs left to go

Training Epoch 15.31322505800464/30 starting at 2021-08-25 15:40:12.485351
[1]  [0/1724] loss: 0.636, ave_loss: 0.636
[2]  [20/1724] loss: 0.774, ave_loss: 0.705
[3]  [40/1724] loss: 0.585, ave_loss: 0.665
[4]  [60/1724] loss: 0.629, ave_loss: 0.656
[5]  [80/1724] loss: 0.624, ave_loss: 0.650
[6]  [100/1724] loss: 0.529, ave_loss: 0.630
[7]  [120/1724] loss: 0.771, ave_loss: 0.650
[8]  [140/1724] loss: 0.650, ave_loss: 0.650
[9]  [160/1724] loss: 0.680, ave_loss: 0.653
[10]  [180/1724] loss: 0.784, ave_loss: 0.666
[11]  [200/1724] loss: 0.639, ave_loss: 0.664
[12]  [220/1724] loss: 0.591, ave_loss: 0.658
[13]  [240/1724] loss: 0.700, ave_loss: 0.661
[14]  [260/1724] loss: 0.695, ave_loss: 0.663
[15]  [280/1724] loss: 0.534, ave_loss: 0.655
[16]  [300/1724] loss: 0.478, ave_loss: 0.644
[17]  [320/1724] loss: 0.594, ave_loss: 0.641
[18]  [340/1724] loss: 0.642, ave_loss: 0.641
[19]  [360/1724] loss: 0.633, ave_loss: 0.640
[20]  [380/1724] loss: 0.863, ave_loss: 0.652
[21]  [400/1724] loss: 0.604, ave_loss: 0.649
[22]  [420/1724] loss: 0.560, ave_loss: 0.645
[23]  [440/1724] loss: 0.734, ave_loss: 0.649
[24]  [460/1724] loss: 0.717, ave_loss: 0.652
[25]  [480/1724] loss: 0.674, ave_loss: 0.653
[26]  [500/1724] loss: 0.779, ave_loss: 0.658
[27]  [520/1724] loss: 0.654, ave_loss: 0.658
[28]  [540/1724] loss: 0.723, ave_loss: 0.660
[29]  [560/1724] loss: 0.782, ave_loss: 0.664
[30]  [580/1724] loss: 0.684, ave_loss: 0.665
[31]  [600/1724] loss: 0.580, ave_loss: 0.662
[32]  [620/1724] loss: 0.494, ave_loss: 0.657
[33]  [640/1724] loss: 0.581, ave_loss: 0.654
[34]  [660/1724] loss: 0.457, ave_loss: 0.649
[35]  [680/1724] loss: 0.733, ave_loss: 0.651
[36]  [700/1724] loss: 0.666, ave_loss: 0.651
[37]  [720/1724] loss: 0.680, ave_loss: 0.652
[38]  [740/1724] loss: 0.638, ave_loss: 0.652
[39]  [760/1724] loss: 0.641, ave_loss: 0.652
[40]  [780/1724] loss: 0.720, ave_loss: 0.653
[41]  [800/1724] loss: 0.577, ave_loss: 0.651
[42]  [820/1724] loss: 0.680, ave_loss: 0.652
[43]  [840/1724] loss: 0.616, ave_loss: 0.651
[44]  [860/1724] loss: 0.677, ave_loss: 0.652
[45]  [880/1724] loss: 0.623, ave_loss: 0.651
[46]  [900/1724] loss: 0.617, ave_loss: 0.651
[47]  [920/1724] loss: 0.560, ave_loss: 0.649
[48]  [940/1724] loss: 0.623, ave_loss: 0.648
[49]  [960/1724] loss: 0.548, ave_loss: 0.646
[50]  [980/1724] loss: 0.525, ave_loss: 0.644
[51]  [1000/1724] loss: 0.789, ave_loss: 0.646
[52]  [1020/1724] loss: 0.630, ave_loss: 0.646
[53]  [1040/1724] loss: 0.676, ave_loss: 0.647
[54]  [1060/1724] loss: 0.637, ave_loss: 0.647
[55]  [1080/1724] loss: 0.572, ave_loss: 0.645
[56]  [1100/1724] loss: 0.629, ave_loss: 0.645
[57]  [1120/1724] loss: 0.724, ave_loss: 0.646
[58]  [1140/1724] loss: 0.558, ave_loss: 0.645
[59]  [1160/1724] loss: 0.587, ave_loss: 0.644
[60]  [1180/1724] loss: 0.637, ave_loss: 0.644
[61]  [1200/1724] loss: 0.639, ave_loss: 0.644
[62]  [1220/1724] loss: 0.608, ave_loss: 0.643
[63]  [1240/1724] loss: 0.748, ave_loss: 0.645
[64]  [1260/1724] loss: 0.673, ave_loss: 0.645
[65]  [1280/1724] loss: 0.588, ave_loss: 0.644
[66]  [1300/1724] loss: 0.535, ave_loss: 0.643
[67]  [1320/1724] loss: 0.611, ave_loss: 0.642
[68]  [1340/1724] loss: 0.571, ave_loss: 0.641
[69]  [1360/1724] loss: 0.682, ave_loss: 0.642
[70]  [1380/1724] loss: 0.617, ave_loss: 0.641
[71]  [1400/1724] loss: 0.669, ave_loss: 0.642
[72]  [1420/1724] loss: 0.663, ave_loss: 0.642
[73]  [1440/1724] loss: 0.770, ave_loss: 0.644
[74]  [1460/1724] loss: 0.650, ave_loss: 0.644
[75]  [1480/1724] loss: 0.558, ave_loss: 0.643
[76]  [1500/1724] loss: 0.611, ave_loss: 0.642
[77]  [1520/1724] loss: 0.637, ave_loss: 0.642
[78]  [1540/1724] loss: 0.682, ave_loss: 0.643
[79]  [1560/1724] loss: 0.696, ave_loss: 0.643
[80]  [1580/1724] loss: 0.707, ave_loss: 0.644
[81]  [1600/1724] loss: 0.609, ave_loss: 0.644
[82]  [1620/1724] loss: 0.706, ave_loss: 0.645
[83]  [1640/1724] loss: 0.592, ave_loss: 0.644
[84]  [1660/1724] loss: 0.547, ave_loss: 0.643
[85]  [1680/1724] loss: 0.634, ave_loss: 0.643
[86]  [1700/1724] loss: 0.528, ave_loss: 0.641
[87]  [1720/1724] loss: 0.722, ave_loss: 0.642
[88]  [1740/1724] loss: 0.505, ave_loss: 0.641

Finished Training finishing at 2021-08-25 15:41:32.983126
printing_out epoch  16.334106728538284 learning rate: 0.00029090509716114235
0.0002821779442463081
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.407e-01
Validation Loss: 5.684e-01
Validation ROC: 0.4433
No improvement, still saving model
12.665893271461716 epochs left to go

Training Epoch 16.334106728538284/30 starting at 2021-08-25 15:41:58.190791
[1]  [0/1724] loss: 0.671, ave_loss: 0.671
[2]  [20/1724] loss: 0.694, ave_loss: 0.683
[3]  [40/1724] loss: 0.711, ave_loss: 0.692
[4]  [60/1724] loss: 0.674, ave_loss: 0.688
[5]  [80/1724] loss: 0.674, ave_loss: 0.685
[6]  [100/1724] loss: 0.666, ave_loss: 0.682
[7]  [120/1724] loss: 0.583, ave_loss: 0.668
[8]  [140/1724] loss: 0.734, ave_loss: 0.676
[9]  [160/1724] loss: 0.632, ave_loss: 0.671
[10]  [180/1724] loss: 0.671, ave_loss: 0.671
[11]  [200/1724] loss: 0.666, ave_loss: 0.671
[12]  [220/1724] loss: 0.709, ave_loss: 0.674
[13]  [240/1724] loss: 0.629, ave_loss: 0.670
[14]  [260/1724] loss: 0.593, ave_loss: 0.665
[15]  [280/1724] loss: 0.684, ave_loss: 0.666
[16]  [300/1724] loss: 0.718, ave_loss: 0.669
[17]  [320/1724] loss: 0.658, ave_loss: 0.669
[18]  [340/1724] loss: 0.624, ave_loss: 0.666
[19]  [360/1724] loss: 0.584, ave_loss: 0.662
[20]  [380/1724] loss: 0.775, ave_loss: 0.668
[21]  [400/1724] loss: 0.680, ave_loss: 0.668
[22]  [420/1724] loss: 0.629, ave_loss: 0.666
[23]  [440/1724] loss: 0.749, ave_loss: 0.670
[24]  [460/1724] loss: 0.583, ave_loss: 0.666
[25]  [480/1724] loss: 0.733, ave_loss: 0.669
[26]  [500/1724] loss: 0.648, ave_loss: 0.668
[27]  [520/1724] loss: 0.757, ave_loss: 0.671
[28]  [540/1724] loss: 0.674, ave_loss: 0.672
[29]  [560/1724] loss: 0.839, ave_loss: 0.677
[30]  [580/1724] loss: 0.647, ave_loss: 0.676
[31]  [600/1724] loss: 0.524, ave_loss: 0.671
[32]  [620/1724] loss: 0.511, ave_loss: 0.666
[33]  [640/1724] loss: 0.658, ave_loss: 0.666
[34]  [660/1724] loss: 0.663, ave_loss: 0.666
[35]  [680/1724] loss: 0.685, ave_loss: 0.667
[36]  [700/1724] loss: 0.606, ave_loss: 0.665
[37]  [720/1724] loss: 0.658, ave_loss: 0.665
[38]  [740/1724] loss: 0.714, ave_loss: 0.666
[39]  [760/1724] loss: 0.663, ave_loss: 0.666
[40]  [780/1724] loss: 0.710, ave_loss: 0.667
[41]  [800/1724] loss: 0.616, ave_loss: 0.666
[42]  [820/1724] loss: 0.795, ave_loss: 0.669
[43]  [840/1724] loss: 0.610, ave_loss: 0.668
[44]  [860/1724] loss: 0.764, ave_loss: 0.670
[45]  [880/1724] loss: 0.681, ave_loss: 0.670
[46]  [900/1724] loss: 0.610, ave_loss: 0.669
[47]  [920/1724] loss: 0.615, ave_loss: 0.668
[48]  [940/1724] loss: 0.573, ave_loss: 0.666
[49]  [960/1724] loss: 0.648, ave_loss: 0.665
[50]  [980/1724] loss: 0.669, ave_loss: 0.665
[51]  [1000/1724] loss: 0.607, ave_loss: 0.664
[52]  [1020/1724] loss: 0.535, ave_loss: 0.662
[53]  [1040/1724] loss: 0.640, ave_loss: 0.661
[54]  [1060/1724] loss: 0.572, ave_loss: 0.660
[55]  [1080/1724] loss: 0.617, ave_loss: 0.659
[56]  [1100/1724] loss: 0.571, ave_loss: 0.657
[57]  [1120/1724] loss: 0.714, ave_loss: 0.658
[58]  [1140/1724] loss: 0.609, ave_loss: 0.657
[59]  [1160/1724] loss: 0.613, ave_loss: 0.657
[60]  [1180/1724] loss: 0.677, ave_loss: 0.657
[61]  [1200/1724] loss: 0.595, ave_loss: 0.656
[62]  [1220/1724] loss: 0.611, ave_loss: 0.655
[63]  [1240/1724] loss: 0.542, ave_loss: 0.653
[64]  [1260/1724] loss: 0.659, ave_loss: 0.654
[65]  [1280/1724] loss: 0.780, ave_loss: 0.655
[66]  [1300/1724] loss: 0.645, ave_loss: 0.655
[67]  [1320/1724] loss: 0.765, ave_loss: 0.657
[68]  [1340/1724] loss: 0.836, ave_loss: 0.660
[69]  [1360/1724] loss: 0.694, ave_loss: 0.660
[70]  [1380/1724] loss: 0.717, ave_loss: 0.661
[71]  [1400/1724] loss: 0.659, ave_loss: 0.661
[72]  [1420/1724] loss: 0.623, ave_loss: 0.660
[73]  [1440/1724] loss: 0.487, ave_loss: 0.658
[74]  [1460/1724] loss: 0.688, ave_loss: 0.658
[75]  [1480/1724] loss: 0.621, ave_loss: 0.658
[76]  [1500/1724] loss: 0.688, ave_loss: 0.658
[77]  [1520/1724] loss: 0.631, ave_loss: 0.658
[78]  [1540/1724] loss: 0.622, ave_loss: 0.657
[79]  [1560/1724] loss: 0.651, ave_loss: 0.657
[80]  [1580/1724] loss: 0.630, ave_loss: 0.657
[81]  [1600/1724] loss: 0.722, ave_loss: 0.658
[82]  [1620/1724] loss: 0.570, ave_loss: 0.657
[83]  [1640/1724] loss: 0.687, ave_loss: 0.657
[84]  [1660/1724] loss: 0.692, ave_loss: 0.658
[85]  [1680/1724] loss: 0.721, ave_loss: 0.658
[86]  [1700/1724] loss: 0.685, ave_loss: 0.659
[87]  [1720/1724] loss: 0.575, ave_loss: 0.658
[88]  [1740/1724] loss: 0.552, ave_loss: 0.656

Finished Training finishing at 2021-08-25 15:43:25.255564
printing_out epoch  17.354988399071924 learning rate: 0.00026445917923740214
0.0002565254038602801
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.565e-01
Validation Loss: 5.762e-01
Validation ROC: 0.4433
No improvement, still saving model
11.645011600928076 epochs left to go

Training Epoch 17.354988399071924/30 starting at 2021-08-25 15:43:57.903516
[1]  [0/1724] loss: 0.612, ave_loss: 0.612
[2]  [20/1724] loss: 0.684, ave_loss: 0.648
[3]  [40/1724] loss: 0.790, ave_loss: 0.695
[4]  [60/1724] loss: 0.573, ave_loss: 0.665
[5]  [80/1724] loss: 0.553, ave_loss: 0.642
[6]  [100/1724] loss: 0.564, ave_loss: 0.629
[7]  [120/1724] loss: 0.769, ave_loss: 0.649
[8]  [140/1724] loss: 0.614, ave_loss: 0.645
[9]  [160/1724] loss: 0.758, ave_loss: 0.657
[10]  [180/1724] loss: 0.610, ave_loss: 0.653
[11]  [200/1724] loss: 0.562, ave_loss: 0.644
[12]  [220/1724] loss: 0.787, ave_loss: 0.656
[13]  [240/1724] loss: 0.739, ave_loss: 0.663
[14]  [260/1724] loss: 0.616, ave_loss: 0.659
[15]  [280/1724] loss: 0.516, ave_loss: 0.650
[16]  [300/1724] loss: 0.554, ave_loss: 0.644
[17]  [320/1724] loss: 0.748, ave_loss: 0.650
[18]  [340/1724] loss: 0.556, ave_loss: 0.645
[19]  [360/1724] loss: 0.585, ave_loss: 0.642
[20]  [380/1724] loss: 0.704, ave_loss: 0.645
[21]  [400/1724] loss: 0.675, ave_loss: 0.646
[22]  [420/1724] loss: 0.560, ave_loss: 0.642
[23]  [440/1724] loss: 0.558, ave_loss: 0.639
[24]  [460/1724] loss: 0.655, ave_loss: 0.639
[25]  [480/1724] loss: 0.739, ave_loss: 0.643
[26]  [500/1724] loss: 0.663, ave_loss: 0.644
[27]  [520/1724] loss: 0.679, ave_loss: 0.645
[28]  [540/1724] loss: 0.703, ave_loss: 0.647
[29]  [560/1724] loss: 0.611, ave_loss: 0.646
[30]  [580/1724] loss: 0.781, ave_loss: 0.651
[31]  [600/1724] loss: 0.666, ave_loss: 0.651
[32]  [620/1724] loss: 0.654, ave_loss: 0.651
[33]  [640/1724] loss: 0.714, ave_loss: 0.653
[34]  [660/1724] loss: 0.695, ave_loss: 0.654
[35]  [680/1724] loss: 0.586, ave_loss: 0.652
[36]  [700/1724] loss: 0.529, ave_loss: 0.649
[37]  [720/1724] loss: 0.624, ave_loss: 0.648
[38]  [740/1724] loss: 0.569, ave_loss: 0.646
[39]  [760/1724] loss: 0.737, ave_loss: 0.649
[40]  [780/1724] loss: 0.646, ave_loss: 0.648
[41]  [800/1724] loss: 0.691, ave_loss: 0.650
[42]  [820/1724] loss: 0.764, ave_loss: 0.652
[43]  [840/1724] loss: 0.575, ave_loss: 0.650
[44]  [860/1724] loss: 0.635, ave_loss: 0.650
[45]  [880/1724] loss: 0.661, ave_loss: 0.650
[46]  [900/1724] loss: 0.697, ave_loss: 0.651
[47]  [920/1724] loss: 0.597, ave_loss: 0.650
[48]  [940/1724] loss: 0.560, ave_loss: 0.648
[49]  [960/1724] loss: 0.675, ave_loss: 0.649
[50]  [980/1724] loss: 0.625, ave_loss: 0.648
[51]  [1000/1724] loss: 0.679, ave_loss: 0.649
[52]  [1020/1724] loss: 0.589, ave_loss: 0.648
[53]  [1040/1724] loss: 0.754, ave_loss: 0.650
[54]  [1060/1724] loss: 0.737, ave_loss: 0.651
[55]  [1080/1724] loss: 0.771, ave_loss: 0.654
[56]  [1100/1724] loss: 0.650, ave_loss: 0.654
[57]  [1120/1724] loss: 0.714, ave_loss: 0.655
[58]  [1140/1724] loss: 0.666, ave_loss: 0.655
[59]  [1160/1724] loss: 0.628, ave_loss: 0.654
[60]  [1180/1724] loss: 0.653, ave_loss: 0.654
[61]  [1200/1724] loss: 0.664, ave_loss: 0.654
[62]  [1220/1724] loss: 0.699, ave_loss: 0.655
[63]  [1240/1724] loss: 0.716, ave_loss: 0.656
[64]  [1260/1724] loss: 0.591, ave_loss: 0.655
[65]  [1280/1724] loss: 0.680, ave_loss: 0.656
[66]  [1300/1724] loss: 0.666, ave_loss: 0.656
[67]  [1320/1724] loss: 0.659, ave_loss: 0.656
[68]  [1340/1724] loss: 0.550, ave_loss: 0.654
[69]  [1360/1724] loss: 0.588, ave_loss: 0.653
[70]  [1380/1724] loss: 0.610, ave_loss: 0.653
[71]  [1400/1724] loss: 0.592, ave_loss: 0.652
[72]  [1420/1724] loss: 0.761, ave_loss: 0.653
[73]  [1440/1724] loss: 0.578, ave_loss: 0.652
[74]  [1460/1724] loss: 0.562, ave_loss: 0.651
[75]  [1480/1724] loss: 0.616, ave_loss: 0.651
[76]  [1500/1724] loss: 0.599, ave_loss: 0.650
[77]  [1520/1724] loss: 0.451, ave_loss: 0.647
[78]  [1540/1724] loss: 0.579, ave_loss: 0.646
[79]  [1560/1724] loss: 0.685, ave_loss: 0.647
[80]  [1580/1724] loss: 0.597, ave_loss: 0.646
[81]  [1600/1724] loss: 0.469, ave_loss: 0.644
[82]  [1620/1724] loss: 0.761, ave_loss: 0.646
[83]  [1640/1724] loss: 0.615, ave_loss: 0.645
[84]  [1660/1724] loss: 0.601, ave_loss: 0.645
[85]  [1680/1724] loss: 0.574, ave_loss: 0.644
[86]  [1700/1724] loss: 0.626, ave_loss: 0.644
[87]  [1720/1724] loss: 0.569, ave_loss: 0.643
[88]  [1740/1724] loss: 0.554, ave_loss: 0.642

Finished Training finishing at 2021-08-25 15:45:29.584570
printing_out epoch  18.37587006960557 learning rate: 0.00024041743567036557
0.0002332049126002546
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.417e-01
Validation Loss: 5.746e-01
Validation ROC: 0.4433
No improvement, still saving model
10.624129930394432 epochs left to go

Training Epoch 18.37587006960557/30 starting at 2021-08-25 15:46:00.859523
[1]  [0/1724] loss: 0.566, ave_loss: 0.566
[2]  [20/1724] loss: 0.721, ave_loss: 0.643
[3]  [40/1724] loss: 0.607, ave_loss: 0.631
[4]  [60/1724] loss: 0.578, ave_loss: 0.618
[5]  [80/1724] loss: 0.691, ave_loss: 0.633
[6]  [100/1724] loss: 0.636, ave_loss: 0.633
[7]  [120/1724] loss: 0.593, ave_loss: 0.627
[8]  [140/1724] loss: 0.600, ave_loss: 0.624
[9]  [160/1724] loss: 0.596, ave_loss: 0.621
[10]  [180/1724] loss: 0.658, ave_loss: 0.625
[11]  [200/1724] loss: 0.580, ave_loss: 0.621
[12]  [220/1724] loss: 0.621, ave_loss: 0.621
[13]  [240/1724] loss: 0.695, ave_loss: 0.626
[14]  [260/1724] loss: 0.637, ave_loss: 0.627
[15]  [280/1724] loss: 0.771, ave_loss: 0.637
[16]  [300/1724] loss: 0.598, ave_loss: 0.634
[17]  [320/1724] loss: 0.754, ave_loss: 0.641
[18]  [340/1724] loss: 0.740, ave_loss: 0.647
[19]  [360/1724] loss: 0.708, ave_loss: 0.650
[20]  [380/1724] loss: 0.751, ave_loss: 0.655
[21]  [400/1724] loss: 0.820, ave_loss: 0.663
[22]  [420/1724] loss: 0.664, ave_loss: 0.663
[23]  [440/1724] loss: 0.631, ave_loss: 0.662
[24]  [460/1724] loss: 0.679, ave_loss: 0.662
[25]  [480/1724] loss: 0.598, ave_loss: 0.660
[26]  [500/1724] loss: 0.592, ave_loss: 0.657
[27]  [520/1724] loss: 0.730, ave_loss: 0.660
[28]  [540/1724] loss: 0.652, ave_loss: 0.660
[29]  [560/1724] loss: 0.672, ave_loss: 0.660
[30]  [580/1724] loss: 0.665, ave_loss: 0.660
[31]  [600/1724] loss: 0.470, ave_loss: 0.654
[32]  [620/1724] loss: 0.745, ave_loss: 0.657
[33]  [640/1724] loss: 0.736, ave_loss: 0.659
[34]  [660/1724] loss: 0.615, ave_loss: 0.658
[35]  [680/1724] loss: 0.684, ave_loss: 0.659
[36]  [700/1724] loss: 0.694, ave_loss: 0.660
[37]  [720/1724] loss: 0.612, ave_loss: 0.658
[38]  [740/1724] loss: 0.621, ave_loss: 0.657
[39]  [760/1724] loss: 0.567, ave_loss: 0.655
[40]  [780/1724] loss: 0.630, ave_loss: 0.654
[41]  [800/1724] loss: 0.703, ave_loss: 0.656
[42]  [820/1724] loss: 0.651, ave_loss: 0.656
[43]  [840/1724] loss: 0.563, ave_loss: 0.653
[44]  [860/1724] loss: 0.600, ave_loss: 0.652
[45]  [880/1724] loss: 0.697, ave_loss: 0.653
[46]  [900/1724] loss: 0.669, ave_loss: 0.654
[47]  [920/1724] loss: 0.619, ave_loss: 0.653
[48]  [940/1724] loss: 0.561, ave_loss: 0.651
[49]  [960/1724] loss: 0.673, ave_loss: 0.651
[50]  [980/1724] loss: 0.569, ave_loss: 0.650
[51]  [1000/1724] loss: 0.611, ave_loss: 0.649
[52]  [1020/1724] loss: 0.584, ave_loss: 0.648
[53]  [1040/1724] loss: 0.760, ave_loss: 0.650
[54]  [1060/1724] loss: 0.534, ave_loss: 0.648
[55]  [1080/1724] loss: 0.554, ave_loss: 0.646
[56]  [1100/1724] loss: 0.534, ave_loss: 0.644
[57]  [1120/1724] loss: 0.559, ave_loss: 0.642
[58]  [1140/1724] loss: 0.624, ave_loss: 0.642
[59]  [1160/1724] loss: 0.724, ave_loss: 0.644
[60]  [1180/1724] loss: 0.584, ave_loss: 0.643
[61]  [1200/1724] loss: 0.706, ave_loss: 0.644
[62]  [1220/1724] loss: 0.634, ave_loss: 0.643
[63]  [1240/1724] loss: 0.550, ave_loss: 0.642
[64]  [1260/1724] loss: 0.643, ave_loss: 0.642
[65]  [1280/1724] loss: 0.649, ave_loss: 0.642
[66]  [1300/1724] loss: 0.736, ave_loss: 0.643
[67]  [1320/1724] loss: 0.740, ave_loss: 0.645
[68]  [1340/1724] loss: 0.651, ave_loss: 0.645
[69]  [1360/1724] loss: 0.662, ave_loss: 0.645
[70]  [1380/1724] loss: 0.569, ave_loss: 0.644
[71]  [1400/1724] loss: 0.628, ave_loss: 0.644
[72]  [1420/1724] loss: 0.676, ave_loss: 0.644
[73]  [1440/1724] loss: 0.731, ave_loss: 0.646
[74]  [1460/1724] loss: 0.566, ave_loss: 0.644
[75]  [1480/1724] loss: 0.605, ave_loss: 0.644
[76]  [1500/1724] loss: 0.625, ave_loss: 0.644
[77]  [1520/1724] loss: 0.540, ave_loss: 0.642
[78]  [1540/1724] loss: 0.542, ave_loss: 0.641
[79]  [1560/1724] loss: 0.529, ave_loss: 0.640
[80]  [1580/1724] loss: 0.618, ave_loss: 0.639
[81]  [1600/1724] loss: 0.678, ave_loss: 0.640
[82]  [1620/1724] loss: 0.643, ave_loss: 0.640
[83]  [1640/1724] loss: 0.619, ave_loss: 0.640
[84]  [1660/1724] loss: 0.715, ave_loss: 0.641
[85]  [1680/1724] loss: 0.595, ave_loss: 0.640
[86]  [1700/1724] loss: 0.632, ave_loss: 0.640
[87]  [1720/1724] loss: 0.620, ave_loss: 0.640
[88]  [1740/1724] loss: 0.645, ave_loss: 0.640

Finished Training finishing at 2021-08-25 15:47:29.972532
printing_out epoch  19.396751740139212 learning rate: 0.00021856130515487778
0.00021200446600023144
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.398e-01
Validation Loss: 5.691e-01
Validation ROC: 0.4433
No improvement, still saving model
9.603248259860788 epochs left to go

Training Epoch 19.396751740139212/30 starting at 2021-08-25 15:48:01.156836
[1]  [0/1724] loss: 0.525, ave_loss: 0.525
[2]  [20/1724] loss: 0.583, ave_loss: 0.554
[3]  [40/1724] loss: 0.629, ave_loss: 0.579
[4]  [60/1724] loss: 0.660, ave_loss: 0.599
[5]  [80/1724] loss: 0.618, ave_loss: 0.603
[6]  [100/1724] loss: 0.504, ave_loss: 0.586
[7]  [120/1724] loss: 0.605, ave_loss: 0.589
[8]  [140/1724] loss: 0.669, ave_loss: 0.599
[9]  [160/1724] loss: 0.732, ave_loss: 0.614
[10]  [180/1724] loss: 0.634, ave_loss: 0.616
[11]  [200/1724] loss: 0.697, ave_loss: 0.623
[12]  [220/1724] loss: 0.624, ave_loss: 0.623
[13]  [240/1724] loss: 0.692, ave_loss: 0.629
[14]  [260/1724] loss: 0.676, ave_loss: 0.632
[15]  [280/1724] loss: 0.531, ave_loss: 0.625
[16]  [300/1724] loss: 0.665, ave_loss: 0.628
[17]  [320/1724] loss: 0.494, ave_loss: 0.620
[18]  [340/1724] loss: 0.646, ave_loss: 0.621
[19]  [360/1724] loss: 0.515, ave_loss: 0.616
[20]  [380/1724] loss: 0.584, ave_loss: 0.614
[21]  [400/1724] loss: 0.508, ave_loss: 0.609
[22]  [420/1724] loss: 0.530, ave_loss: 0.605
[23]  [440/1724] loss: 0.620, ave_loss: 0.606
[24]  [460/1724] loss: 0.653, ave_loss: 0.608
[25]  [480/1724] loss: 0.705, ave_loss: 0.612
[26]  [500/1724] loss: 0.671, ave_loss: 0.614
[27]  [520/1724] loss: 0.645, ave_loss: 0.615
[28]  [540/1724] loss: 0.641, ave_loss: 0.616
[29]  [560/1724] loss: 0.599, ave_loss: 0.616
[30]  [580/1724] loss: 0.618, ave_loss: 0.616
[31]  [600/1724] loss: 0.746, ave_loss: 0.620
[32]  [620/1724] loss: 0.572, ave_loss: 0.618
[33]  [640/1724] loss: 0.687, ave_loss: 0.620
[34]  [660/1724] loss: 0.708, ave_loss: 0.623
[35]  [680/1724] loss: 0.582, ave_loss: 0.622
[36]  [700/1724] loss: 0.649, ave_loss: 0.623
[37]  [720/1724] loss: 0.689, ave_loss: 0.624
[38]  [740/1724] loss: 0.568, ave_loss: 0.623
[39]  [760/1724] loss: 0.605, ave_loss: 0.622
[40]  [780/1724] loss: 0.644, ave_loss: 0.623
[41]  [800/1724] loss: 0.573, ave_loss: 0.622
[42]  [820/1724] loss: 0.637, ave_loss: 0.622
[43]  [840/1724] loss: 0.563, ave_loss: 0.621
[44]  [860/1724] loss: 0.732, ave_loss: 0.623
[45]  [880/1724] loss: 0.631, ave_loss: 0.623
[46]  [900/1724] loss: 0.643, ave_loss: 0.624
[47]  [920/1724] loss: 0.559, ave_loss: 0.623
[48]  [940/1724] loss: 0.651, ave_loss: 0.623
[49]  [960/1724] loss: 0.576, ave_loss: 0.622
[50]  [980/1724] loss: 0.627, ave_loss: 0.622
[51]  [1000/1724] loss: 0.663, ave_loss: 0.623
[52]  [1020/1724] loss: 0.686, ave_loss: 0.624
[53]  [1040/1724] loss: 0.640, ave_loss: 0.625
[54]  [1060/1724] loss: 0.574, ave_loss: 0.624
[55]  [1080/1724] loss: 0.715, ave_loss: 0.625
[56]  [1100/1724] loss: 0.656, ave_loss: 0.626
[57]  [1120/1724] loss: 0.585, ave_loss: 0.625
[58]  [1140/1724] loss: 0.708, ave_loss: 0.627
[59]  [1160/1724] loss: 0.408, ave_loss: 0.623
[60]  [1180/1724] loss: 0.786, ave_loss: 0.626
[61]  [1200/1724] loss: 0.673, ave_loss: 0.626
[62]  [1220/1724] loss: 0.551, ave_loss: 0.625
[63]  [1240/1724] loss: 0.666, ave_loss: 0.626
[64]  [1260/1724] loss: 0.688, ave_loss: 0.627
[65]  [1280/1724] loss: 0.594, ave_loss: 0.626
[66]  [1300/1724] loss: 0.686, ave_loss: 0.627
[67]  [1320/1724] loss: 0.538, ave_loss: 0.626
[68]  [1340/1724] loss: 0.797, ave_loss: 0.628
[69]  [1360/1724] loss: 0.550, ave_loss: 0.627
[70]  [1380/1724] loss: 0.549, ave_loss: 0.626
[71]  [1400/1724] loss: 0.716, ave_loss: 0.627
[72]  [1420/1724] loss: 0.556, ave_loss: 0.626
[73]  [1440/1724] loss: 0.499, ave_loss: 0.625
[74]  [1460/1724] loss: 0.619, ave_loss: 0.625
[75]  [1480/1724] loss: 0.653, ave_loss: 0.625
[76]  [1500/1724] loss: 0.556, ave_loss: 0.624
[77]  [1520/1724] loss: 0.747, ave_loss: 0.626
[78]  [1540/1724] loss: 0.810, ave_loss: 0.628
[79]  [1560/1724] loss: 0.702, ave_loss: 0.629
[80]  [1580/1724] loss: 0.712, ave_loss: 0.630
[81]  [1600/1724] loss: 0.592, ave_loss: 0.629
[82]  [1620/1724] loss: 0.717, ave_loss: 0.631
[83]  [1640/1724] loss: 0.708, ave_loss: 0.631
[84]  [1660/1724] loss: 0.642, ave_loss: 0.632
[85]  [1680/1724] loss: 0.562, ave_loss: 0.631
[86]  [1700/1724] loss: 0.629, ave_loss: 0.631
[87]  [1720/1724] loss: 0.782, ave_loss: 0.632
[88]  [1740/1724] loss: 0.583, ave_loss: 0.632

Finished Training finishing at 2021-08-25 15:49:39.880575
printing_out epoch  20.417633410672853 learning rate: 0.00019869209559534342
0.00019273133272748312
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.319e-01
Validation Loss: 5.639e-01
Validation ROC: 0.4433
No improvement, still saving model
8.582366589327147 epochs left to go

Training Epoch 20.417633410672853/30 starting at 2021-08-25 15:50:11.254408
[1]  [0/1724] loss: 0.656, ave_loss: 0.656
[2]  [20/1724] loss: 0.637, ave_loss: 0.646
[3]  [40/1724] loss: 0.749, ave_loss: 0.681
[4]  [60/1724] loss: 0.730, ave_loss: 0.693
[5]  [80/1724] loss: 0.525, ave_loss: 0.659
[6]  [100/1724] loss: 0.574, ave_loss: 0.645
[7]  [120/1724] loss: 0.749, ave_loss: 0.660
[8]  [140/1724] loss: 0.485, ave_loss: 0.638
[9]  [160/1724] loss: 0.624, ave_loss: 0.637
[10]  [180/1724] loss: 0.634, ave_loss: 0.636
[11]  [200/1724] loss: 0.702, ave_loss: 0.642
[12]  [220/1724] loss: 0.800, ave_loss: 0.655
[13]  [240/1724] loss: 0.688, ave_loss: 0.658
[14]  [260/1724] loss: 0.622, ave_loss: 0.655
[15]  [280/1724] loss: 0.531, ave_loss: 0.647
[16]  [300/1724] loss: 0.636, ave_loss: 0.646
[17]  [320/1724] loss: 0.539, ave_loss: 0.640
[18]  [340/1724] loss: 0.595, ave_loss: 0.638
[19]  [360/1724] loss: 0.604, ave_loss: 0.636
[20]  [380/1724] loss: 0.660, ave_loss: 0.637
[21]  [400/1724] loss: 0.633, ave_loss: 0.637
[22]  [420/1724] loss: 0.668, ave_loss: 0.638
[23]  [440/1724] loss: 0.755, ave_loss: 0.643
[24]  [460/1724] loss: 0.583, ave_loss: 0.641
[25]  [480/1724] loss: 0.553, ave_loss: 0.637
[26]  [500/1724] loss: 0.730, ave_loss: 0.641
[27]  [520/1724] loss: 0.702, ave_loss: 0.643
[28]  [540/1724] loss: 0.677, ave_loss: 0.644
[29]  [560/1724] loss: 0.698, ave_loss: 0.646
[30]  [580/1724] loss: 0.532, ave_loss: 0.642
[31]  [600/1724] loss: 0.647, ave_loss: 0.642
[32]  [620/1724] loss: 0.664, ave_loss: 0.643
[33]  [640/1724] loss: 0.599, ave_loss: 0.642
[34]  [660/1724] loss: 0.645, ave_loss: 0.642
[35]  [680/1724] loss: 0.780, ave_loss: 0.646
[36]  [700/1724] loss: 0.605, ave_loss: 0.645
[37]  [720/1724] loss: 0.621, ave_loss: 0.644
[38]  [740/1724] loss: 0.556, ave_loss: 0.642
[39]  [760/1724] loss: 0.729, ave_loss: 0.644
[40]  [780/1724] loss: 0.601, ave_loss: 0.643
[41]  [800/1724] loss: 0.602, ave_loss: 0.642
[42]  [820/1724] loss: 0.593, ave_loss: 0.641
[43]  [840/1724] loss: 0.635, ave_loss: 0.641
[44]  [860/1724] loss: 0.621, ave_loss: 0.640
[45]  [880/1724] loss: 0.709, ave_loss: 0.642
[46]  [900/1724] loss: 0.805, ave_loss: 0.645
[47]  [920/1724] loss: 0.737, ave_loss: 0.647
[48]  [940/1724] loss: 0.694, ave_loss: 0.648
[49]  [960/1724] loss: 0.700, ave_loss: 0.649
[50]  [980/1724] loss: 0.790, ave_loss: 0.652
[51]  [1000/1724] loss: 0.676, ave_loss: 0.653
[52]  [1020/1724] loss: 0.824, ave_loss: 0.656
[53]  [1040/1724] loss: 0.553, ave_loss: 0.654
[54]  [1060/1724] loss: 0.720, ave_loss: 0.655
[55]  [1080/1724] loss: 0.716, ave_loss: 0.656
[56]  [1100/1724] loss: 0.662, ave_loss: 0.656
[57]  [1120/1724] loss: 0.575, ave_loss: 0.655
[58]  [1140/1724] loss: 0.709, ave_loss: 0.656
[59]  [1160/1724] loss: 0.618, ave_loss: 0.655
[60]  [1180/1724] loss: 0.710, ave_loss: 0.656
[61]  [1200/1724] loss: 0.608, ave_loss: 0.655
[62]  [1220/1724] loss: 0.568, ave_loss: 0.654
[63]  [1240/1724] loss: 0.657, ave_loss: 0.654
[64]  [1260/1724] loss: 0.795, ave_loss: 0.656
[65]  [1280/1724] loss: 0.675, ave_loss: 0.656
[66]  [1300/1724] loss: 0.788, ave_loss: 0.658
[67]  [1320/1724] loss: 0.717, ave_loss: 0.659
[68]  [1340/1724] loss: 0.545, ave_loss: 0.658
[69]  [1360/1724] loss: 0.643, ave_loss: 0.657
[70]  [1380/1724] loss: 0.609, ave_loss: 0.657
[71]  [1400/1724] loss: 0.586, ave_loss: 0.656
[72]  [1420/1724] loss: 0.601, ave_loss: 0.655
[73]  [1440/1724] loss: 0.597, ave_loss: 0.654
[74]  [1460/1724] loss: 0.708, ave_loss: 0.655
[75]  [1480/1724] loss: 0.579, ave_loss: 0.654
[76]  [1500/1724] loss: 0.690, ave_loss: 0.654
[77]  [1520/1724] loss: 0.613, ave_loss: 0.654
[78]  [1540/1724] loss: 0.700, ave_loss: 0.654
[79]  [1560/1724] loss: 0.684, ave_loss: 0.655
[80]  [1580/1724] loss: 0.665, ave_loss: 0.655
[81]  [1600/1724] loss: 0.703, ave_loss: 0.656
[82]  [1620/1724] loss: 0.576, ave_loss: 0.655
[83]  [1640/1724] loss: 0.762, ave_loss: 0.656
[84]  [1660/1724] loss: 0.639, ave_loss: 0.656
[85]  [1680/1724] loss: 0.601, ave_loss: 0.655
[86]  [1700/1724] loss: 0.578, ave_loss: 0.654
[87]  [1720/1724] loss: 0.622, ave_loss: 0.654
[88]  [1740/1724] loss: 0.759, ave_loss: 0.655

Finished Training finishing at 2021-08-25 15:52:15.113980
printing_out epoch  21.438515081206496 learning rate: 0.00018062917781394856
0.0001752103024795301
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.549e-01
Validation Loss: 5.763e-01
Validation ROC: 0.4085
No improvement, still saving model
7.561484918793504 epochs left to go

Training Epoch 21.438515081206496/30 starting at 2021-08-25 15:52:43.004977
[1]  [0/1724] loss: 0.678, ave_loss: 0.678
[2]  [20/1724] loss: 0.763, ave_loss: 0.720
[3]  [40/1724] loss: 0.619, ave_loss: 0.687
[4]  [60/1724] loss: 0.664, ave_loss: 0.681
[5]  [80/1724] loss: 0.603, ave_loss: 0.665
[6]  [100/1724] loss: 0.852, ave_loss: 0.697
[7]  [120/1724] loss: 0.609, ave_loss: 0.684
[8]  [140/1724] loss: 0.620, ave_loss: 0.676
[9]  [160/1724] loss: 0.628, ave_loss: 0.671
[10]  [180/1724] loss: 0.639, ave_loss: 0.668
[11]  [200/1724] loss: 0.805, ave_loss: 0.680
[12]  [220/1724] loss: 0.607, ave_loss: 0.674
[13]  [240/1724] loss: 0.534, ave_loss: 0.663
[14]  [260/1724] loss: 0.647, ave_loss: 0.662
[15]  [280/1724] loss: 0.540, ave_loss: 0.654
[16]  [300/1724] loss: 0.701, ave_loss: 0.657
[17]  [320/1724] loss: 0.618, ave_loss: 0.655
[18]  [340/1724] loss: 0.474, ave_loss: 0.645
[19]  [360/1724] loss: 0.569, ave_loss: 0.641
[20]  [380/1724] loss: 0.647, ave_loss: 0.641
[21]  [400/1724] loss: 0.532, ave_loss: 0.636
[22]  [420/1724] loss: 0.720, ave_loss: 0.640
[23]  [440/1724] loss: 0.625, ave_loss: 0.639
[24]  [460/1724] loss: 0.715, ave_loss: 0.642
[25]  [480/1724] loss: 0.647, ave_loss: 0.642
[26]  [500/1724] loss: 0.587, ave_loss: 0.640
[27]  [520/1724] loss: 0.670, ave_loss: 0.641
[28]  [540/1724] loss: 0.566, ave_loss: 0.639
[29]  [560/1724] loss: 0.689, ave_loss: 0.640
[30]  [580/1724] loss: 0.548, ave_loss: 0.637
[31]  [600/1724] loss: 0.626, ave_loss: 0.637
[32]  [620/1724] loss: 0.594, ave_loss: 0.636
[33]  [640/1724] loss: 0.788, ave_loss: 0.640
[34]  [660/1724] loss: 0.633, ave_loss: 0.640
[35]  [680/1724] loss: 0.775, ave_loss: 0.644
[36]  [700/1724] loss: 0.615, ave_loss: 0.643
[37]  [720/1724] loss: 0.547, ave_loss: 0.640
[38]  [740/1724] loss: 0.618, ave_loss: 0.640
[39]  [760/1724] loss: 0.622, ave_loss: 0.639
[40]  [780/1724] loss: 0.632, ave_loss: 0.639
[41]  [800/1724] loss: 0.760, ave_loss: 0.642
[42]  [820/1724] loss: 0.799, ave_loss: 0.646
[43]  [840/1724] loss: 0.578, ave_loss: 0.644
[44]  [860/1724] loss: 0.613, ave_loss: 0.644
[45]  [880/1724] loss: 0.676, ave_loss: 0.644
[46]  [900/1724] loss: 0.593, ave_loss: 0.643
[47]  [920/1724] loss: 0.496, ave_loss: 0.640
[48]  [940/1724] loss: 0.659, ave_loss: 0.640
[49]  [960/1724] loss: 0.685, ave_loss: 0.641
[50]  [980/1724] loss: 0.714, ave_loss: 0.643
[51]  [1000/1724] loss: 0.548, ave_loss: 0.641
[52]  [1020/1724] loss: 0.683, ave_loss: 0.642
[53]  [1040/1724] loss: 0.672, ave_loss: 0.642
[54]  [1060/1724] loss: 0.548, ave_loss: 0.641
[55]  [1080/1724] loss: 0.553, ave_loss: 0.639
[56]  [1100/1724] loss: 0.603, ave_loss: 0.638
[57]  [1120/1724] loss: 0.659, ave_loss: 0.639
[58]  [1140/1724] loss: 0.666, ave_loss: 0.639
[59]  [1160/1724] loss: 0.643, ave_loss: 0.639
[60]  [1180/1724] loss: 0.600, ave_loss: 0.639
[61]  [1200/1724] loss: 0.640, ave_loss: 0.639
[62]  [1220/1724] loss: 0.577, ave_loss: 0.638
[63]  [1240/1724] loss: 0.748, ave_loss: 0.639
[64]  [1260/1724] loss: 0.606, ave_loss: 0.639
[65]  [1280/1724] loss: 0.639, ave_loss: 0.639
[66]  [1300/1724] loss: 0.748, ave_loss: 0.641
[67]  [1320/1724] loss: 0.586, ave_loss: 0.640
[68]  [1340/1724] loss: 0.684, ave_loss: 0.640
[69]  [1360/1724] loss: 0.657, ave_loss: 0.641
[70]  [1380/1724] loss: 0.718, ave_loss: 0.642
[71]  [1400/1724] loss: 0.687, ave_loss: 0.642
[72]  [1420/1724] loss: 0.627, ave_loss: 0.642
[73]  [1440/1724] loss: 0.579, ave_loss: 0.641
[74]  [1460/1724] loss: 0.730, ave_loss: 0.642
[75]  [1480/1724] loss: 0.577, ave_loss: 0.642
[76]  [1500/1724] loss: 0.547, ave_loss: 0.640
[77]  [1520/1724] loss: 0.787, ave_loss: 0.642
[78]  [1540/1724] loss: 0.792, ave_loss: 0.644
[79]  [1560/1724] loss: 0.595, ave_loss: 0.644
[80]  [1580/1724] loss: 0.625, ave_loss: 0.643
[81]  [1600/1724] loss: 0.623, ave_loss: 0.643
[82]  [1620/1724] loss: 0.723, ave_loss: 0.644
[83]  [1640/1724] loss: 0.499, ave_loss: 0.642
[84]  [1660/1724] loss: 0.782, ave_loss: 0.644
[85]  [1680/1724] loss: 0.528, ave_loss: 0.643
[86]  [1700/1724] loss: 0.599, ave_loss: 0.642
[87]  [1720/1724] loss: 0.613, ave_loss: 0.642
[88]  [1740/1724] loss: 0.558, ave_loss: 0.641

Finished Training finishing at 2021-08-25 15:54:12.926896
printing_out epoch  22.45939675174014 learning rate: 0.00016420834346722596
0.00015928209316320917
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.408e-01
Validation Loss: 5.752e-01
Validation ROC: 0.4085
No improvement, still saving model
6.54060324825986 epochs left to go

Training Epoch 22.45939675174014/30 starting at 2021-08-25 15:54:45.125318
[1]  [0/1724] loss: 0.684, ave_loss: 0.684
[2]  [20/1724] loss: 0.688, ave_loss: 0.686
[3]  [40/1724] loss: 0.515, ave_loss: 0.629
[4]  [60/1724] loss: 0.554, ave_loss: 0.610
[5]  [80/1724] loss: 0.627, ave_loss: 0.614
[6]  [100/1724] loss: 0.582, ave_loss: 0.608
[7]  [120/1724] loss: 0.691, ave_loss: 0.620
[8]  [140/1724] loss: 0.769, ave_loss: 0.639
[9]  [160/1724] loss: 0.663, ave_loss: 0.641
[10]  [180/1724] loss: 0.524, ave_loss: 0.630
[11]  [200/1724] loss: 0.552, ave_loss: 0.623
[12]  [220/1724] loss: 0.700, ave_loss: 0.629
[13]  [240/1724] loss: 0.635, ave_loss: 0.630
[14]  [260/1724] loss: 0.720, ave_loss: 0.636
[15]  [280/1724] loss: 0.588, ave_loss: 0.633
[16]  [300/1724] loss: 0.765, ave_loss: 0.641
[17]  [320/1724] loss: 0.460, ave_loss: 0.630
[18]  [340/1724] loss: 0.694, ave_loss: 0.634
[19]  [360/1724] loss: 0.667, ave_loss: 0.636
[20]  [380/1724] loss: 0.735, ave_loss: 0.641
[21]  [400/1724] loss: 0.625, ave_loss: 0.640
[22]  [420/1724] loss: 0.793, ave_loss: 0.647
[23]  [440/1724] loss: 0.704, ave_loss: 0.649
[24]  [460/1724] loss: 0.547, ave_loss: 0.645
[25]  [480/1724] loss: 0.602, ave_loss: 0.643
[26]  [500/1724] loss: 0.682, ave_loss: 0.645
[27]  [520/1724] loss: 0.627, ave_loss: 0.644
[28]  [540/1724] loss: 0.603, ave_loss: 0.643
[29]  [560/1724] loss: 0.585, ave_loss: 0.641
[30]  [580/1724] loss: 0.663, ave_loss: 0.641
[31]  [600/1724] loss: 0.768, ave_loss: 0.645
[32]  [620/1724] loss: 0.644, ave_loss: 0.645
[33]  [640/1724] loss: 0.663, ave_loss: 0.646
[34]  [660/1724] loss: 0.593, ave_loss: 0.644
[35]  [680/1724] loss: 0.751, ave_loss: 0.647
[36]  [700/1724] loss: 0.519, ave_loss: 0.644
[37]  [720/1724] loss: 0.587, ave_loss: 0.642
[38]  [740/1724] loss: 0.698, ave_loss: 0.644
[39]  [760/1724] loss: 0.723, ave_loss: 0.646
[40]  [780/1724] loss: 0.732, ave_loss: 0.648
[41]  [800/1724] loss: 0.683, ave_loss: 0.649
[42]  [820/1724] loss: 0.723, ave_loss: 0.651
[43]  [840/1724] loss: 0.637, ave_loss: 0.650
[44]  [860/1724] loss: 0.645, ave_loss: 0.650
[45]  [880/1724] loss: 0.618, ave_loss: 0.649
[46]  [900/1724] loss: 0.622, ave_loss: 0.649
[47]  [920/1724] loss: 0.593, ave_loss: 0.648
[48]  [940/1724] loss: 0.631, ave_loss: 0.647
[49]  [960/1724] loss: 0.750, ave_loss: 0.649
[50]  [980/1724] loss: 0.656, ave_loss: 0.650
[51]  [1000/1724] loss: 0.415, ave_loss: 0.645
[52]  [1020/1724] loss: 0.703, ave_loss: 0.646
[53]  [1040/1724] loss: 0.562, ave_loss: 0.644
[54]  [1060/1724] loss: 0.670, ave_loss: 0.645
[55]  [1080/1724] loss: 0.550, ave_loss: 0.643
[56]  [1100/1724] loss: 0.725, ave_loss: 0.645
[57]  [1120/1724] loss: 0.608, ave_loss: 0.644
[58]  [1140/1724] loss: 0.686, ave_loss: 0.645
[59]  [1160/1724] loss: 0.530, ave_loss: 0.643
[60]  [1180/1724] loss: 0.625, ave_loss: 0.643
[61]  [1200/1724] loss: 0.600, ave_loss: 0.642
[62]  [1220/1724] loss: 0.537, ave_loss: 0.640
[63]  [1240/1724] loss: 0.632, ave_loss: 0.640
[64]  [1260/1724] loss: 0.488, ave_loss: 0.638
[65]  [1280/1724] loss: 0.704, ave_loss: 0.639
[66]  [1300/1724] loss: 0.560, ave_loss: 0.637
[67]  [1320/1724] loss: 0.717, ave_loss: 0.639
[68]  [1340/1724] loss: 0.556, ave_loss: 0.637
[69]  [1360/1724] loss: 0.498, ave_loss: 0.635
[70]  [1380/1724] loss: 0.529, ave_loss: 0.634
[71]  [1400/1724] loss: 0.645, ave_loss: 0.634
[72]  [1420/1724] loss: 0.682, ave_loss: 0.635
[73]  [1440/1724] loss: 0.596, ave_loss: 0.634
[74]  [1460/1724] loss: 0.528, ave_loss: 0.633
[75]  [1480/1724] loss: 0.711, ave_loss: 0.634
[76]  [1500/1724] loss: 0.548, ave_loss: 0.633
[77]  [1520/1724] loss: 0.659, ave_loss: 0.633
[78]  [1540/1724] loss: 0.686, ave_loss: 0.634
[79]  [1560/1724] loss: 0.599, ave_loss: 0.633
[80]  [1580/1724] loss: 0.698, ave_loss: 0.634
[81]  [1600/1724] loss: 0.454, ave_loss: 0.632
[82]  [1620/1724] loss: 0.673, ave_loss: 0.632
[83]  [1640/1724] loss: 0.717, ave_loss: 0.633
[84]  [1660/1724] loss: 0.750, ave_loss: 0.635
[85]  [1680/1724] loss: 0.540, ave_loss: 0.634
[86]  [1700/1724] loss: 0.563, ave_loss: 0.633
[87]  [1720/1724] loss: 0.687, ave_loss: 0.633
[88]  [1740/1724] loss: 0.509, ave_loss: 0.632

Finished Training finishing at 2021-08-25 15:56:21.100663
printing_out epoch  23.48027842227378 learning rate: 0.0001492803122429327
0.0001448019028756447
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.320e-01
Validation Loss: 5.665e-01
Validation ROC: 0.4085
No improvement, still saving model
5.519721577726219 epochs left to go

Training Epoch 23.48027842227378/30 starting at 2021-08-25 15:56:50.438326
[1]  [0/1724] loss: 0.699, ave_loss: 0.699
[2]  [20/1724] loss: 0.652, ave_loss: 0.675
[3]  [40/1724] loss: 0.672, ave_loss: 0.674
[4]  [60/1724] loss: 0.648, ave_loss: 0.668
[5]  [80/1724] loss: 0.694, ave_loss: 0.673
[6]  [100/1724] loss: 0.597, ave_loss: 0.660
[7]  [120/1724] loss: 0.517, ave_loss: 0.640
[8]  [140/1724] loss: 0.636, ave_loss: 0.639
[9]  [160/1724] loss: 0.660, ave_loss: 0.642
[10]  [180/1724] loss: 0.587, ave_loss: 0.636
[11]  [200/1724] loss: 0.673, ave_loss: 0.639
[12]  [220/1724] loss: 0.633, ave_loss: 0.639
[13]  [240/1724] loss: 0.623, ave_loss: 0.638
[14]  [260/1724] loss: 0.551, ave_loss: 0.631
[15]  [280/1724] loss: 0.698, ave_loss: 0.636
[16]  [300/1724] loss: 0.559, ave_loss: 0.631
[17]  [320/1724] loss: 0.596, ave_loss: 0.629
[18]  [340/1724] loss: 0.519, ave_loss: 0.623
[19]  [360/1724] loss: 0.673, ave_loss: 0.626
[20]  [380/1724] loss: 0.585, ave_loss: 0.624
[21]  [400/1724] loss: 0.592, ave_loss: 0.622
[22]  [420/1724] loss: 0.775, ave_loss: 0.629
[23]  [440/1724] loss: 0.636, ave_loss: 0.629
[24]  [460/1724] loss: 0.619, ave_loss: 0.629
[25]  [480/1724] loss: 0.653, ave_loss: 0.630
[26]  [500/1724] loss: 0.648, ave_loss: 0.631
[27]  [520/1724] loss: 0.579, ave_loss: 0.629
[28]  [540/1724] loss: 0.645, ave_loss: 0.629
[29]  [560/1724] loss: 0.808, ave_loss: 0.635
[30]  [580/1724] loss: 0.677, ave_loss: 0.637
[31]  [600/1724] loss: 0.703, ave_loss: 0.639
[32]  [620/1724] loss: 0.579, ave_loss: 0.637
[33]  [640/1724] loss: 0.542, ave_loss: 0.634
[34]  [660/1724] loss: 0.528, ave_loss: 0.631
[35]  [680/1724] loss: 0.705, ave_loss: 0.633
[36]  [700/1724] loss: 0.673, ave_loss: 0.634
[37]  [720/1724] loss: 0.527, ave_loss: 0.631
[38]  [740/1724] loss: 0.719, ave_loss: 0.634
[39]  [760/1724] loss: 0.635, ave_loss: 0.634
[40]  [780/1724] loss: 0.574, ave_loss: 0.632
[41]  [800/1724] loss: 0.576, ave_loss: 0.631
[42]  [820/1724] loss: 0.635, ave_loss: 0.631
[43]  [840/1724] loss: 0.542, ave_loss: 0.629
[44]  [860/1724] loss: 0.776, ave_loss: 0.632
[45]  [880/1724] loss: 0.684, ave_loss: 0.633
[46]  [900/1724] loss: 0.769, ave_loss: 0.636
[47]  [920/1724] loss: 0.505, ave_loss: 0.634
[48]  [940/1724] loss: 0.600, ave_loss: 0.633
[49]  [960/1724] loss: 0.683, ave_loss: 0.634
[50]  [980/1724] loss: 0.687, ave_loss: 0.635
[51]  [1000/1724] loss: 0.564, ave_loss: 0.634
[52]  [1020/1724] loss: 0.481, ave_loss: 0.631
[53]  [1040/1724] loss: 0.599, ave_loss: 0.630
[54]  [1060/1724] loss: 0.689, ave_loss: 0.631
[55]  [1080/1724] loss: 0.681, ave_loss: 0.632
[56]  [1100/1724] loss: 0.722, ave_loss: 0.634
[57]  [1120/1724] loss: 0.663, ave_loss: 0.634
[58]  [1140/1724] loss: 0.469, ave_loss: 0.631
[59]  [1160/1724] loss: 0.562, ave_loss: 0.630
[60]  [1180/1724] loss: 0.727, ave_loss: 0.632
[61]  [1200/1724] loss: 0.674, ave_loss: 0.632
[62]  [1220/1724] loss: 0.750, ave_loss: 0.634
[63]  [1240/1724] loss: 0.672, ave_loss: 0.635
[64]  [1260/1724] loss: 0.608, ave_loss: 0.634
[65]  [1280/1724] loss: 0.620, ave_loss: 0.634
[66]  [1300/1724] loss: 0.627, ave_loss: 0.634
[67]  [1320/1724] loss: 0.717, ave_loss: 0.635
[68]  [1340/1724] loss: 0.599, ave_loss: 0.635
[69]  [1360/1724] loss: 0.573, ave_loss: 0.634
[70]  [1380/1724] loss: 0.785, ave_loss: 0.636
[71]  [1400/1724] loss: 0.662, ave_loss: 0.636
[72]  [1420/1724] loss: 0.607, ave_loss: 0.636
[73]  [1440/1724] loss: 0.601, ave_loss: 0.636
[74]  [1460/1724] loss: 0.630, ave_loss: 0.636
[75]  [1480/1724] loss: 0.518, ave_loss: 0.634
[76]  [1500/1724] loss: 0.528, ave_loss: 0.633
[77]  [1520/1724] loss: 0.666, ave_loss: 0.633
[78]  [1540/1724] loss: 0.620, ave_loss: 0.633
[79]  [1560/1724] loss: 0.601, ave_loss: 0.632
[80]  [1580/1724] loss: 0.602, ave_loss: 0.632
[81]  [1600/1724] loss: 0.532, ave_loss: 0.631
[82]  [1620/1724] loss: 0.660, ave_loss: 0.631
[83]  [1640/1724] loss: 0.686, ave_loss: 0.632
[84]  [1660/1724] loss: 0.602, ave_loss: 0.631
[85]  [1680/1724] loss: 0.531, ave_loss: 0.630
[86]  [1700/1724] loss: 0.627, ave_loss: 0.630
[87]  [1720/1724] loss: 0.718, ave_loss: 0.631
[88]  [1740/1724] loss: 0.589, ave_loss: 0.631

Finished Training finishing at 2021-08-25 15:58:24.627536
printing_out epoch  24.501160092807424 learning rate: 0.00013570937476630243
0.00013163809352331336
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.308e-01
Validation Loss: 5.635e-01
Validation ROC: 0.4085
No improvement, still saving model
4.4988399071925755 epochs left to go

Training Epoch 24.501160092807424/30 starting at 2021-08-25 15:58:55.473076
[1]  [0/1724] loss: 0.741, ave_loss: 0.741
[2]  [20/1724] loss: 0.590, ave_loss: 0.665
[3]  [40/1724] loss: 0.646, ave_loss: 0.659
[4]  [60/1724] loss: 0.602, ave_loss: 0.645
[5]  [80/1724] loss: 0.663, ave_loss: 0.648
[6]  [100/1724] loss: 0.613, ave_loss: 0.642
[7]  [120/1724] loss: 0.703, ave_loss: 0.651
[8]  [140/1724] loss: 0.827, ave_loss: 0.673
[9]  [160/1724] loss: 0.591, ave_loss: 0.664
[10]  [180/1724] loss: 0.664, ave_loss: 0.664
[11]  [200/1724] loss: 0.728, ave_loss: 0.670
[12]  [220/1724] loss: 0.629, ave_loss: 0.666
[13]  [240/1724] loss: 0.525, ave_loss: 0.655
[14]  [260/1724] loss: 0.578, ave_loss: 0.650
[15]  [280/1724] loss: 0.655, ave_loss: 0.650
[16]  [300/1724] loss: 0.625, ave_loss: 0.649
[17]  [320/1724] loss: 0.565, ave_loss: 0.644
[18]  [340/1724] loss: 0.550, ave_loss: 0.639
[19]  [360/1724] loss: 0.596, ave_loss: 0.636
[20]  [380/1724] loss: 0.873, ave_loss: 0.648
[21]  [400/1724] loss: 0.664, ave_loss: 0.649
[22]  [420/1724] loss: 0.807, ave_loss: 0.656
[23]  [440/1724] loss: 0.622, ave_loss: 0.655
[24]  [460/1724] loss: 0.770, ave_loss: 0.659
[25]  [480/1724] loss: 0.520, ave_loss: 0.654
[26]  [500/1724] loss: 0.557, ave_loss: 0.650
[27]  [520/1724] loss: 0.730, ave_loss: 0.653
[28]  [540/1724] loss: 0.632, ave_loss: 0.652
[29]  [560/1724] loss: 0.622, ave_loss: 0.651
[30]  [580/1724] loss: 0.824, ave_loss: 0.657
[31]  [600/1724] loss: 0.549, ave_loss: 0.654
[32]  [620/1724] loss: 0.624, ave_loss: 0.653
[33]  [640/1724] loss: 0.441, ave_loss: 0.646
[34]  [660/1724] loss: 0.696, ave_loss: 0.648
[35]  [680/1724] loss: 0.702, ave_loss: 0.649
[36]  [700/1724] loss: 0.624, ave_loss: 0.649
[37]  [720/1724] loss: 0.704, ave_loss: 0.650
[38]  [740/1724] loss: 0.632, ave_loss: 0.650
[39]  [760/1724] loss: 0.717, ave_loss: 0.651
[40]  [780/1724] loss: 0.572, ave_loss: 0.649
[41]  [800/1724] loss: 0.604, ave_loss: 0.648
[42]  [820/1724] loss: 0.611, ave_loss: 0.647
[43]  [840/1724] loss: 0.691, ave_loss: 0.648
[44]  [860/1724] loss: 0.602, ave_loss: 0.647
[45]  [880/1724] loss: 0.612, ave_loss: 0.647
[46]  [900/1724] loss: 0.676, ave_loss: 0.647
[47]  [920/1724] loss: 0.546, ave_loss: 0.645
[48]  [940/1724] loss: 0.640, ave_loss: 0.645
[49]  [960/1724] loss: 0.623, ave_loss: 0.644
[50]  [980/1724] loss: 0.781, ave_loss: 0.647
[51]  [1000/1724] loss: 0.576, ave_loss: 0.646
[52]  [1020/1724] loss: 0.642, ave_loss: 0.646
[53]  [1040/1724] loss: 0.732, ave_loss: 0.647
[54]  [1060/1724] loss: 0.543, ave_loss: 0.645
[55]  [1080/1724] loss: 0.513, ave_loss: 0.643
[56]  [1100/1724] loss: 0.682, ave_loss: 0.644
[57]  [1120/1724] loss: 0.591, ave_loss: 0.643
[58]  [1140/1724] loss: 0.605, ave_loss: 0.642
[59]  [1160/1724] loss: 0.658, ave_loss: 0.642
[60]  [1180/1724] loss: 0.614, ave_loss: 0.642
[61]  [1200/1724] loss: 0.650, ave_loss: 0.642
[62]  [1220/1724] loss: 0.523, ave_loss: 0.640
[63]  [1240/1724] loss: 0.651, ave_loss: 0.640
[64]  [1260/1724] loss: 0.542, ave_loss: 0.639
[65]  [1280/1724] loss: 0.650, ave_loss: 0.639
[66]  [1300/1724] loss: 0.551, ave_loss: 0.638
[67]  [1320/1724] loss: 0.661, ave_loss: 0.638
[68]  [1340/1724] loss: 0.640, ave_loss: 0.638
[69]  [1360/1724] loss: 0.670, ave_loss: 0.638
[70]  [1380/1724] loss: 0.595, ave_loss: 0.638
[71]  [1400/1724] loss: 0.634, ave_loss: 0.638
[72]  [1420/1724] loss: 0.699, ave_loss: 0.639
[73]  [1440/1724] loss: 0.613, ave_loss: 0.638
[74]  [1460/1724] loss: 0.733, ave_loss: 0.640
[75]  [1480/1724] loss: 0.580, ave_loss: 0.639
[76]  [1500/1724] loss: 0.688, ave_loss: 0.639
[77]  [1520/1724] loss: 0.562, ave_loss: 0.638
[78]  [1540/1724] loss: 0.625, ave_loss: 0.638
[79]  [1560/1724] loss: 0.646, ave_loss: 0.638
[80]  [1580/1724] loss: 0.671, ave_loss: 0.639
[81]  [1600/1724] loss: 0.496, ave_loss: 0.637
[82]  [1620/1724] loss: 0.673, ave_loss: 0.637
[83]  [1640/1724] loss: 0.650, ave_loss: 0.638
[84]  [1660/1724] loss: 0.752, ave_loss: 0.639
[85]  [1680/1724] loss: 0.808, ave_loss: 0.641
[86]  [1700/1724] loss: 0.607, ave_loss: 0.641
[87]  [1720/1724] loss: 0.621, ave_loss: 0.640
[88]  [1740/1724] loss: 0.630, ave_loss: 0.640

Finished Training finishing at 2021-08-25 16:00:33.601288
printing_out epoch  25.52204176334107 learning rate: 0.00012337215887845676
0.00011967099411210306
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.402e-01
Validation Loss: 5.655e-01
Validation ROC: 0.4085
No improvement, still saving model
3.4779582366589317 epochs left to go

Training Epoch 25.52204176334107/30 starting at 2021-08-25 16:01:10.861040
[1]  [0/1724] loss: 0.663, ave_loss: 0.663
[2]  [20/1724] loss: 0.558, ave_loss: 0.611
[3]  [40/1724] loss: 0.755, ave_loss: 0.659
[4]  [60/1724] loss: 0.606, ave_loss: 0.646
[5]  [80/1724] loss: 0.611, ave_loss: 0.639
[6]  [100/1724] loss: 0.702, ave_loss: 0.649
[7]  [120/1724] loss: 0.739, ave_loss: 0.662
[8]  [140/1724] loss: 0.651, ave_loss: 0.661
[9]  [160/1724] loss: 0.711, ave_loss: 0.666
[10]  [180/1724] loss: 0.523, ave_loss: 0.652
[11]  [200/1724] loss: 0.535, ave_loss: 0.641
[12]  [220/1724] loss: 0.528, ave_loss: 0.632
[13]  [240/1724] loss: 0.713, ave_loss: 0.638
[14]  [260/1724] loss: 0.624, ave_loss: 0.637
[15]  [280/1724] loss: 0.592, ave_loss: 0.634
[16]  [300/1724] loss: 0.679, ave_loss: 0.637
[17]  [320/1724] loss: 0.593, ave_loss: 0.634
[18]  [340/1724] loss: 0.707, ave_loss: 0.638
[19]  [360/1724] loss: 0.593, ave_loss: 0.636
[20]  [380/1724] loss: 0.612, ave_loss: 0.635
[21]  [400/1724] loss: 0.704, ave_loss: 0.638
[22]  [420/1724] loss: 0.756, ave_loss: 0.643
[23]  [440/1724] loss: 0.673, ave_loss: 0.645
[24]  [460/1724] loss: 0.647, ave_loss: 0.645
[25]  [480/1724] loss: 0.861, ave_loss: 0.653
[26]  [500/1724] loss: 0.635, ave_loss: 0.653
[27]  [520/1724] loss: 0.743, ave_loss: 0.656
[28]  [540/1724] loss: 0.669, ave_loss: 0.656
[29]  [560/1724] loss: 0.624, ave_loss: 0.655
[30]  [580/1724] loss: 0.620, ave_loss: 0.654
[31]  [600/1724] loss: 0.693, ave_loss: 0.655
[32]  [620/1724] loss: 0.672, ave_loss: 0.656
[33]  [640/1724] loss: 0.561, ave_loss: 0.653
[34]  [660/1724] loss: 0.753, ave_loss: 0.656
[35]  [680/1724] loss: 0.447, ave_loss: 0.650
[36]  [700/1724] loss: 0.535, ave_loss: 0.647
[37]  [720/1724] loss: 0.597, ave_loss: 0.646
[38]  [740/1724] loss: 0.632, ave_loss: 0.645
[39]  [760/1724] loss: 0.507, ave_loss: 0.642
[40]  [780/1724] loss: 0.686, ave_loss: 0.643
[41]  [800/1724] loss: 0.753, ave_loss: 0.645
[42]  [820/1724] loss: 0.603, ave_loss: 0.644
[43]  [840/1724] loss: 0.610, ave_loss: 0.644
[44]  [860/1724] loss: 0.634, ave_loss: 0.643
[45]  [880/1724] loss: 0.686, ave_loss: 0.644
[46]  [900/1724] loss: 0.605, ave_loss: 0.643
[47]  [920/1724] loss: 0.645, ave_loss: 0.644
[48]  [940/1724] loss: 0.666, ave_loss: 0.644
[49]  [960/1724] loss: 0.618, ave_loss: 0.643
[50]  [980/1724] loss: 0.519, ave_loss: 0.641
[51]  [1000/1724] loss: 0.692, ave_loss: 0.642
[52]  [1020/1724] loss: 0.486, ave_loss: 0.639
[53]  [1040/1724] loss: 0.736, ave_loss: 0.641
[54]  [1060/1724] loss: 0.537, ave_loss: 0.639
[55]  [1080/1724] loss: 0.565, ave_loss: 0.638
[56]  [1100/1724] loss: 0.550, ave_loss: 0.636
[57]  [1120/1724] loss: 0.673, ave_loss: 0.637
[58]  [1140/1724] loss: 0.785, ave_loss: 0.639
[59]  [1160/1724] loss: 0.735, ave_loss: 0.641
[60]  [1180/1724] loss: 0.668, ave_loss: 0.641
[61]  [1200/1724] loss: 0.684, ave_loss: 0.642
[62]  [1220/1724] loss: 0.628, ave_loss: 0.642
[63]  [1240/1724] loss: 0.676, ave_loss: 0.642
[64]  [1260/1724] loss: 0.652, ave_loss: 0.642
[65]  [1280/1724] loss: 0.681, ave_loss: 0.643
[66]  [1300/1724] loss: 0.543, ave_loss: 0.641
[67]  [1320/1724] loss: 0.562, ave_loss: 0.640
[68]  [1340/1724] loss: 0.712, ave_loss: 0.641
[69]  [1360/1724] loss: 0.679, ave_loss: 0.642
[70]  [1380/1724] loss: 0.498, ave_loss: 0.640
[71]  [1400/1724] loss: 0.670, ave_loss: 0.640
[72]  [1420/1724] loss: 0.569, ave_loss: 0.639
[73]  [1440/1724] loss: 0.585, ave_loss: 0.639
[74]  [1460/1724] loss: 0.702, ave_loss: 0.639
[75]  [1480/1724] loss: 0.621, ave_loss: 0.639
[76]  [1500/1724] loss: 0.557, ave_loss: 0.638
[77]  [1520/1724] loss: 0.607, ave_loss: 0.638
[78]  [1540/1724] loss: 0.780, ave_loss: 0.639
[79]  [1560/1724] loss: 0.636, ave_loss: 0.639
[80]  [1580/1724] loss: 0.540, ave_loss: 0.638
[81]  [1600/1724] loss: 0.743, ave_loss: 0.639
[82]  [1620/1724] loss: 0.667, ave_loss: 0.640
[83]  [1640/1724] loss: 0.561, ave_loss: 0.639
[84]  [1660/1724] loss: 0.652, ave_loss: 0.639
[85]  [1680/1724] loss: 0.751, ave_loss: 0.640
[86]  [1700/1724] loss: 0.801, ave_loss: 0.642
[87]  [1720/1724] loss: 0.684, ave_loss: 0.643
[88]  [1740/1724] loss: 0.546, ave_loss: 0.642

Finished Training finishing at 2021-08-25 16:02:47.503188
printing_out epoch  26.54292343387471 learning rate: 0.00011215650807132433
0.0001087918128291846
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.416e-01
Validation Loss: 5.688e-01
Validation ROC: 0.4085
No improvement, still saving model
2.4570765661252914 epochs left to go

Training Epoch 26.54292343387471/30 starting at 2021-08-25 16:03:18.448442
[1]  [0/1724] loss: 0.590, ave_loss: 0.590
[2]  [20/1724] loss: 0.669, ave_loss: 0.629
[3]  [40/1724] loss: 0.797, ave_loss: 0.685
[4]  [60/1724] loss: 0.601, ave_loss: 0.664
[5]  [80/1724] loss: 0.577, ave_loss: 0.647
[6]  [100/1724] loss: 0.658, ave_loss: 0.649
[7]  [120/1724] loss: 0.530, ave_loss: 0.632
[8]  [140/1724] loss: 0.590, ave_loss: 0.627
[9]  [160/1724] loss: 0.486, ave_loss: 0.611
[10]  [180/1724] loss: 0.611, ave_loss: 0.611
[11]  [200/1724] loss: 0.516, ave_loss: 0.602
[12]  [220/1724] loss: 0.605, ave_loss: 0.603
[13]  [240/1724] loss: 0.662, ave_loss: 0.607
[14]  [260/1724] loss: 0.696, ave_loss: 0.613
[15]  [280/1724] loss: 0.608, ave_loss: 0.613
[16]  [300/1724] loss: 0.679, ave_loss: 0.617
[17]  [320/1724] loss: 0.652, ave_loss: 0.619
[18]  [340/1724] loss: 0.544, ave_loss: 0.615
[19]  [360/1724] loss: 0.644, ave_loss: 0.617
[20]  [380/1724] loss: 0.563, ave_loss: 0.614
[21]  [400/1724] loss: 0.679, ave_loss: 0.617
[22]  [420/1724] loss: 0.638, ave_loss: 0.618
[23]  [440/1724] loss: 0.680, ave_loss: 0.621
[24]  [460/1724] loss: 0.590, ave_loss: 0.619
[25]  [480/1724] loss: 0.520, ave_loss: 0.615
[26]  [500/1724] loss: 0.521, ave_loss: 0.612
[27]  [520/1724] loss: 0.492, ave_loss: 0.607
[28]  [540/1724] loss: 0.575, ave_loss: 0.606
[29]  [560/1724] loss: 0.740, ave_loss: 0.611
[30]  [580/1724] loss: 0.536, ave_loss: 0.608
[31]  [600/1724] loss: 0.630, ave_loss: 0.609
[32]  [620/1724] loss: 0.584, ave_loss: 0.608
[33]  [640/1724] loss: 0.702, ave_loss: 0.611
[34]  [660/1724] loss: 0.647, ave_loss: 0.612
[35]  [680/1724] loss: 0.757, ave_loss: 0.616
[36]  [700/1724] loss: 0.752, ave_loss: 0.620
[37]  [720/1724] loss: 0.618, ave_loss: 0.620
[38]  [740/1724] loss: 0.708, ave_loss: 0.622
[39]  [760/1724] loss: 0.604, ave_loss: 0.622
[40]  [780/1724] loss: 0.584, ave_loss: 0.621
[41]  [800/1724] loss: 0.610, ave_loss: 0.621
[42]  [820/1724] loss: 0.614, ave_loss: 0.620
[43]  [840/1724] loss: 0.613, ave_loss: 0.620
[44]  [860/1724] loss: 0.557, ave_loss: 0.619
[45]  [880/1724] loss: 0.655, ave_loss: 0.620
[46]  [900/1724] loss: 0.623, ave_loss: 0.620
[47]  [920/1724] loss: 0.702, ave_loss: 0.621
[48]  [940/1724] loss: 0.581, ave_loss: 0.621
[49]  [960/1724] loss: 0.668, ave_loss: 0.622
[50]  [980/1724] loss: 0.620, ave_loss: 0.622
[51]  [1000/1724] loss: 0.655, ave_loss: 0.622
[52]  [1020/1724] loss: 0.679, ave_loss: 0.623
[53]  [1040/1724] loss: 0.630, ave_loss: 0.623
[54]  [1060/1724] loss: 0.604, ave_loss: 0.623
[55]  [1080/1724] loss: 0.585, ave_loss: 0.622
[56]  [1100/1724] loss: 0.673, ave_loss: 0.623
[57]  [1120/1724] loss: 0.646, ave_loss: 0.624
[58]  [1140/1724] loss: 0.610, ave_loss: 0.623
[59]  [1160/1724] loss: 0.534, ave_loss: 0.622
[60]  [1180/1724] loss: 0.692, ave_loss: 0.623
[61]  [1200/1724] loss: 0.629, ave_loss: 0.623
[62]  [1220/1724] loss: 0.538, ave_loss: 0.622
[63]  [1240/1724] loss: 0.628, ave_loss: 0.622
[64]  [1260/1724] loss: 0.522, ave_loss: 0.620
[65]  [1280/1724] loss: 0.629, ave_loss: 0.620
[66]  [1300/1724] loss: 0.639, ave_loss: 0.621
[67]  [1320/1724] loss: 0.545, ave_loss: 0.620
[68]  [1340/1724] loss: 0.769, ave_loss: 0.622
[69]  [1360/1724] loss: 0.649, ave_loss: 0.622
[70]  [1380/1724] loss: 0.779, ave_loss: 0.624
[71]  [1400/1724] loss: 0.764, ave_loss: 0.626
[72]  [1420/1724] loss: 0.712, ave_loss: 0.628
[73]  [1440/1724] loss: 0.651, ave_loss: 0.628
[74]  [1460/1724] loss: 0.670, ave_loss: 0.629
[75]  [1480/1724] loss: 0.620, ave_loss: 0.628
[76]  [1500/1724] loss: 0.628, ave_loss: 0.628
[77]  [1520/1724] loss: 0.560, ave_loss: 0.628
[78]  [1540/1724] loss: 0.795, ave_loss: 0.630
[79]  [1560/1724] loss: 0.715, ave_loss: 0.631
[80]  [1580/1724] loss: 0.550, ave_loss: 0.630
[81]  [1600/1724] loss: 0.543, ave_loss: 0.629
[82]  [1620/1724] loss: 0.588, ave_loss: 0.628
[83]  [1640/1724] loss: 0.562, ave_loss: 0.627
[84]  [1660/1724] loss: 0.491, ave_loss: 0.626
[85]  [1680/1724] loss: 0.669, ave_loss: 0.626
[86]  [1700/1724] loss: 0.614, ave_loss: 0.626
[87]  [1720/1724] loss: 0.744, ave_loss: 0.627
[88]  [1740/1724] loss: 0.609, ave_loss: 0.627

Finished Training finishing at 2021-08-25 16:04:51.885075
printing_out epoch  27.563805104408353 learning rate: 0.00010196046188302211
9.890164802653145e-05
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.273e-01
Validation Loss: 5.659e-01
Validation ROC: 0.4085
No improvement, still saving model
1.4361948955916475 epochs left to go

Training Epoch 27.563805104408353/30 starting at 2021-08-25 16:05:24.476509
[1]  [0/1724] loss: 0.545, ave_loss: 0.545
[2]  [20/1724] loss: 0.699, ave_loss: 0.622
[3]  [40/1724] loss: 0.532, ave_loss: 0.592
[4]  [60/1724] loss: 0.706, ave_loss: 0.620
[5]  [80/1724] loss: 0.535, ave_loss: 0.603
[6]  [100/1724] loss: 0.647, ave_loss: 0.611
[7]  [120/1724] loss: 0.671, ave_loss: 0.619
[8]  [140/1724] loss: 0.652, ave_loss: 0.623
[9]  [160/1724] loss: 0.531, ave_loss: 0.613
[10]  [180/1724] loss: 0.756, ave_loss: 0.627
[11]  [200/1724] loss: 0.594, ave_loss: 0.624
[12]  [220/1724] loss: 0.705, ave_loss: 0.631
[13]  [240/1724] loss: 0.703, ave_loss: 0.637
[14]  [260/1724] loss: 0.801, ave_loss: 0.648
[15]  [280/1724] loss: 0.736, ave_loss: 0.654
[16]  [300/1724] loss: 0.661, ave_loss: 0.655
[17]  [320/1724] loss: 0.635, ave_loss: 0.653
[18]  [340/1724] loss: 0.727, ave_loss: 0.657
[19]  [360/1724] loss: 0.543, ave_loss: 0.651
[20]  [380/1724] loss: 0.599, ave_loss: 0.649
[21]  [400/1724] loss: 0.674, ave_loss: 0.650
[22]  [420/1724] loss: 0.701, ave_loss: 0.652
[23]  [440/1724] loss: 0.673, ave_loss: 0.653
[24]  [460/1724] loss: 0.629, ave_loss: 0.652
[25]  [480/1724] loss: 0.579, ave_loss: 0.649
[26]  [500/1724] loss: 0.552, ave_loss: 0.646
[27]  [520/1724] loss: 0.596, ave_loss: 0.644
[28]  [540/1724] loss: 0.613, ave_loss: 0.643
[29]  [560/1724] loss: 0.578, ave_loss: 0.640
[30]  [580/1724] loss: 0.762, ave_loss: 0.644
[31]  [600/1724] loss: 0.705, ave_loss: 0.646
[32]  [620/1724] loss: 0.561, ave_loss: 0.644
[33]  [640/1724] loss: 0.576, ave_loss: 0.642
[34]  [660/1724] loss: 0.653, ave_loss: 0.642
[35]  [680/1724] loss: 0.763, ave_loss: 0.645
[36]  [700/1724] loss: 0.677, ave_loss: 0.646
[37]  [720/1724] loss: 0.742, ave_loss: 0.649
[38]  [740/1724] loss: 0.740, ave_loss: 0.651
[39]  [760/1724] loss: 0.603, ave_loss: 0.650
[40]  [780/1724] loss: 0.649, ave_loss: 0.650
[41]  [800/1724] loss: 0.621, ave_loss: 0.649
[42]  [820/1724] loss: 0.692, ave_loss: 0.650
[43]  [840/1724] loss: 0.663, ave_loss: 0.651
[44]  [860/1724] loss: 0.596, ave_loss: 0.649
[45]  [880/1724] loss: 0.682, ave_loss: 0.650
[46]  [900/1724] loss: 0.671, ave_loss: 0.651
[47]  [920/1724] loss: 0.749, ave_loss: 0.653
[48]  [940/1724] loss: 0.615, ave_loss: 0.652
[49]  [960/1724] loss: 0.505, ave_loss: 0.649
[50]  [980/1724] loss: 0.653, ave_loss: 0.649
[51]  [1000/1724] loss: 0.732, ave_loss: 0.651
[52]  [1020/1724] loss: 0.614, ave_loss: 0.650
[53]  [1040/1724] loss: 0.584, ave_loss: 0.649
[54]  [1060/1724] loss: 0.677, ave_loss: 0.649
[55]  [1080/1724] loss: 0.571, ave_loss: 0.648
[56]  [1100/1724] loss: 0.654, ave_loss: 0.648
[57]  [1120/1724] loss: 0.587, ave_loss: 0.647
[58]  [1140/1724] loss: 0.634, ave_loss: 0.647
[59]  [1160/1724] loss: 0.592, ave_loss: 0.646
[60]  [1180/1724] loss: 0.627, ave_loss: 0.645
[61]  [1200/1724] loss: 0.673, ave_loss: 0.646
[62]  [1220/1724] loss: 0.713, ave_loss: 0.647
[63]  [1240/1724] loss: 0.591, ave_loss: 0.646
[64]  [1260/1724] loss: 0.561, ave_loss: 0.645
[65]  [1280/1724] loss: 0.833, ave_loss: 0.648
[66]  [1300/1724] loss: 0.548, ave_loss: 0.646
[67]  [1320/1724] loss: 0.556, ave_loss: 0.645
[68]  [1340/1724] loss: 0.644, ave_loss: 0.645
[69]  [1360/1724] loss: 0.654, ave_loss: 0.645
[70]  [1380/1724] loss: 0.641, ave_loss: 0.645
[71]  [1400/1724] loss: 0.683, ave_loss: 0.645
[72]  [1420/1724] loss: 0.603, ave_loss: 0.645
[73]  [1440/1724] loss: 0.543, ave_loss: 0.643
[74]  [1460/1724] loss: 0.727, ave_loss: 0.645
[75]  [1480/1724] loss: 0.614, ave_loss: 0.644
[76]  [1500/1724] loss: 0.578, ave_loss: 0.643
[77]  [1520/1724] loss: 0.834, ave_loss: 0.646
[78]  [1540/1724] loss: 0.749, ave_loss: 0.647
[79]  [1560/1724] loss: 0.539, ave_loss: 0.646
[80]  [1580/1724] loss: 0.647, ave_loss: 0.646
[81]  [1600/1724] loss: 0.712, ave_loss: 0.646
[82]  [1620/1724] loss: 0.599, ave_loss: 0.646
[83]  [1640/1724] loss: 0.668, ave_loss: 0.646
[84]  [1660/1724] loss: 0.557, ave_loss: 0.645
[85]  [1680/1724] loss: 0.639, ave_loss: 0.645
[86]  [1700/1724] loss: 0.671, ave_loss: 0.645
[87]  [1720/1724] loss: 0.636, ave_loss: 0.645
[88]  [1740/1724] loss: 0.703, ave_loss: 0.646

Finished Training finishing at 2021-08-25 16:07:00.605605
printing_out epoch  28.584686774941996 learning rate: 9.269132898456555e-05
8.991058911502858e-05
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.459e-01
Validation Loss: 5.679e-01
Validation ROC: 0.4086
No improvement, still saving model
0.4153132250580036 epochs left to go

Training Epoch 28.584686774941996/30 starting at 2021-08-25 16:07:34.566606
[1]  [0/1724] loss: 0.646, ave_loss: 0.646
[2]  [20/1724] loss: 0.551, ave_loss: 0.599
[3]  [40/1724] loss: 0.507, ave_loss: 0.568
[4]  [60/1724] loss: 0.734, ave_loss: 0.610
[5]  [80/1724] loss: 0.650, ave_loss: 0.618
[6]  [100/1724] loss: 0.568, ave_loss: 0.609
[7]  [120/1724] loss: 0.734, ave_loss: 0.627
[8]  [140/1724] loss: 0.546, ave_loss: 0.617
[9]  [160/1724] loss: 0.756, ave_loss: 0.632
[10]  [180/1724] loss: 0.527, ave_loss: 0.622
[11]  [200/1724] loss: 0.683, ave_loss: 0.627
[12]  [220/1724] loss: 0.579, ave_loss: 0.623
[13]  [240/1724] loss: 0.645, ave_loss: 0.625
[14]  [260/1724] loss: 0.800, ave_loss: 0.638
[15]  [280/1724] loss: 0.632, ave_loss: 0.637
[16]  [300/1724] loss: 0.634, ave_loss: 0.637
[17]  [320/1724] loss: 0.552, ave_loss: 0.632
[18]  [340/1724] loss: 0.701, ave_loss: 0.636
[19]  [360/1724] loss: 0.714, ave_loss: 0.640
[20]  [380/1724] loss: 0.574, ave_loss: 0.637
[21]  [400/1724] loss: 0.675, ave_loss: 0.638
[22]  [420/1724] loss: 0.436, ave_loss: 0.629
[23]  [440/1724] loss: 0.585, ave_loss: 0.627
[24]  [460/1724] loss: 0.538, ave_loss: 0.624
[25]  [480/1724] loss: 0.554, ave_loss: 0.621
[26]  [500/1724] loss: 0.656, ave_loss: 0.622
[27]  [520/1724] loss: 0.581, ave_loss: 0.621
[28]  [540/1724] loss: 0.637, ave_loss: 0.621
[29]  [560/1724] loss: 0.639, ave_loss: 0.622
[30]  [580/1724] loss: 0.588, ave_loss: 0.621
[31]  [600/1724] loss: 0.667, ave_loss: 0.622
[32]  [620/1724] loss: 0.654, ave_loss: 0.623
[33]  [640/1724] loss: 0.577, ave_loss: 0.622
[34]  [660/1724] loss: 0.512, ave_loss: 0.619
[35]  [680/1724] loss: 0.704, ave_loss: 0.621
[36]  [700/1724] loss: 0.630, ave_loss: 0.621
[37]  [720/1724] loss: 0.678, ave_loss: 0.623
[38]  [740/1724] loss: 0.611, ave_loss: 0.623
[39]  [760/1724] loss: 0.649, ave_loss: 0.623
[40]  [780/1724] loss: 0.639, ave_loss: 0.624
[41]  [800/1724] loss: 0.518, ave_loss: 0.621
[42]  [820/1724] loss: 0.621, ave_loss: 0.621
[43]  [840/1724] loss: 0.636, ave_loss: 0.621
[44]  [860/1724] loss: 0.632, ave_loss: 0.622
[45]  [880/1724] loss: 0.582, ave_loss: 0.621
[46]  [900/1724] loss: 0.525, ave_loss: 0.619
[47]  [920/1724] loss: 0.631, ave_loss: 0.619
[48]  [940/1724] loss: 0.629, ave_loss: 0.619
[49]  [960/1724] loss: 0.728, ave_loss: 0.621
[50]  [980/1724] loss: 0.677, ave_loss: 0.622
[51]  [1000/1724] loss: 0.601, ave_loss: 0.622
[52]  [1020/1724] loss: 0.555, ave_loss: 0.621
[53]  [1040/1724] loss: 0.722, ave_loss: 0.623
[54]  [1060/1724] loss: 0.670, ave_loss: 0.624
[55]  [1080/1724] loss: 0.578, ave_loss: 0.623
[56]  [1100/1724] loss: 0.502, ave_loss: 0.621
[57]  [1120/1724] loss: 0.561, ave_loss: 0.619
[58]  [1140/1724] loss: 0.588, ave_loss: 0.619
[59]  [1160/1724] loss: 0.622, ave_loss: 0.619
[60]  [1180/1724] loss: 0.524, ave_loss: 0.617
[61]  [1200/1724] loss: 0.690, ave_loss: 0.619
[62]  [1220/1724] loss: 0.579, ave_loss: 0.618
[63]  [1240/1724] loss: 0.640, ave_loss: 0.618
[64]  [1260/1724] loss: 0.716, ave_loss: 0.620
[65]  [1280/1724] loss: 0.713, ave_loss: 0.621
[66]  [1300/1724] loss: 0.495, ave_loss: 0.619
[67]  [1320/1724] loss: 0.479, ave_loss: 0.617
[68]  [1340/1724] loss: 0.649, ave_loss: 0.618
[69]  [1360/1724] loss: 0.545, ave_loss: 0.617
[70]  [1380/1724] loss: 0.665, ave_loss: 0.617
[71]  [1400/1724] loss: 0.607, ave_loss: 0.617
[72]  [1420/1724] loss: 0.725, ave_loss: 0.619
[73]  [1440/1724] loss: 0.572, ave_loss: 0.618
[74]  [1460/1724] loss: 0.643, ave_loss: 0.618
[75]  [1480/1724] loss: 0.525, ave_loss: 0.617
[76]  [1500/1724] loss: 0.528, ave_loss: 0.616
[77]  [1520/1724] loss: 0.613, ave_loss: 0.616
[78]  [1540/1724] loss: 0.591, ave_loss: 0.616
[79]  [1560/1724] loss: 0.627, ave_loss: 0.616
[80]  [1580/1724] loss: 0.643, ave_loss: 0.616
[81]  [1600/1724] loss: 0.612, ave_loss: 0.616
[82]  [1620/1724] loss: 0.637, ave_loss: 0.616
[83]  [1640/1724] loss: 0.625, ave_loss: 0.616
[84]  [1660/1724] loss: 0.630, ave_loss: 0.617
[85]  [1680/1724] loss: 0.697, ave_loss: 0.618
[86]  [1700/1724] loss: 0.800, ave_loss: 0.620
[87]  [1720/1724] loss: 0.678, ave_loss: 0.620
[88]  [1740/1724] loss: 0.534, ave_loss: 0.619

Finished Training finishing at 2021-08-25 16:09:13.578700
printing_out epoch  29.605568445475637 learning rate: 8.426484453142322e-05
8.173689919548052e-05
y_prime, 849 434
y_gold, 849 434
disruptive, 849
=========Summary======== for epoch88
Training Loss numpy: 6.193e-01
Validation Loss: 5.575e-01
Validation ROC: 0.4085
No improvement, still saving model
saving results
model-path is:  ./model_checkpoints/torch_modelfull_model.pt
